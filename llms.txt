# HealthArchive - Developer Assistant Context

This file provides high-level context for automated developer assistants working on HealthArchive.

## AGENTS.md

# AGENTS.md – HealthArchive.ca backend

## Project overview

- Backend for HealthArchive.ca:
  - Orchestrates web crawls via the internal **`archive_tool`** subpackage + Docker.
  - Indexes WARCs into `Snapshot` rows in a relational DB via SQLAlchemy.
  - Exposes public HTTP APIs for the frontend (search, sources, snapshots).
  - Exposes admin and metrics endpoints for operators.
  - Runs a worker loop that:
    - Picks queued jobs,
    - Runs `archive_tool`,
    - Indexes WARCs into snapshots.

Canonical docs to consult first:

- **Documentation Site**: Run `make docs-serve` locally to view the backend docs site (links out to other repos; does not mirror their docs).
- **LLM Context File**: generated by `scripts/generate_llms_txt.py` (written to docs/llms.txt; regenerated by `make docs-build` / `make docs-serve`).
- `mkdocs.yml` – navigation source of truth.
- `docs/architecture.md` – architecture & implementation details.
- `docs/documentation-guidelines.md` – documentation policy and canonical sources.
- `docs/development/live-testing.md` – step-by-step local testing flows.
- `docs/deployment/environments-and-configuration.md` – cross-repo env vars and host matrix.
- `src/archive_tool/docs/documentation.md` – internals of the `archive_tool` crawler CLI.
- `docs/deployment/hosting-and-live-server-to-dos.md` – hosting/env/DNS/CORS checklist.
- `docs/deployment/staging-rollout-checklist.md` – staging rollout steps.
- `docs/deployment/production-rollout-checklist.md` – production rollout steps.
- `docs/operations/monitoring-and-ci-checklist.md` – monitoring, uptime, and CI guidance.
- `docs/operations/monitoring-and-alerting.md` – **NEW (2026):** Critical crawl metrics and alert thresholds strategies.
- `docs/deployment/production-single-vps.md` – current production runbook (Hetzner + Tailscale-only SSH, nightly backups, NAS pull).
- `docs/planning/roadmap.md` – backlog of not-yet-implemented work.
- `docs/planning/implemented/` – historical implementation plans (executed).
- Frontend bilingual/dev architecture: `healtharchive-frontend/docs/development/bilingual-dev-guide.md`

When you're doing anything beyond tiny local changes, **open those docs and sync your mental model first**.

---

## Production VPS Access

**CRITICAL: Automated assistants do NOT have access to the production VPS.**

- You CANNOT run commands on the VPS (no SSH, no remote execution)
- You CANNOT directly access production logs, databases, or filesystems
- You CAN ONLY suggest commands for the human operator to run manually
- When providing diagnostics or troubleshooting, format commands as examples for the operator to copy/paste
- Never attempt `ssh vps` or similar remote access - it will always fail

Only the human operator has direct VPS access. All VPS operations must be performed by the operator.

---

## Dev environment & commands

From the repo root (Python project):

- Create venv (if not already done) – adjust to my actual workflow:

  ```bash
  make venv
  ```

- Run checks (what CI blocks on):

  ```bash
  make ci
  ```

- Optional full suite (slower / stricter):

  ```bash
  make check-full
  ```

- **Documentation**:

  ```bash
  make docs-serve  # View the docs site locally
  make docs-build  # Build static site to site/
  ```

For how to:

- Start the FastAPI app for local dev,
- Run specific CLI flows end-to-end,
- Wire up Docker + `archive_tool`,

→ **Follow `docs/development/live-testing.md` rather than inventing new commands**.

When you add/modify functionality, you should:

- Keep checks passing (`make ci`).
- Prefer adding/adjusting tests close to the code you touch (e.g., API route tests, indexing pipeline tests, worker tests).

---

## Git workflow (commits & pushes)

Default for assistant work: **do not commit or push** unless the human operator explicitly asks.

Guidelines:

- If asked to commit: prefer small, logically grouped commits over big “catch-all” commits.
- Use the existing message style (e.g., `fix: ...`, `docs: ...`, `ops: ...`).
- Run the closest relevant local checks before pushing (usually `make ci`; use `make check-full` before deploys).
- Never commit secrets, `.env` files, or machine-specific artifacts.

---

## Agent tool compatibility files

- `AGENTS.md` is the canonical agent instruction file for this repo.
- `CLAUDE.md` is committed as a symlink to `AGENTS.md` for tool compatibility.
- `GEMINI.md` is committed as a symlink to `AGENTS.md` for tool compatibility.

## Attribution and authorship policy

- Commits, docs, code comments, and release notes must list only human contributors as authors.
- Do not add AI-assistant attribution (for example: Codex/Claude/Gemini/ChatGPT) in authored-by/co-authored-by notes, prose, or metadata.
- If tooling is used, describe technical changes directly without attributing authorship to tools.

---

## Common engineering best practices (as you go)

When you change behavior, do routine hygiene in the same series of commits:

- Update relevant docs (especially runbooks/playbooks under `docs/**`) so operators and future devs can follow the new reality.
- For new procedural docs, use the templates:
  - Runbook: `docs/_templates/runbook-template.md`
  - Playbook: `docs/_templates/playbook-template.md`
  - Incident note: `docs/_templates/incident-template.md`
  - Decision record: `docs/_templates/decision-template.md`
- **Update Navigation**: When adding new files, add them to `mkdocs.yml` if you want them in the sidebar.
- Add/adjust tests for new behavior and bug fixes to prevent regressions.
  - Critical business logic modules require >80% line coverage.
  - Use `pytest --cov=path/to/module tests/test_module.py` to verify locally.
- Update `.gitignore` when you introduce new local artifacts, generated files, or caches.
- Keep things tidy: remove dead code, unused imports, and accidental debug logging; keep scripts safe to re-run.
- If you introduce new project conventions/workflows, update `AGENTS.md` to reflect them.

---

## Core concepts you must respect

### Data model

- SQLAlchemy models defined under `src/ha_backend/models.py`:

  - `Source` – logical content origin (e.g., `"hc"`, `"phac"`).
  - `ArchiveJob` – a crawl job and its lifecycle (`queued`, `running`, `completed`, `failed`, `indexed`, `index_failed`, etc.).
  - `Snapshot` – a single captured page with URL, capture timestamp, WARC path, language, etc.

If you change models:

- Think through migrations and existing queries/routes.
- Don’t drop/change columns in a way that silently breaks APIs.
- Update Pydantic schemas in `ha_backend/api/schemas.py` (and admin schemas) coherently.

### Job lifecycle / worker

- Job creation via `SourceJobConfig` in `ha_backend/job_registry.py`:

  - Seeds, `name_template`, and `tool_options` live there.

- `ArchiveJob.config` JSON is the **single source of truth** for building `archive_tool` CLI args.
- Worker loop (`ha_backend/worker/main.py`):

  - Picks `queued`/`retryable` jobs.
  - Runs `run_persistent_job(job_id)`.
  - Applies retry semantics.
  - On success, runs `index_job(job_id)`.

If you change job states, retry logic, or indexing behavior, update:

- Worker logic,
- Admin views (status counts, job details),
- Metrics (`/metrics`).

---

## archive_tool integration – internal crawler subpackage

The `archive_tool` package lives under `src/archive_tool/` and is maintained as
part of this repository. It originated as an earlier standalone crawler repo
but should now be treated as an in-tree, first-class component of the backend.

- The backend primarily talks to it via the CLI (`archive-tool` or the
  configured command) and imports a small set of helpers (`archive_tool.state`,
  `archive_tool.utils`) for WARC and state discovery.
- You **may** change and refactor `src/archive_tool/**` when needed, but:

  - Keep the CLI contract used by `ha_backend/jobs.py` and
    `ha_backend/job_registry.py` in sync (flags such as `--seeds`,
    `--name`, `--output-dir`, monitoring/VPN options, `--relax-perms`,
    etc.).
  - If you change how `.archive_state.json` or temp dirs are laid out,
    update WARC discovery (`ha_backend/indexing/warc_discovery.py`) and
    cleanup (`ha_backend/cli.py:cmd_cleanup_job`) at the same time.
  - Preserve or intentionally migrate any behavior relied on by tests under
    `tests/` (worker flows, job status transitions, indexing expectations).

- When in doubt:

  - Treat `ha_backend/jobs.py` and `ha_backend/job_registry.py` as the main
    integration points for CLI construction and configuration.
  - Treat `ha_backend/indexing/warc_discovery.py` and `cmd_cleanup_job` as
    the integration points for WARC/state discovery and cleanup.
  - Add or adjust tests close to the code you change.

The backend and `archive_tool` are expected to evolve together; it is fine for
changes to span both sides as long as they are coherent and tested.

---

## Indexing pipeline (WARCs → Snapshots)

Key pieces:

- `ha_backend/indexing/warc_discovery.py` – use `CrawlState` + `find_all_warc_files` to discover WARCs.
- `ha_backend/indexing/warc_reader.py` – stream HTML records from WARCs.
- `ha_backend/indexing/text_extraction.py` – extract title/text/snippet/language.
- `ha_backend/indexing/mapping.py` – map an archive record to a `Snapshot` ORM instance.
- `ha_backend/indexing/pipeline.py` – `index_job(job_id)` orchestrates the whole indexing step.

If you change indexing:

- Don’t break the assumption that **indexed snapshots reference a WARC path / record that `viewer.py` can replay**.
- Keep per-record errors logged but non-fatal where feasible.
- Keep pagination and performance in mind with large WARC sets.

---

## HTTP API contract

Public routes (for the frontend):

- `GET /api/health` – status and basic stats.
- `GET /api/sources` – summarized counts by source.
- `GET /api/search` – paginated search with `q`, `source`, `page`, `pageSize`.
- `GET /api/snapshot/{id}` – metadata for one snapshot.
- `GET /api/snapshots/raw/{id}` – raw HTML replay.
- `GET /api/usage` – aggregated daily usage metrics for public reporting.
- `GET /api/changes` – change-event feed (edition-aware by default).
- `GET /api/changes/compare` – precomputed diff between adjacent captures.
- `GET /api/changes/rss` – RSS feed for change events.
- `GET /api/exports` – export manifest (formats + limits).
- `GET /api/exports/snapshots` – snapshot metadata export (JSONL/CSV).
- `GET /api/exports/changes` – change event export (JSONL/CSV).
- `GET /api/snapshots/{id}/timeline` – timeline for a page group.
- `POST /api/reports` – public issue report intake.

Admin & metrics:

- `/api/admin/**` – job lists, details, snapshots per job, status counts.
- `/metrics` – Prometheus-style counts, behind `require_admin`.

When you add/modify endpoints:

- Keep public/admin responsibilities separate.
- Maintain existing query semantics:

  - `page >= 1`, `1 <= pageSize <= 100`, etc.
  - Filtering by `source` using `Source.code`.

- Update the relevant Pydantic schemas and tests accordingly.

---

## Security & admin auth

Admin auth is via `require_admin`:

- Controlled by `HEALTHARCHIVE_ENV` and `HEALTHARCHIVE_ADMIN_TOKEN`.
- In `production`/`staging`, a missing admin token should fail closed.
- When the token is set, admin/metrics require either:

  - `Authorization: Bearer <token>`, or
  - `X-Admin-Token: <token>`.

Do **not** weaken this behavior:

- Don’t expose admin endpoints without auth in non-dev environments.
- Don’t log secrets or tokens.

---

## Cleanup & retention

- Cleanup is done per job via `ha-backend cleanup-job --id ID --mode temp`.
- It removes:

  - temp crawl dirs (`.tmp*`),
  - `.archive_state.json`,
    consistent with `archive_tool`’s own cleanup behavior.

- Only safe when jobs are `indexed` or explicitly `index_failed` and not being retried.

Don’t make cleanup more aggressive (e.g. deleting WARCs or ZIMs) without carefully updating docs and CLI semantics.

---

## Testing & expectations

- Tests live under `tests/` and use `pytest`.
- Many tests:

  - Set `HEALTHARCHIVE_DATABASE_URL` to a temporary path.
  - Re-create the schema via `Base.metadata.drop_all()` / `create_all()`.

When you change behavior:

- Add/adjust tests rather than disabling existing ones.
- Keep DB setup/teardown patterns consistent.

---

## Safety rails / things not to touch casually

- Don’t:

  - Change `HEALTHARCHIVE_ARCHIVE_ROOT` semantics in a way that would break existing job locations on disk without explicit migration.
  - Remove or relax job status transitions and retry guards.
  - Expose `/api/admin/**` or `/metrics` publicly.

- Be cautious with:

  - ORM model changes (`ArchiveJob`, `Snapshot`, `Source`).
  - CORS configuration in `ha_backend/api/__init__.py`.
  - Anything under `src/archive_tool/**` beyond doc updates.

For non-trivial changes, **explain your plan and assumptions in the chat before editing**.


---

## README.md

# HealthArchive.ca – Backend

This repository contains the backend services and archiving pipeline for
[HealthArchive.ca](https://healtharchive.ca).

The backend has three main responsibilities:

- **Run crawl jobs** for sources like Health Canada (`hc`) and PHAC (`phac`)
  by calling the `archive_tool` CLI (which wraps `zimit` in Docker).
- **Index WARCs into snapshots** (URL + timestamp + HTML text, etc.) in a
  relational database.
- **Expose HTTP APIs** that the Next.js frontend uses for search, source
  summaries, and snapshot viewing.

For a deep architecture and implementation walkthrough, see
`docs/architecture.md`. For a step‑by‑step local live‑testing guide, see
`docs/development/live-testing.md`. For the current production runbook (single
VPS + Tailscale-only SSH + nightly backups), see
`docs/deployment/production-single-vps.md`.
This README is intentionally shorter and focused on practical usage.

Related repositories (project is multi-repo):

- Frontend UI: https://github.com/jerdaw/healtharchive-frontend
- **Documentation Site**: Run `make docs-serve` in the backend repo for a searchable web UI.

Production entrypoints (VPS):

- Deploy gate (includes baseline drift + public surface checks):
  - `./scripts/vps-deploy.sh --apply --baseline-mode live`
- Baseline drift policy (desired state in git):
  - `docs/operations/production-baseline-policy.toml`
- Systemd automation templates (timers + enablement steps):
  - `docs/deployment/systemd/README.md`

---

## Project layout (high level)

```text
.
├── README.md
├── docs/                     # Documentation Source (MkDocs)
│   ├── architecture.md       # Detailed architecture and implementation guide
│   ├── development/          # Local dev + live-testing flows
│   ├── deployment/           # Deployment/runbooks/checklists
│   ├── operations/           # Monitoring/uptime/CI/Ops guidance
│   ├── frontend-external/    # Link-out pointers to frontend docs (canonical in frontend repo)
│   └── datasets-external/    # Link-out pointers to datasets repo/docs
├── mkdocs.yml                # Documentation Navigation Source of Truth
├── pyproject.toml            # Package + dependency metadata
├── requirements.txt          # Convenience requirements file (mirrors pyproject)
├── alembic/                  # Database migrations
├── src/
│   ├── ha_backend/           # Backend package
│   │   ├── api/              # FastAPI app, public + admin routes
│   │   ├── cli.py            # ha-backend CLI entrypoint
│   │   ├── config.py         # Archive root + DB + tool config
│   │   ├── db.py             # SQLAlchemy engine/session helpers
│   │   ├── indexing/         # WARC discovery, parsing, text extraction, mapping
│   │   ├── job_registry.py   # Per-source job templates (hc, phac)
│   │   ├── jobs.py           # Persistent job runner → archive_tool
│   │   ├── logging_config.py # Shared logging configuration
│   │   ├── models.py         # ORM models (Source, ArchiveJob, Snapshot)
│   │   ├── seeds.py          # Initial Source seeding
│   │   └── worker/           # Long-running worker loop for queued jobs
│   └── archive_tool/         # Crawler/orchestrator subpackage, with its own docs
└── tests/                    # Pytest suite
```

The `archive_tool` package started as a separate repository and is now
maintained in-tree as the backend's crawler/orchestrator subpackage. It is
invoked primarily via its CLI (`archive-tool`) and integrates closely with the
backend's job, worker, and indexing code. Its internal documentation lives
under `src/archive_tool/docs/documentation.md`.

---

## Installation & setup

### 1. Prerequisites

- Python **3.11+**
- Docker (required by `archive_tool` / `zimit` for crawls)
- A Python virtual environment (recommended)

### 2. Install dependencies

From the repo root:

```bash
make venv
```

This provides:

- `ha-backend` – backend CLI
- `archive-tool` – console script pointing at the in-repo `archive_tool` package

### 3. Database

By default the backend uses a SQLite file at `sqlite:///healtharchive.db` in
the repo root, or whatever you point `HEALTHARCHIVE_DATABASE_URL` at.

To verify connectivity:

```bash
ha-backend check-db
```

For production, you will typically point `HEALTHARCHIVE_DATABASE_URL` at a
Postgres instance and run Alembic migrations:

```bash
alembic upgrade head
```

For local development it is common to isolate everything under the repo
directory:

```bash
export HEALTHARCHIVE_DATABASE_URL=sqlite:///$(pwd)/.dev-healtharchive.db
export HEALTHARCHIVE_ARCHIVE_ROOT=$(pwd)/.dev-archive-root
export HEALTHARCHIVE_ADMIN_TOKEN=localdev-admin  # optional for admin routes
# Optional CORS overrides (defaults already cover localhost + prod domains)
# export HEALTHARCHIVE_CORS_ORIGINS=http://localhost:3000,http://localhost:5173
alembic upgrade head
```

If you want to use Postgres locally via Docker for testing:

```bash
docker run --name ha-pg \
  -e POSTGRES_USER=healtharchive \
  -e POSTGRES_PASSWORD=healtharchive \
  -e POSTGRES_DB=healtharchive \
  -p 5432:5432 -d postgres:16

export HEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://healtharchive:healtharchive@localhost:5432/healtharchive
alembic upgrade head
```

### 4. Archive root & archive_tool

The backend writes job output under an archive root directory:

- `HEALTHARCHIVE_ARCHIVE_ROOT` (env) **or**
- `/mnt/nasd/nobak/healtharchive/jobs` by default.

To verify the archive root and `archive_tool`:

```bash
ha-backend check-env           # shows archive root and checks writability
ha-backend check-archive-tool  # runs 'archive-tool --help'
```

### 5. Optional Git hooks

This repo uses [pre-commit](https://pre-commit.com) and provides an optional
pre-push hook helper:

```bash
pre-commit install
bash scripts/install-pre-push-hook.sh
```

---

## Running the API

The FastAPI app lives at `ha_backend.api:app`. Once your virtualenv and DB
are configured:

```bash
uvicorn ha_backend.api:app --reload
```

Key public endpoints (all prefixed with `/api`):

- `GET /api/health`
  Basic health check (`status`, DB connectivity, job and snapshot counts).

- `GET /api/stats`
  Lightweight public archive stats for the frontend (snapshot totals, unique page
  count, source count, latest capture date).

- `GET /api/sources`
  Per-source summaries derived from indexed snapshots.

  When `HEALTHARCHIVE_REPLAY_BASE_URL` is set, the response also includes:

  - `entryRecordId` – a “best effort” entry-point snapshot ID for browsing a source
  - `entryBrowseUrl` – a timestamp-locked replay URL for that entry point

- `GET /api/search`
  Full-text style search over snapshots (with filters for `source`, pagination,
  etc.).

  When `HEALTHARCHIVE_REPLAY_BASE_URL` is set, each result may include:

  - `jobId` + `captureTimestamp` – used to lock replay to a specific capture
  - `browseUrl` – a timestamp-locked replay URL for browsing within the backup

  Ranking controls:

  - Default ranking is controlled by `HA_SEARCH_RANKING_VERSION` (`v1` or `v2`).
  - Per-request override: add `ranking=v1|v2` to `/api/search`.

- `GET /api/snapshot/{id}`
  Snapshot metadata for a single record.

  When `HEALTHARCHIVE_REPLAY_BASE_URL` is set, this includes a `browseUrl`
  suitable for embedding a replay engine (pywb) for full-fidelity browsing.

- `GET /api/usage`
  Aggregated usage metrics (daily counts) for public reporting.

- `GET /api/changes`
  Precomputed change events feed (filters by source, edition/job, date range).

- `GET /api/changes/compare`
  Precomputed diff between two adjacent captures (A → B).

- `GET /api/changes/rss`
  RSS feed for the latest edition-aware change events.

- `GET /api/exports`
  Export manifest describing available research exports and limits.

- `GET /api/exports/snapshots`
  Snapshot metadata export (JSONL/CSV, metadata-only).

- `GET /api/exports/changes`
  Change event export (JSONL/CSV, metadata-only).

- `POST /api/reports`
  Public issue intake endpoint for broken snapshots, metadata errors, missing coverage, or takedown requests.

- `GET /api/snapshots/raw/{id}`
  Returns the archived HTML document for embedding in the frontend.

- `GET /api/snapshots/{id}/timeline`
  Timeline of captures for the same normalized URL group.

Admin + observability endpoints (protected by a simple admin token):

- `GET /api/admin/jobs` – list jobs (filters: `source`, `status`)
- `GET /api/admin/jobs/{id}` – detailed job info
- `GET /api/admin/jobs/status-counts` – job counts by status
- `GET /api/admin/jobs/{id}/snapshots` – list snapshots for a job
- `GET /api/admin/search-debug` – admin-only search scoring breakdown
- `GET /api/admin/reports` – list issue reports (admin)
- `GET /api/admin/reports/{id}` – issue report detail (admin)
- `GET /metrics` – Prometheus-style metrics (jobs, cleanup_status, snapshots)

Admin endpoints require a token when `HEALTHARCHIVE_ADMIN_TOKEN` is set (see
“Admin auth” below).

These endpoints are intended for internal operators and monitoring systems
only. The public Next.js frontend does **not** call `/api/admin/*` or
`/metrics` directly.

### Dev .env helper

For convenience, you can copy `.env.example` to `.env` (git-ignored) and source
it in your shell:

```bash
cp .env.example .env
source .env
alembic upgrade head
uvicorn ha_backend.api:app --reload --port 8001
```

Do not commit real secrets in `.env`; use host-managed env vars for staging/prod.

---

## Search evaluation tools

This repo includes lightweight scripts to capture and compare search results:

- Capture a standard query set (v1 vs v2):
  - `./scripts/search-eval-capture.sh --out-dir /tmp/ha-search-eval --page-size 20 --ranking v1`
  - `./scripts/search-eval-capture.sh --out-dir /tmp/ha-search-eval --page-size 20 --ranking v2`
- Diff two capture directories:
  - `python ./scripts/search-eval-diff.py --a /tmp/ha-search-eval/<TS_A> --b /tmp/ha-search-eval/<TS_B> --top 20`

Docs:

- `docs/operations/search-quality.md`
- `docs/operations/search-golden-queries.md`

### CORS / frontend origins

The API enables CORS for the public endpoints. Allowed origins come from
`HEALTHARCHIVE_CORS_ORIGINS` (comma-separated). Defaults cover local dev and
production:

```
http://localhost:3000, http://localhost:5173, https://healtharchive.ca, https://www.healtharchive.ca
```

Set `HEALTHARCHIVE_CORS_ORIGINS` when your frontend runs on a different host
or port (e.g., a preview/staging domain). Admin routes remain token-gated even
when CORS is enabled.

---

## Running the worker

The worker process polls for queued jobs and runs both the crawl (`archive_tool`)
and indexing pipeline.

Start it via the CLI:

```bash
ha-backend start-worker
```

Options:

- `--poll-interval SECONDS` – sleep delay when no work is found (default 30).
- `--once` – process at most one job and exit (useful for cron / debugging).

The worker:

- Looks for `ArchiveJob` rows with `status in ("queued", "retryable")`.
- Runs `run_persistent_job(job_id)` which calls `archive_tool` as a subprocess.
- On crawl success, runs `index_job(job_id)` to ingest WARCs into `Snapshot`s.
- Applies a simple retry policy (`MAX_CRAWL_RETRIES`) before marking jobs
  permanently `failed`.

---

## Creating and managing jobs

The backend exposes a small CLI layer for managing `ArchiveJob` rows.

### Seed sources

Ensure `Source` rows for `hc` and `phac` exist:

```bash
ha-backend seed-sources
```

### Create a job from registry defaults

For example, a monthly Health Canada job:

```bash
ha-backend create-job --source hc
```

This:

- Uses the `SourceJobConfig` for `hc` (seeds, naming template, tool options).
- Creates an `ArchiveJob` row with `status="queued"` and a unique `output_dir`.

### Run a specific DB-backed job once

```bash
ha-backend run-db-job --id 42
```

This calls `archive_tool` with the stored seeds, `output_dir`, and tool
options. It updates `status`, timestamps, and `crawler_exit_code`.

### Index an existing job

If you ran a crawl separately and just want to index WARCs:

```bash
ha-backend index-job --id 42
```

If you have an existing `archive_tool` output directory on disk (e.g. from a
manual run) and want to attach it to the DB for indexing, use:

```bash
ha-backend register-job-dir --source hc --output-dir /path/to/job_dir [--name NAME]
ha-backend index-job --id <printed ID>
```

**Permissions note:** crawls run as root inside Docker. The registry defaults now
enable `relax_perms` so temp WARCs are chmod’d readable after the crawl, allowing
indexing without a host-side `sudo chown`. If you disable `relax_perms`, you may
need to chown `.tmp*` before indexing.

### Compute change events (diffs)

Change tracking is computed off the request path using precomputed events.

```bash
# Incremental (last 30 days by default)
ha-backend compute-changes --max-events 200

# Backfill historical changes
ha-backend compute-changes --backfill --max-events 500
```

These commands populate `snapshot_changes` rows used by `/api/changes` and
`/api/changes/compare`.

### List and inspect jobs

```bash
ha-backend list-jobs
ha-backend show-job --id 42
```

### Validate a job's configuration (dry-run)

To validate that a job's configuration is coherent (seeds, tool options, and
zimit args) without actually running a crawl, you can invoke the integrated
`archive_tool` CLI in dry-run mode via:

```bash
ha-backend validate-job-config --id 42
```

This:

- Reconstructs the `archive_tool` CLI arguments from `ArchiveJob.config`.
- Runs `archive-tool` with `--dry-run` so it validates the configuration and
  prints a summary.
- Does **not** change the job's status or timestamps.

### Retry and cleanup

- Retry a failed crawl or reindex:

  ```bash
  ha-backend retry-job --id 42
  ```

  - For `status="failed"` → sets `status="retryable"` for another crawl.
  - For `status="index_failed"` → sets `status="completed"` so indexing can re-run.
  - For other statuses, the command logs that there is nothing to retry.

- Cleanup temp dirs and state for an **indexed** or **index_failed** job:

  ```bash
  ha-backend cleanup-job --id 42
  ```

  This:

  - Uses `archive_tool`’s `CrawlState` and `cleanup_temp_dirs(...)` to delete
    `.tmp*` directories and the `.archive_state.json` file under `output_dir`.
  - Leaves the job directory and any final ZIM in place.
  - Updates `ArchiveJob.cleanup_status = "temp_cleaned"` and `cleaned_at` when
    there was actually a state file and/or temp dirs to remove.

> **Note:** `cleanup-job` is destructive for temporary crawl artifacts
> (including WARCs under `.tmp*`). Only run it after you are confident the
> job has been fully indexed (or indexing has failed in a way you do not
> plan to recover from) and any desired ZIMs or exports are verified.

---

## Configuration (environment variables)

The backend reads configuration from environment variables with sensible
defaults:

- `HEALTHARCHIVE_DATABASE_URL`
  SQLAlchemy URL for the DB. Defaults to `sqlite:///healtharchive.db` in the
  repo root.

- `HEALTHARCHIVE_ARCHIVE_ROOT`
  Base directory for job output dirs (passed as `--output-dir` to `archive_tool`).
  Defaults to `/mnt/nasd/nobak/healtharchive/jobs`.

- `HEALTHARCHIVE_TOOL_CMD`
  Command used to invoke the archiver. Defaults to `archive-tool`.

- `HEALTHARCHIVE_ENV`
  High-level environment hint used by admin auth. Recognised values:

  - `"development"` (default when unset): admin endpoints are open when
    `HEALTHARCHIVE_ADMIN_TOKEN` is unset (dev convenience).
  - `"staging"` or `"production"`: admin endpoints fail closed with HTTP 500
    if `HEALTHARCHIVE_ADMIN_TOKEN` is not configured.

- `HEALTHARCHIVE_ADMIN_TOKEN`
  Optional admin token. If set, `/api/admin/*` and `/metrics` require either:

  - `Authorization: Bearer <token>` or
  - `X-Admin-Token: <token>`
    If unset and `HEALTHARCHIVE_ENV` is `"development"` (or unset), admin
    endpoints are open (intended only for local development). In staging and
    production you should **always** set a long, random token and store it as a
    secret in your hosting platform (never committed to the repo); when
    `HEALTHARCHIVE_ENV` is `"staging"` or `"production"` and this token is
    missing, admin and metrics endpoints return HTTP 500.

- `HEALTHARCHIVE_LOG_LEVEL`
  Global log level (`DEBUG`, `INFO`, etc.). Defaults to `INFO`.

- `HEALTHARCHIVE_CORS_ORIGINS`
  Comma-separated list of allowed Origins for CORS on the public API routes.
  If unset, a built-in default is used:

  - `http://localhost:3000`
  - `http://localhost:5173`
  - `https://healtharchive.ca`
  - `https://www.healtharchive.ca`

  In production and staging you should set this explicitly so that only
  expected frontend hosts can call the API from a browser. Examples:

  - **Production (frontend at healtharchive.ca):**

    ```bash
    export HEALTHARCHIVE_CORS_ORIGINS="https://healtharchive.ca,https://www.healtharchive.ca"
    ```

- **Preview (frontend at healtharchive.vercel.app):**

  ```bash
  export HEALTHARCHIVE_CORS_ORIGINS="https://healtharchive.vercel.app"
  ```

  You can also include `http://localhost:3000` if you want local development
  to talk directly to a remote API instance.

For a more complete checklist covering staging/production configuration,
DNS, and Vercel env vars, see:

- `docs/deployment/hosting-and-live-server-to-dos.md`
- `docs/deployment/environments-and-configuration.md`
- `docs/deployment/production-single-vps.md`
- `docs/deployment/staging-rollout-checklist.md`
- `docs/deployment/production-rollout-checklist.md`
- `docs/operations/monitoring-and-ci-checklist.md`.

---

## Continuous integration

A GitHub Actions workflow (`.github/workflows/backend-ci.yml`) is intended to
run on pushes to `main` and on pull requests. It:

- Checks out the repository.
- Sets up Python 3.11.
- Runs `make ci` (fast gate: format check, lint, typecheck, tests).
- Runs an end-to-end smoke test (backend + frontend) on `main` pushes / manual runs.

The CI job uses a temporary SQLite database via:

```bash
HEALTHARCHIVE_DATABASE_URL=sqlite:///./ci-healtharchive.db
```

so no external DB or Docker services are required. Crawls are not executed in
CI; tests focus on unit-level behavior (DB models, APIs, job orchestration,
etc.).

---

## Detailed architecture

For a full walkthrough of:

- ORM models and status lifecycle
- Job registry and how per-source jobs are configured
- `archive_tool` integration and adaptive strategies
- Indexing pipeline and snapshot schema
- HTTP API routes and JSON schemas
- Worker loop and retry semantics
- Cleanup and retention strategy (future)
- How the backend integrates with the in-repo `archive_tool` crawler

see `docs/architecture.md`.

### Frontend integration smoke test

Once a frontend is pointed at this backend (via `NEXT_PUBLIC_API_BASE_URL` on
the frontend side and `HEALTHARCHIVE_CORS_ORIGINS` here), you can perform a
quick end-to-end smoke test:

1. **Verify API health from the frontend host**

   From a shell:

   ```bash
   curl -i "$API_BASE_URL/api/health"
   curl -i "$API_BASE_URL/api/sources"
   ```

   You should see HTTP 200 responses and JSON bodies. If you add an `Origin`
   header matching the frontend (e.g. `https://healtharchive.ca`), the response
   should include:

   ```text
   Access-Control-Allow-Origin: https://healtharchive.ca
   Vary: Origin
   ```

2. **Exercise the UI**

   From the frontend domain (staging or production):

   - Visit `/archive`:
     - With the backend up, the filters should show `Filters (live API)` and
       search/pagination should be backed by real snapshot data.
     - If you intentionally stop the backend (in staging), the UI should show
       a small “Backend unreachable” banner (when enabled) and fall back to
       the demo dataset with a clear notice.
   - Visit `/archive/browse-by-source` and `/snapshot/[id]` to confirm source
     summaries and snapshot details load correctly against the live API.

The `archive_tool` subpackage also has its own detailed documentation in
`src/archive_tool/docs/documentation.md` describing its internal state
machine and Docker orchestration, and how it cooperates with the backend.

Developed, in part, with the use of artificial intelligence tooling.


---

## docs/architecture.md

# HealthArchive Backend – Architecture & Implementation Guide

This document is an in‑depth walkthrough of the **HealthArchive.ca backend**
(`healtharchive-backend` repo). It covers:

- How the backend is structured.
- How it integrates with the `archive_tool` crawler subpackage.
- The data model and job lifecycle.
- The indexing pipeline (WARCs → snapshots).
- HTTP APIs (public + admin) and metrics.
- Worker loop, retries, and cleanup/retention (future).

For `archive_tool` internals (log parsing, Docker orchestration, run modes),
see `src/archive_tool/docs/documentation.md`. For a shorter, task‑oriented
overview of common commands and local testing flows, see
`development/live-testing.md`. For deployment‑oriented configuration
(staging/prod env vars, DNS, Vercel), see
`deployment/hosting-and-live-server-to-dos.md`.

---

## 1. High‑level architecture

### 1.1 Components

- **archive_tool** (internal subpackage under `src/archive_tool/`):
  - CLI wrapper around `zimit` + Docker.
  - Manages temporary output dirs, WARCs, and final ZIM build.
  - Tracks persistent state in `.archive_state.json` + `.tmp*` directories.
  - Implements stall/error detection, adaptive worker reductions, and VPN
    rotation (when enabled).

- **Backend package** (`src/ha_backend/`):
  - Orchestrates crawl **jobs** using `archive_tool` as a subprocess.
  - Stores job and snapshot metadata in a relational database via SQLAlchemy.
  - Indexes WARCs into `Snapshot` rows.
  - Exposes HTTP APIs via FastAPI.
  - Provides a worker loop to process queued jobs.
  - Offers CLI commands for admins (job creation, status, retry, cleanup).

- **External dependencies**:
  - Docker & `ghcr.io/openzim/zimit` image.
  - Database (SQLite by default; Postgres recommended in production).
  - Optional VPN client/command for rotation (e.g., `nordvpn`).

### 1.2 Data flow overview

1. **Job creation**:
   - Admin runs `ha-backend create-job --source hc`.
   - Backend:
     - Ensures a `Source` row exists.
     - Uses `SourceJobConfig` to build seeds, tool options, and `output_dir`.
     - Inserts an `ArchiveJob` with `status="queued"`.

2. **Crawl (archive_tool)**:
   - Worker or CLI runs `run_persistent_job(job_id)`:
     - Builds `archive_tool` CLI args from `ArchiveJob.config` and `output_dir`.
     - Runs `archive_tool` as a subprocess (no in‑process calls).
     - Marks job `running` → `completed` or `failed` with `crawler_exit_code`
       and `crawler_status`.
   - `archive_tool`:
     - Validates Docker.
     - Determines run mode (Fresh/Resume/New‑with‑Consolidation/Overwrite).
     - Spawns `docker run ghcr.io/openzim/zimit zimit ...`.
     - Tracks temp dirs and state, discovers WARCs, and optionally runs a
       final ZIM build (depending on its configuration).

3. **Indexing (WARCs → Snapshot)**:
   - Worker calls `index_job(job_id)` when crawl succeeds.
   - Backend:
     - Uses `CrawlState` + `find_all_warc_files` to locate WARCs under
       `output_dir`.
     - Streams WARC records, extracts HTML, text, language, etc.
     - Writes `Snapshot` rows for each captured page.
     - Marks job `indexed` with `indexed_page_count`.

4. **Change tracking (Snapshot → Change events)**:
   - A background task (`ha-backend compute-changes`) computes **precomputed**
     change events between adjacent captures of the same `normalized_url_group`.
   - Outputs `SnapshotChange` rows with:
     - provenance (from/to snapshot IDs, timestamps),
     - summary stats (sections/lines changed),
     - and a renderable diff artifact when available.
   - This work is intentionally **off the request path** to keep APIs fast.

5. **Serving**:
   - FastAPI app:
     - `GET /api/search` queries `Snapshot` for search results.
     - `GET /api/stats` provides lightweight public archive totals for frontend metrics.
     - `GET /api/sources` summarises captures per `Source`.
     - `GET /api/snapshot/{id}` returns metadata for a single snapshot.
     - `GET /api/snapshots/raw/{id}` replays archived HTML from a WARC.
     - `GET /api/changes` and `GET /api/changes/compare` expose change feeds and diffs.
     - `GET /api/snapshots/{id}/timeline` returns a capture timeline for a page group.

5. **Admin & cleanup**:
   - Admin API:
     - `GET /api/admin/jobs` / `{id}` for job status and config.
     - `GET /metrics` for Prometheus‑style metrics.
   - CLI:
     - `ha-backend retry-job` to reattempt failed jobs.
     - `ha-backend cleanup-job` to delete temp dirs/state for indexed jobs,
       updating `cleanup_status`.

---

## 2. Configuration & environment

### 2.1 Config module (`ha_backend/config.py`)

Key roles:

- Locate the **archive root** (`--output-dir` base) and `archive_tool` command.
- Read the **database URL**.

Admin‑related configuration is handled separately in `ha_backend/api/deps.py`,
which reads `HEALTHARCHIVE_ADMIN_TOKEN` from the environment. When this token
is **unset**, admin and metrics endpoints are effectively open and should only
be used in local development. In staging and production you should always set
`HEALTHARCHIVE_ADMIN_TOKEN` to a long, random value and treat it as a secret.

#### ArchiveToolConfig

```python
@dataclass
class ArchiveToolConfig:
    archive_root: Path = DEFAULT_ARCHIVE_ROOT
    archive_tool_cmd: str = DEFAULT_ARCHIVE_TOOL_CMD

    def ensure_archive_root(self) -> None:
        self.archive_root.mkdir(parents=True, exist_ok=True)
```

Defaults:

- `DEFAULT_ARCHIVE_ROOT` = `/mnt/nasd/nobak/healtharchive/jobs`
- `DEFAULT_ARCHIVE_TOOL_CMD` = `"archive-tool"`

Env overrides:

- `HEALTHARCHIVE_ARCHIVE_ROOT` → archive root.
- `HEALTHARCHIVE_TOOL_CMD` → CLI to call (e.g., `archive-tool`, `python run_archive.py`).

#### DatabaseConfig

```python
@dataclass
class DatabaseConfig:
    database_url: str = DEFAULT_DATABASE_URL
```

Defaults:

- `DEFAULT_DATABASE_URL = "sqlite:///healtharchive.db"` in the repo root.

Env override:

- `HEALTHARCHIVE_DATABASE_URL`.

### 2.2 Logging (`ha_backend/logging_config.py`)

Centralized logging configuration:

- Reads `HEALTHARCHIVE_LOG_LEVEL` (default `INFO`).
- On first call, uses `logging.basicConfig(...)` with:
  - Format: `"%(asctime)s [%(levelname)s] %(name)s: %(message)s"`.
- Adjusts noisy loggers:
  - `sqlalchemy.engine` → `WARNING`.
  - `uvicorn.access` → `INFO`.

Used in:

- `ha_backend.api.__init__` (API startup).
- `ha_backend.cli.main` (CLI entrypoint).

---

## 3. Data model (SQLAlchemy ORM)

Defined in `src/ha_backend/models.py`, with `Base` from `ha_backend.db`.

### 3.1 Source

Represents a logical content origin (e.g., Health Canada, PHAC).

Important fields:

- `id: int` (PK)
- `code: str` – short code (`"hc"`, `"phac"`) – unique, indexed.
- `name: str` – human‑readable name.
- `base_url: str | None`
- `description: str | None`
- `enabled: bool`
- Timestamps: `created_at`, `updated_at`

Relationships:

- `jobs: List[ArchiveJob]` – all jobs for this source.
- `snapshots: List[Snapshot]` – all snapshots for this source.

### 3.2 ArchiveJob

Represents a single `archive_tool` run (or family of runs) for a source.

Key fields:

- Identity:
  - `id: int` (PK)
  - `source_id: int | None` → FK to `sources.id`
  - `name: str` – must match `--name` for `archive_tool`; used in ZIM naming.
  - `output_dir: str` – host path used as `--output-dir` for `archive_tool`.

- Lifecycle/status:
  - `status: str` – high‑level state; typical values:
    - `queued`
    - `running`
    - `retryable`
    - `failed`
    - `completed` (crawl succeeded)
    - `indexing`
    - `indexed`
    - `index_failed`
  - `queued_at`, `started_at`, `finished_at`: timestamps.
  - `retry_count: int` – number of times the worker retried the crawl.

- Configuration:
  - `config: JSON | None` – “opaque” config used to reconstruct the CLI:

    ```json
    {
      "seeds": ["https://..."],
      "zimit_passthrough_args": ["--profile", "foo"],
      "tool_options": {
        "cleanup": false,
        "overwrite": false,
        "skip_final_build": false,
        "enable_monitoring": false,
        "enable_adaptive_workers": false,
        "enable_vpn_rotation": false,
        "initial_workers": 2,
        "log_level": "INFO",
        "...": "..."
      }
    }
    ```

- Crawl metrics:
  - `crawler_exit_code: int | None` – exit code from the `archive_tool` process.
  - `crawler_status: str | None` – summarised status (e.g. `"success"`, `"failed"`).
  - `crawler_stage: str | None` – last known stage (not heavily used yet).
  - `last_stats_json: JSON | None` – parsed crawl stats from the latest combined log, when available.
  - `pages_crawled`, `pages_total`, `pages_failed`: simple integer metrics derived from `last_stats_json` (best-effort).

- WARC/ZIM counts:
  - `warc_file_count: int` – number of WARCs discovered for this job.
  - `indexed_page_count: int` – number of `Snapshot`s created during indexing.

- Filesystem paths:
  - `final_zim_path: str | None` – if a ZIM is produced by `archive_tool` or manual `warc2zim`.
  - `combined_log_path: str | None` – path to the latest combined log, used for stats/debugging.
  - `state_file_path: str | None` – path to `.archive_state.json` within `output_dir` (may be `None` after cleanup).

- Cleanup state (future):
  - `cleanup_status: str` – describes whether any cleanup has occurred:
    - `"none"` (default) – temp dirs & state still present (or never existed).
    - `"temp_cleaned"` – `cleanup-job` or an equivalent operation removed temp dirs/state.
    - Future values could represent more aggressive cleanup.
  - `cleaned_at: datetime | None` – when cleanup was performed.

Relationships:

- `source: Source | None` – parent source.
- `snapshots: List[Snapshot]` – all snapshots produced by this job.

### 3.3 Snapshot

Represents a single captured web page (an HTML response) extracted from a WARC.

Key fields:

- Identity:
  - `id: int` (PK)
  - `job_id: int | None` → FK to `archive_jobs.id`
  - `source_id: int | None` → FK to `sources.id`

- URL & grouping:
  - `url: str` – full URL of the capture (including query string).
  - `normalized_url_group: str | None` – optional canonicalised URL for grouping (e.g., removing query or anchors).

- Timing:
  - `capture_timestamp: datetime` – from `WARC-Date` or HTTP headers.

- HTTP & content:
  - `mime_type: str | None`
  - `status_code: int | None`
  - `title: str | None` – extracted from `<title>` or headings.
  - `snippet: str | None` – short preview text.
  - `language: str | None` – ISO language (e.g. `"en"`, `"fr"`).

- Storage / replay:
  - `warc_path: str` – path to the `.warc.gz` file on disk.
  - `warc_record_id: str | None` – WARC record identifier or offset (see `indexing.viewer`).
  - `raw_snapshot_path: str | None` – optional path to a static HTML export, if you create such stubs.
  - `content_hash: str | None` – hash of the HTML body for deduplication.

Relationships:

- `job: ArchiveJob | None`
- `source: Source | None`

---

## 4. Job registry & creation (`ha_backend/job_registry.py`) {: #4-job-registry--creation-ha_backendjob_registrypy }

The job registry defines default behavior and seeds for each source code (`"hc"`, `"phac"`).

### 4.1 SourceJobConfig

```python
@dataclass
class SourceJobConfig:
    source_code: str
    name_template: str
    default_seeds: List[str]
    default_zimit_passthrough_args: List[str]
    default_tool_options: Dict[str, Any]
    schedule_hint: Optional[str] = None
```

Examples:

- `hc` (Health Canada):

  - `name_template = "hc-{date:%Y%m%d}"`
  - `default_seeds = ["https://www.canada.ca/en/health-canada.html"]`
  - `default_tool_options`:
    - `cleanup = False`
    - `overwrite = False`
    - `enable_monitoring = True` (required for adaptive strategies)
    - `enable_adaptive_workers = True`
    - `enable_adaptive_restart = True`
    - `enable_vpn_rotation = False` (disabled by default)
    - `initial_workers = 2`
    - `stall_timeout_minutes = 60`
    - `docker_shm_size = "1g"`
    - `skip_final_build = True` (annual campaign: search/indexing uses WARCs)
    - `error_threshold_timeout = 50`
    - `error_threshold_http = 50`
    - `backoff_delay_minutes = 2`
    - `max_container_restarts = 20`
    - `log_level = "INFO"`

- `phac` (Public Health Agency of Canada) is similar with a PHAC home page seed.

### 4.2 Job name and output dir

- `generate_job_name(source_cfg, now)`:
  - Renders `name_template` using `{date:%Y%m%d}` from UTC timestamp.
  - E.g. `hc-20251209`.

- `build_output_dir_for_job(source_code, job_name, archive_root, now)`:

  ```text
  <archive_root>/<source_code>/<YYYYMMDDThhmmssZ>__<job_name>
  ```

  Example:

  ```text
  /mnt/nasd/nobak/healtharchive/jobs/hc/20251209T210911Z__hc-20251209
  ```

### 4.3 Job config JSON

- `build_job_config(source_cfg, extra_seeds=None, overrides=None)`:
  - Merges `default_seeds` + extra seeds.
  - Copies `default_zimit_passthrough_args`.
  - Copies and updates `default_tool_options` with any `overrides`.
  - Performs basic validation of `tool_options` to fail fast on
    misconfiguration:

    - If `enable_adaptive_workers=True` but `enable_monitoring` is not `True`,
      a `ValueError` is raised.
    - If `enable_vpn_rotation=True` but `enable_monitoring` is not `True`,
      a `ValueError` is raised.
    - If `enable_vpn_rotation=True` but `vpn_connect_command` is missing or
      empty, a `ValueError` is raised.

Result structure:

```json
{
  "seeds": ["https://...", "..."],
  "zimit_passthrough_args": [],
  "tool_options": {
    "cleanup": false,
    "overwrite": false,
    "skip_final_build": true,
    "enable_monitoring": true,
    "enable_adaptive_workers": true,
    "enable_adaptive_restart": true,
    "enable_vpn_rotation": false,
    "initial_workers": 2,
    "stall_timeout_minutes": 60,
    "docker_shm_size": "1g",
    "error_threshold_timeout": 50,
    "error_threshold_http": 50,
    "backoff_delay_minutes": 2,
    "max_container_restarts": 20,
    "log_level": "INFO"
  }
}
```

### 4.4 create_job_for_source

```python
def create_job_for_source(
    source_code: str,
    *,
    session: Session,
    overrides: Optional[Dict[str, Any]] = None,
) -> ORMArchiveJob:
```

Steps:

1. Look up `SourceJobConfig` for `source_code`.
2. Ensure a `Source` row with that code exists (or raise).
3. Resolve `archive_root` from config.
4. Generate `job_name` and `output_dir`.
5. Build `job_config`.
6. Insert an `ArchiveJob`:
   - `status="queued"`, `queued_at=now`, `config=job_config`.

The CLI command `ha-backend create-job --source hc` is a thin wrapper around this.

---

## 5. archive_tool integration & job runner (`ha_backend/jobs.py`) {: #5-archive_tool-integration--job-runner-ha_backendjobspy }

### 5.1 RuntimeArchiveJob

`RuntimeArchiveJob` is a small helper for ad‑hoc runs (`ha-backend run-job`) that:

- Holds just a `name` and `seeds: list[str]`.
- Creates a timestamped job directory under the archive root (unless overridden).
- Builds the `archive_tool` CLI command.
- Executes it via `subprocess.run(...)`.

This path is used by:

- `ha-backend run-job` – direct, non‑persistent jobs.

### 5.2 run_persistent_job – DB‑backed jobs {: #52-run_persistent_job--db-backed-jobs }

```python
def run_persistent_job(job_id: int) -> int:
    ...
```

Responsibilities:

1. **Load job and mark running**:

   - Using `get_session()`:

     - Fetch `ArchiveJob` by ID.
     - Validate `status in ("queued", "retryable")`.
     - Extract `config`, splitting into:
       - `tool_options`
       - `zimit_passthrough_args`
       - `seeds`
     - Validate that `seeds` is non‑empty.
     - Record `output_dir` and `name`.
     - Set:
       - `status = "running"`
       - `started_at = now`

2. **Build CLI options from tool_options**:

   - Core:

     ```python
     initial_workers = int(tool_options.initial_workers)
     cleanup = bool(tool_options.cleanup)
     overwrite = bool(tool_options.overwrite)
     log_level = str(tool_options.log_level)
     ```

   - Monitoring options:

     Only if `enable_monitoring` is `True`:

     - Adds `--enable-monitoring`.
     - Optionally:
       - `monitor_interval_seconds` → `--monitor-interval-seconds N`
       - `stall_timeout_minutes` → `--stall-timeout-minutes N`
       - `error_threshold_timeout` → `--error-threshold-timeout N`
       - `error_threshold_http` → `--error-threshold-http N`

   - Adaptive workers:

     Only if both `enable_monitoring` and `enable_adaptive_workers` are `True`:

     - Adds `--enable-adaptive-workers`.
     - Optionally:
       - `min_workers` → `--min-workers N`
       - `max_worker_reductions` → `--max-worker-reductions N`

   - VPN rotation:

     Only if `enable_monitoring`, `enable_vpn_rotation`, and `vpn_connect_command`
     are all present:

     - Adds:

       ```bash
       --enable-vpn-rotation
       --vpn-connect-command "<vpn_connect_command>"
       ```

     - Optionally:
       - `max_vpn_rotations` → `--max-vpn-rotations N`
       - `vpn_rotation_frequency_minutes` → `--vpn-rotation-frequency-minutes N`

   - Backoff:

     Only when monitoring is enabled and `backoff_delay_minutes` is set:

     - `--backoff-delay-minutes N`.

   - Zimit passthrough:

     - `zimit_passthrough_args` are appended directly (no explicit `"--"`
       separator is required): `archive_tool` uses `argparse.parse_known_args()`
       and passes unknown args through to `zimit`.
     - For `ha-backend run-job`, a leading `"--"` is accepted and stripped for
       convenience when passing through flags interactively.

   - The final `extra_args` passed to `RuntimeArchiveJob.run(...)` look like:

     ```bash
     [archive_tool_flags..., zimit_passthrough_args...]
     ```

3. **Execute archive_tool**:

   - Instantiates `RuntimeArchiveJob(name, seeds)`.
   - Calls:

     ```python
     rc = runtime_job.run(
         initial_workers=initial_workers,
         cleanup=cleanup,
         overwrite=overwrite,
         log_level=log_level,
         extra_args=full_extra_args,
         stream_output=True,
         output_dir_override=Path(output_dir_str),
     )
     ```

   - `output_dir_override` ensures a specific job directory under the archive
     root (matching the DB record) is used, and created if needed.

4. **Update job status**:

   - After the subprocess returns:

     - `crawler_exit_code = rc`
     - `finished_at = now`
     - `combined_log_path` is recorded best-effort (newest `archive_*.combined.log`)
     - `status = "completed"` and `crawler_status = "success"` if `rc == 0`
     - Otherwise:
       - `status = "retryable"`, `crawler_status = "infra_error"` for storage/mount failures
       - `status = "failed"`, `crawler_status = "infra_error_config"` for CLI/config/runtime errors
         (e.g., invalid `zimit_passthrough_args`)
       - `status = "failed"`, `crawler_status = "failed"` for normal crawl failures

The worker uses `run_persistent_job(job_id)` for each queued job.

### 5.3 Maintaining the archive_tool integration

The backend and ``archive_tool`` share a small but important contract:

- **Configuration JSON**:

  - `ArchiveJob.config` stores a dict that is the serialised form of
    `ArchiveJobConfig` from `ha_backend.archive_contract`:

    ```json
    {
      "seeds": ["https://...", "..."],
      "zimit_passthrough_args": ["--scopeType", "host"],
      "tool_options": {
        "cleanup": false,
        "overwrite": false,
        "skip_final_build": true,
        "enable_monitoring": true,
        "enable_adaptive_workers": true,
        "enable_adaptive_restart": true,
        "enable_vpn_rotation": false,
        "initial_workers": 2,
        "log_level": "INFO",
        "relax_perms": true,
        "stall_timeout_minutes": 60,
        "docker_shm_size": "1g",
        "error_threshold_timeout": 50,
        "error_threshold_http": 50,
        "max_container_restarts": 20,
        "backoff_delay_minutes": 2
      }
    }
    ```

  - `SourceJobConfig.default_tool_options` in `ha_backend.job_registry` is the
    source of truth for defaults; overrides are merged via
    `build_job_config(...)` which uses `ArchiveToolOptions` +
    `validate_tool_options(...)` to enforce invariants that mirror
    `archive_tool.cli` (e.g. monitoring required for adaptive/VPN).

- **CLI construction**:

  - `ha_backend.jobs.run_persistent_job` is the only place that maps
    `tool_options` fields to `archive_tool` CLI flags. It expects the argument
    model described in `src/archive_tool/docs/documentation.md` and
    `archive_tool/cli.py`.
  - If you add or rename CLI options in `archive_tool`:

    - Extend `ArchiveToolOptions` and `ArchiveJobConfig` to carry the new
      fields.
    - Update `run_persistent_job` to add/remove the corresponding flags.
    - Adjust tests under `tests/test_job_registry.py`,
      `tests/test_archive_contract.py`, and `tests/test_jobs_persistent.py`
      that assert config and CLI behaviour.

- **Stats and logs**:

  - `archive_tool` writes combined logs
    `archive_<stage_name>_*.combined.log` under each job's `output_dir` and
    emits `"Crawl statistics"` JSON lines that
    `archive_tool.utils.parse_last_stats_from_log` can parse.
  - `ha_backend.crawl_stats.update_job_stats_from_logs`:

    - Locates the latest combined log for a job.
    - Calls `parse_last_stats_from_log(log_path)` to obtain a stats dict.
    - Stores it in `ArchiveJob.last_stats_json`.
    - Updates `pages_crawled`, `pages_total`, `pages_failed`, and
      `combined_log_path` as a best-effort summary.

  - `/metrics` exposes these page counters via:

    - `healtharchive_jobs_pages_crawled_total`
    - `healtharchive_jobs_pages_failed_total`
    - per-source variants, backed by the `pages_*` fields on `ArchiveJob`.

- **WARC discovery and cleanup**:

  - `ha_backend.indexing.warc_discovery.discover_warcs_for_job` relies on
    `archive_tool.state.CrawlState` and `archive_tool.utils.find_all_warc_files`
    / `find_latest_temp_dir_fallback` for WARC discovery and temp dir
    tracking.
  - `ha_backend.cli.cmd_cleanup_job` uses `CrawlState` and
    `archive_tool.utils.cleanup_temp_dirs` to remove `.tmp*` directories and
    `.archive_state.json` safely once jobs are indexed.

If you change log formats, state layout, or directory structure in
`archive_tool`, update the corresponding backend helpers (`ArchiveJobConfig`,
`run_persistent_job`, `update_job_stats_from_logs`, WARC discovery, and
cleanup) and their tests to keep the contract in sync.

---

## 6. Indexing pipeline (`ha_backend/indexing/*`)

The indexing pipeline converts the WARCs produced by `archive_tool` into
structured `Snapshot` rows.

### 6.1 WARC discovery (`warc_discovery.py`)

```python
from archive_tool.state import CrawlState
from archive_tool.utils import find_all_warc_files, find_latest_temp_dir_fallback
```

```python
def discover_warcs_for_job(
    job: ArchiveJob,
    *,
    allow_fallback: bool = True,
) -> List[Path]:
```

Steps:

1. Resolve `host_output_dir = Path(job.output_dir).resolve()`.
2. Instantiate `CrawlState(host_output_dir, initial_workers=1)`:
   - This loads `.archive_state.json` if present.
3. Get `temp_dirs = state.get_temp_dir_paths()`:
   - Returns only existing directories and prunes missing ones from state.
4. If `temp_dirs` is empty and `allow_fallback`:
   - Use `find_latest_temp_dir_fallback(host_output_dir)` to scan for `.tmp*`
     directories.
5. If still empty → return `[]`.
6. Call `find_all_warc_files(temp_dirs)`:
   - Returns a de‑duplicated list of `*.warc.gz` files under each
     `collections/crawl-*/archive` directory.

This ensures the backend uses **exactly the same** WARC discovery logic as
`archive_tool` itself.

### 6.2 WARC reading (`warc_reader.py`)

Wraps `warcio` to stream HTML response records from a `.warc.gz` file.

Exports a generator like:

```python
def iter_html_records(warc_path: Path) -> Iterator[ArchiveRecord]:
    ...
```

Where `ArchiveRecord` provides:

- `url: str`
- `capture_timestamp: datetime`
- `headers: dict[str, str]`
- `body_bytes: bytes`
- `warc_path: Path`
- `warc_record_id: str | None`

### 6.3 Text extraction (`text_extraction.py`)

Helpers:

- `extract_title(html: str) -> str` – heuristics over `<title>` / headings.
- `extract_text(html: str) -> str` – uses BeautifulSoup to pull visible text.
- `make_snippet(text: str) -> str` – short preview (~N chars/words).
- `detect_language(text: str, headers: dict) -> str` – simple language detection,
  leveraging headers or heuristics (kept basic for now).

### 6.4 Mapping records to Snapshot (`mapping.py`)

`record_to_snapshot(job, source, rec, title, snippet, language)`:

- Takes:
  - `ArchiveJob`
  - `Source`
  - `ArchiveRecord` from `iter_html_records`
  - `title`, `snippet`, `language` from text extraction
- Produces a new `Snapshot` instance with:
  - `job_id`, `source_id`
  - `url`, `normalized_url_group`
  - `capture_timestamp`
  - `mime_type`, `status_code`
  - `title`, `snippet`, `language`
  - `warc_path`, `warc_record_id`
  - `content_hash` (if computed)

### 6.5 Orchestration (`pipeline.py`)

```python
def index_job(job_id: int) -> int:
```

Steps:

1. Load `ArchiveJob` by ID, ensure:
   - `job.source` is not `None`.
   - `job.status in ("completed", "index_failed", "indexed")`.
2. Validate `output_dir` exists.
3. Discover WARCs:
   - `warc_paths = discover_warcs_for_job(job)`.
   - Sets `job.warc_file_count = len(warc_paths)`.
   - If no WARCs found:
     - Logs warning.
     - Sets `job.status = "index_failed"` and returns `1`.
4. Clear previous snapshots for this job:
   - `DELETE FROM snapshots WHERE job_id = :job_id`.
5. Mark job as indexing:
   - `job.indexed_page_count = 0`, `job.status = "indexing"`.
6. For each WARC path:
   - Iterate `iter_html_records(warc_path)`.
   - Decode `html = rec.body_bytes.decode("utf-8", errors="replace")`.
   - Use text extraction functions to get `title`, `text`, `snippet`, `language`.
   - Call `record_to_snapshot(...)` to construct a `Snapshot`.
   - `session.add(snapshot)`; flush every 500 additions.
   - Count snapshots in `n_snapshots`.
   - On per‑record errors, log and continue.
7. On success:
   - Set `job.indexed_page_count = n_snapshots`.
   - Set `job.status = "indexed"`.
   - Return `0`.
8. On unexpected error:
   - Log at error level.
   - Set `job.status = "index_failed"`.
   - Return `1`.

---

## 7. Viewer helper (`ha_backend/indexing/viewer.py`)

The viewer helper is used by `GET /api/snapshots/raw/{id}` to reconstruct the
HTML for a snapshot from its WARC.

Design:

- Either:
  - Use `warc_record_id` to seek directly to a known record, or
  - Fallback to scanning `warc_path` for the first matching URL + timestamp.

The API route:

- Validates that `Snapshot` and its `warc_path` exist.
- Calls `find_record_for_snapshot(snapshot)`:
  - Returns an `ArchiveRecord` or `None`.
- Decodes `record.body_bytes` as UTF‑8 with replacement.
- Writes `HTMLResponse(content=html, media_type="text/html")`.

This is used by the Next.js frontend for the embedded snapshot viewer.

---

## 8. HTTP API (`ha_backend/api/*`)

### 8.1 Public schemas (`schemas.py`)

Public Pydantic models:

- `SourceSummarySchema` – used by `/api/sources`:

  ```python
  sourceCode: str
  sourceName: str
  recordCount: int
  firstCapture: str
  lastCapture: str
  latestRecordId: Optional[int]
  ```

- `SnapshotSummarySchema` – used by `/api/search`:

  - `id`, `title`, `sourceCode`, `sourceName`, `language`, `captureDate`,
    `originalUrl`, `snippet`, `rawSnapshotUrl`.

- `SearchResponseSchema`:

  - `results: List[SnapshotSummarySchema]`, `total`, `page`, `pageSize`.

- `ArchiveStatsSchema` – used by `/api/stats`:

  - `snapshotsTotal`, `pagesTotal`, `sourcesTotal`, `latestCaptureDate`, `latestCaptureAgeDays`.

- `SnapshotDetailSchema` – used by `/api/snapshot/{id}`:

  - Contains metadata for a single snapshot including `mimeType` and
    `statusCode`, plus `rawSnapshotUrl`.

### 8.2 Public routes (`routes_public.py`)

- `GET /api/health`:

  - Returns JSON with:

    ```json
    {
      "status": "ok",
      "checks": {
        "db": "ok",
        "jobs": {
          "queued": 1,
          "indexed": 5,
          ...
        },
        "snapshots": {
          "total": 12345
        }
      }
    }
    ```

  - If the DB connectivity check fails, returns HTTP 500 with
    `{"status": "error", "checks": {"db": "error"}}`.

- `GET /api/stats`:

  - Returns lightweight, cacheable archive totals used by the frontend:

    ```json
    {
      "snapshotsTotal": 12345,
      "pagesTotal": 6789,
      "sourcesTotal": 2,
      "latestCaptureDate": "2025-04-19",
      "latestCaptureAgeDays": 3
    }
    ```

- `GET /api/sources`:

  - Aggregates `Snapshot` by `source_id`:
    - Counts, first/last capture dates, latest snapshot ID.

- `GET /api/search`:

  - Query params:
    - `q: str | None` – keyword.
    - `source: str | None` – source code (e.g. `"hc"`).
    - `sort: "relevance" | "newest" | None` – ordering mode.
    - `view: "snapshots" | "pages" | None` – results grouping mode.
    - `includeNon2xx: bool` – include non‑2xx HTTP status captures (defaults to `false`).
    - `from: YYYY-MM-DD | None` – filter captures from this UTC date, inclusive.
    - `to: YYYY-MM-DD | None` – filter captures up to this UTC date, inclusive.
    - `page: int` – 1‑based page index (default `1`, must be `>= 1`).
    - `pageSize: int` – results per page (default `20`, minimum `1`, maximum `100`).
  - Filters:
    - `Source.code == source.lower()` when `source` set.
    - By default (`includeNon2xx=false`), filters out snapshots with a known non‑2xx
      `status_code` (keeps `status_code IS NULL` and `200–299`).
    - Keyword filter / query intent:
      - URL lookup: when `q` looks like a URL (or starts with `url:`), treat it as
        a *page* lookup and filter by the normalized URL group (with a small set of
        common scheme/`www.` variants).
      - Boolean/field syntax: when `q` contains `AND`/`OR`/`NOT`, parentheses, `-term`,
        or `title:`/`snippet:`/`url:` prefixes, parse it and apply a boolean filter
        using case-insensitive substring matching.
      - Plain text:
        - On Postgres with `sort="relevance"`: full‑text search (FTS) against
          `snapshots.search_vector`.
          - If FTS yields no results, fall back to tokenized substring matching.
          - If that still yields no results and `pg_trgm` is available, fall back to
            pg_trgm word-level trigram similarity for fuzzy matching (misspellings).
        - Otherwise: tokenized substring matching on `title`, `snippet`, and `url`.
  - Ordering:
    - Default sort:
      - When `q` is present: `sort="relevance"`.
      - When `q` is absent: `sort="newest"`.
	    - `sort="relevance"` (when `q` present):
	      - On Postgres: uses FTS (`websearch_to_tsquery` + `ts_rank_cd`) against
	        `snapshots.search_vector`, with small heuristics (phrase-in-title boost,
	        URL depth/querystring penalties) and an optional authority boost from
	        `page_signals.inlink_count` (when available).
	      - On SQLite/other DBs: uses a DB‑agnostic match score (title > URL > snippet),
	        then (when available) a small authority tie-break from `page_signals`,
	        then recency.
    - `sort="newest"`: orders by recency.
    - When `includeNon2xx=true`, 2xx snapshots are still prioritised ahead of 3xx,
      unknown, and 4xx/5xx captures.
  - Grouping:
    - Default view: `view="snapshots"` (returns individual captures; `total` counts snapshots).
    - `view="pages"` returns only the **latest** snapshot for each page group
      (`normalized_url_group`, falling back to `url` with query/fragment stripped), and
      `total` counts page groups.
    - When `view="pages"` is used for browse (no `q` and no date range), the API can optionally
      use the `pages` table as a fast path (controlled by `HA_PAGES_FASTPATH`). This is a
      metadata-only optimization and does not affect replay fidelity.
    - When available, `pageSnapshotsCount` is included on `view="pages"` results to show the
      number of captures for that page group.
  - Pagination semantics:
    - `total` is the total number of matching items across all pages (snapshots
      for `view="snapshots"`, page groups for `view="pages"`).
    - `results` contains at most `pageSize` snapshots for the requested `page`
      (in `view="pages"`, these are the latest snapshots for each page group).
    - Requesting a page past the end of the result set returns `200 OK` with `results: []` and `total` unchanged.
    - Supplying an invalid `page` (`< 1`) or `pageSize` (`< 1` or `> 100`) yields `422 Unprocessable Entity` from FastAPI’s validation.

- `GET /api/snapshot/{id}`:

  - Loads `Snapshot` + `Source`.
  - Returns `SnapshotDetailSchema`.
  - 404 if snapshot or source missing.

- `GET /api/snapshots/raw/{id}`:

  - Validates `Snapshot` exists and `warc_path` points to an existing file.
  - Uses `find_record_for_snapshot(snapshot)` to get a WARC record.
  - Returns an HTML page via `HTMLResponse` that includes the reconstructed archived HTML
    plus a lightweight HealthArchive top bar (navigation links + disclaimer) so it can be
    viewed standalone.

### 8.3 Admin auth (`deps.py`)

`require_admin` is a FastAPI dependency used to protect admin and metrics
endpoints.

Behavior:

- Reads `HEALTHARCHIVE_ENV` and `HEALTHARCHIVE_ADMIN_TOKEN` from the
  environment.
- If `HEALTHARCHIVE_ENV` is `"production"` or `"staging"` and
  `HEALTHARCHIVE_ADMIN_TOKEN` is **unset**:
  - Admin and metrics endpoints **fail closed** with HTTP 500 and a clear
    error detail (`"Admin token not configured for this environment"`).
- In other environments (or when `HEALTHARCHIVE_ENV` is unset) and the admin
  token is **unset**:
  - Admin endpoints are **open** (dev mode convenience).
- When `HEALTHARCHIVE_ADMIN_TOKEN` is set:
  - Requires the same token via either:
    - `Authorization: Bearer <token>` header, or
    - `X-Admin-Token: <token>` header.
  - On mismatch/missing token → `HTTP 403`.

### 8.4 Admin schemas (`schemas_admin.py`)

Key models:

- `JobSummarySchema` – used for lists:

  - Contains the key job fields plus:

    ```python
    cleanupStatus: str
    cleanedAt: Optional[datetime]
    ```

- `JobDetailSchema` – extended view for a single job:

  - Includes status, worker counters, pages, WARC counts, ZIM/log/state paths,
    `config` (JSON), and `lastStats` (JSON, reserved).
  - Also includes `cleanupStatus` and `cleanedAt`.

- `JobSnapshotSummarySchema` – minimal `Snapshot` view in a job context.

- `JobListResponseSchema` – wrapper for job list results.

- `JobStatusCountsSchema` – dictionary of `{status: count}`.

### 8.5 Admin routes (`routes_admin.py`)

All routes are under `/api/admin` and use `require_admin` for auth. They are
intended for internal operator tooling (CLI or a future admin console), not
for the public web UI.

- `GET /api/admin/jobs` → `JobListResponseSchema`:
  - Filters:
    - `source: str | None` – by source code.
    - `status: str | None` – by job status.
    - `limit` (1–500, default 50), `offset` (≥0).
  - Joins `ArchiveJob` with `Source` (outer join).

- `GET /api/admin/jobs/{job_id}` → `JobDetailSchema`:
  - Joins `ArchiveJob` with `Source`.
  - 404 if job not found.

- `GET /api/admin/jobs/status-counts` → `JobStatusCountsSchema`:
  - SQL: `SELECT status, COUNT(*) FROM archive_jobs GROUP BY status`.

- `GET /api/admin/jobs/{job_id}/snapshots` → `List[JobSnapshotSummarySchema]`:
  - Lists snapshots for a given job with pagination (`limit`, `offset`).

### 8.6 Metrics (Prometheus‑style)

Defined directly in `ha_backend.api.__init__`:

- `GET /metrics`:
  - Protected by `require_admin` (same token behavior) and intended for
    scrape‑only use by monitoring systems (e.g., Prometheus) and internal
    tooling.
  - Computes:
    - `healtharchive_jobs_total{status="..."}`
    - `healtharchive_jobs_cleanup_status_total{cleanup_status="..."}`
    - `healtharchive_snapshots_total`
    - `healtharchive_snapshots_total{source="hc"}`, etc.

### 8.7 CORS

- CORS is enabled on the public API routes. Allowed origins are derived from
  `HEALTHARCHIVE_CORS_ORIGINS` (comma-separated). Defaults cover local dev and
  production (`http://localhost:3000`, `http://localhost:5173`,
  `https://healtharchive.ca`, `https://www.healtharchive.ca`).
- Admin and metrics routes remain token-gated even when CORS allows browser
  access to public routes.

Typical environment setups:

- **Local development**:

  ```bash
  # often no override needed; defaults already include localhost:3000/5173
  export HEALTHARCHIVE_DATABASE_URL=sqlite:///$(pwd)/.dev-healtharchive.db
  export HEALTHARCHIVE_ARCHIVE_ROOT=$(pwd)/.dev-archive-root
  # Optional CORS override if your frontend runs on a different origin:
  # export HEALTHARCHIVE_CORS_ORIGINS=http://localhost:3000
  ```

- **Staging** (example):

  ```bash
  # frontend served from https://healtharchive.vercel.app
  export HEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.vercel.app
  ```

- **Production** (example):

  ```bash
  # frontend served from https://healtharchive.ca and https://www.healtharchive.ca
  export HEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.ca,https://www.healtharchive.ca
  ```

In all cases, CORS affects only the browser’s ability to call public routes;
admin and metrics endpoints still require the admin token when configured.

---

## 9. Worker loop (`ha_backend/worker/main.py`)

The worker processes jobs end‑to‑end: crawl and index.

### 9.1 Selection

`_select_next_crawl_job(session)`:

- Query:

  ```python
  session.query(ArchiveJob) \
    .join(Source) \
    .filter(ArchiveJob.status.in_(["queued", "retryable"])) \
    .order_by(ArchiveJob.queued_at.asc().nullsfirst(),
              ArchiveJob.created_at.asc()) \
    .first()
  ```

- Chooses the oldest queued/retryable job, preferring jobs with the earliest
  `queued_at`.

### 9.2 Processing a single job

`_process_single_job()`:

1. Select a job → get `job_id`.
2. Run `run_persistent_job(job_id)`:
   - Executes `archive_tool` and returns a process exit code.
3. Reload job in a new session and apply retry semantics:
   - If `crawl_rc != 0` or `job.status == "failed"`:
     - If `job.retry_count < MAX_CRAWL_RETRIES`:
       - Increment `job.retry_count`.
       - Set `job.status = "retryable"`.
     - Else:
       - Log error; job remains in `failed`.
   - Else (crawl succeeded):
     - Log that indexing will start.
4. If crawl succeeded:
   - Run `index_job(job_id)`.
   - Log success/failure for indexing.

Returns `True` if a job was processed, `False` if no jobs were found.

### 9.3 Main loop

`run_worker_loop(poll_interval=30, run_once=False)`:

- Logs startup with the given interval and `run_once`.
- In a loop:
  - Calls `_process_single_job()`.
  - If `run_once` → break after first iteration.
  - If no job processed:
    - Logs and sleeps for `poll_interval` seconds.
- Handles `KeyboardInterrupt` gracefully.

---

## 10. Cleanup & retention (future)

Job‑level cleanup is focused on removing **temporary crawl artifacts** (`.tmp*`
dirs and `.archive_state.json`) after indexing is complete.

### 10.1 Cleanup flags on ArchiveJob

New fields:

- `cleanup_status: str`:
  - `"none"` – no cleanup performed (default).
  - `"temp_cleaned"` – temporary dirs and state file have been deleted.
  - Future values could represent more aggressive cleanup modes.
- `cleaned_at: datetime | None` – when cleanup occurred.

These fields are exposed through:

- Admin schemas (`JobSummarySchema`, `JobDetailSchema`).
- Metrics (`healtharchive_jobs_cleanup_status_total`).

### 10.2 CLI command: cleanup-job

`ha-backend cleanup-job --id JOB_ID [--mode temp] [--force]`

Implementation notes:

- Currently supports only `--mode temp`:
  - Any other mode → error.

- Behavior:

  1. Load the `ArchiveJob` by ID.
  2. If job is missing → error, exit 1.
  3. If replay is enabled globally (`HEALTHARCHIVE_REPLAY_BASE_URL` is set) and
     `--force` is **not** provided:
     - Refuse cleanup and exit 1.
     - Rationale: `--mode temp` can delete WARCs required for replay.
  4. If `job.status` is **not** one of:
     - `"indexed"` – indexing completed successfully, or
     - `"index_failed"` – indexing failed and you have decided not to retry,
     then refuse cleanup and exit 1.
     - This ensures we don’t delete temp dirs while a job might still be
       resumed or indexing is in progress.
  5. Validate `output_dir` exists and is a directory.
  6. Use `archive_tool.state.CrawlState(output_dir, initial_workers=1)` to
     instantiate state and locate the state file.
  7. Use `state.get_temp_dir_paths()` to get known temp dirs; fall back to
     `find_latest_temp_dir_fallback` if none are tracked.
  8. If neither temp dirs nor the state file exist:
     - Print a message that there is nothing to clean up and **do not** change
       `cleanup_status` or `cleaned_at`.
  9. Otherwise (if temp dirs and/or state file exist):
     - Call `cleanup_temp_dirs(temp_dirs, state.state_file_path)`:
       - Deletes `.tmp*` directories and the `.archive_state.json`.
     - Update job:
        - `cleanup_status = "temp_cleaned"`
        - `cleaned_at = now`
        - `state_file_path = None`

Operational warning:

- `cleanup-job --mode temp` will delete WARCs if they live under the job’s
  `.tmp*` directory (common for legacy imports and some crawl layouts).
  If you intend to serve the job via replay (pywb), do not run cleanup for that
  job — replay depends on WARCs remaining on disk.
  If replay is enabled globally, you must pass `--force` to run cleanup; treat
  this as an emergency override.

> **Caution:** This cleanup removes WARCs stored under `.tmp*` directories,
> consistent with `archive_tool`’s own `--cleanup` behavior. In v1 you should
> only run it once you have:
> - Indexed the job successfully (`status="indexed"`), and
> - Verified any desired ZIM or exports derived from these WARCs.

### 10.3 Metrics for cleanup

`/metrics` includes:

- `healtharchive_jobs_cleanup_status_total{cleanup_status="none"}`
- `healtharchive_jobs_cleanup_status_total{cleanup_status="temp_cleaned"}`

This gives a quick overview of how many jobs still have temp artifacts versus
those that have been cleaned.

---

## 11. CLI commands summary

All commands are available via the `ha-backend` entrypoint.

- Environment / connectivity:
  - `check-env` – show archive root and ensure it exists.
  - `check-archive-tool` – run `archive-tool --help`.
  - `check-db` – simple DB connectivity check.

- Direct, non‑persistent job:
  - `run-job` – run `archive_tool` immediately with explicit `--name`, `--seeds`,
    `--initial-workers`, etc.

- Persistent jobs (DB‑backed):
  - `create-job --source CODE` – create `ArchiveJob` using registry defaults.
  - `run-db-job --id ID` – run `archive_tool` for an existing job.
  - `index-job --id ID` – index an existing job’s WARCs into snapshots.
  - `register-job-dir --source CODE --output-dir PATH [--name NAME]` –
    attach a DB `ArchiveJob` to an existing archive_tool output directory
    (useful when a crawl has already been run and you want to index its
    WARCs).
  - Job configs default to `relax_perms=True` for dev (adds `--relax-perms` so
    temp WARCs are chmod’d readable on the host after a crawl).

- Seeding:
  - `seed-sources` – insert baseline `Source` rows for `hc`, `phac`.

- Admin / introspection:
  - `list-jobs` – list recent jobs with basic fields.
  - `show-job --id ID` – detailed job info including config.
  - `retry-job --id ID` – mark:
    - `failed` jobs as `retryable` (for another crawl).
    - `index_failed` jobs as `completed` (for re-indexing).
  - `cleanup-job --id ID [--mode temp] [--force]` – cleanup temp dirs/state for jobs in
    status `indexed` or `index_failed`.
  - `replay-index-job --id ID` – create/refresh the pywb collection + CDX index
    for a job (so snapshots can be browsed via replay).
  - `start-worker [--poll-interval N] [--once]` – start the worker loop.

---

## 12. Testing & development

- Tests are written with `pytest` and live under `tests/`.
- To run checks:

  ```bash
  make venv
  make check
  ```

- Many tests configure a temporary SQLite DB by:
  - Setting `HEALTHARCHIVE_DATABASE_URL` to a temp file.
  - Resetting `db_module._engine` and `_SessionLocal`.
  - Calling `Base.metadata.drop_all()` / `create_all()` to fully reset the schema.

This allows development and CI to run in isolated environments without
touching real data.

---

## 13. Relationship to archive_tool and the frontend

- **archive_tool**:
  - Lives under `src/archive_tool/` and is maintained as part of this repo.
    It originated as an earlier standalone crawler project but is now the
    in-tree crawler/orchestrator subpackage for the backend.
  - The backend calls it strictly via the CLI (`archive-tool`) as a subprocess.
  - Its internal behavior (Docker orchestration, run modes, monitoring,
    adaptive strategies) is documented in `src/archive_tool/docs/documentation.md`.

- **Frontend (healtharchive-frontend)**:
  - Next.js 16 app using the backend’s HTTP APIs:
    - `/api/health`
    - `/api/sources`
    - `/api/search`
    - `/api/snapshot/{id}`
    - `/api/snapshots/raw/{id}`
  - The frontend currently still supports a demo dataset, but is gradually
    being wired to these real APIs.

Together, the backend + `archive_tool` + frontend form a pipeline from:

> Web → crawl (Docker + `zimit`) → WARCs → Snapshots in DB → searchable
> archive UI at HealthArchive.ca.


---

## docs/documentation-guidelines.md

# Documentation Guidelines (internal)

Keep documentation accurate, minimal, and easy to maintain across repos.

## Canonical sources

- Docs portal (published): https://docs.healtharchive.ca
- Docs portal (local): Run `make docs-serve` in the backend repo root.
- Navigation config: `mkdocs.yml` (source of truth for sidebar structure).
- Cross-repo environment wiring: `docs/deployment/environments-and-configuration.md`
- Ops roadmap/todo: `docs/operations/healtharchive-ops-roadmap.md`
- Future roadmap backlog (not-yet-implemented work): `docs/planning/roadmap.md`
- Implemented plans archive (historical records): `docs/planning/implemented/`
- Frontend documentation (canonical): https://github.com/jerdaw/healtharchive-frontend/tree/main/docs
- Datasets documentation (canonical): https://github.com/jerdaw/healtharchive-datasets

## Multi-repo boundary (avoid bleed)

This documentation site is built from the **backend repo only**.

- Frontend docs are canonical in the frontend repo (`docs/**`) and should be linked-to, not copied into this site.
- Datasets docs are canonical in the datasets repo and should be linked-to, not copied into this site.
- Frontend PRs should not break backend docs builds (and vice versa).

## Cross-repo linking (avoid drift)

When referencing another repo from docs in this repo:

- **For documentation references**: Use GitHub URLs
  ```markdown
  # Good
  See the [frontend i18n guide](https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/i18n.md)

  # Avoid
  See `healtharchive-frontend/docs/i18n.md`
  ```

- **For command examples**: Workspace-relative paths are fine
  ```bash
  # This is appropriate in a development guide
  cd ../healtharchive-frontend && npm ci
  ```

- **For project names in prose**: Use simple names
  ```markdown
  The healtharchive-frontend repository handles the public UI.
  ```

- Treat cross-repo references as pointers. Do not copy text across repos unless it is an intentional public-safe excerpt.
- Links to backend docs can use relative paths within this repo or `docs.healtharchive.ca` URLs.

### External pointer pages

If you want another repo’s docs to be discoverable from the docs portal, add a
small pointer page under `docs/*-external/` and add it to `mkdocs.yml` `nav`.
Do not mirror the other repo’s docs into this site.

## Navigation policy

### What goes in mkdocs.yml nav

- All README index pages
- Docs that are frequently accessed or critical for operations
- At least one representative doc from each major category
- Core playbooks (operator responsibilities, deploy & verify, incident response)

### What stays README-only

- Detailed playbooks beyond the core set (discoverable via playbooks/README.md)
- Historical/archived roadmaps (implemented/)
- Log files and templates
- Highly specialized procedures

### Organizing new docs

When adding new docs:

1. Add to the appropriate directory
2. Update the directory's `README.md` index
3. If critical or frequently accessed, add to `mkdocs.yml` nav
4. Ensure cross-links from related docs

## Using templates

Templates are stored in `docs/_templates/`. To use:

1. Copy the template to the appropriate directory
2. Rename with appropriate filename (remove `-template` suffix)
3. Fill in all sections
4. Add to directory README index
5. Add to `mkdocs.yml` nav if appropriate

Available templates:

- `_templates/runbook-template.md` — For deployment procedures
- `_templates/playbook-template.md` — For operational tasks
- `_templates/incident-template.md` — For incident postmortems
- `_templates/decision-template.md` — For architectural decisions
- `_templates/restore-test-log-template.md` — For quarterly restore test logs
- `_templates/adoption-signals-log-template.md` — For quarterly adoption signals
- `_templates/mentions-log-template.md` — For mentions log entries
- `_templates/ops-ui-friction-log-template.md` — For internal friction logging

## When adding or changing docs

- Prefer one canonical source. Use pointers elsewhere instead of copying text.
- Keep docs close to the code they describe.
- **Registry**: New critical docs should be added to the `nav` section of `mkdocs.yml`. All docs should be added to their directory's `README.md` index.
- Use MkDocs Material features like **Admonitions** (`!!! note`), **Tabs**, and **Mermaid** diagrams.
- Documentation should be English-only; do not duplicate it in other languages.
- Do not include AI-assistant authorship attribution in docs metadata/prose; document only human authors/contributors.
- Avoid "phase" labels or other implementation-ordering labels outside `docs/planning/roadmap.md` and `docs/planning/implemented/`. The order that something was implemented in is not something that needs documentation; rather documentation should focus on key elements of what was implemented, how it was implemented, and how it is to be used.
- Keep public copy public-safe (no secrets, private emails, or internal IPs).
- If you sync your workspace via Syncthing, treat `.stignore` as "sync ignore" (like `.gitignore`) and ensure it excludes build artifacts and machine-local dev artifacts (e.g., `.venv/`, `node_modules/`, `.dev-archive-root/`). Secrets may sync via Syncthing, but must remain git-ignored.

## Documentation framework (Diátaxis)

HealthArchive documentation follows the **Diátaxis framework** for clarity and user-centered organization. Diátaxis divides documentation into four types based on user needs:

### Four Documentation Types

| Type | Purpose | User Action | Examples |
|------|---------|-------------|----------|
| **Tutorials** | Learning-oriented | Following steps to gain skills | First contribution guide, architecture walkthrough |
| **How-To Guides** | Task-oriented | Solving specific problems | Playbooks, runbooks, checklists |
| **Reference** | Information-oriented | Looking up details | API docs, CLI reference, data model |
| **Explanation** | Understanding-oriented | Understanding concepts | Architecture, decisions, guidelines |

**Key principle**: Keep these types separate. Don't mix tutorials with reference material, or how-to guides with explanations.

**Learn more**: [diataxis.fr](https://diataxis.fr/)

### Mapping to Our Taxonomy

Our existing document types map to Diátaxis categories:

**Tutorials** (Learning):
- Lives under `docs/tutorials/`
- Examples: `tutorials/first-contribution.md`, `tutorials/architecture-walkthrough.md`, `tutorials/debug-crawl.md`
- Characteristics: Step-by-step, hands-on, designed for learning

**How-To Guides** (Tasks):
- **Runbooks**: Deployment procedures in `docs/deployment/` (template: `_templates/runbook-template.md`)
- **Playbooks**: Operational tasks in `docs/operations/playbooks/` or `docs/development/playbooks/` (template: `_templates/playbook-template.md`)
- **Checklists**: Minimal verification lists
- Characteristics: Goal-oriented, assume some knowledge, focused on results

**Reference** (Information):
- Lives under `docs/reference/` or specialized files (`api.md`, etc.)
- Examples: `reference/data-model.md`, `reference/cli-commands.md`, `reference/archive-tool.md`
- Also: API documentation (`api.md`), Architecture sections
- Characteristics: Factual, precise, structured for lookup

**Explanation** (Understanding):
- **Decision records**: In `docs/decisions/` (template: `_templates/decision-template.md`)
- **Policies/contracts**: Invariants and boundaries
- **Guidelines**: This file, `documentation-process-audit.md`
- **Architecture**: `architecture.md` (blends reference and explanation)
- Characteristics: Background, context, "why" not "how"

### Additional Document Types

These support but don't replace the four main types:

- **Index (`README.md`)**: Navigation only; points to canonical docs
- **Log/record**: Dated, append-only operational evidence (restore tests, adoption signals)
- **Template**: Scaffolds in `docs/_templates/`
- **Pointer**: Short files linking to canonical docs (e.g., `frontend-external/`)

## Document types (detailed taxonomy)

Use consistent doc types so people know what to expect:

## Quality bar (definition of done)

For anything procedural (runbook/playbook/checklist), include:

- **Purpose**: why this doc exists and what it covers.
- **Audience + access**: who should run it, and from where (local vs VPS; `haadmin` vs `root`).
- **Preconditions**: required state and inputs (paths, env vars, service names).
- **Steps**: explicit commands (prefer stable scripts), ordered, with “what this changes”.
- **Verification**: what “done” means (health checks, drift check, smoke tests).
- **Safety**: common footguns, irreversible actions, and rollback/recovery notes.
- **References**: links to canonical docs, incident notes, or roadmaps.

For anything public-facing (policy pages, changelog, partner kit):

- Keep it **public-safe** (no secrets/emails/internal hostnames; avoid sensitive incident details).
- Prefer stable claims tied to stable artifacts (URLs, tags, filenames, commit SHAs).
- Record meaningful changes in the public changelog:
  - Process: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md

## Lifecycle (avoid drift)

Docs should reflect **current reality**. If something is intentionally outdated:

- Put a short note at the top: what changed, and where the new canonical doc lives.
- Prefer updating the doc over adding a second “new doc” (avoid forks).
- For long historical artifacts, move them under `docs/planning/implemented/` (dated).

Suggested cadence (keep it lightweight):

- **After any production change**: update the relevant runbook/playbook and keep deploy/verify steps accurate.
- **After sev0/sev1 incidents**: ensure recovery steps are captured and follow-ups exist (roadmap or TODOs).
- **Quarterly**: skim the production runbook + incident response playbook and fix any drift discovered during routine ops.

## Roadmap workflow

This project separates **backlog** vs **implementation plans** vs **canonical docs** to reduce drift.

- Short pointer (for new contributors): `roadmap-process.md`
- `docs/planning/roadmap.md` is the single backlog of not-yet-implemented items.
- When you start work, create a focused implementation plan under `docs/planning/`.
- When the work is done, update canonical docs (deployment/ops/dev) so the result is maintainable.
- Then move the implementation plan into `docs/planning/implemented/` with a dated filename.

Rule of thumb: documentation should describe **what exists and how to use/operate it**, not the order it was implemented.

## Implementation plan lifecycle

Implementation plans follow a specific lifecycle to prevent documentation sprawl:

### 1. Active planning
Create in `docs/planning/` with dated filename: `YYYY-MM-DD-<short-slug>.md`

### 2. During implementation
Update with progress notes as needed. Keep the focus on outcomes and decisions.

### 3. After completion
- Update all canonical docs (deployment/operations/development) with outcomes
- Compress the plan to summary format (see below)
- Move to `docs/planning/implemented/`

### 4. Compressed format for implemented plans

Completed plans >200 lines should be compressed to this format (~40-80 lines):

```markdown
# [Title] (Implemented YYYY-MM-DD)

**Status:** Implemented | **Scope:** [1-2 sentences describing what was done]

## Outcomes
- [Bullet list of what was delivered]

## Canonical Docs Updated
- [Links to docs that now contain this information]

## Decisions Created (if any)
- [Links to decision records]

## Historical Context
[Brief note that detailed implementation history is preserved in git]
```

**Why compress?** Detailed phase-by-phase narratives become historical journals that duplicate content now in canonical docs. Compression:
- Reduces maintenance burden
- Prevents documentation sprawl
- Preserves outcomes while keeping the archive scannable

**Detailed history:** Git preserves the full implementation narrative for anyone who needs it.

## Naming and organization

- Use descriptive filenames (`runbook`, `checklist`, `guidelines`) and avoid phase prefixes.
- File titles and filenames should reflect the document’s actual purpose and content. If the purpose or content changes, rename the file and update links as needed.
- Put roadmaps and active implementation plans in `docs/planning`.
- Move completed implementation plans into `docs/planning/implemented/` (dated).
- Put operational procedures in `docs/operations`.
- Put incident notes / lightweight postmortems in `docs/operations/incidents/` (template: `docs/_templates/incident-template.md`).
- Put ops playbooks (task-oriented checklists) in `docs/operations/playbooks/`.
- Put deployment/runbooks in `docs/deployment`.
- Put developer workflows (local setup, testing, debugging) in `docs/development`.
- Put dev playbooks (task workflows) in `docs/development/playbooks/`.


---

