{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HealthArchive Documentation","text":"<p>This documentation portal covers the HealthArchive backend and links to frontend and datasets documentation.</p>"},{"location":"#quick-start-by-role","title":"Quick Start by Role","text":"<p>Choose your path:</p> <ul> <li>\ud83d\udc64 Operators: Start with Operations Overview \u2192 Operator Responsibilities</li> <li>\ud83d\udcbb Developers: Start with Development Guide \u2192 Live Testing</li> <li>\ud83d\udd27 Deploying: Start with Production Runbook</li> <li>\ud83d\udcca API consumers: Start with API Documentation</li> <li>\ud83d\udcda Researchers: Start with Project Overview \u2192 Datasets</li> </ul>"},{"location":"#key-resources","title":"Key Resources","text":"Need Documentation Architecture overview Architecture Production deployment Production Runbook Local development setup Dev Setup Incident response Incident Response Search API API Documentation Monitoring setup Monitoring"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>This docs portal is built from the backend repo only. Frontend and datasets docs are canonical in their repos and are linked-to from this site:</p> <ul> <li>Frontend pointers: <code>frontend-external/README.md</code></li> <li>Datasets pointers: <code>datasets-external/README.md</code></li> </ul>"},{"location":"#recommended-reading-order","title":"Recommended reading order","text":"<ol> <li>Project docs portal (multi-repo navigation)</li> <li><code>project.md</code></li> <li>Architecture &amp; implementation (how the code works)</li> <li><code>architecture.md</code></li> <li>Documentation guidelines (how docs stay sane)</li> <li><code>documentation-guidelines.md</code></li> <li><code>documentation-process-audit.md</code> (audit of doc processes; 2026-01-09)</li> <li><code>decisions/README.md</code> (decision records for high-stakes choices)</li> <li>Local development / live testing (how to run it locally)</li> <li><code>development/live-testing.md</code></li> <li><code>development/dev-environment-setup.md</code> (local setup + local vs VPS guidance)</li> <li><code>development/testing-guidelines.md</code> (backend test expectations)</li> <li>Deployment (how to run it on a server)</li> <li><code>deployment/production-single-vps.md</code> (current production runbook)</li> <li><code>deployment/systemd/README.md</code> (systemd units: annual scheduler, crawl monitoring + auto-recovery, baseline drift, replay reconcile + smoke tests, change tracking, annual search verify, coverage guardrails, cleanup automation, worker priority)</li> <li><code>deployment/replay-service-pywb.md</code> (pywb replay service for full-fidelity browsing)</li> <li><code>deployment/search-rollout.md</code> (enable v2 search + rollback)</li> <li><code>deployment/pages-table-rollout.md</code> (pages table backfill + browse fast path)</li> <li><code>deployment/hosting-and-live-server-to-dos.md</code> (deployment checklist + Vercel wiring)</li> <li><code>deployment/environments-and-configuration.md</code> (cross\u2011repo env vars + host matrix)</li> <li><code>deployment/production-rollout-checklist.md</code> (generic production checklist)</li> <li><code>deployment/staging-rollout-checklist.md</code> (optional future staging)</li> <li>Operations (how to keep it healthy)</li> <li><code>operations/README.md</code> (index of ops docs)</li> <li>Roadmaps and implementation plans</li> <li><code>roadmaps/README.md</code></li> <li><code>roadmap-process.md</code> (short pointer)</li> </ol>"},{"location":"#notes","title":"Notes","text":"<ul> <li>No secrets live in this repo. Any token/password values shown in docs must be   placeholders.</li> <li>The <code>archive_tool</code> crawler has its own internal documentation at   <code>src/archive_tool/docs/documentation.md</code>.</li> </ul>"},{"location":"api-consumer-guide/","title":"API Consumer Guide","text":"<p>A practical guide for researchers, journalists, and developers who want to programmatically access the HealthArchive.</p> <p>Audience: API consumers, data researchers, integration developers Time: 20-30 minutes to read, lifetime to master Prerequisites: Basic HTTP/REST knowledge, command line or programming experience</p>"},{"location":"api-consumer-guide/#quick-start","title":"Quick Start","text":"<p>Base URL: <code>https://api.healtharchive.ca</code></p> <p>Authentication: None required for public endpoints (search, stats, snapshots)</p> <p>Try it now: <pre><code>curl \"https://api.healtharchive.ca/api/stats\"\n</code></pre></p>"},{"location":"api-consumer-guide/#api-overview","title":"API Overview","text":"<p>HealthArchive provides a RESTful JSON API for searching and retrieving archived Canadian health government content.</p>"},{"location":"api-consumer-guide/#public-endpoints","title":"Public Endpoints","text":"Endpoint Purpose Auth Required <code>GET /api/health</code> Health check No <code>GET /api/stats</code> Archive statistics No <code>GET /api/sources</code> List archived sources No <code>GET /api/search</code> Search snapshots No <code>GET /api/snapshot/{id}</code> Get snapshot metadata No <code>GET /api/snapshots/raw/{id}</code> View archived HTML No"},{"location":"api-consumer-guide/#admin-endpoints","title":"Admin Endpoints","text":"<p>(For operators only, require token)</p> Endpoint Purpose <code>GET /api/admin/jobs</code> List crawl jobs <code>GET /api/admin/jobs/{id}</code> Get job details <code>GET /metrics</code> Prometheus metrics <p>This guide focuses on public endpoints.</p>"},{"location":"api-consumer-guide/#interactive-api-documentation","title":"Interactive API Documentation","text":"<p>Explore the API interactively:</p> <p>Swagger UI: https://api.healtharchive.ca/docs</p> <p>Features: - Try requests directly in browser - See request/response examples - View full schema definitions</p>"},{"location":"api-consumer-guide/#core-concepts","title":"Core Concepts","text":""},{"location":"api-consumer-guide/#sources","title":"Sources","text":"<p>A Source represents a content origin (e.g., Health Canada, PHAC).</p> <p>Available sources: - <code>hc</code> - Health Canada - <code>phac</code> - Public Health Agency of Canada</p>"},{"location":"api-consumer-guide/#snapshots","title":"Snapshots","text":"<p>A Snapshot is a single captured web page at a specific point in time.</p> <p>Key attributes: - <code>url</code>: Original web address - <code>captureDate</code>: When it was archived - <code>title</code>: Page title - <code>snippet</code>: Text preview - <code>language</code>: Content language (<code>en</code> or <code>fr</code>)</p>"},{"location":"api-consumer-guide/#views","title":"Views","text":"<p>Search results can be returned in two views:</p> <ol> <li>Snapshots view (<code>view=snapshots</code>, default): Returns individual captures</li> <li>Pages view (<code>view=pages</code>): Returns only the latest capture per page</li> </ol>"},{"location":"api-consumer-guide/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-consumer-guide/#1-get-archive-statistics","title":"1. Get Archive Statistics","text":"<p>Use case: Display total archive size, latest capture, etc.</p> <pre><code>curl \"https://api.healtharchive.ca/api/stats\"\n</code></pre> <p>Response: <pre><code>{\n  \"snapshotsTotal\": 45678,\n  \"pagesTotal\": 12345,\n  \"sourcesTotal\": 2,\n  \"latestCaptureDate\": \"2026-01-18\",\n  \"latestCaptureAgeDays\": 0\n}\n</code></pre></p> <p>Fields: - <code>snapshotsTotal</code>: Total captures across all sources - <code>pagesTotal</code>: Unique pages (excluding duplicates) - <code>sourcesTotal</code>: Number of sources - <code>latestCaptureDate</code>: Most recent capture timestamp - <code>latestCaptureAgeDays</code>: Days since latest capture</p>"},{"location":"api-consumer-guide/#2-list-all-sources","title":"2. List All Sources","text":"<p>Use case: Understand what's in the archive</p> <pre><code>curl \"https://api.healtharchive.ca/api/sources\"\n</code></pre> <p>Response: <pre><code>[\n  {\n    \"sourceCode\": \"hc\",\n    \"sourceName\": \"Health Canada\",\n    \"recordCount\": 30123,\n    \"firstCapture\": \"2024-06-01T00:00:00Z\",\n    \"lastCapture\": \"2026-01-18T21:15:42Z\",\n    \"latestRecordId\": 12345\n  },\n  {\n    \"sourceCode\": \"phac\",\n    \"sourceName\": \"Public Health Agency of Canada\",\n    \"recordCount\": 15555,\n    \"firstCapture\": \"2024-06-01T00:00:00Z\",\n    \"lastCapture\": \"2026-01-18T20:30:15Z\",\n    \"latestRecordId\": 12346\n  }\n]\n</code></pre></p>"},{"location":"api-consumer-guide/#3-search-for-content","title":"3. Search for Content","text":"<p>Use case: Find pages about a specific topic</p>"},{"location":"api-consumer-guide/#basic-keyword-search","title":"Basic Keyword Search","text":"<pre><code>curl \"https://api.healtharchive.ca/api/search?q=covid vaccines\"\n</code></pre> <p>Response: <pre><code>{\n  \"results\": [\n    {\n      \"id\": 1,\n      \"title\": \"COVID-19 vaccines: Authorization and safety\",\n      \"sourceCode\": \"hc\",\n      \"sourceName\": \"Health Canada\",\n      \"language\": \"en\",\n      \"captureDate\": \"2026-01-18T21:15:42Z\",\n      \"originalUrl\": \"https://www.canada.ca/en/health-canada/services/drugs-health-products/covid19-industry/drugs-vaccines-treatments/vaccines.html\",\n      \"snippet\": \"Health Canada has approved several COVID-19 vaccines for use in Canada...\",\n      \"rawSnapshotUrl\": \"/api/snapshots/raw/1\"\n    }\n  ],\n  \"total\": 127,\n  \"page\": 1,\n  \"pageSize\": 20\n}\n</code></pre></p>"},{"location":"api-consumer-guide/#filter-by-source","title":"Filter by Source","text":"<pre><code>curl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;source=phac\"\n</code></pre> <p>Only returns results from PHAC.</p>"},{"location":"api-consumer-guide/#sort-by-date-newest-first","title":"Sort by Date (Newest First)","text":"<pre><code>curl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;sort=newest\"\n</code></pre> <p>Sort options: - <code>relevance</code> (default when <code>q</code> is present): Best match first - <code>newest</code>: Most recent captures first</p>"},{"location":"api-consumer-guide/#filter-by-date-range","title":"Filter by Date Range","text":"<pre><code># Captures from 2025 only\ncurl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;from=2025-01-01&amp;to=2025-12-31\"\n</code></pre>"},{"location":"api-consumer-guide/#include-non-2xx-http-status","title":"Include Non-2xx HTTP Status","text":"<p>By default, only successful (200-299) responses are returned. To include redirects, errors, etc.:</p> <pre><code>curl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;includeNon2xx=true\"\n</code></pre>"},{"location":"api-consumer-guide/#pagination","title":"Pagination","text":"<pre><code># Get page 2, 50 results per page\ncurl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;page=2&amp;pageSize=50\"\n</code></pre> <p>Limits: - <code>page</code>: Min 1 (default: 1) - <code>pageSize</code>: Min 1, Max 100 (default: 20)</p>"},{"location":"api-consumer-guide/#4-advanced-search-syntax","title":"4. Advanced Search Syntax","text":""},{"location":"api-consumer-guide/#boolean-operators","title":"Boolean Operators","text":"<pre><code># AND (both terms must appear)\ncurl \"https://api.healtharchive.ca/api/search?q=covid+AND+vaccine\"\n\n# OR (either term)\ncurl \"https://api.healtharchive.ca/api/search?q=covid+OR+coronavirus\"\n\n# NOT (exclude term)\ncurl \"https://api.healtharchive.ca/api/search?q=vaccine+NOT+flu\"\n\n# Parentheses for grouping\ncurl \"https://api.healtharchive.ca/api/search?q=(covid+OR+coronavirus)+AND+vaccine\"\n</code></pre>"},{"location":"api-consumer-guide/#field-specific-search","title":"Field-Specific Search","text":"<pre><code># Search only in title\ncurl \"https://api.healtharchive.ca/api/search?q=title:vaccines\"\n\n# Search only in snippet (text content)\ncurl \"https://api.healtharchive.ca/api/search?q=snippet:mRNA\"\n\n# Search only in URL\ncurl \"https://api.healtharchive.ca/api/search?q=url:health-canada\"\n</code></pre>"},{"location":"api-consumer-guide/#url-lookup","title":"URL Lookup","text":"<p>Find all captures of a specific page:</p> <pre><code>curl \"https://api.healtharchive.ca/api/search?q=url:https://www.canada.ca/en/health-canada.html\"\n</code></pre> <p>Or use the <code>url:</code> prefix:</p> <pre><code>curl \"https://api.healtharchive.ca/api/search?q=url:canada.ca/en/health-canada\"\n</code></pre>"},{"location":"api-consumer-guide/#5-browse-pages-latest-captures-only","title":"5. Browse Pages (Latest Captures Only)","text":"<p>Use case: Get a list of unique pages, not all captures</p> <pre><code>curl \"https://api.healtharchive.ca/api/search?view=pages&amp;source=hc&amp;sort=newest\"\n</code></pre> <p>Difference from snapshots view: - <code>view=snapshots</code>: Returns all captures (same page may appear multiple times) - <code>view=pages</code>: Returns only the most recent capture per page</p> <p>Response includes <code>pageSnapshotsCount</code>: <pre><code>{\n  \"results\": [\n    {\n      \"id\": 12345,\n      \"title\": \"Health Canada\",\n      \"pageSnapshotsCount\": 15,\n      ...\n    }\n  ]\n}\n</code></pre></p> <p><code>pageSnapshotsCount</code> tells you how many times this page was captured.</p>"},{"location":"api-consumer-guide/#6-get-snapshot-metadata","title":"6. Get Snapshot Metadata","text":"<p>Use case: Retrieve full details for a specific snapshot</p> <pre><code>curl \"https://api.healtharchive.ca/api/snapshot/12345\"\n</code></pre> <p>Response: <pre><code>{\n  \"id\": 12345,\n  \"title\": \"COVID-19 vaccines\",\n  \"sourceCode\": \"hc\",\n  \"sourceName\": \"Health Canada\",\n  \"language\": \"en\",\n  \"captureDate\": \"2026-01-18T21:15:42Z\",\n  \"originalUrl\": \"https://www.canada.ca/en/health-canada/services/drugs-health-products/covid19-industry/drugs-vaccines-treatments/vaccines.html\",\n  \"mimeType\": \"text/html\",\n  \"statusCode\": 200,\n  \"snippet\": \"Health Canada has approved...\",\n  \"rawSnapshotUrl\": \"/api/snapshots/raw/12345\"\n}\n</code></pre></p>"},{"location":"api-consumer-guide/#7-view-archived-html","title":"7. View Archived HTML","text":"<p>Use case: Retrieve the actual archived page content</p> <pre><code>curl \"https://api.healtharchive.ca/api/snapshots/raw/12345\"\n</code></pre> <p>Response: HTML page with HealthArchive header banner</p> <p>In browser: Visit <code>https://api.healtharchive.ca/api/snapshots/raw/12345</code> to see rendered page</p> <p>Note: This is the archived content exactly as it was captured, plus a small HealthArchive navigation bar.</p>"},{"location":"api-consumer-guide/#language-support","title":"Language Support","text":"<p>HealthArchive indexes content in English and French.</p>"},{"location":"api-consumer-guide/#search-by-language","title":"Search by Language","text":"<pre><code># English content only\ncurl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;language=en\"\n\n# French content only\ncurl \"https://api.healtharchive.ca/api/search?q=vaccins&amp;language=fr\"\n</code></pre> <p>Tip: HealthArchive auto-detects language, but some pages may be incorrectly classified.</p>"},{"location":"api-consumer-guide/#pagination-performance","title":"Pagination &amp; Performance","text":""},{"location":"api-consumer-guide/#response-times","title":"Response Times","text":"<ul> <li>Search: ~100-500ms (depending on query complexity)</li> <li>Stats: ~50ms (heavily cached)</li> <li>Sources: ~100ms</li> <li>Snapshot metadata: ~50ms</li> <li>Raw HTML: ~200-500ms (reads WARC from disk)</li> </ul>"},{"location":"api-consumer-guide/#rate-limiting","title":"Rate Limiting","text":"<p>Current policy: No rate limits (subject to change)</p> <p>Best practices: - Cache responses when appropriate - Use <code>pageSize</code> wisely (larger pages = slower) - Implement exponential backoff if you encounter errors</p>"},{"location":"api-consumer-guide/#pagination-best-practices","title":"Pagination Best Practices","text":"<p>For complete datasets: <pre><code>import requests\n\nbase_url = \"https://api.healtharchive.ca/api/search\"\npage = 1\npage_size = 100  # Max allowed\nall_results = []\n\nwhile True:\n    response = requests.get(base_url, params={\n        \"q\": \"vaccines\",\n        \"page\": page,\n        \"pageSize\": page_size\n    })\n    data = response.json()\n\n    all_results.extend(data[\"results\"])\n\n    if page * page_size &gt;= data[\"total\"]:\n        break\n\n    page += 1\n\nprint(f\"Retrieved {len(all_results)} results\")\n</code></pre></p>"},{"location":"api-consumer-guide/#code-examples","title":"Code Examples","text":""},{"location":"api-consumer-guide/#python","title":"Python","text":"<pre><code>import requests\n\ndef search_healtharchive(query, source=None, sort=\"relevance\", page=1):\n    \"\"\"Search HealthArchive API.\"\"\"\n    url = \"https://api.healtharchive.ca/api/search\"\n    params = {\n        \"q\": query,\n        \"sort\": sort,\n        \"page\": page,\n        \"pageSize\": 20\n    }\n    if source:\n        params[\"source\"] = source\n\n    response = requests.get(url, params=params)\n    response.raise_for_status()\n    return response.json()\n\n# Example usage\nresults = search_healtharchive(\"covid vaccines\", source=\"hc\")\nfor snapshot in results[\"results\"]:\n    print(f\"{snapshot['title']} - {snapshot['captureDate']}\")\n</code></pre>"},{"location":"api-consumer-guide/#javascript-nodejs","title":"JavaScript (Node.js)","text":"<pre><code>const fetch = require('node-fetch');\n\nasync function searchHealthArchive(query, options = {}) {\n    const baseUrl = 'https://api.healtharchive.ca/api/search';\n    const params = new URLSearchParams({\n        q: query,\n        sort: options.sort || 'relevance',\n        page: options.page || 1,\n        pageSize: options.pageSize || 20,\n        ...(options.source &amp;&amp; { source: options.source })\n    });\n\n    const response = await fetch(`${baseUrl}?${params}`);\n    return response.json();\n}\n\n// Example usage\n(async () =&gt; {\n    const results = await searchHealthArchive('covid vaccines', { source: 'hc' });\n    results.results.forEach(snapshot =&gt; {\n        console.log(`${snapshot.title} - ${snapshot.captureDate}`);\n    });\n})();\n</code></pre>"},{"location":"api-consumer-guide/#r","title":"R","text":"<pre><code>library(httr)\nlibrary(jsonlite)\n\nsearch_healtharchive &lt;- function(query, source = NULL, sort = \"relevance\", page = 1) {\n  base_url &lt;- \"https://api.healtharchive.ca/api/search\"\n\n  params &lt;- list(\n    q = query,\n    sort = sort,\n    page = page,\n    pageSize = 20\n  )\n\n  if (!is.null(source)) {\n    params$source &lt;- source\n  }\n\n  response &lt;- GET(base_url, query = params)\n  stop_for_status(response)\n\n  content(response, as = \"parsed\")\n}\n\n# Example usage\nresults &lt;- search_healtharchive(\"covid vaccines\", source = \"hc\")\nfor (snapshot in results$results) {\n  cat(sprintf(\"%s - %s\\n\", snapshot$title, snapshot$captureDate))\n}\n</code></pre>"},{"location":"api-consumer-guide/#shell-curl-jq","title":"Shell (curl + jq)","text":"<pre><code>#!/bin/bash\n\n# Search and format results\ncurl -s \"https://api.healtharchive.ca/api/search?q=vaccines&amp;source=hc\" | \\\n  jq -r '.results[] | \"\\(.title) - \\(.captureDate)\"'\n\n# Get total count\ncurl -s \"https://api.healtharchive.ca/api/search?q=vaccines\" | \\\n  jq '.total'\n\n# Download all snapshot URLs\ncurl -s \"https://api.healtharchive.ca/api/search?q=vaccines&amp;pageSize=100\" | \\\n  jq -r '.results[] | .originalUrl' &gt; urls.txt\n</code></pre>"},{"location":"api-consumer-guide/#research-workflows","title":"Research Workflows","text":""},{"location":"api-consumer-guide/#1-historical-analysis","title":"1. Historical Analysis","text":"<p>Goal: Track how Health Canada's COVID-19 vaccine page changed over time</p> <pre><code>import requests\nfrom datetime import datetime\n\nurl_to_track = \"https://www.canada.ca/en/health-canada/services/drugs-health-products/covid19-industry/drugs-vaccines-treatments/vaccines.html\"\n\nresponse = requests.get(\n    \"https://api.healtharchive.ca/api/search\",\n    params={\n        \"q\": f\"url:{url_to_track}\",\n        \"view\": \"snapshots\",  # Get all captures\n        \"sort\": \"newest\",\n        \"pageSize\": 100\n    }\n)\n\nsnapshots = response.json()[\"results\"]\n\nprint(f\"Found {len(snapshots)} captures of this page\")\n\nfor snapshot in snapshots:\n    capture_date = datetime.fromisoformat(snapshot[\"captureDate\"].replace(\"Z\", \"+00:00\"))\n    print(f\"{capture_date.strftime('%Y-%m-%d')}: {snapshot['title']}\")\n</code></pre>"},{"location":"api-consumer-guide/#2-comparative-analysis","title":"2. Comparative Analysis","text":"<p>Goal: Compare coverage of a topic across sources</p> <pre><code>import requests\n\ntopic = \"vaccination\"\n\nfor source in [\"hc\", \"phac\"]:\n    response = requests.get(\n        \"https://api.healtharchive.ca/api/search\",\n        params={\"q\": topic, \"source\": source, \"pageSize\": 1}\n    )\n    total = response.json()[\"total\"]\n    print(f\"{source.upper()}: {total} snapshots mention '{topic}'\")\n</code></pre>"},{"location":"api-consumer-guide/#3-bulk-download-metadata","title":"3. Bulk Download Metadata","text":"<p>Goal: Export all metadata for offline analysis</p> <pre><code>import requests\nimport json\n\nall_snapshots = []\npage = 1\n\nwhile True:\n    response = requests.get(\n        \"https://api.healtharchive.ca/api/search\",\n        params={\n            \"q\": \"\",  # Empty query = browse all\n            \"page\": page,\n            \"pageSize\": 100,\n            \"sort\": \"newest\"\n        }\n    )\n    data = response.json()\n    all_snapshots.extend(data[\"results\"])\n\n    if page * 100 &gt;= data[\"total\"]:\n        break\n\n    page += 1\n\n# Save to JSON\nwith open(\"healtharchive_metadata.json\", \"w\") as f:\n    json.dump(all_snapshots, f, indent=2)\n\nprint(f\"Exported {len(all_snapshots)} snapshots\")\n</code></pre>"},{"location":"api-consumer-guide/#citation-attribution","title":"Citation &amp; Attribution","text":""},{"location":"api-consumer-guide/#citing-healtharchive","title":"Citing HealthArchive","text":"<p>When using HealthArchive data in research:</p> <pre><code>HealthArchive. (2026). Archive of Canadian Health Government Websites.\nRetrieved [Date] from https://healtharchive.ca\n</code></pre>"},{"location":"api-consumer-guide/#citing-specific-snapshots","title":"Citing Specific Snapshots","text":"<pre><code>Health Canada. (2026, January 18). COVID-19 vaccines: Authorization and safety.\nArchived by HealthArchive. Retrieved from https://api.healtharchive.ca/api/snapshots/raw/12345\nOriginal URL: https://www.canada.ca/en/health-canada/services/drugs-health-products/covid19-industry/drugs-vaccines-treatments/vaccines.html\n</code></pre>"},{"location":"api-consumer-guide/#data-access-datasets","title":"Data Access &amp; Datasets","text":""},{"location":"api-consumer-guide/#bulk-data-downloads","title":"Bulk Data Downloads","text":"<p>For large-scale research, consider using dataset releases:</p> <p>Datasets Repository: github.com/jerdaw/healtharchive-datasets</p> <p>Benefits: - Pre-packaged metadata exports - Checksums for integrity verification - Version-controlled releases - Citable DOIs (future)</p>"},{"location":"api-consumer-guide/#api-vs-datasets","title":"API vs Datasets","text":"Use Case Use API Use Dataset Real-time search \u2705 \u274c Small queries (&lt; 1000 results) \u2705 \u274c Complete metadata export \u274c \u2705 Reproducible research ~ \u2705 Offline analysis \u274c \u2705"},{"location":"api-consumer-guide/#error-handling","title":"Error Handling","text":""},{"location":"api-consumer-guide/#common-http-status-codes","title":"Common HTTP Status Codes","text":"Code Meaning Action 200 Success Process response 404 Snapshot/resource not found Check ID, may have been deleted 422 Validation error Fix query parameters 500 Server error Retry with exponential backoff 503 Service unavailable Maintenance, retry later"},{"location":"api-consumer-guide/#example-error-response","title":"Example Error Response","text":"<pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\"query\", \"page\"],\n      \"msg\": \"ensure this value is greater than or equal to 1\",\n      \"type\": \"value_error.number.not_ge\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api-consumer-guide/#robust-error-handling-python","title":"Robust Error Handling (Python)","text":"<pre><code>import requests\nimport time\n\ndef search_with_retry(query, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            response = requests.get(\n                \"https://api.healtharchive.ca/api/search\",\n                params={\"q\": query},\n                timeout=10\n            )\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code &gt;= 500:\n                # Server error, retry with backoff\n                wait = 2 ** attempt\n                print(f\"Server error, retrying in {wait}s...\")\n                time.sleep(wait)\n            else:\n                # Client error, don't retry\n                raise\n        except requests.exceptions.Timeout:\n            print(f\"Timeout, retrying...\")\n            time.sleep(2 ** attempt)\n\n    raise Exception(f\"Failed after {max_retries} retries\")\n</code></pre>"},{"location":"api-consumer-guide/#api-limits-fair-use","title":"API Limits &amp; Fair Use","text":""},{"location":"api-consumer-guide/#current-limits","title":"Current Limits","text":"<ul> <li>Rate limiting: None (subject to change)</li> <li>Query complexity: No hard limits, but very broad queries may timeout</li> <li>Page size: Max 100 results per page</li> </ul>"},{"location":"api-consumer-guide/#fair-use-guidelines","title":"Fair Use Guidelines","text":"<p>To keep the API available for everyone:</p> <ol> <li>Cache aggressively: Don't request the same data repeatedly</li> <li>Use appropriate page sizes: Don't always use <code>pageSize=100</code> if you only need 20</li> <li>Implement backoff: Retry with exponential backoff on errors</li> <li>Consider datasets: For bulk access, use dataset releases instead of paginating through API</li> <li>Report issues: If you encounter consistent errors, let us know</li> </ol>"},{"location":"api-consumer-guide/#future-changes","title":"Future Changes","text":"<p>We may introduce: - Rate limiting (per IP or API key) - API keys for higher limits - Tiered access (free vs. paid)</p> <p>Stay informed: Monitor github.com/jerdaw/healtharchive-backend for announcements</p>"},{"location":"api-consumer-guide/#faq","title":"FAQ","text":"<p>Q: Is there an API key or authentication? A: Public endpoints require no authentication. Admin endpoints require a token.</p> <p>Q: Can I download the entire archive? A: Use dataset releases for bulk access. API is designed for queries, not full dumps.</p> <p>Q: How often is the archive updated? A: Annual full crawls, with potential ad-hoc crawls for significant events.</p> <p>Q: What if a snapshot I need is missing? A: Check the capture dates via <code>/api/sources</code>. We can only provide what was archived.</p> <p>Q: Can I request a specific page be archived? A: Currently no on-demand archiving. Future feature under consideration.</p> <p>Q: How long are snapshots retained? A: Indefinitely, subject to storage constraints. See Data Handling Policy.</p> <p>Q: Is there a GraphQL API? A: Not yet. REST/JSON only for now.</p> <p>Q: Can I embed archived pages in my site? A: Yes, use <code>&lt;iframe src=\"https://api.healtharchive.ca/api/snapshots/raw/{id}\"&gt;&lt;/iframe&gt;</code>. Attribute HealthArchive.</p>"},{"location":"api-consumer-guide/#support-contact","title":"Support &amp; Contact","text":"<ul> <li>Technical issues: GitHub Issues</li> <li>General questions: GitHub Discussions</li> <li>API documentation: Interactive docs</li> </ul>"},{"location":"api-consumer-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API: Try interactive documentation</li> <li>Download datasets: Visit healtharchive-datasets</li> <li>Read the architecture: Architecture Guide</li> <li>Stay updated: Watch the backend repo for changes</li> </ul> <p>Happy researching! \ud83d\udcca</p>"},{"location":"api/","title":"API Documentation","text":"<p>This page provides interactive documentation for the HealthArchive Backend API. You can explore the available endpoints, view schemas, and test requests directly from this interface.</p> <p></p>"},{"location":"architecture/","title":"HealthArchive Backend \u2013 Architecture &amp; Implementation Guide","text":"<p>This document is an in\u2011depth walkthrough of the HealthArchive.ca backend (<code>healtharchive-backend</code> repo). It covers:</p> <ul> <li>How the backend is structured.</li> <li>How it integrates with the <code>archive_tool</code> crawler subpackage.</li> <li>The data model and job lifecycle.</li> <li>The indexing pipeline (WARCs \u2192 snapshots).</li> <li>HTTP APIs (public + admin) and metrics.</li> <li>Worker loop, retries, and cleanup/retention (future).</li> </ul> <p>For <code>archive_tool</code> internals (log parsing, Docker orchestration, run modes), see <code>src/archive_tool/docs/documentation.md</code>. For a shorter, task\u2011oriented overview of common commands and local testing flows, see <code>development/live-testing.md</code>. For deployment\u2011oriented configuration (staging/prod env vars, DNS, Vercel), see <code>deployment/hosting-and-live-server-to-dos.md</code>.</p>"},{"location":"architecture/#1-highlevel-architecture","title":"1. High\u2011level architecture","text":""},{"location":"architecture/#11-components","title":"1.1 Components","text":"<ul> <li>archive_tool (internal subpackage under <code>src/archive_tool/</code>):</li> <li>CLI wrapper around <code>zimit</code> + Docker.</li> <li>Manages temporary output dirs, WARCs, and final ZIM build.</li> <li>Tracks persistent state in <code>.archive_state.json</code> + <code>.tmp*</code> directories.</li> <li> <p>Implements stall/error detection, adaptive worker reductions, and VPN     rotation (when enabled).</p> </li> <li> <p>Backend package (<code>src/ha_backend/</code>):</p> </li> <li>Orchestrates crawl jobs using <code>archive_tool</code> as a subprocess.</li> <li>Stores job and snapshot metadata in a relational database via SQLAlchemy.</li> <li>Indexes WARCs into <code>Snapshot</code> rows.</li> <li>Exposes HTTP APIs via FastAPI.</li> <li>Provides a worker loop to process queued jobs.</li> <li> <p>Offers CLI commands for admins (job creation, status, retry, cleanup).</p> </li> <li> <p>External dependencies:</p> </li> <li>Docker &amp; <code>ghcr.io/openzim/zimit</code> image.</li> <li>Database (SQLite by default; Postgres recommended in production).</li> <li>Optional VPN client/command for rotation (e.g., <code>nordvpn</code>).</li> </ul>"},{"location":"architecture/#12-data-flow-overview","title":"1.2 Data flow overview","text":"<ol> <li>Job creation:</li> <li>Admin runs <code>ha-backend create-job --source hc</code>.</li> <li> <p>Backend:</p> <ul> <li>Ensures a <code>Source</code> row exists.</li> <li>Uses <code>SourceJobConfig</code> to build seeds, tool options, and <code>output_dir</code>.</li> <li>Inserts an <code>ArchiveJob</code> with <code>status=\"queued\"</code>.</li> </ul> </li> <li> <p>Crawl (archive_tool):</p> </li> <li>Worker or CLI runs <code>run_persistent_job(job_id)</code>:<ul> <li>Builds <code>archive_tool</code> CLI args from <code>ArchiveJob.config</code> and <code>output_dir</code>.</li> <li>Runs <code>archive_tool</code> as a subprocess (no in\u2011process calls).</li> <li>Marks job <code>running</code> \u2192 <code>completed</code> or <code>failed</code> with <code>crawler_exit_code</code>    and <code>crawler_status</code>.</li> </ul> </li> <li> <p><code>archive_tool</code>:</p> <ul> <li>Validates Docker.</li> <li>Determines run mode (Fresh/Resume/New\u2011with\u2011Consolidation/Overwrite).</li> <li>Spawns <code>docker run ghcr.io/openzim/zimit zimit ...</code>.</li> <li>Tracks temp dirs and state, discovers WARCs, and optionally runs a    final ZIM build (depending on its configuration).</li> </ul> </li> <li> <p>Indexing (WARCs \u2192 Snapshot):</p> </li> <li>Worker calls <code>index_job(job_id)</code> when crawl succeeds.</li> <li> <p>Backend:</p> <ul> <li>Uses <code>CrawlState</code> + <code>find_all_warc_files</code> to locate WARCs under    <code>output_dir</code>.</li> <li>Streams WARC records, extracts HTML, text, language, etc.</li> <li>Writes <code>Snapshot</code> rows for each captured page.</li> <li>Marks job <code>indexed</code> with <code>indexed_page_count</code>.</li> </ul> </li> <li> <p>Change tracking (Snapshot \u2192 Change events):</p> </li> <li>A background task (<code>ha-backend compute-changes</code>) computes precomputed      change events between adjacent captures of the same <code>normalized_url_group</code>.</li> <li>Outputs <code>SnapshotChange</code> rows with:<ul> <li>provenance (from/to snapshot IDs, timestamps),</li> <li>summary stats (sections/lines changed),</li> <li>and a renderable diff artifact when available.</li> </ul> </li> <li> <p>This work is intentionally off the request path to keep APIs fast.</p> </li> <li> <p>Serving:</p> </li> <li> <p>FastAPI app:</p> <ul> <li><code>GET /api/search</code> queries <code>Snapshot</code> for search results.</li> <li><code>GET /api/stats</code> provides lightweight public archive totals for frontend metrics.</li> <li><code>GET /api/sources</code> summarises captures per <code>Source</code>.</li> <li><code>GET /api/snapshot/{id}</code> returns metadata for a single snapshot.</li> <li><code>GET /api/snapshots/raw/{id}</code> replays archived HTML from a WARC.</li> <li><code>GET /api/changes</code> and <code>GET /api/changes/compare</code> expose change feeds and diffs.</li> <li><code>GET /api/snapshots/{id}/timeline</code> returns a capture timeline for a page group.</li> </ul> </li> <li> <p>Admin &amp; cleanup:</p> </li> <li>Admin API:<ul> <li><code>GET /api/admin/jobs</code> / <code>{id}</code> for job status and config.</li> <li><code>GET /metrics</code> for Prometheus\u2011style metrics.</li> </ul> </li> <li>CLI:<ul> <li><code>ha-backend retry-job</code> to reattempt failed jobs.</li> <li><code>ha-backend cleanup-job</code> to delete temp dirs/state for indexed jobs,    updating <code>cleanup_status</code>.</li> </ul> </li> </ol>"},{"location":"architecture/#2-configuration-environment","title":"2. Configuration &amp; environment","text":""},{"location":"architecture/#21-config-module-ha_backendconfigpy","title":"2.1 Config module (<code>ha_backend/config.py</code>)","text":"<p>Key roles:</p> <ul> <li>Locate the archive root (<code>--output-dir</code> base) and <code>archive_tool</code> command.</li> <li>Read the database URL.</li> </ul> <p>Admin\u2011related configuration is handled separately in <code>ha_backend/api/deps.py</code>, which reads <code>HEALTHARCHIVE_ADMIN_TOKEN</code> from the environment. When this token is unset, admin and metrics endpoints are effectively open and should only be used in local development. In staging and production you should always set <code>HEALTHARCHIVE_ADMIN_TOKEN</code> to a long, random value and treat it as a secret.</p>"},{"location":"architecture/#archivetoolconfig","title":"ArchiveToolConfig","text":"<pre><code>@dataclass\nclass ArchiveToolConfig:\n    archive_root: Path = DEFAULT_ARCHIVE_ROOT\n    archive_tool_cmd: str = DEFAULT_ARCHIVE_TOOL_CMD\n\n    def ensure_archive_root(self) -&gt; None:\n        self.archive_root.mkdir(parents=True, exist_ok=True)\n</code></pre> <p>Defaults:</p> <ul> <li><code>DEFAULT_ARCHIVE_ROOT</code> = <code>/mnt/nasd/nobak/healtharchive/jobs</code></li> <li><code>DEFAULT_ARCHIVE_TOOL_CMD</code> = <code>\"archive-tool\"</code></li> </ul> <p>Env overrides:</p> <ul> <li><code>HEALTHARCHIVE_ARCHIVE_ROOT</code> \u2192 archive root.</li> <li><code>HEALTHARCHIVE_TOOL_CMD</code> \u2192 CLI to call (e.g., <code>archive-tool</code>, <code>python run_archive.py</code>).</li> </ul>"},{"location":"architecture/#databaseconfig","title":"DatabaseConfig","text":"<pre><code>@dataclass\nclass DatabaseConfig:\n    database_url: str = DEFAULT_DATABASE_URL\n</code></pre> <p>Defaults:</p> <ul> <li><code>DEFAULT_DATABASE_URL = \"sqlite:///healtharchive.db\"</code> in the repo root.</li> </ul> <p>Env override:</p> <ul> <li><code>HEALTHARCHIVE_DATABASE_URL</code>.</li> </ul>"},{"location":"architecture/#22-logging-ha_backendlogging_configpy","title":"2.2 Logging (<code>ha_backend/logging_config.py</code>)","text":"<p>Centralized logging configuration:</p> <ul> <li>Reads <code>HEALTHARCHIVE_LOG_LEVEL</code> (default <code>INFO</code>).</li> <li>On first call, uses <code>logging.basicConfig(...)</code> with:</li> <li>Format: <code>\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"</code>.</li> <li>Adjusts noisy loggers:</li> <li><code>sqlalchemy.engine</code> \u2192 <code>WARNING</code>.</li> <li><code>uvicorn.access</code> \u2192 <code>INFO</code>.</li> </ul> <p>Used in:</p> <ul> <li><code>ha_backend.api.__init__</code> (API startup).</li> <li><code>ha_backend.cli.main</code> (CLI entrypoint).</li> </ul>"},{"location":"architecture/#3-data-model-sqlalchemy-orm","title":"3. Data model (SQLAlchemy ORM)","text":"<p>Defined in <code>src/ha_backend/models.py</code>, with <code>Base</code> from <code>ha_backend.db</code>.</p>"},{"location":"architecture/#31-source","title":"3.1 Source","text":"<p>Represents a logical content origin (e.g., Health Canada, PHAC).</p> <p>Important fields:</p> <ul> <li><code>id: int</code> (PK)</li> <li><code>code: str</code> \u2013 short code (<code>\"hc\"</code>, <code>\"phac\"</code>) \u2013 unique, indexed.</li> <li><code>name: str</code> \u2013 human\u2011readable name.</li> <li><code>base_url: str | None</code></li> <li><code>description: str | None</code></li> <li><code>enabled: bool</code></li> <li>Timestamps: <code>created_at</code>, <code>updated_at</code></li> </ul> <p>Relationships:</p> <ul> <li><code>jobs: List[ArchiveJob]</code> \u2013 all jobs for this source.</li> <li><code>snapshots: List[Snapshot]</code> \u2013 all snapshots for this source.</li> </ul>"},{"location":"architecture/#32-archivejob","title":"3.2 ArchiveJob","text":"<p>Represents a single <code>archive_tool</code> run (or family of runs) for a source.</p> <p>Key fields:</p> <ul> <li>Identity:</li> <li><code>id: int</code> (PK)</li> <li><code>source_id: int | None</code> \u2192 FK to <code>sources.id</code></li> <li><code>name: str</code> \u2013 must match <code>--name</code> for <code>archive_tool</code>; used in ZIM naming.</li> <li> <p><code>output_dir: str</code> \u2013 host path used as <code>--output-dir</code> for <code>archive_tool</code>.</p> </li> <li> <p>Lifecycle/status:</p> </li> <li><code>status: str</code> \u2013 high\u2011level state; typical values:<ul> <li><code>queued</code></li> <li><code>running</code></li> <li><code>retryable</code></li> <li><code>failed</code></li> <li><code>completed</code> (crawl succeeded)</li> <li><code>indexing</code></li> <li><code>indexed</code></li> <li><code>index_failed</code></li> </ul> </li> <li><code>queued_at</code>, <code>started_at</code>, <code>finished_at</code>: timestamps.</li> <li> <p><code>retry_count: int</code> \u2013 number of times the worker retried the crawl.</p> </li> <li> <p>Configuration:</p> </li> <li> <p><code>config: JSON | None</code> \u2013 \u201copaque\u201d config used to reconstruct the CLI:</p> <pre><code>{\n  \"seeds\": [\"https://...\"],\n  \"zimit_passthrough_args\": [\"--profile\", \"foo\"],\n  \"tool_options\": {\n    \"cleanup\": false,\n    \"overwrite\": false,\n    \"skip_final_build\": false,\n    \"enable_monitoring\": false,\n    \"enable_adaptive_workers\": false,\n    \"enable_vpn_rotation\": false,\n    \"initial_workers\": 2,\n    \"log_level\": \"INFO\",\n    \"...\": \"...\"\n  }\n}\n</code></pre> </li> <li> <p>Crawl metrics:</p> </li> <li><code>crawler_exit_code: int | None</code> \u2013 exit code from the <code>archive_tool</code> process.</li> <li><code>crawler_status: str | None</code> \u2013 summarised status (e.g. <code>\"success\"</code>, <code>\"failed\"</code>).</li> <li><code>crawler_stage: str | None</code> \u2013 last known stage (not heavily used yet).</li> <li><code>last_stats_json: JSON | None</code> \u2013 parsed crawl stats from the latest combined log, when available.</li> <li> <p><code>pages_crawled</code>, <code>pages_total</code>, <code>pages_failed</code>: simple integer metrics derived from <code>last_stats_json</code> (best-effort).</p> </li> <li> <p>WARC/ZIM counts:</p> </li> <li><code>warc_file_count: int</code> \u2013 number of WARCs discovered for this job.</li> <li> <p><code>indexed_page_count: int</code> \u2013 number of <code>Snapshot</code>s created during indexing.</p> </li> <li> <p>Filesystem paths:</p> </li> <li><code>final_zim_path: str | None</code> \u2013 if a ZIM is produced by <code>archive_tool</code> or manual <code>warc2zim</code>.</li> <li><code>combined_log_path: str | None</code> \u2013 path to the latest combined log, used for stats/debugging.</li> <li> <p><code>state_file_path: str | None</code> \u2013 path to <code>.archive_state.json</code> within <code>output_dir</code> (may be <code>None</code> after cleanup).</p> </li> <li> <p>Cleanup state (future):</p> </li> <li><code>cleanup_status: str</code> \u2013 describes whether any cleanup has occurred:<ul> <li><code>\"none\"</code> (default) \u2013 temp dirs &amp; state still present (or never existed).</li> <li><code>\"temp_cleaned\"</code> \u2013 <code>cleanup-job</code> or an equivalent operation removed temp dirs/state.</li> <li>Future values could represent more aggressive cleanup.</li> </ul> </li> <li><code>cleaned_at: datetime | None</code> \u2013 when cleanup was performed.</li> </ul> <p>Relationships:</p> <ul> <li><code>source: Source | None</code> \u2013 parent source.</li> <li><code>snapshots: List[Snapshot]</code> \u2013 all snapshots produced by this job.</li> </ul>"},{"location":"architecture/#33-snapshot","title":"3.3 Snapshot","text":"<p>Represents a single captured web page (an HTML response) extracted from a WARC.</p> <p>Key fields:</p> <ul> <li>Identity:</li> <li><code>id: int</code> (PK)</li> <li><code>job_id: int | None</code> \u2192 FK to <code>archive_jobs.id</code></li> <li> <p><code>source_id: int | None</code> \u2192 FK to <code>sources.id</code></p> </li> <li> <p>URL &amp; grouping:</p> </li> <li><code>url: str</code> \u2013 full URL of the capture (including query string).</li> <li> <p><code>normalized_url_group: str | None</code> \u2013 optional canonicalised URL for grouping (e.g., removing query or anchors).</p> </li> <li> <p>Timing:</p> </li> <li> <p><code>capture_timestamp: datetime</code> \u2013 from <code>WARC-Date</code> or HTTP headers.</p> </li> <li> <p>HTTP &amp; content:</p> </li> <li><code>mime_type: str | None</code></li> <li><code>status_code: int | None</code></li> <li><code>title: str | None</code> \u2013 extracted from <code>&lt;title&gt;</code> or headings.</li> <li><code>snippet: str | None</code> \u2013 short preview text.</li> <li> <p><code>language: str | None</code> \u2013 ISO language (e.g. <code>\"en\"</code>, <code>\"fr\"</code>).</p> </li> <li> <p>Storage / replay:</p> </li> <li><code>warc_path: str</code> \u2013 path to the <code>.warc.gz</code> file on disk.</li> <li><code>warc_record_id: str | None</code> \u2013 WARC record identifier or offset (see <code>indexing.viewer</code>).</li> <li><code>raw_snapshot_path: str | None</code> \u2013 optional path to a static HTML export, if you create such stubs.</li> <li><code>content_hash: str | None</code> \u2013 hash of the HTML body for deduplication.</li> </ul> <p>Relationships:</p> <ul> <li><code>job: ArchiveJob | None</code></li> <li><code>source: Source | None</code></li> </ul>"},{"location":"architecture/#4-job-registry--creation-ha_backendjob_registrypy","title":"4. Job registry &amp; creation (<code>ha_backend/job_registry.py</code>)","text":"<p>The job registry defines default behavior and seeds for each source code (<code>\"hc\"</code>, <code>\"phac\"</code>).</p>"},{"location":"architecture/#41-sourcejobconfig","title":"4.1 SourceJobConfig","text":"<pre><code>@dataclass\nclass SourceJobConfig:\n    source_code: str\n    name_template: str\n    default_seeds: List[str]\n    default_zimit_passthrough_args: List[str]\n    default_tool_options: Dict[str, Any]\n    schedule_hint: Optional[str] = None\n</code></pre> <p>Examples:</p> <ul> <li> <p><code>hc</code> (Health Canada):</p> </li> <li> <p><code>name_template = \"hc-{date:%Y%m%d}\"</code></p> </li> <li><code>default_seeds = [\"https://www.canada.ca/en/health-canada.html\"]</code></li> <li> <p><code>default_tool_options</code>:</p> <ul> <li><code>cleanup = False</code></li> <li><code>overwrite = False</code></li> <li><code>enable_monitoring = True</code> (required for adaptive strategies)</li> <li><code>enable_adaptive_workers = True</code></li> <li><code>enable_adaptive_restart = True</code></li> <li><code>enable_vpn_rotation = False</code> (disabled by default)</li> <li><code>initial_workers = 2</code></li> <li><code>stall_timeout_minutes = 60</code></li> <li><code>docker_shm_size = \"1g\"</code></li> <li><code>skip_final_build = True</code> (annual campaign: search/indexing uses WARCs)</li> <li><code>error_threshold_timeout = 50</code></li> <li><code>error_threshold_http = 50</code></li> <li><code>backoff_delay_minutes = 2</code></li> <li><code>max_container_restarts = 20</code></li> <li><code>log_level = \"INFO\"</code></li> </ul> </li> <li> <p><code>phac</code> (Public Health Agency of Canada) is similar with a PHAC home page seed.</p> </li> </ul>"},{"location":"architecture/#42-job-name-and-output-dir","title":"4.2 Job name and output dir","text":"<ul> <li><code>generate_job_name(source_cfg, now)</code>:</li> <li>Renders <code>name_template</code> using <code>{date:%Y%m%d}</code> from UTC timestamp.</li> <li> <p>E.g. <code>hc-20251209</code>.</p> </li> <li> <p><code>build_output_dir_for_job(source_code, job_name, archive_root, now)</code>:</p> </li> </ul> <pre><code>&lt;archive_root&gt;/&lt;source_code&gt;/&lt;YYYYMMDDThhmmssZ&gt;__&lt;job_name&gt;\n</code></pre> <p>Example:</p> <pre><code>/mnt/nasd/nobak/healtharchive/jobs/hc/20251209T210911Z__hc-20251209\n</code></pre>"},{"location":"architecture/#43-job-config-json","title":"4.3 Job config JSON","text":"<ul> <li><code>build_job_config(source_cfg, extra_seeds=None, overrides=None)</code>:</li> <li>Merges <code>default_seeds</code> + extra seeds.</li> <li>Copies <code>default_zimit_passthrough_args</code>.</li> <li>Copies and updates <code>default_tool_options</code> with any <code>overrides</code>.</li> <li> <p>Performs basic validation of <code>tool_options</code> to fail fast on     misconfiguration:</p> <ul> <li>If <code>enable_adaptive_workers=True</code> but <code>enable_monitoring</code> is not <code>True</code>,   a <code>ValueError</code> is raised.</li> <li>If <code>enable_vpn_rotation=True</code> but <code>enable_monitoring</code> is not <code>True</code>,   a <code>ValueError</code> is raised.</li> <li>If <code>enable_vpn_rotation=True</code> but <code>vpn_connect_command</code> is missing or   empty, a <code>ValueError</code> is raised.</li> </ul> </li> </ul> <p>Result structure:</p> <pre><code>{\n  \"seeds\": [\"https://...\", \"...\"],\n  \"zimit_passthrough_args\": [],\n  \"tool_options\": {\n    \"cleanup\": false,\n    \"overwrite\": false,\n    \"skip_final_build\": true,\n    \"enable_monitoring\": true,\n    \"enable_adaptive_workers\": true,\n    \"enable_adaptive_restart\": true,\n    \"enable_vpn_rotation\": false,\n    \"initial_workers\": 2,\n    \"stall_timeout_minutes\": 60,\n    \"docker_shm_size\": \"1g\",\n    \"error_threshold_timeout\": 50,\n    \"error_threshold_http\": 50,\n    \"backoff_delay_minutes\": 2,\n    \"max_container_restarts\": 20,\n    \"log_level\": \"INFO\"\n  }\n}\n</code></pre>"},{"location":"architecture/#44-create_job_for_source","title":"4.4 create_job_for_source","text":"<pre><code>def create_job_for_source(\n    source_code: str,\n    *,\n    session: Session,\n    overrides: Optional[Dict[str, Any]] = None,\n) -&gt; ORMArchiveJob:\n</code></pre> <p>Steps:</p> <ol> <li>Look up <code>SourceJobConfig</code> for <code>source_code</code>.</li> <li>Ensure a <code>Source</code> row with that code exists (or raise).</li> <li>Resolve <code>archive_root</code> from config.</li> <li>Generate <code>job_name</code> and <code>output_dir</code>.</li> <li>Build <code>job_config</code>.</li> <li>Insert an <code>ArchiveJob</code>:</li> <li><code>status=\"queued\"</code>, <code>queued_at=now</code>, <code>config=job_config</code>.</li> </ol> <p>The CLI command <code>ha-backend create-job --source hc</code> is a thin wrapper around this.</p>"},{"location":"architecture/#5-archive_tool-integration--job-runner-ha_backendjobspy","title":"5. archive_tool integration &amp; job runner (<code>ha_backend/jobs.py</code>)","text":""},{"location":"architecture/#51-runtimearchivejob","title":"5.1 RuntimeArchiveJob","text":"<p><code>RuntimeArchiveJob</code> is a small helper for ad\u2011hoc runs (<code>ha-backend run-job</code>) that:</p> <ul> <li>Holds just a <code>name</code> and <code>seeds: list[str]</code>.</li> <li>Creates a timestamped job directory under the archive root (unless overridden).</li> <li>Builds the <code>archive_tool</code> CLI command.</li> <li>Executes it via <code>subprocess.run(...)</code>.</li> </ul> <p>This path is used by:</p> <ul> <li><code>ha-backend run-job</code> \u2013 direct, non\u2011persistent jobs.</li> </ul>"},{"location":"architecture/#52-run_persistent_job--db-backed-jobs","title":"5.2 run_persistent_job \u2013 DB\u2011backed jobs","text":"<pre><code>def run_persistent_job(job_id: int) -&gt; int:\n    ...\n</code></pre> <p>Responsibilities:</p> <ol> <li> <p>Load job and mark running:</p> </li> <li> <p>Using <code>get_session()</code>:</p> <ul> <li>Fetch <code>ArchiveJob</code> by ID.</li> <li>Validate <code>status in (\"queued\", \"retryable\")</code>.</li> <li>Extract <code>config</code>, splitting into:</li> <li><code>tool_options</code></li> <li><code>zimit_passthrough_args</code></li> <li><code>seeds</code></li> <li>Validate that <code>seeds</code> is non\u2011empty.</li> <li>Record <code>output_dir</code> and <code>name</code>.</li> <li>Set:</li> <li><code>status = \"running\"</code></li> <li><code>started_at = now</code></li> </ul> </li> <li> <p>Build CLI options from tool_options:</p> </li> <li> <p>Core:</p> <pre><code>initial_workers = int(tool_options.initial_workers)\ncleanup = bool(tool_options.cleanup)\noverwrite = bool(tool_options.overwrite)\nlog_level = str(tool_options.log_level)\n</code></pre> </li> <li> <p>Monitoring options:</p> <p>Only if <code>enable_monitoring</code> is <code>True</code>:</p> <ul> <li>Adds <code>--enable-monitoring</code>.</li> <li>Optionally:</li> <li><code>monitor_interval_seconds</code> \u2192 <code>--monitor-interval-seconds N</code></li> <li><code>stall_timeout_minutes</code> \u2192 <code>--stall-timeout-minutes N</code></li> <li><code>error_threshold_timeout</code> \u2192 <code>--error-threshold-timeout N</code></li> <li><code>error_threshold_http</code> \u2192 <code>--error-threshold-http N</code></li> </ul> </li> <li> <p>Adaptive workers:</p> <p>Only if both <code>enable_monitoring</code> and <code>enable_adaptive_workers</code> are <code>True</code>:</p> <ul> <li>Adds <code>--enable-adaptive-workers</code>.</li> <li>Optionally:</li> <li><code>min_workers</code> \u2192 <code>--min-workers N</code></li> <li><code>max_worker_reductions</code> \u2192 <code>--max-worker-reductions N</code></li> </ul> </li> <li> <p>VPN rotation:</p> <p>Only if <code>enable_monitoring</code>, <code>enable_vpn_rotation</code>, and <code>vpn_connect_command</code>  are all present:</p> <ul> <li>Adds:</li> </ul> <pre><code>--enable-vpn-rotation\n--vpn-connect-command \"&lt;vpn_connect_command&gt;\"\n</code></pre> <ul> <li>Optionally:</li> <li><code>max_vpn_rotations</code> \u2192 <code>--max-vpn-rotations N</code></li> <li><code>vpn_rotation_frequency_minutes</code> \u2192 <code>--vpn-rotation-frequency-minutes N</code></li> </ul> </li> <li> <p>Backoff:</p> <p>Only when monitoring is enabled and <code>backoff_delay_minutes</code> is set:</p> <ul> <li><code>--backoff-delay-minutes N</code>.</li> </ul> </li> <li> <p>Zimit passthrough:</p> <ul> <li><code>zimit_passthrough_args</code> are appended directly (no explicit <code>\"--\"</code>    separator is required): <code>archive_tool</code> uses <code>argparse.parse_known_args()</code>    and passes unknown args through to <code>zimit</code>.</li> <li>For <code>ha-backend run-job</code>, a leading <code>\"--\"</code> is accepted and stripped for    convenience when passing through flags interactively.</li> </ul> </li> <li> <p>The final <code>extra_args</code> passed to <code>RuntimeArchiveJob.run(...)</code> look like:</p> <pre><code>[archive_tool_flags..., zimit_passthrough_args...]\n</code></pre> </li> <li> <p>Execute archive_tool:</p> </li> <li> <p>Instantiates <code>RuntimeArchiveJob(name, seeds)</code>.</p> </li> <li> <p>Calls:</p> <pre><code>rc = runtime_job.run(\n    initial_workers=initial_workers,\n    cleanup=cleanup,\n    overwrite=overwrite,\n    log_level=log_level,\n    extra_args=full_extra_args,\n    stream_output=True,\n    output_dir_override=Path(output_dir_str),\n)\n</code></pre> </li> <li> <p><code>output_dir_override</code> ensures a specific job directory under the archive      root (matching the DB record) is used, and created if needed.</p> </li> <li> <p>Update job status:</p> </li> <li> <p>After the subprocess returns:</p> <ul> <li><code>crawler_exit_code = rc</code></li> <li><code>finished_at = now</code></li> <li><code>combined_log_path</code> is recorded best-effort (newest <code>archive_*.combined.log</code>)</li> <li><code>status = \"completed\"</code> and <code>crawler_status = \"success\"</code> if <code>rc == 0</code></li> <li>Otherwise:</li> <li><code>status = \"retryable\"</code>, <code>crawler_status = \"infra_error\"</code> for storage/mount failures</li> <li><code>status = \"failed\"</code>, <code>crawler_status = \"infra_error_config\"</code> for CLI/config/runtime errors      (e.g., invalid <code>zimit_passthrough_args</code>)</li> <li><code>status = \"failed\"</code>, <code>crawler_status = \"failed\"</code> for normal crawl failures</li> </ul> </li> </ol> <p>The worker uses <code>run_persistent_job(job_id)</code> for each queued job.</p>"},{"location":"architecture/#53-maintaining-the-archive_tool-integration","title":"5.3 Maintaining the archive_tool integration","text":"<p>The backend and <code>archive_tool</code> share a small but important contract:</p> <ul> <li> <p>Configuration JSON:</p> </li> <li> <p><code>ArchiveJob.config</code> stores a dict that is the serialised form of     <code>ArchiveJobConfig</code> from <code>ha_backend.archive_contract</code>:</p> <pre><code>{\n  \"seeds\": [\"https://...\", \"...\"],\n  \"zimit_passthrough_args\": [\"--scopeType\", \"host\"],\n  \"tool_options\": {\n    \"cleanup\": false,\n    \"overwrite\": false,\n    \"skip_final_build\": true,\n    \"enable_monitoring\": true,\n    \"enable_adaptive_workers\": true,\n    \"enable_adaptive_restart\": true,\n    \"enable_vpn_rotation\": false,\n    \"initial_workers\": 2,\n    \"log_level\": \"INFO\",\n    \"relax_perms\": true,\n    \"stall_timeout_minutes\": 60,\n    \"docker_shm_size\": \"1g\",\n    \"error_threshold_timeout\": 50,\n    \"error_threshold_http\": 50,\n    \"max_container_restarts\": 20,\n    \"backoff_delay_minutes\": 2\n  }\n}\n</code></pre> </li> <li> <p><code>SourceJobConfig.default_tool_options</code> in <code>ha_backend.job_registry</code> is the     source of truth for defaults; overrides are merged via     <code>build_job_config(...)</code> which uses <code>ArchiveToolOptions</code> +     <code>validate_tool_options(...)</code> to enforce invariants that mirror     <code>archive_tool.cli</code> (e.g. monitoring required for adaptive/VPN).</p> </li> <li> <p>CLI construction:</p> </li> <li> <p><code>ha_backend.jobs.run_persistent_job</code> is the only place that maps     <code>tool_options</code> fields to <code>archive_tool</code> CLI flags. It expects the argument     model described in <code>src/archive_tool/docs/documentation.md</code> and     <code>archive_tool/cli.py</code>.</p> </li> <li> <p>If you add or rename CLI options in <code>archive_tool</code>:</p> <ul> <li>Extend <code>ArchiveToolOptions</code> and <code>ArchiveJobConfig</code> to carry the new   fields.</li> <li>Update <code>run_persistent_job</code> to add/remove the corresponding flags.</li> <li>Adjust tests under <code>tests/test_job_registry.py</code>,   <code>tests/test_archive_contract.py</code>, and <code>tests/test_jobs_persistent.py</code>   that assert config and CLI behaviour.</li> </ul> </li> <li> <p>Stats and logs:</p> </li> <li> <p><code>archive_tool</code> writes combined logs     <code>archive_&lt;stage_name&gt;_*.combined.log</code> under each job's <code>output_dir</code> and     emits <code>\"Crawl statistics\"</code> JSON lines that     <code>archive_tool.utils.parse_last_stats_from_log</code> can parse.</p> </li> <li> <p><code>ha_backend.crawl_stats.update_job_stats_from_logs</code>:</p> <ul> <li>Locates the latest combined log for a job.</li> <li>Calls <code>parse_last_stats_from_log(log_path)</code> to obtain a stats dict.</li> <li>Stores it in <code>ArchiveJob.last_stats_json</code>.</li> <li>Updates <code>pages_crawled</code>, <code>pages_total</code>, <code>pages_failed</code>, and   <code>combined_log_path</code> as a best-effort summary.</li> </ul> </li> <li> <p><code>/metrics</code> exposes these page counters via:</p> <ul> <li><code>healtharchive_jobs_pages_crawled_total</code></li> <li><code>healtharchive_jobs_pages_failed_total</code></li> <li>per-source variants, backed by the <code>pages_*</code> fields on <code>ArchiveJob</code>.</li> </ul> </li> <li> <p>WARC discovery and cleanup:</p> </li> <li> <p><code>ha_backend.indexing.warc_discovery.discover_warcs_for_job</code> relies on     <code>archive_tool.state.CrawlState</code> and <code>archive_tool.utils.find_all_warc_files</code>     / <code>find_latest_temp_dir_fallback</code> for WARC discovery and temp dir     tracking.</p> </li> <li><code>ha_backend.cli.cmd_cleanup_job</code> uses <code>CrawlState</code> and     <code>archive_tool.utils.cleanup_temp_dirs</code> to remove <code>.tmp*</code> directories and     <code>.archive_state.json</code> safely once jobs are indexed.</li> </ul> <p>If you change log formats, state layout, or directory structure in <code>archive_tool</code>, update the corresponding backend helpers (<code>ArchiveJobConfig</code>, <code>run_persistent_job</code>, <code>update_job_stats_from_logs</code>, WARC discovery, and cleanup) and their tests to keep the contract in sync.</p>"},{"location":"architecture/#6-indexing-pipeline-ha_backendindexing","title":"6. Indexing pipeline (<code>ha_backend/indexing/*</code>)","text":"<p>The indexing pipeline converts the WARCs produced by <code>archive_tool</code> into structured <code>Snapshot</code> rows.</p>"},{"location":"architecture/#61-warc-discovery-warc_discoverypy","title":"6.1 WARC discovery (<code>warc_discovery.py</code>)","text":"<pre><code>from archive_tool.state import CrawlState\nfrom archive_tool.utils import find_all_warc_files, find_latest_temp_dir_fallback\n</code></pre> <pre><code>def discover_warcs_for_job(\n    job: ArchiveJob,\n    *,\n    allow_fallback: bool = True,\n) -&gt; List[Path]:\n</code></pre> <p>Steps:</p> <ol> <li>Resolve <code>host_output_dir = Path(job.output_dir).resolve()</code>.</li> <li>Instantiate <code>CrawlState(host_output_dir, initial_workers=1)</code>:</li> <li>This loads <code>.archive_state.json</code> if present.</li> <li>Get <code>temp_dirs = state.get_temp_dir_paths()</code>:</li> <li>Returns only existing directories and prunes missing ones from state.</li> <li>If <code>temp_dirs</code> is empty and <code>allow_fallback</code>:</li> <li>Use <code>find_latest_temp_dir_fallback(host_output_dir)</code> to scan for <code>.tmp*</code>      directories.</li> <li>If still empty \u2192 return <code>[]</code>.</li> <li>Call <code>find_all_warc_files(temp_dirs)</code>:</li> <li>Returns a de\u2011duplicated list of <code>*.warc.gz</code> files under each      <code>collections/crawl-*/archive</code> directory.</li> </ol> <p>This ensures the backend uses exactly the same WARC discovery logic as <code>archive_tool</code> itself.</p>"},{"location":"architecture/#62-warc-reading-warc_readerpy","title":"6.2 WARC reading (<code>warc_reader.py</code>)","text":"<p>Wraps <code>warcio</code> to stream HTML response records from a <code>.warc.gz</code> file.</p> <p>Exports a generator like:</p> <pre><code>def iter_html_records(warc_path: Path) -&gt; Iterator[ArchiveRecord]:\n    ...\n</code></pre> <p>Where <code>ArchiveRecord</code> provides:</p> <ul> <li><code>url: str</code></li> <li><code>capture_timestamp: datetime</code></li> <li><code>headers: dict[str, str]</code></li> <li><code>body_bytes: bytes</code></li> <li><code>warc_path: Path</code></li> <li><code>warc_record_id: str | None</code></li> </ul>"},{"location":"architecture/#63-text-extraction-text_extractionpy","title":"6.3 Text extraction (<code>text_extraction.py</code>)","text":"<p>Helpers:</p> <ul> <li><code>extract_title(html: str) -&gt; str</code> \u2013 heuristics over <code>&lt;title&gt;</code> / headings.</li> <li><code>extract_text(html: str) -&gt; str</code> \u2013 uses BeautifulSoup to pull visible text.</li> <li><code>make_snippet(text: str) -&gt; str</code> \u2013 short preview (~N chars/words).</li> <li><code>detect_language(text: str, headers: dict) -&gt; str</code> \u2013 simple language detection,   leveraging headers or heuristics (kept basic for now).</li> </ul>"},{"location":"architecture/#64-mapping-records-to-snapshot-mappingpy","title":"6.4 Mapping records to Snapshot (<code>mapping.py</code>)","text":"<p><code>record_to_snapshot(job, source, rec, title, snippet, language)</code>:</p> <ul> <li>Takes:</li> <li><code>ArchiveJob</code></li> <li><code>Source</code></li> <li><code>ArchiveRecord</code> from <code>iter_html_records</code></li> <li><code>title</code>, <code>snippet</code>, <code>language</code> from text extraction</li> <li>Produces a new <code>Snapshot</code> instance with:</li> <li><code>job_id</code>, <code>source_id</code></li> <li><code>url</code>, <code>normalized_url_group</code></li> <li><code>capture_timestamp</code></li> <li><code>mime_type</code>, <code>status_code</code></li> <li><code>title</code>, <code>snippet</code>, <code>language</code></li> <li><code>warc_path</code>, <code>warc_record_id</code></li> <li><code>content_hash</code> (if computed)</li> </ul>"},{"location":"architecture/#65-orchestration-pipelinepy","title":"6.5 Orchestration (<code>pipeline.py</code>)","text":"<pre><code>def index_job(job_id: int) -&gt; int:\n</code></pre> <p>Steps:</p> <ol> <li>Load <code>ArchiveJob</code> by ID, ensure:</li> <li><code>job.source</code> is not <code>None</code>.</li> <li><code>job.status in (\"completed\", \"index_failed\", \"indexed\")</code>.</li> <li>Validate <code>output_dir</code> exists.</li> <li>Discover WARCs:</li> <li><code>warc_paths = discover_warcs_for_job(job)</code>.</li> <li>Sets <code>job.warc_file_count = len(warc_paths)</code>.</li> <li>If no WARCs found:<ul> <li>Logs warning.</li> <li>Sets <code>job.status = \"index_failed\"</code> and returns <code>1</code>.</li> </ul> </li> <li>Clear previous snapshots for this job:</li> <li><code>DELETE FROM snapshots WHERE job_id = :job_id</code>.</li> <li>Mark job as indexing:</li> <li><code>job.indexed_page_count = 0</code>, <code>job.status = \"indexing\"</code>.</li> <li>For each WARC path:</li> <li>Iterate <code>iter_html_records(warc_path)</code>.</li> <li>Decode <code>html = rec.body_bytes.decode(\"utf-8\", errors=\"replace\")</code>.</li> <li>Use text extraction functions to get <code>title</code>, <code>text</code>, <code>snippet</code>, <code>language</code>.</li> <li>Call <code>record_to_snapshot(...)</code> to construct a <code>Snapshot</code>.</li> <li><code>session.add(snapshot)</code>; flush every 500 additions.</li> <li>Count snapshots in <code>n_snapshots</code>.</li> <li>On per\u2011record errors, log and continue.</li> <li>On success:</li> <li>Set <code>job.indexed_page_count = n_snapshots</code>.</li> <li>Set <code>job.status = \"indexed\"</code>.</li> <li>Return <code>0</code>.</li> <li>On unexpected error:</li> <li>Log at error level.</li> <li>Set <code>job.status = \"index_failed\"</code>.</li> <li>Return <code>1</code>.</li> </ol>"},{"location":"architecture/#7-viewer-helper-ha_backendindexingviewerpy","title":"7. Viewer helper (<code>ha_backend/indexing/viewer.py</code>)","text":"<p>The viewer helper is used by <code>GET /api/snapshots/raw/{id}</code> to reconstruct the HTML for a snapshot from its WARC.</p> <p>Design:</p> <ul> <li>Either:</li> <li>Use <code>warc_record_id</code> to seek directly to a known record, or</li> <li>Fallback to scanning <code>warc_path</code> for the first matching URL + timestamp.</li> </ul> <p>The API route:</p> <ul> <li>Validates that <code>Snapshot</code> and its <code>warc_path</code> exist.</li> <li>Calls <code>find_record_for_snapshot(snapshot)</code>:</li> <li>Returns an <code>ArchiveRecord</code> or <code>None</code>.</li> <li>Decodes <code>record.body_bytes</code> as UTF\u20118 with replacement.</li> <li>Writes <code>HTMLResponse(content=html, media_type=\"text/html\")</code>.</li> </ul> <p>This is used by the Next.js frontend for the embedded snapshot viewer.</p>"},{"location":"architecture/#8-http-api-ha_backendapi","title":"8. HTTP API (<code>ha_backend/api/*</code>)","text":""},{"location":"architecture/#81-public-schemas-schemaspy","title":"8.1 Public schemas (<code>schemas.py</code>)","text":"<p>Public Pydantic models:</p> <ul> <li><code>SourceSummarySchema</code> \u2013 used by <code>/api/sources</code>:</li> </ul> <pre><code>sourceCode: str\nsourceName: str\nrecordCount: int\nfirstCapture: str\nlastCapture: str\nlatestRecordId: Optional[int]\n</code></pre> <ul> <li> <p><code>SnapshotSummarySchema</code> \u2013 used by <code>/api/search</code>:</p> </li> <li> <p><code>id</code>, <code>title</code>, <code>sourceCode</code>, <code>sourceName</code>, <code>language</code>, <code>captureDate</code>,     <code>originalUrl</code>, <code>snippet</code>, <code>rawSnapshotUrl</code>.</p> </li> <li> <p><code>SearchResponseSchema</code>:</p> </li> <li> <p><code>results: List[SnapshotSummarySchema]</code>, <code>total</code>, <code>page</code>, <code>pageSize</code>.</p> </li> <li> <p><code>ArchiveStatsSchema</code> \u2013 used by <code>/api/stats</code>:</p> </li> <li> <p><code>snapshotsTotal</code>, <code>pagesTotal</code>, <code>sourcesTotal</code>, <code>latestCaptureDate</code>, <code>latestCaptureAgeDays</code>.</p> </li> <li> <p><code>SnapshotDetailSchema</code> \u2013 used by <code>/api/snapshot/{id}</code>:</p> </li> <li> <p>Contains metadata for a single snapshot including <code>mimeType</code> and     <code>statusCode</code>, plus <code>rawSnapshotUrl</code>.</p> </li> </ul>"},{"location":"architecture/#82-public-routes-routes_publicpy","title":"8.2 Public routes (<code>routes_public.py</code>)","text":"<ul> <li> <p><code>GET /api/health</code>:</p> </li> <li> <p>Returns JSON with:</p> <pre><code>{\n  \"status\": \"ok\",\n  \"checks\": {\n    \"db\": \"ok\",\n    \"jobs\": {\n      \"queued\": 1,\n      \"indexed\": 5,\n      ...\n    },\n    \"snapshots\": {\n      \"total\": 12345\n    }\n  }\n}\n</code></pre> </li> <li> <p>If the DB connectivity check fails, returns HTTP 500 with     <code>{\"status\": \"error\", \"checks\": {\"db\": \"error\"}}</code>.</p> </li> <li> <p><code>GET /api/stats</code>:</p> </li> <li> <p>Returns lightweight, cacheable archive totals used by the frontend:</p> <pre><code>{\n  \"snapshotsTotal\": 12345,\n  \"pagesTotal\": 6789,\n  \"sourcesTotal\": 2,\n  \"latestCaptureDate\": \"2025-04-19\",\n  \"latestCaptureAgeDays\": 3\n}\n</code></pre> </li> <li> <p><code>GET /api/sources</code>:</p> </li> <li> <p>Aggregates <code>Snapshot</code> by <code>source_id</code>:</p> <ul> <li>Counts, first/last capture dates, latest snapshot ID.</li> </ul> </li> <li> <p><code>GET /api/search</code>:</p> </li> <li> <p>Query params:</p> <ul> <li><code>q: str | None</code> \u2013 keyword.</li> <li><code>source: str | None</code> \u2013 source code (e.g. <code>\"hc\"</code>).</li> <li><code>sort: \"relevance\" | \"newest\" | None</code> \u2013 ordering mode.</li> <li><code>view: \"snapshots\" | \"pages\" | None</code> \u2013 results grouping mode.</li> <li><code>includeNon2xx: bool</code> \u2013 include non\u20112xx HTTP status captures (defaults to <code>false</code>).</li> <li><code>from: YYYY-MM-DD | None</code> \u2013 filter captures from this UTC date, inclusive.</li> <li><code>to: YYYY-MM-DD | None</code> \u2013 filter captures up to this UTC date, inclusive.</li> <li><code>page: int</code> \u2013 1\u2011based page index (default <code>1</code>, must be <code>&gt;= 1</code>).</li> <li><code>pageSize: int</code> \u2013 results per page (default <code>20</code>, minimum <code>1</code>, maximum <code>100</code>).</li> </ul> </li> <li>Filters:<ul> <li><code>Source.code == source.lower()</code> when <code>source</code> set.</li> <li>By default (<code>includeNon2xx=false</code>), filters out snapshots with a known non\u20112xx   <code>status_code</code> (keeps <code>status_code IS NULL</code> and <code>200\u2013299</code>).</li> <li>Keyword filter / query intent:</li> <li>URL lookup: when <code>q</code> looks like a URL (or starts with <code>url:</code>), treat it as     a page lookup and filter by the normalized URL group (with a small set of     common scheme/<code>www.</code> variants).</li> <li>Boolean/field syntax: when <code>q</code> contains <code>AND</code>/<code>OR</code>/<code>NOT</code>, parentheses, <code>-term</code>,     or <code>title:</code>/<code>snippet:</code>/<code>url:</code> prefixes, parse it and apply a boolean filter     using case-insensitive substring matching.</li> <li>Plain text:<ul> <li>On Postgres with <code>sort=\"relevance\"</code>: full\u2011text search (FTS) against   <code>snapshots.search_vector</code>.</li> <li>If FTS yields no results, fall back to tokenized substring matching.</li> <li>If that still yields no results and <code>pg_trgm</code> is available, fall back to     pg_trgm word-level trigram similarity for fuzzy matching (misspellings).</li> <li>Otherwise: tokenized substring matching on <code>title</code>, <code>snippet</code>, and <code>url</code>.</li> </ul> </li> </ul> </li> <li>Ordering:<ul> <li>Default sort:</li> <li>When <code>q</code> is present: <code>sort=\"relevance\"</code>.</li> <li>When <code>q</code> is absent: <code>sort=\"newest\"</code>.<ul> <li><code>sort=\"relevance\"</code> (when <code>q</code> present):</li> <li>On Postgres: uses FTS (<code>websearch_to_tsquery</code> + <code>ts_rank_cd</code>) against     <code>snapshots.search_vector</code>, with small heuristics (phrase-in-title boost,     URL depth/querystring penalties) and an optional authority boost from     <code>page_signals.inlink_count</code> (when available).</li> <li>On SQLite/other DBs: uses a DB\u2011agnostic match score (title &gt; URL &gt; snippet),     then (when available) a small authority tie-break from <code>page_signals</code>,     then recency.</li> </ul> </li> <li><code>sort=\"newest\"</code>: orders by recency.</li> <li>When <code>includeNon2xx=true</code>, 2xx snapshots are still prioritised ahead of 3xx,   unknown, and 4xx/5xx captures.</li> </ul> </li> <li>Grouping:<ul> <li>Default view: <code>view=\"snapshots\"</code> (returns individual captures; <code>total</code> counts snapshots).</li> <li><code>view=\"pages\"</code> returns only the latest snapshot for each page group   (<code>normalized_url_group</code>, falling back to <code>url</code> with query/fragment stripped), and   <code>total</code> counts page groups.</li> <li>When <code>view=\"pages\"</code> is used for browse (no <code>q</code> and no date range), the API can optionally   use the <code>pages</code> table as a fast path (controlled by <code>HA_PAGES_FASTPATH</code>). This is a   metadata-only optimization and does not affect replay fidelity.</li> <li>When available, <code>pageSnapshotsCount</code> is included on <code>view=\"pages\"</code> results to show the   number of captures for that page group.</li> </ul> </li> <li> <p>Pagination semantics:</p> <ul> <li><code>total</code> is the total number of matching items across all pages (snapshots   for <code>view=\"snapshots\"</code>, page groups for <code>view=\"pages\"</code>).</li> <li><code>results</code> contains at most <code>pageSize</code> snapshots for the requested <code>page</code>   (in <code>view=\"pages\"</code>, these are the latest snapshots for each page group).</li> <li>Requesting a page past the end of the result set returns <code>200 OK</code> with <code>results: []</code> and <code>total</code> unchanged.</li> <li>Supplying an invalid <code>page</code> (<code>&lt; 1</code>) or <code>pageSize</code> (<code>&lt; 1</code> or <code>&gt; 100</code>) yields <code>422 Unprocessable Entity</code> from FastAPI\u2019s validation.</li> </ul> </li> <li> <p><code>GET /api/snapshot/{id}</code>:</p> </li> <li> <p>Loads <code>Snapshot</code> + <code>Source</code>.</p> </li> <li>Returns <code>SnapshotDetailSchema</code>.</li> <li> <p>404 if snapshot or source missing.</p> </li> <li> <p><code>GET /api/snapshots/raw/{id}</code>:</p> </li> <li> <p>Validates <code>Snapshot</code> exists and <code>warc_path</code> points to an existing file.</p> </li> <li>Uses <code>find_record_for_snapshot(snapshot)</code> to get a WARC record.</li> <li>Returns an HTML page via <code>HTMLResponse</code> that includes the reconstructed archived HTML     plus a lightweight HealthArchive top bar (navigation links + disclaimer) so it can be     viewed standalone.</li> </ul>"},{"location":"architecture/#83-admin-auth-depspy","title":"8.3 Admin auth (<code>deps.py</code>)","text":"<p><code>require_admin</code> is a FastAPI dependency used to protect admin and metrics endpoints.</p> <p>Behavior:</p> <ul> <li>Reads <code>HEALTHARCHIVE_ENV</code> and <code>HEALTHARCHIVE_ADMIN_TOKEN</code> from the   environment.</li> <li>If <code>HEALTHARCHIVE_ENV</code> is <code>\"production\"</code> or <code>\"staging\"</code> and   <code>HEALTHARCHIVE_ADMIN_TOKEN</code> is unset:</li> <li>Admin and metrics endpoints fail closed with HTTP 500 and a clear     error detail (<code>\"Admin token not configured for this environment\"</code>).</li> <li>In other environments (or when <code>HEALTHARCHIVE_ENV</code> is unset) and the admin   token is unset:</li> <li>Admin endpoints are open (dev mode convenience).</li> <li>When <code>HEALTHARCHIVE_ADMIN_TOKEN</code> is set:</li> <li>Requires the same token via either:<ul> <li><code>Authorization: Bearer &lt;token&gt;</code> header, or</li> <li><code>X-Admin-Token: &lt;token&gt;</code> header.</li> </ul> </li> <li>On mismatch/missing token \u2192 <code>HTTP 403</code>.</li> </ul>"},{"location":"architecture/#84-admin-schemas-schemas_adminpy","title":"8.4 Admin schemas (<code>schemas_admin.py</code>)","text":"<p>Key models:</p> <ul> <li> <p><code>JobSummarySchema</code> \u2013 used for lists:</p> </li> <li> <p>Contains the key job fields plus:</p> <pre><code>cleanupStatus: str\ncleanedAt: Optional[datetime]\n</code></pre> </li> <li> <p><code>JobDetailSchema</code> \u2013 extended view for a single job:</p> </li> <li> <p>Includes status, worker counters, pages, WARC counts, ZIM/log/state paths,     <code>config</code> (JSON), and <code>lastStats</code> (JSON, reserved).</p> </li> <li> <p>Also includes <code>cleanupStatus</code> and <code>cleanedAt</code>.</p> </li> <li> <p><code>JobSnapshotSummarySchema</code> \u2013 minimal <code>Snapshot</code> view in a job context.</p> </li> <li> <p><code>JobListResponseSchema</code> \u2013 wrapper for job list results.</p> </li> <li> <p><code>JobStatusCountsSchema</code> \u2013 dictionary of <code>{status: count}</code>.</p> </li> </ul>"},{"location":"architecture/#85-admin-routes-routes_adminpy","title":"8.5 Admin routes (<code>routes_admin.py</code>)","text":"<p>All routes are under <code>/api/admin</code> and use <code>require_admin</code> for auth. They are intended for internal operator tooling (CLI or a future admin console), not for the public web UI.</p> <ul> <li><code>GET /api/admin/jobs</code> \u2192 <code>JobListResponseSchema</code>:</li> <li>Filters:<ul> <li><code>source: str | None</code> \u2013 by source code.</li> <li><code>status: str | None</code> \u2013 by job status.</li> <li><code>limit</code> (1\u2013500, default 50), <code>offset</code> (\u22650).</li> </ul> </li> <li> <p>Joins <code>ArchiveJob</code> with <code>Source</code> (outer join).</p> </li> <li> <p><code>GET /api/admin/jobs/{job_id}</code> \u2192 <code>JobDetailSchema</code>:</p> </li> <li>Joins <code>ArchiveJob</code> with <code>Source</code>.</li> <li> <p>404 if job not found.</p> </li> <li> <p><code>GET /api/admin/jobs/status-counts</code> \u2192 <code>JobStatusCountsSchema</code>:</p> </li> <li> <p>SQL: <code>SELECT status, COUNT(*) FROM archive_jobs GROUP BY status</code>.</p> </li> <li> <p><code>GET /api/admin/jobs/{job_id}/snapshots</code> \u2192 <code>List[JobSnapshotSummarySchema]</code>:</p> </li> <li>Lists snapshots for a given job with pagination (<code>limit</code>, <code>offset</code>).</li> </ul>"},{"location":"architecture/#86-metrics-prometheusstyle","title":"8.6 Metrics (Prometheus\u2011style)","text":"<p>Defined directly in <code>ha_backend.api.__init__</code>:</p> <ul> <li><code>GET /metrics</code>:</li> <li>Protected by <code>require_admin</code> (same token behavior) and intended for     scrape\u2011only use by monitoring systems (e.g., Prometheus) and internal     tooling.</li> <li>Computes:<ul> <li><code>healtharchive_jobs_total{status=\"...\"}</code></li> <li><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"...\"}</code></li> <li><code>healtharchive_snapshots_total</code></li> <li><code>healtharchive_snapshots_total{source=\"hc\"}</code>, etc.</li> </ul> </li> </ul>"},{"location":"architecture/#87-cors","title":"8.7 CORS","text":"<ul> <li>CORS is enabled on the public API routes. Allowed origins are derived from   <code>HEALTHARCHIVE_CORS_ORIGINS</code> (comma-separated). Defaults cover local dev and   production (<code>http://localhost:3000</code>, <code>http://localhost:5173</code>,   <code>https://healtharchive.ca</code>, <code>https://www.healtharchive.ca</code>).</li> <li>Admin and metrics routes remain token-gated even when CORS allows browser   access to public routes.</li> </ul> <p>Typical environment setups:</p> <ul> <li>Local development:</li> </ul> <pre><code># often no override needed; defaults already include localhost:3000/5173\nexport HEALTHARCHIVE_DATABASE_URL=sqlite:///$(pwd)/.dev-healtharchive.db\nexport HEALTHARCHIVE_ARCHIVE_ROOT=$(pwd)/.dev-archive-root\n# Optional CORS override if your frontend runs on a different origin:\n# export HEALTHARCHIVE_CORS_ORIGINS=http://localhost:3000\n</code></pre> <ul> <li>Staging (example):</li> </ul> <pre><code># frontend served from https://healtharchive.vercel.app\nexport HEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.vercel.app\n</code></pre> <ul> <li>Production (example):</li> </ul> <pre><code># frontend served from https://healtharchive.ca and https://www.healtharchive.ca\nexport HEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.ca,https://www.healtharchive.ca\n</code></pre> <p>In all cases, CORS affects only the browser\u2019s ability to call public routes; admin and metrics endpoints still require the admin token when configured.</p>"},{"location":"architecture/#9-worker-loop-ha_backendworkermainpy","title":"9. Worker loop (<code>ha_backend/worker/main.py</code>)","text":"<p>The worker processes jobs end\u2011to\u2011end: crawl and index.</p>"},{"location":"architecture/#91-selection","title":"9.1 Selection","text":"<p><code>_select_next_crawl_job(session)</code>:</p> <ul> <li>Query:</li> </ul> <pre><code>session.query(ArchiveJob) \\\n  .join(Source) \\\n  .filter(ArchiveJob.status.in_([\"queued\", \"retryable\"])) \\\n  .order_by(ArchiveJob.queued_at.asc().nullsfirst(),\n            ArchiveJob.created_at.asc()) \\\n  .first()\n</code></pre> <ul> <li>Chooses the oldest queued/retryable job, preferring jobs with the earliest   <code>queued_at</code>.</li> </ul>"},{"location":"architecture/#92-processing-a-single-job","title":"9.2 Processing a single job","text":"<p><code>_process_single_job()</code>:</p> <ol> <li>Select a job \u2192 get <code>job_id</code>.</li> <li>Run <code>run_persistent_job(job_id)</code>:</li> <li>Executes <code>archive_tool</code> and returns a process exit code.</li> <li>Reload job in a new session and apply retry semantics:</li> <li>If <code>crawl_rc != 0</code> or <code>job.status == \"failed\"</code>:<ul> <li>If <code>job.retry_count &lt; MAX_CRAWL_RETRIES</code>:</li> <li>Increment <code>job.retry_count</code>.</li> <li>Set <code>job.status = \"retryable\"</code>.</li> <li>Else:</li> <li>Log error; job remains in <code>failed</code>.</li> </ul> </li> <li>Else (crawl succeeded):<ul> <li>Log that indexing will start.</li> </ul> </li> <li>If crawl succeeded:</li> <li>Run <code>index_job(job_id)</code>.</li> <li>Log success/failure for indexing.</li> </ol> <p>Returns <code>True</code> if a job was processed, <code>False</code> if no jobs were found.</p>"},{"location":"architecture/#93-main-loop","title":"9.3 Main loop","text":"<p><code>run_worker_loop(poll_interval=30, run_once=False)</code>:</p> <ul> <li>Logs startup with the given interval and <code>run_once</code>.</li> <li>In a loop:</li> <li>Calls <code>_process_single_job()</code>.</li> <li>If <code>run_once</code> \u2192 break after first iteration.</li> <li>If no job processed:<ul> <li>Logs and sleeps for <code>poll_interval</code> seconds.</li> </ul> </li> <li>Handles <code>KeyboardInterrupt</code> gracefully.</li> </ul>"},{"location":"architecture/#10-cleanup-retention-future","title":"10. Cleanup &amp; retention (future)","text":"<p>Job\u2011level cleanup is focused on removing temporary crawl artifacts (<code>.tmp*</code> dirs and <code>.archive_state.json</code>) after indexing is complete.</p>"},{"location":"architecture/#101-cleanup-flags-on-archivejob","title":"10.1 Cleanup flags on ArchiveJob","text":"<p>New fields:</p> <ul> <li><code>cleanup_status: str</code>:</li> <li><code>\"none\"</code> \u2013 no cleanup performed (default).</li> <li><code>\"temp_cleaned\"</code> \u2013 temporary dirs and state file have been deleted.</li> <li>Future values could represent more aggressive cleanup modes.</li> <li><code>cleaned_at: datetime | None</code> \u2013 when cleanup occurred.</li> </ul> <p>These fields are exposed through:</p> <ul> <li>Admin schemas (<code>JobSummarySchema</code>, <code>JobDetailSchema</code>).</li> <li>Metrics (<code>healtharchive_jobs_cleanup_status_total</code>).</li> </ul>"},{"location":"architecture/#102-cli-command-cleanup-job","title":"10.2 CLI command: cleanup-job","text":"<p><code>ha-backend cleanup-job --id JOB_ID [--mode temp] [--force]</code></p> <p>Implementation notes:</p> <ul> <li>Currently supports only <code>--mode temp</code>:</li> <li> <p>Any other mode \u2192 error.</p> </li> <li> <p>Behavior:</p> </li> <li> <p>Load the <code>ArchiveJob</code> by ID.</p> </li> <li>If job is missing \u2192 error, exit 1.</li> <li>If replay is enabled globally (<code>HEALTHARCHIVE_REPLAY_BASE_URL</code> is set) and      <code>--force</code> is not provided:<ul> <li>Refuse cleanup and exit 1.</li> <li>Rationale: <code>--mode temp</code> can delete WARCs required for replay.</li> </ul> </li> <li>If <code>job.status</code> is not one of:<ul> <li><code>\"indexed\"</code> \u2013 indexing completed successfully, or</li> <li><code>\"index_failed\"</code> \u2013 indexing failed and you have decided not to retry,  then refuse cleanup and exit 1.</li> <li>This ensures we don\u2019t delete temp dirs while a job might still be    resumed or indexing is in progress.</li> </ul> </li> <li>Validate <code>output_dir</code> exists and is a directory.</li> <li>Use <code>archive_tool.state.CrawlState(output_dir, initial_workers=1)</code> to      instantiate state and locate the state file.</li> <li>Use <code>state.get_temp_dir_paths()</code> to get known temp dirs; fall back to      <code>find_latest_temp_dir_fallback</code> if none are tracked.</li> <li>If neither temp dirs nor the state file exist:<ul> <li>Print a message that there is nothing to clean up and do not change    <code>cleanup_status</code> or <code>cleaned_at</code>.</li> </ul> </li> <li>Otherwise (if temp dirs and/or state file exist):<ul> <li>Call <code>cleanup_temp_dirs(temp_dirs, state.state_file_path)</code>:</li> <li>Deletes <code>.tmp*</code> directories and the <code>.archive_state.json</code>.</li> <li>Update job:<ul> <li><code>cleanup_status = \"temp_cleaned\"</code></li> <li><code>cleaned_at = now</code></li> <li><code>state_file_path = None</code></li> </ul> </li> </ul> </li> </ul> <p>Operational warning:</p> <ul> <li><code>cleanup-job --mode temp</code> will delete WARCs if they live under the job\u2019s   <code>.tmp*</code> directory (common for legacy imports and some crawl layouts).   If you intend to serve the job via replay (pywb), do not run cleanup for that   job \u2014 replay depends on WARCs remaining on disk.   If replay is enabled globally, you must pass <code>--force</code> to run cleanup; treat   this as an emergency override.</li> </ul> <p>Caution: This cleanup removes WARCs stored under <code>.tmp*</code> directories, consistent with <code>archive_tool</code>\u2019s own <code>--cleanup</code> behavior. In v1 you should only run it once you have: - Indexed the job successfully (<code>status=\"indexed\"</code>), and - Verified any desired ZIM or exports derived from these WARCs.</p>"},{"location":"architecture/#103-metrics-for-cleanup","title":"10.3 Metrics for cleanup","text":"<p><code>/metrics</code> includes:</p> <ul> <li><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"none\"}</code></li> <li><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"temp_cleaned\"}</code></li> </ul> <p>This gives a quick overview of how many jobs still have temp artifacts versus those that have been cleaned.</p>"},{"location":"architecture/#11-cli-commands-summary","title":"11. CLI commands summary","text":"<p>All commands are available via the <code>ha-backend</code> entrypoint.</p> <ul> <li>Environment / connectivity:</li> <li><code>check-env</code> \u2013 show archive root and ensure it exists.</li> <li><code>check-archive-tool</code> \u2013 run <code>archive-tool --help</code>.</li> <li> <p><code>check-db</code> \u2013 simple DB connectivity check.</p> </li> <li> <p>Direct, non\u2011persistent job:</p> </li> <li> <p><code>run-job</code> \u2013 run <code>archive_tool</code> immediately with explicit <code>--name</code>, <code>--seeds</code>,     <code>--initial-workers</code>, etc.</p> </li> <li> <p>Persistent jobs (DB\u2011backed):</p> </li> <li><code>create-job --source CODE</code> \u2013 create <code>ArchiveJob</code> using registry defaults.</li> <li><code>run-db-job --id ID</code> \u2013 run <code>archive_tool</code> for an existing job.</li> <li><code>index-job --id ID</code> \u2013 index an existing job\u2019s WARCs into snapshots.</li> <li><code>register-job-dir --source CODE --output-dir PATH [--name NAME]</code> \u2013     attach a DB <code>ArchiveJob</code> to an existing archive_tool output directory     (useful when a crawl has already been run and you want to index its     WARCs).</li> <li> <p>Job configs default to <code>relax_perms=True</code> for dev (adds <code>--relax-perms</code> so     temp WARCs are chmod\u2019d readable on the host after a crawl).</p> </li> <li> <p>Seeding:</p> </li> <li> <p><code>seed-sources</code> \u2013 insert baseline <code>Source</code> rows for <code>hc</code>, <code>phac</code>.</p> </li> <li> <p>Admin / introspection:</p> </li> <li><code>list-jobs</code> \u2013 list recent jobs with basic fields.</li> <li><code>show-job --id ID</code> \u2013 detailed job info including config.</li> <li><code>retry-job --id ID</code> \u2013 mark:<ul> <li><code>failed</code> jobs as <code>retryable</code> (for another crawl).</li> <li><code>index_failed</code> jobs as <code>completed</code> (for re-indexing).</li> </ul> </li> <li><code>cleanup-job --id ID [--mode temp] [--force]</code> \u2013 cleanup temp dirs/state for jobs in     status <code>indexed</code> or <code>index_failed</code>.</li> <li><code>replay-index-job --id ID</code> \u2013 create/refresh the pywb collection + CDX index     for a job (so snapshots can be browsed via replay).</li> <li><code>start-worker [--poll-interval N] [--once]</code> \u2013 start the worker loop.</li> </ul>"},{"location":"architecture/#12-testing-development","title":"12. Testing &amp; development","text":"<ul> <li>Tests are written with <code>pytest</code> and live under <code>tests/</code>.</li> <li>To run checks:</li> </ul> <pre><code>make venv\nmake check\n</code></pre> <ul> <li>Many tests configure a temporary SQLite DB by:</li> <li>Setting <code>HEALTHARCHIVE_DATABASE_URL</code> to a temp file.</li> <li>Resetting <code>db_module._engine</code> and <code>_SessionLocal</code>.</li> <li>Calling <code>Base.metadata.drop_all()</code> / <code>create_all()</code> to fully reset the schema.</li> </ul> <p>This allows development and CI to run in isolated environments without touching real data.</p>"},{"location":"architecture/#13-relationship-to-archive_tool-and-the-frontend","title":"13. Relationship to archive_tool and the frontend","text":"<ul> <li>archive_tool:</li> <li>Lives under <code>src/archive_tool/</code> and is maintained as part of this repo.     It originated as an earlier standalone crawler project but is now the     in-tree crawler/orchestrator subpackage for the backend.</li> <li>The backend calls it strictly via the CLI (<code>archive-tool</code>) as a subprocess.</li> <li> <p>Its internal behavior (Docker orchestration, run modes, monitoring,     adaptive strategies) is documented in <code>src/archive_tool/docs/documentation.md</code>.</p> </li> <li> <p>Frontend (healtharchive-frontend):</p> </li> <li>Next.js 16 app using the backend\u2019s HTTP APIs:<ul> <li><code>/api/health</code></li> <li><code>/api/sources</code></li> <li><code>/api/search</code></li> <li><code>/api/snapshot/{id}</code></li> <li><code>/api/snapshots/raw/{id}</code></li> </ul> </li> <li>The frontend currently still supports a demo dataset, but is gradually     being wired to these real APIs.</li> </ul> <p>Together, the backend + <code>archive_tool</code> + frontend form a pipeline from:</p> <p>Web \u2192 crawl (Docker + <code>zimit</code>) \u2192 WARCs \u2192 Snapshots in DB \u2192 searchable archive UI at HealthArchive.ca.</p>"},{"location":"documentation-guidelines/","title":"Documentation Guidelines (internal)","text":"<p>Keep documentation accurate, minimal, and easy to maintain across repos.</p>"},{"location":"documentation-guidelines/#canonical-sources","title":"Canonical sources","text":"<ul> <li>Docs portal (published): https://docs.healtharchive.ca</li> <li>Docs portal (local): Run <code>make docs-serve</code> in the backend repo root.</li> <li>Navigation config: <code>mkdocs.yml</code> (source of truth for sidebar structure).</li> <li>Cross-repo environment wiring: <code>docs/deployment/environments-and-configuration.md</code></li> <li>Ops roadmap/todo: <code>docs/operations/healtharchive-ops-roadmap.md</code></li> <li>Future roadmap backlog (not-yet-implemented work): <code>docs/roadmaps/roadmap.md</code></li> <li>Implemented plans archive (historical records): <code>docs/roadmaps/implemented/</code></li> <li>Frontend documentation (canonical): https://github.com/jerdaw/healtharchive-frontend/tree/main/docs</li> <li>Datasets documentation (canonical): https://github.com/jerdaw/healtharchive-datasets</li> </ul>"},{"location":"documentation-guidelines/#multi-repo-boundary-avoid-bleed","title":"Multi-repo boundary (avoid bleed)","text":"<p>This documentation site is built from the backend repo only.</p> <ul> <li>Frontend docs are canonical in the frontend repo (<code>docs/**</code>) and should be linked-to, not copied into this site.</li> <li>Datasets docs are canonical in the datasets repo and should be linked-to, not copied into this site.</li> <li>Frontend PRs should not break backend docs builds (and vice versa).</li> </ul>"},{"location":"documentation-guidelines/#cross-repo-linking-avoid-drift","title":"Cross-repo linking (avoid drift)","text":"<p>When referencing another repo from docs in this repo:</p> <ul> <li> <p>For documentation references: Use GitHub URLs   <pre><code># Good\nSee the [frontend i18n guide](https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/i18n.md)\n\n# Avoid\nSee `healtharchive-frontend/docs/i18n.md`\n</code></pre></p> </li> <li> <p>For command examples: Workspace-relative paths are fine   <pre><code># This is appropriate in a development guide\ncd ../healtharchive-frontend &amp;&amp; npm ci\n</code></pre></p> </li> <li> <p>For project names in prose: Use simple names   <pre><code>The healtharchive-frontend repository handles the public UI.\n</code></pre></p> </li> <li> <p>Treat cross-repo references as pointers. Do not copy text across repos unless it is an intentional public-safe excerpt.</p> </li> <li>Links to backend docs can use relative paths within this repo or <code>docs.healtharchive.ca</code> URLs.</li> </ul>"},{"location":"documentation-guidelines/#external-pointer-pages","title":"External pointer pages","text":"<p>If you want another repo\u2019s docs to be discoverable from the docs portal, add a small pointer page under <code>docs/*-external/</code> and add it to <code>mkdocs.yml</code> <code>nav</code>. Do not mirror the other repo\u2019s docs into this site.</p>"},{"location":"documentation-guidelines/#navigation-policy","title":"Navigation policy","text":""},{"location":"documentation-guidelines/#what-goes-in-mkdocsyml-nav","title":"What goes in mkdocs.yml nav","text":"<ul> <li>All README index pages</li> <li>Docs that are frequently accessed or critical for operations</li> <li>At least one representative doc from each major category</li> <li>Core playbooks (operator responsibilities, deploy &amp; verify, incident response)</li> </ul>"},{"location":"documentation-guidelines/#what-stays-readme-only","title":"What stays README-only","text":"<ul> <li>Detailed playbooks beyond the core set (discoverable via playbooks/README.md)</li> <li>Historical/archived roadmaps (implemented/)</li> <li>Log files and templates</li> <li>Highly specialized procedures</li> </ul>"},{"location":"documentation-guidelines/#organizing-new-docs","title":"Organizing new docs","text":"<p>When adding new docs:</p> <ol> <li>Add to the appropriate directory</li> <li>Update the directory's <code>README.md</code> index</li> <li>If critical or frequently accessed, add to <code>mkdocs.yml</code> nav</li> <li>Ensure cross-links from related docs</li> </ol>"},{"location":"documentation-guidelines/#using-templates","title":"Using templates","text":"<p>Templates are stored in <code>docs/_templates/</code>. To use:</p> <ol> <li>Copy the template to the appropriate directory</li> <li>Rename with appropriate filename (remove <code>-template</code> suffix)</li> <li>Fill in all sections</li> <li>Add to directory README index</li> <li>Add to <code>mkdocs.yml</code> nav if appropriate</li> </ol> <p>Available templates:</p> <ul> <li><code>runbook-template.md</code> \u2014 For deployment procedures</li> <li><code>playbook-template.md</code> \u2014 For operational tasks</li> <li><code>incident-template.md</code> \u2014 For incident postmortems</li> <li><code>decision-template.md</code> \u2014 For architectural decisions</li> <li><code>restore-test-log-template.md</code> \u2014 For quarterly restore test logs</li> <li><code>adoption-signals-log-template.md</code> \u2014 For quarterly adoption signals</li> <li><code>mentions-log-template.md</code> \u2014 For mentions log entries</li> <li><code>ops-ui-friction-log-template.md</code> \u2014 For internal friction logging</li> </ul>"},{"location":"documentation-guidelines/#when-adding-or-changing-docs","title":"When adding or changing docs","text":"<ul> <li>Prefer one canonical source. Use pointers elsewhere instead of copying text.</li> <li>Keep docs close to the code they describe.</li> <li>Registry: New critical docs should be added to the <code>nav</code> section of <code>mkdocs.yml</code>. All docs should be added to their directory's <code>README.md</code> index.</li> <li>Use MkDocs Material features like Admonitions (<code>!!! note</code>), Tabs, and Mermaid diagrams.</li> <li>Documentation should be English-only; do not duplicate it in other languages.</li> <li>Avoid \"phase\" labels or other implementation-ordering labels outside <code>docs/roadmaps</code> and explicit implementation plans. The order that something was implemented in is not something that needs documentation; rather documentation should focus on key elements of what was implemented, how it was implemented, and how it is to be used.</li> <li>Keep public copy public-safe (no secrets, private emails, or internal IPs).</li> <li>If you sync your workspace via Syncthing, treat <code>.stignore</code> as \"sync ignore\" (like <code>.gitignore</code>) and ensure it excludes build artifacts and machine-local dev artifacts (e.g., <code>.venv/</code>, <code>node_modules/</code>, <code>.dev-archive-root/</code>). Secrets may sync via Syncthing, but must remain git-ignored.</li> </ul>"},{"location":"documentation-guidelines/#documentation-framework-diataxis","title":"Documentation framework (Di\u00e1taxis)","text":"<p>HealthArchive documentation follows the Di\u00e1taxis framework for clarity and user-centered organization. Di\u00e1taxis divides documentation into four types based on user needs:</p>"},{"location":"documentation-guidelines/#four-documentation-types","title":"Four Documentation Types","text":"Type Purpose User Action Examples Tutorials Learning-oriented Following steps to gain skills First contribution guide, architecture walkthrough How-To Guides Task-oriented Solving specific problems Playbooks, runbooks, checklists Reference Information-oriented Looking up details API docs, CLI reference, data model Explanation Understanding-oriented Understanding concepts Architecture, decisions, guidelines <p>Key principle: Keep these types separate. Don't mix tutorials with reference material, or how-to guides with explanations.</p> <p>Learn more: diataxis.fr</p>"},{"location":"documentation-guidelines/#mapping-to-our-taxonomy","title":"Mapping to Our Taxonomy","text":"<p>Our existing document types map to Di\u00e1taxis categories:</p> <p>Tutorials (Learning): - Lives under <code>docs/tutorials/</code> - Examples: <code>first-contribution.md</code>, <code>architecture-walkthrough.md</code>, <code>debug-crawl.md</code> - Characteristics: Step-by-step, hands-on, designed for learning</p> <p>How-To Guides (Tasks): - Runbooks: Deployment procedures in <code>docs/deployment/</code> (template: <code>runbook-template.md</code>) - Playbooks: Operational tasks in <code>docs/operations/playbooks/</code> or <code>docs/development/playbooks/</code> (template: <code>playbook-template.md</code>) - Checklists: Minimal verification lists - Characteristics: Goal-oriented, assume some knowledge, focused on results</p> <p>Reference (Information): - Lives under <code>docs/reference/</code> or specialized files (<code>api.md</code>, etc.) - Examples: <code>data-model.md</code>, <code>cli-commands.md</code>, <code>archive-tool.md</code> - Also: API documentation (<code>api.md</code>), Architecture sections - Characteristics: Factual, precise, structured for lookup</p> <p>Explanation (Understanding): - Decision records: In <code>docs/decisions/</code> (template: <code>decision-template.md</code>) - Policies/contracts: Invariants and boundaries - Guidelines: This file, <code>documentation-process-audit.md</code> - Architecture: <code>architecture.md</code> (blends reference and explanation) - Characteristics: Background, context, \"why\" not \"how\"</p>"},{"location":"documentation-guidelines/#additional-document-types","title":"Additional Document Types","text":"<p>These support but don't replace the four main types:</p> <ul> <li>Index (<code>README.md</code>): Navigation only; points to canonical docs</li> <li>Log/record: Dated, append-only operational evidence (restore tests, adoption signals)</li> <li>Template: Scaffolds in <code>docs/_templates/</code></li> <li>Pointer: Short files linking to canonical docs (e.g., <code>frontend-external/</code>)</li> </ul>"},{"location":"documentation-guidelines/#document-types-detailed-taxonomy","title":"Document types (detailed taxonomy)","text":"<p>Use consistent doc types so people know what to expect:</p>"},{"location":"documentation-guidelines/#quality-bar-definition-of-done","title":"Quality bar (definition of done)","text":"<p>For anything procedural (runbook/playbook/checklist), include:</p> <ul> <li>Purpose: why this doc exists and what it covers.</li> <li>Audience + access: who should run it, and from where (local vs VPS; <code>haadmin</code> vs <code>root</code>).</li> <li>Preconditions: required state and inputs (paths, env vars, service names).</li> <li>Steps: explicit commands (prefer stable scripts), ordered, with \u201cwhat this changes\u201d.</li> <li>Verification: what \u201cdone\u201d means (health checks, drift check, smoke tests).</li> <li>Safety: common footguns, irreversible actions, and rollback/recovery notes.</li> <li>References: links to canonical docs, incident notes, or roadmaps.</li> </ul> <p>For anything public-facing (policy pages, changelog, partner kit):</p> <ul> <li>Keep it public-safe (no secrets/emails/internal hostnames; avoid sensitive incident details).</li> <li>Prefer stable claims tied to stable artifacts (URLs, tags, filenames, commit SHAs).</li> <li>Record meaningful changes in the public changelog:</li> <li>Process: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md</li> </ul>"},{"location":"documentation-guidelines/#lifecycle-avoid-drift","title":"Lifecycle (avoid drift)","text":"<p>Docs should reflect current reality. If something is intentionally outdated:</p> <ul> <li>Put a short note at the top: what changed, and where the new canonical doc lives.</li> <li>Prefer updating the doc over adding a second \u201cnew doc\u201d (avoid forks).</li> <li>For long historical artifacts, move them under <code>docs/roadmaps/implemented/</code> (dated).</li> </ul> <p>Suggested cadence (keep it lightweight):</p> <ul> <li>After any production change: update the relevant runbook/playbook and keep deploy/verify steps accurate.</li> <li>After sev0/sev1 incidents: ensure recovery steps are captured and follow-ups exist (roadmap or TODOs).</li> <li>Quarterly: skim the production runbook + incident response playbook and fix any drift discovered during routine ops.</li> </ul>"},{"location":"documentation-guidelines/#roadmap-workflow","title":"Roadmap workflow","text":"<p>This project separates backlog vs implementation plans vs canonical docs to reduce drift.</p> <ul> <li>Short pointer (for new contributors): <code>roadmap-process.md</code></li> <li><code>docs/roadmaps/roadmap.md</code> is the single backlog of not-yet-implemented items.</li> <li>When you start work, create a focused implementation plan under <code>docs/roadmaps/</code>.</li> <li>When the work is done, update canonical docs (deployment/ops/dev) so the result is maintainable.</li> <li>Then move the implementation plan into <code>docs/roadmaps/implemented/</code> with a dated filename.</li> </ul> <p>Rule of thumb: documentation should describe what exists and how to use/operate it, not the order it was implemented.</p>"},{"location":"documentation-guidelines/#naming-and-organization","title":"Naming and organization","text":"<ul> <li>Use descriptive filenames (<code>runbook</code>, <code>checklist</code>, <code>guidelines</code>) and avoid phase prefixes.</li> <li>File titles and filenames should reflect the document\u2019s actual purpose and content. If the purpose or content changes, rename the file and update links as needed.</li> <li>Put roadmaps and active implementation plans in <code>docs/roadmaps</code>.</li> <li>Move completed implementation plans into <code>docs/roadmaps/implemented/</code> (dated).</li> <li>Put operational procedures in <code>docs/operations</code>.</li> <li>Put incident notes / lightweight postmortems in <code>docs/operations/incidents/</code> (template: <code>docs/operations/incidents/incident-template.md</code>).</li> <li>Put ops playbooks (task-oriented checklists) in <code>docs/operations/playbooks/</code>.</li> <li>Put deployment/runbooks in <code>docs/deployment</code>.</li> <li>Put developer workflows (local setup, testing, debugging) in <code>docs/development</code>.</li> <li>Put dev playbooks (task workflows) in <code>docs/development/playbooks/</code>.</li> </ul>"},{"location":"documentation-process-audit/","title":"Documentation process audit (2026-01-09)","text":"<p>Scope: HealthArchive project documentation processes and subprocesses across:</p> <ul> <li><code>healtharchive-backend</code> (ops, runbooks, incident notes, canonical internal docs)</li> <li><code>healtharchive-frontend</code> (public policy/reporting surfaces, UX copy, changelog)</li> <li><code>healtharchive-datasets</code> (dataset release documentation and integrity posture)</li> <li>The local \u201cworkspace of sibling repos\u201d convention used in <code>/home/jer/LocalSync/healtharchive/</code></li> </ul> <p>Goal: assess whether the documentation system is well-designed, maintainable, and aligned with modern best practices (docs-as-code + operational excellence), and identify concrete upgrades.</p>"},{"location":"documentation-process-audit/#executive-summary","title":"Executive summary","text":"<p>Overall: the project\u2019s documentation system is already unusually strong for its size. It is structured around a clear \u201cdocs-as-code\u201d posture, high-signal operational procedures, and drift-resistant separation of backlog vs implementation plans vs canonical docs.</p> <p>Key strengths (high confidence):</p> <ul> <li>Drift prevention by design: single canonical sources + pointer docs, and explicit backlog/plan/canonical separation (<code>docs/roadmaps/**</code> vs <code>docs/**</code>).</li> <li>Operations maturity: production runbook, playbooks, cadence checklists, monitoring/CI setup guidance, and safety posture are explicit and actionable.</li> <li>Incident management: severity rubric + incident template + operator response playbook + at least one real incident note showing good practice.</li> <li>Public vs private boundaries: explicit contracts for privacy-preserving usage metrics, issue-report retention, and non-public admin/metrics access.</li> <li>Reproducibility: dataset release integrity rules (checksums + manifest invariants) documented and operationalized.</li> </ul> <p>Primary remaining gaps (fixable, low risk):</p> <ul> <li>Templates / consistency: you had strong examples, but no standard templates for new runbooks/playbooks, and changelog updates were not documented as an SOP.</li> <li>Lifecycle + review cadence: docs avoided duplication well, but \u201chow we keep docs correct over time\u201d could be more explicit (lightweight review + deprecation pattern).</li> <li>Public communication integration: incident notes were solid, but the \u201cwhen do we update <code>/changelog</code> and/or <code>/status</code>?\u201d expectation wasn\u2019t explicit enough.</li> </ul>"},{"location":"documentation-process-audit/#inventory-of-documentation-process-surfaces","title":"Inventory of documentation \u201cprocess surfaces\u201d","text":"<p>This is the set of documents that define how documentation is produced, maintained, and used (not every domain-specific doc).</p>"},{"location":"documentation-process-audit/#governance-doc-architecture","title":"Governance / doc architecture","text":"<ul> <li>Canonical documentation policy and source-of-truth rules:</li> <li><code>healtharchive-backend/docs/documentation-guidelines.md</code></li> <li>Index structure (discoverability):</li> <li><code>healtharchive-backend/docs/README.md</code></li> <li><code>healtharchive-backend/docs/operations/README.md</code></li> <li><code>healtharchive-backend/docs/operations/playbooks/README.md</code></li> <li><code>healtharchive-backend/docs/roadmaps/README.md</code></li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/README.md</li> </ul>"},{"location":"documentation-process-audit/#planning-change-management","title":"Planning / change management","text":"<ul> <li>Backlog and implementation plan workflow:</li> <li><code>healtharchive-backend/docs/roadmap-process.md</code></li> <li><code>healtharchive-backend/docs/roadmaps/roadmap.md</code></li> <li><code>healtharchive-backend/docs/roadmaps/implemented/</code></li> </ul>"},{"location":"documentation-process-audit/#incidents-and-post-incident-learning","title":"Incidents and post-incident learning","text":"<ul> <li>Incident SOP and artifacts:</li> <li><code>healtharchive-backend/docs/operations/incidents/README.md</code></li> <li><code>healtharchive-backend/docs/operations/incidents/incident-template.md</code></li> <li><code>healtharchive-backend/docs/operations/incidents/severity.md</code></li> <li><code>healtharchive-backend/docs/operations/playbooks/incident-response.md</code></li> </ul>"},{"location":"documentation-process-audit/#operations-and-reliability-subprocesses-repeatable-routines","title":"Operations and reliability subprocesses (repeatable routines)","text":"<ul> <li>Cadence and routines:</li> <li><code>healtharchive-backend/docs/operations/ops-cadence-checklist.md</code></li> <li>Monitoring/CI and deploy gating:</li> <li><code>healtharchive-backend/docs/operations/monitoring-and-ci-checklist.md</code></li> <li><code>healtharchive-backend/docs/operations/playbooks/deploy-and-verify.md</code></li> <li><code>healtharchive-backend/docs/operations/baseline-drift.md</code></li> <li>Backup/restore validation:</li> <li><code>healtharchive-backend/docs/operations/restore-test-procedure.md</code></li> <li><code>healtharchive-backend/docs/operations/restore-test-log-template.md</code></li> <li>Data handling and privacy posture:</li> <li><code>healtharchive-backend/docs/operations/data-handling-retention.md</code></li> <li><code>healtharchive-backend/docs/operations/observability-and-private-stats.md</code></li> </ul>"},{"location":"documentation-process-audit/#public-facing-reporting-surfaces-documentation-for-users","title":"Public-facing reporting surfaces (documentation for users)","text":"<ul> <li>Changelog content lives in code, but is effectively \u201cpublic documentation\u201d:</li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/src/content/changelog.ts</li> <li>(process) https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md</li> <li>Status/impact pages are public reporting surfaces (operational transparency):</li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/src/app/%5Blocale%5D/status/page.tsx</li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/src/app/%5Blocale%5D/impact/page.tsx</li> </ul>"},{"location":"documentation-process-audit/#release-reproducibility-subprocesses","title":"Release + reproducibility subprocesses","text":"<ul> <li>Dataset releases and integrity expectations:</li> <li>https://github.com/jerdaw/healtharchive-datasets/blob/main/README.md</li> <li><code>healtharchive-backend/docs/operations/export-integrity-contract.md</code></li> <li><code>healtharchive-backend/docs/operations/dataset-release-runbook.md</code></li> </ul>"},{"location":"documentation-process-audit/#evaluation-against-modern-best-practices","title":"Evaluation against modern best practices","text":"<p>This section uses a simple \u201cGreen / Yellow / Red\u201d maturity signal.</p>"},{"location":"documentation-process-audit/#1-discoverability-information-architecture-green","title":"1) Discoverability &amp; information architecture \u2014 Green","text":"<p>Evidence:</p> <ul> <li>Dedicated indices exist for backend docs, ops docs, playbooks, and roadmaps.</li> <li>File naming is descriptive and stable (runbook, checklist, playbook).</li> <li>Cross-repo \u201ccanonical doc\u201d pointers exist (env wiring, partner kit, data dictionary).</li> </ul> <p>Residual risks:</p> <ul> <li>Some \u201cproject-level\u201d navigation depends on having sibling repos locally (fine for operators, less ideal for single-repo readers on GitHub).</li> </ul>"},{"location":"documentation-process-audit/#2-single-source-of-truth-drift-control-green","title":"2) Single source of truth / drift control \u2014 Green","text":"<p>Evidence:</p> <ul> <li>Explicit canonical sources + pointer strategy.</li> <li>Explicit separation:</li> <li>backlog (<code>roadmaps/roadmap.md</code>)</li> <li>active plans (<code>docs/roadmaps/*.md</code>)</li> <li>canonical docs (deployment/ops/dev)</li> </ul> <p>Residual risks:</p> <ul> <li>Any duplicated non-git copies (e.g., ops roadmap) are inherently drift-prone; currently mitigated by explicit \u201ckeep synced\u201d guidance.</li> </ul>"},{"location":"documentation-process-audit/#3-operational-excellence-runbooks-playbooks-verification-green","title":"3) Operational excellence (runbooks, playbooks, verification) \u2014 Green","text":"<p>Evidence:</p> <ul> <li>Production runbook is explicit about topology, security posture, and setup steps:</li> <li><code>healtharchive-backend/docs/deployment/production-single-vps.md</code></li> <li>Deploy is treated as a verified procedure with a defined gate (\u201cgreen main\u201d + VPS verification).</li> <li>Baseline drift is operationalized as policy+observed+diff.</li> <li>Restore tests and dataset verification have explicit SOPs and templates.</li> </ul> <p>Residual risks:</p> <ul> <li>As more workflows accumulate, templates become important to prevent playbooks/runbooks diverging in structure/quality.</li> </ul>"},{"location":"documentation-process-audit/#4-incident-response-learning-system-green-with-small-upgrades","title":"4) Incident response &amp; learning system \u2014 Green (with small upgrades)","text":"<p>Evidence:</p> <ul> <li>Incident notes have a clear SOP, a severity rubric, and a good template.</li> <li>The template includes: impact, detection, timeline, root cause, recovery, verification, action items.</li> <li>The repo has at least one high-quality real incident note, with follow-ups tied to a roadmap.</li> </ul> <p>Residual risks:</p> <ul> <li>Public communication expectations (status/changelog) were implicit; now explicit guidance exists, but you may still want to decide a project stance:</li> <li>\u201cWe always publish a public-safe note for sev0/sev1\u201d vs \u201conly when it changes user expectations\u201d.</li> </ul>"},{"location":"documentation-process-audit/#5-public-transparency-user-facing-documentation-yellow","title":"5) Public transparency &amp; user-facing documentation \u2014 Yellow","text":"<p>Evidence:</p> <ul> <li>The site includes <code>/governance</code>, <code>/terms</code>, <code>/privacy</code>, <code>/changelog</code>, <code>/report</code>, <code>/status</code>, <code>/impact</code>.</li> <li>Copy inventory and disclaimer matrices exist to keep safety posture coherent.</li> </ul> <p>Gaps:</p> <ul> <li>The changelog is a core public accountability surface, but without an explicit SOP it risks becoming stale or inconsistent (especially across EN/FR).</li> </ul>"},{"location":"documentation-process-audit/#6-security-privacy-documentation-posture-green","title":"6) Security + privacy documentation posture \u2014 Green","text":"<p>Evidence:</p> <ul> <li>Clear \u201cno secrets in git\u201d posture across docs.</li> <li>Admin/metrics are explicitly private-only; tailnet access model is documented.</li> <li>Data retention and PHI risk are explicitly addressed for issue reports and logs.</li> </ul> <p>Residual risks:</p> <ul> <li>If the project ever adds more operators, formalize \u201cwho has access to what\u201d and credential rotation as explicit operator subprocesses.</li> </ul>"},{"location":"documentation-process-audit/#7-reproducibility-and-research-integrity-green","title":"7) Reproducibility and research integrity \u2014 Green","text":"<p>Evidence:</p> <ul> <li>Export endpoints have defined ordering/pagination invariants.</li> <li>Dataset releases are immutable objects with checksum verification and manifest invariants.</li> <li>Corrections are expected to be documented rather than silently rewriting history.</li> </ul>"},{"location":"documentation-process-audit/#improvements-implemented-in-this-audit-2026-01-09","title":"Improvements implemented in this audit (2026-01-09)","text":"<p>These are low-risk upgrades that make doc creation and maintenance more consistent:</p> <ul> <li>Docs reference sanity checks (broken links/path refs):</li> <li>Backend: <code>healtharchive-backend/scripts/check_docs_references.py</code> (wired into <code>healtharchive-backend/Makefile</code>)</li> <li>Frontend: https://github.com/jerdaw/healtharchive-frontend/blob/main/scripts/check-doc-references.mjs (wired into <code>package.json</code>)</li> <li>Datasets: https://github.com/jerdaw/healtharchive-datasets/blob/main/scripts/check_docs_references.py (wired into <code>Makefile</code>)</li> <li>Standardized templates:</li> <li><code>healtharchive-backend/docs/deployment/runbook-template.md</code></li> <li><code>healtharchive-backend/docs/operations/playbooks/playbook-template.md</code></li> <li>Decision records mechanism:</li> <li><code>healtharchive-backend/docs/decisions/README.md</code></li> <li><code>healtharchive-backend/docs/decisions/decision-template.md</code></li> <li>Clearer doc taxonomy, quality bar, and lifecycle guidance:</li> <li><code>healtharchive-backend/docs/documentation-guidelines.md</code></li> <li>Public changelog SOP (source of truth, format, localization rules):</li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md</li> <li>Stronger \u201cincident \u2192 public-safe note\u201d expectation (optional but recommended for sev0/sev1):</li> <li><code>healtharchive-backend/docs/operations/incidents/README.md</code></li> <li><code>healtharchive-backend/docs/operations/incidents/incident-template.md</code></li> <li><code>healtharchive-backend/docs/operations/incidents/severity.md</code></li> <li><code>healtharchive-backend/docs/operations/ops-cadence-checklist.md</code></li> <li>Process nudges in PR templates:</li> <li><code>healtharchive-backend/.github/pull_request_template.md</code></li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/.github/pull_request_template.md</li> </ul>"},{"location":"documentation-process-audit/#recommendations-next-steps","title":"Recommendations (next steps)","text":""},{"location":"documentation-process-audit/#p0-high-value-low-effort","title":"P0 (high value, low effort)","text":"<ul> <li>Decide an explicit public incident disclosure posture:</li> <li>Option A: always add a public-safe <code>/changelog</code> entry for sev0/sev1 incidents.</li> <li>Option B: only add a public-safe entry when it changes user expectations (outage, integrity risk, policy change).</li> <li>Make doc maintenance part of normal ops:</li> <li>During the quarterly cadence, skim the production runbook + incident response playbook and fix drift discovered during real operations.</li> </ul>"},{"location":"documentation-process-audit/#p1-medium-value-moderate-effort","title":"P1 (medium value, moderate effort)","text":"<ul> <li>(Implemented) Docs link/path sanity checks + decision records are now in place.</li> </ul>"},{"location":"documentation-process-audit/#p2-later-if-team-grows","title":"P2 (later / if team grows)","text":"<ul> <li>If/when there are multiple regular committers:</li> <li>switch to PR-only merges (branch protection required checks),</li> <li>introduce CODEOWNERS for high-risk areas (deployment/ops/policy pages),</li> <li>require review for public-policy copy changes.</li> </ul>"},{"location":"documentation-process-audit/#top-notch-principles-to-keep","title":"\u201cTop notch\u201d principles to keep","text":"<ul> <li>Prefer stable, scripted entrypoints over fragile shell snippets.</li> <li>Keep internal docs public-safe by default (assume they may be shared).</li> <li>Separate \u201cwhat exists and how to operate it\u201d from \u201chow we got here\u201d (roadmaps/implemented plans).</li> <li>Treat verification as first-class: every operational procedure should define what \u201cdone\u201d means.</li> </ul>"},{"location":"project/","title":"HealthArchive Documentation Hub","text":"<p>HealthArchive is a multi-repo project that archives Canadian health government websites for research and accountability. This page helps you navigate the documentation across all repositories.</p>"},{"location":"project/#quick-start-by-role","title":"\ud83d\ude80 Quick Start by Role","text":"<p>Choose your entry point based on what you want to do:</p>"},{"location":"project/#im-an-operator","title":"\ud83d\udc64 I'm an Operator","text":"<p>Goal: Deploy, monitor, and maintain the production system</p> <p>Start Here: 1. Production Runbook - Complete production setup guide 2. Operator Responsibilities - Must-do checklist 3. Deploy &amp; Verify - Safe deployment process 4. Incident Response - Emergency procedures</p> <p>Key Resources: - Ops Cadence Checklist - Daily/weekly/quarterly tasks - Monitoring Checklist - Set up alerts and checks - All Playbooks - 30+ operational procedures</p>"},{"location":"project/#im-a-developer","title":"\ud83d\udcbb I'm a Developer","text":"<p>Goal: Contribute code, fix bugs, add features</p> <p>Start Here: 1. Quick Start Guide - Get running in 5 minutes 2. Your First Contribution - Step-by-step tutorial 3. Dev Environment Setup - Detailed local setup 4. Live Testing Guide - Run the full pipeline locally</p> <p>Key Resources: - Architecture Walkthrough - Visual guide to how it all works - Architecture Deep Dive - Complete technical reference - Testing Guidelines - How to write and run tests - Contributing Guide - Code standards and workflow</p>"},{"location":"project/#im-an-api-consumer-researcher","title":"\ud83d\udd27 I'm an API Consumer / Researcher","text":"<p>Goal: Search the archive and retrieve historical snapshots</p> <p>Start Here: 1. API Consumer Guide - Complete API walkthrough with examples 2. Interactive API Docs - Try the API in your browser 3. API Reference - Full OpenAPI specification</p> <p>Quick API Test: <pre><code>curl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;source=hc\"\n</code></pre></p> <p>Key Resources: - Dataset Downloads: healtharchive-datasets - Bulk metadata exports - Data Handling Policy - Retention and privacy - Live Site: healtharchive.ca - Web interface</p>"},{"location":"project/#im-a-student-new-to-the-project","title":"\ud83d\udcda I'm a Student / New to the Project","text":"<p>Goal: Learn how HealthArchive works</p> <p>Recommended Reading Order: 1. Quick Start - High-level overview 2. Architecture Walkthrough - Follow a page from crawl to search 3. Architecture Reference - Deep technical details 4. Documentation Guidelines - How docs stay organized</p> <p>Tutorials: - Your First Contribution - Hands-on coding tutorial - Debugging a Failed Crawl - Practical troubleshooting - Live Testing - Run it yourself locally</p>"},{"location":"project/#multi-repo-architecture","title":"\ud83d\udce6 Multi-Repo Architecture","text":"<p>HealthArchive is split across three repositories with clear boundaries:</p>"},{"location":"project/#backend-this-repo","title":"\ud83d\udd19 Backend (This Repo)","text":"<p>Purpose: API, crawler, database, operations, and all internal infrastructure</p> <p>Location: github.com/jerdaw/healtharchive-backend</p> <p>Documentation: docs.healtharchive.ca (you are here)</p> <p>What Lives Here: - \u2705 Crawler (archive_tool) and job orchestration - \u2705 Database models and indexing pipeline - \u2705 RESTful JSON API (FastAPI) - \u2705 Operations runbooks and playbooks - \u2705 Deployment guides and systemd units - \u2705 Architecture and developer docs - \u2705 Decision records and incident notes</p> <p>Tech Stack: Python, FastAPI, SQLAlchemy, PostgreSQL, Docker, systemd</p>"},{"location":"project/#frontend","title":"\ud83c\udf10 Frontend","text":"<p>Purpose: Public-facing website and user interface</p> <p>Location: github.com/jerdaw/healtharchive-frontend</p> <p>Live Site: healtharchive.ca</p> <p>What Lives Here: - \u2705 Next.js web application (search UI, snapshot viewer) - \u2705 Public content (status page, impact statement, changelog) - \u2705 Internationalization (i18n) - English and French - \u2705 UI/UX documentation</p> <p>Tech Stack: Next.js 16, React, TypeScript, Tailwind CSS</p> <p>Frontend Docs in This Repo (pointers only): - Frontend Overview - I18n Guide - Implementation Guide</p> <p>Canonical Frontend Docs: frontend/docs/</p>"},{"location":"project/#datasets","title":"\ud83d\udcca Datasets","text":"<p>Purpose: Versioned, citable metadata-only dataset releases</p> <p>Location: github.com/jerdaw/healtharchive-datasets</p> <p>What Lives Here: - \u2705 Snapshot metadata exports (JSON/CSV) - \u2705 Checksums and integrity manifests - \u2705 Dataset release documentation - \u2705 Dataset integrity policies</p> <p>Why Separate?: Enables versioned, citable releases independent of code changes</p> <p>Datasets Docs in This Repo: datasets-external/README.md (pointer only)</p> <p>Canonical Datasets Docs: datasets/README.md</p>"},{"location":"project/#where-things-live-source-of-truth-map","title":"\ud83d\uddfa\ufe0f Where Things Live (Source of Truth Map)","text":"Content Type Lives In Link Operations &amp; Runbooks Backend repo docs.healtharchive.ca/operations Architecture &amp; Dev Guides Backend repo docs.healtharchive.ca/architecture API Documentation Backend repo docs.healtharchive.ca/api Public Changelog Frontend repo github.com/jerdaw/healtharchive-frontend/.../changelog-process.md Status Page Frontend repo (code) github.com/jerdaw/healtharchive-frontend/.../status/page.tsx Impact Statement Frontend repo (code) github.com/jerdaw/healtharchive-frontend/.../impact/page.tsx Dataset Releases Datasets repo github.com/jerdaw/healtharchive-datasets I18n Guidelines Frontend repo github.com/jerdaw/healtharchive-frontend/.../i18n.md <p>Principle: Each doc has one canonical source. Other repos link to it.</p>"},{"location":"project/#cross-repo-linking","title":"\ud83d\udd17 Cross-Repo Linking","text":""},{"location":"project/#in-github-issuesprs","title":"In GitHub Issues/PRs","text":"<p>Use full GitHub URLs: <pre><code>See the [production runbook](https://github.com/jerdaw/healtharchive-backend/blob/main/docs/deployment/production-single-vps.md)\n</code></pre></p>"},{"location":"project/#in-documentation","title":"In Documentation","text":"<p>For docs users: Use the docs site URLs: <pre><code>See the [Production Runbook](https://docs.healtharchive.ca/deployment/production-single-vps/)\n</code></pre></p> <p>For cross-repo references: Use full GitHub URLs: <pre><code>Frontend changelog process: [changelog-process.md](https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md)\n</code></pre></p>"},{"location":"project/#local-development-multi-repo-workspace","title":"Local Development (Multi-Repo Workspace)","text":"<p>If you have all repos cloned as siblings: <pre><code>/home/user/healtharchive/\n\u251c\u2500\u2500 healtharchive-backend/\n\u251c\u2500\u2500 healtharchive-frontend/\n\u2514\u2500\u2500 healtharchive-datasets/\n</code></pre></p> <p>Some docs use relative paths like <code>../healtharchive-frontend/...</code> for convenience.</p> <p>Note: These paths only work in local workspaces, not on GitHub.</p>"},{"location":"project/#system-architecture-high-level","title":"\ud83c\udfd7\ufe0f System Architecture (High Level)","text":"<pre><code>graph TB\n    subgraph \"HealthArchive Backend\"\n        CLI[CLI Commands] --&gt;|Create| Jobs[(Database)]\n        Worker[Worker Process] --&gt;|Poll| Jobs\n        Worker --&gt;|Execute| Crawler[Archive Tool]\n        Crawler --&gt;|Docker| Zimit[Zimit Crawler]\n        Zimit --&gt;|Write| WARC[WARC Files]\n        Worker --&gt;|Index| WARC\n        WARC --&gt;|Extract| Snapshots[(Snapshots)]\n        API[FastAPI API] --&gt;|Query| Snapshots\n    end\n\n    subgraph \"HealthArchive Frontend\"\n        UI[Next.js UI] --&gt;|API Calls| API\n        API --&gt;|JSON| UI\n        UI --&gt;|Display| Users[End Users]\n    end\n\n    subgraph \"HealthArchive Datasets\"\n        Export[Export Script] --&gt;|Read| Snapshots\n        Export --&gt;|Write| Datasets[Dataset Files]\n        Researchers --&gt;|Download| Datasets\n    end</code></pre> <p>Data Flow: 1. Crawl: CLI creates job \u2192 Worker runs crawler \u2192 Docker writes WARCs 2. Index: Worker parses WARCs \u2192 Extracts text \u2192 Stores snapshots in DB 3. Serve: API queries DB \u2192 Returns JSON \u2192 Frontend displays results 4. Export: Scripts export metadata \u2192 Version as datasets \u2192 Researchers download</p> <p>See: Architecture Walkthrough for detailed data flow</p>"},{"location":"project/#documentation-structure","title":"\ud83d\udcd6 Documentation Structure","text":"<p>This documentation follows the Di\u00e1taxis framework for clarity:</p> Type Purpose Where Tutorials Learning-oriented, step-by-step tutorials/ How-To Guides Task-oriented, problem-solving operations/playbooks/, development/ Reference Information-oriented, lookup api.md, architecture.md, reference/ Explanation Understanding-oriented, concepts documentation-guidelines.md, decisions/, operations/ <p>Navigation: Use the sidebar to explore by category</p>"},{"location":"project/#getting-help","title":"\ud83c\udd98 Getting Help","text":""},{"location":"project/#by-issue-type","title":"By Issue Type","text":"Issue Where to Go API questions API Consumer Guide \u2192 API Docs Deployment problems Production Runbook \u2192 Playbooks Code questions Architecture Guide \u2192 GitHub Discussions Bugs or feature requests GitHub Issues Operational incidents Incident Response"},{"location":"project/#community","title":"Community","text":"<ul> <li>GitHub Discussions: backend | frontend</li> <li>Issues: backend | frontend | datasets</li> <li>Contributor Guide: CONTRIBUTING.md</li> </ul>"},{"location":"project/#documentation-updates","title":"\ud83d\udd04 Documentation Updates","text":"<p>Found something wrong? Documentation lives in git and accepts pull requests!</p> <ol> <li>Backend docs: Edit files in <code>docs/</code></li> <li>Frontend docs: Edit files in frontend <code>docs/</code></li> <li>Datasets docs: Edit datasets README</li> </ol> <p>Guidelines: Documentation Guidelines</p>"},{"location":"project/#project-status","title":"\ud83d\udcca Project Status","text":"Metric Value Details Snapshots Archived Check /api/stats Live count Sources 2 (Health Canada, PHAC) /api/sources Crawl Frequency Annual + ad-hoc Ops Roadmap API Status Production Health Check Frontend Status Production healtharchive.ca <p>Latest Incidents: See operations/incidents/</p>"},{"location":"project/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<p>Based on your role, here's what to do next:</p>"},{"location":"project/#operators","title":"Operators","text":"<ol> <li>\u2705 Review Production Runbook</li> <li>\u2705 Complete Monitoring Checklist</li> <li>\u2705 Bookmark Incident Response</li> </ol>"},{"location":"project/#developers","title":"Developers","text":"<ol> <li>\u2705 Complete Quick Start</li> <li>\u2705 Follow Your First Contribution</li> <li>\u2705 Read Architecture Walkthrough</li> </ol>"},{"location":"project/#researchers","title":"Researchers","text":"<ol> <li>\u2705 Read API Consumer Guide</li> <li>\u2705 Try Interactive API Docs</li> <li>\u2705 Explore Datasets</li> </ol>"},{"location":"project/#essential-documentation-index","title":"\ud83d\udcda Essential Documentation Index","text":"<p>Getting Started: - Quick Start - Project Overview (you are here)</p> <p>For Operators: - Production Runbook - All Playbooks - Ops Cadence</p> <p>For Developers: - First Contribution - Architecture Guide - Dev Setup</p> <p>For Researchers: - API Guide - API Reference - Datasets</p> <p>Reference: - Documentation Guidelines - Decision Records - Roadmaps</p>"},{"location":"project/#about-this-documentation","title":"\ud83d\udca1 About This Documentation","text":"<p>This documentation portal is built with MkDocs Material and deployed to docs.healtharchive.ca.</p> <p>Source: docs/ Build: <code>make docs-build</code> Serve Locally: <code>make docs-serve</code></p> <p>Last Updated: Auto-generated on every push to <code>main</code></p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get up and running with HealthArchive in 5 minutes.</p>"},{"location":"quickstart/#what-is-healtharchive","title":"What is HealthArchive?","text":"<p>HealthArchive is a web archiving service that preserves Canadian health government sources (Health Canada, PHAC). It crawls, indexes, and makes searchable snapshots of public health information for research and accountability.</p>"},{"location":"quickstart/#choose-your-path","title":"Choose Your Path","text":"<p>Pick the guide that matches your role:</p>"},{"location":"quickstart/#im-an-operator","title":"\ud83d\udc64 I'm an Operator","text":"<p>Goal: Deploy, monitor, and maintain the production system.</p> <ol> <li>Read the Production Runbook for deployment setup</li> <li>Review Operator Responsibilities for your must-do checklist</li> <li>Bookmark Incident Response for emergencies</li> </ol> <p>Quick Deploy: <pre><code># On the VPS\ncd /opt/healtharchive-backend\n./scripts/vps-deploy.sh --apply --baseline-mode live\n</code></pre></p>"},{"location":"quickstart/#im-a-developer","title":"\ud83d\udcbb I'm a Developer","text":"<p>Goal: Contribute code, fix bugs, add features.</p> <ol> <li> <p>Clone and setup:    <pre><code>git clone https://github.com/jerdaw/healtharchive-backend.git\ncd healtharchive-backend\nmake venv\n</code></pre></p> </li> <li> <p>Configure environment:    <pre><code>cp .env.example .env\nsource .env\n</code></pre></p> </li> <li> <p>Run database migrations:    <pre><code>alembic upgrade head\n</code></pre></p> </li> <li> <p>Start the API:    <pre><code>uvicorn ha_backend.api:app --reload --port 8001\n</code></pre></p> </li> <li> <p>Run tests:    <pre><code>make ci\n</code></pre></p> </li> </ol> <p>Next: Follow the Architecture Walkthrough tutorial to understand how everything fits together.</p>"},{"location":"quickstart/#im-an-api-consumer-researcher","title":"\ud83d\udd27 I'm an API Consumer / Researcher","text":"<p>Goal: Search the archive and retrieve historical snapshots.</p> <p>API Base URL: <code>https://api.healtharchive.ca</code></p> <p>Quick Examples:</p> <pre><code># Search for content about vaccines\ncurl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;sort=relevance\"\n\n# Get archive stats\ncurl \"https://api.healtharchive.ca/api/stats\"\n\n# List all sources\ncurl \"https://api.healtharchive.ca/api/sources\"\n\n# Get a specific snapshot\ncurl \"https://api.healtharchive.ca/api/snapshot/42\"\n</code></pre> <p>Interactive API Docs: api.healtharchive.ca (OpenAPI/Swagger UI)</p> <p>Next: Read the API Consumer Guide for detailed examples and use cases.</p>"},{"location":"quickstart/#multi-repo-project","title":"Multi-Repo Project","text":"<p>HealthArchive uses a multi-repo architecture:</p> <ul> <li>Backend (this repo): API, crawler, database, operations</li> <li>GitHub: jerdaw/healtharchive-backend</li> <li> <p>Docs: docs.healtharchive.ca</p> </li> <li> <p>Frontend: Public website UI</p> </li> <li>GitHub: jerdaw/healtharchive-frontend</li> <li> <p>Live Site: healtharchive.ca</p> </li> <li> <p>Datasets: Versioned data releases</p> </li> <li>GitHub: jerdaw/healtharchive-datasets</li> </ul> <p>See the Project Overview for detailed navigation.</p>"},{"location":"quickstart/#common-tasks","title":"Common Tasks","text":"Task Command Run all checks <code>make ci</code> Start API server <code>uvicorn ha_backend.api:app --reload --port 8001</code> Start worker <code>ha-backend start-worker --poll-interval 30</code> Create a crawl job <code>ha-backend create-job --source hc</code> Run a job <code>ha-backend run-db-job --id 42</code> Index WARCs <code>ha-backend index-job --id 42</code> List jobs <code>ha-backend list-jobs</code> Serve docs locally <code>make docs-serve</code>"},{"location":"quickstart/#need-help","title":"Need Help?","text":"<ul> <li>Architecture Deep Dive: Architecture Guide</li> <li>Local Development: Live Testing</li> <li>API Reference: API Documentation</li> <li>Troubleshooting: Check the How-To Guides</li> <li>Report Issues: GitHub Issues</li> </ul>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":""},{"location":"quickstart/#for-operators","title":"For Operators","text":"<ol> <li>Complete production deployment</li> <li>Set up monitoring and alerts</li> <li>Review the Ops Cadence Checklist</li> </ol>"},{"location":"quickstart/#for-developers","title":"For Developers","text":"<ol> <li>Complete Your First Contribution tutorial</li> <li>Read the Architecture Walkthrough</li> <li>Review Testing Guidelines</li> </ol>"},{"location":"quickstart/#for-researchers","title":"For Researchers","text":"<ol> <li>Explore the API Documentation</li> <li>Download datasets from healtharchive-datasets</li> <li>Read about Data Handling</li> </ol>"},{"location":"roadmap-process/","title":"Roadmap process (pointer)","text":"<p>This repo separates:</p> <ul> <li>the backlog (what is not implemented),</li> <li>active implementation plans (what we are currently doing),</li> <li>and canonical docs (what exists and how to run/operate it),</li> </ul> <p>to reduce documentation drift.</p> <p>Canonical guidance:</p> <ul> <li>Roadmap workflow: <code>documentation-guidelines.md</code></li> <li>Roadmaps index: <code>roadmaps/README.md</code></li> <li>Backlog: <code>roadmaps/roadmap.md</code></li> <li>Implemented plans archive: <code>roadmaps/implemented/README.md</code></li> </ul>"},{"location":"_templates/","title":"Documentation Templates","text":"<p>This directory contains templates for creating consistent documentation across the project.</p>"},{"location":"_templates/#available-templates","title":"Available Templates","text":"Template Purpose Destination <code>runbook-template.md</code> Deployment/operational procedures <code>docs/deployment/</code> <code>playbook-template.md</code> Task-oriented checklists <code>docs/operations/playbooks/</code> or <code>docs/development/playbooks/</code> <code>incident-template.md</code> Incident postmortems <code>docs/operations/incidents/</code> <code>decision-template.md</code> Architectural/policy decisions <code>docs/decisions/</code> <code>restore-test-log-template.md</code> Quarterly restore test logs <code>/srv/healtharchive/ops/restore-tests/</code> (VPS) <code>adoption-signals-log-template.md</code> Quarterly adoption signals <code>/srv/healtharchive/ops/adoption/</code> (VPS) <code>mentions-log-template.md</code> Public mentions log entries <code>docs/operations/mentions-log.md</code> <code>ops-ui-friction-log-template.md</code> Internal friction tracking Local ops notes (not git)"},{"location":"_templates/#how-to-use","title":"How to Use","text":"<ol> <li>Copy the appropriate template to the destination directory</li> <li>Rename the file (remove <code>-template</code> suffix)</li> <li>Fill in all sections with your content</li> <li>Update the directory's <code>README.md</code> index to include the new doc</li> <li>Add to navigation in <code>mkdocs.yml</code> if the doc is critical/frequently accessed</li> </ol>"},{"location":"_templates/#template-conventions","title":"Template Conventions","text":""},{"location":"_templates/#runbooks","title":"Runbooks","text":"<ul> <li>Purpose: Step-by-step operational procedures</li> <li>Audience: Operators with appropriate access</li> <li>Structure: Purpose, Scope, Preconditions, Architecture, Procedure, Verification, Rollback, Troubleshooting, References</li> </ul>"},{"location":"_templates/#playbooks","title":"Playbooks","text":"<ul> <li>Purpose: Short task-oriented checklists</li> <li>Audience: Operators performing recurring work</li> <li>Structure: Purpose, Preconditions, Steps, Verification, Safety, References</li> </ul>"},{"location":"_templates/#incident-notes","title":"Incident Notes","text":"<ul> <li>Purpose: Lightweight postmortems for operational learning</li> <li>Audience: Internal operators</li> <li>Structure: Metadata, Timeline, Impact, Root Cause, Resolution, Follow-ups, References</li> </ul>"},{"location":"_templates/#decision-records","title":"Decision Records","text":"<ul> <li>Purpose: Document high-stakes architectural/policy choices</li> <li>Audience: All contributors</li> <li>Structure: Context, Decision, Rationale, Alternatives, Consequences, Verification, References</li> </ul>"},{"location":"_templates/#maintenance","title":"Maintenance","text":"<ul> <li>Templates should be updated when patterns evolve</li> <li>Keep templates minimal and focused on structure</li> <li>Avoid prescriptive content that changes frequently</li> <li>Templates are excluded from the published docs site navigation but remain in the repo</li> </ul>"},{"location":"_templates/#references","title":"References","text":"<ul> <li>Documentation guidelines: <code>../documentation-guidelines.md</code></li> <li>Quality bar requirements: See \"Definition of Done\" in guidelines</li> </ul>"},{"location":"_templates/adoption-signals-log-template/","title":"Adoption signals log (template; private)","text":"<p>Keep this log public-safe:</p> <ul> <li>No emails, names, or personal details.</li> <li>Links + aggregate counts only.</li> </ul> <p>Recommended cadence: quarterly.</p> <p>Suggested location on the VPS:</p> <ul> <li><code>/srv/healtharchive/ops/adoption/</code></li> <li>One file per quarter (e.g., <code>YYYY-QN.md</code>)</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#adoption-signals-yyyy-qn-utc","title":"Adoption signals \u2014 YYYY-QN (UTC)","text":"<ul> <li>Quarter (UTC): YYYY-QN</li> <li>Operator:</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#dataset-releases","title":"Dataset releases","text":"<ul> <li>Release tags published this quarter:</li> <li><code>healtharchive-dataset-YYYY-MM-DD</code></li> <li>Notes (what changed / anything notable):</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#downloads-usage-aggregate","title":"Downloads / usage (aggregate)","text":"<ul> <li>GitHub release downloads: (if you check them; optional)</li> <li>snapshots: (count)</li> <li>changes: (count)</li> <li>Site usage metrics (aggregate): (optional)</li> <li>notes:</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#public-mentions-citations-links-only","title":"Public mentions / citations (links only)","text":"<ul> <li>(none)</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#research-reuse-signals-public-safe","title":"Research reuse signals (public-safe)","text":"<ul> <li>\u201cUsed in class/lab/project\u201d (link if public; otherwise omit).</li> <li>\u201cReferenced in newsletter/blog\u201d (link).</li> <li>\u201cCited in paper/preprint\u201d (link).</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#follow-ups","title":"Follow-ups","text":"<ul> <li>(none)</li> </ul>"},{"location":"_templates/decision-template/","title":"Decision:  (YYYY-MM-DD) <p>Status: draft | accepted | superseded</p>","text":""},{"location":"_templates/decision-template/#context","title":"Context","text":"<ul> <li>What problem are we solving?</li> <li>What constraints matter (security, privacy, ops capacity, reproducibility)?</li> <li>What triggered the decision (incident, planned change, external request)?</li> </ul>"},{"location":"_templates/decision-template/#decision","title":"Decision","text":"<p>State the decision clearly in 1\u20133 bullets.</p> <ul> <li>We will\u2026</li> <li>We will not\u2026</li> </ul>"},{"location":"_templates/decision-template/#rationale","title":"Rationale","text":"<p>Why this is the right tradeoff for HealthArchive right now.</p>"},{"location":"_templates/decision-template/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Option A \u2014 (why rejected)</li> <li>Option B \u2014 (why rejected)</li> </ul>"},{"location":"_templates/decision-template/#consequences","title":"Consequences","text":""},{"location":"_templates/decision-template/#positive","title":"Positive","text":"<ul> <li>\u2026</li> </ul>"},{"location":"_templates/decision-template/#negative-risks","title":"Negative / risks","text":"<ul> <li>\u2026</li> </ul>"},{"location":"_templates/decision-template/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>How do we confirm the decision is correctly implemented (checks, scripts, policy)?</li> <li>What is the rollback plan if it proves unsafe?</li> </ul>"},{"location":"_templates/decision-template/#references","title":"References","text":"<ul> <li>Related canonical docs:</li> <li>Related playbooks/runbooks:</li> <li>Related incident notes:</li> <li>PRs / issues / external links:</li> </ul>"},{"location":"_templates/incident-template/","title":"Incident:  (YYYY-MM-DD) <p>Status: draft | closed</p>","text":""},{"location":"_templates/incident-template/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): YYYY-MM-DD</li> <li>Severity (see <code>severity.md</code>): sev0 | sev1 | sev2 | sev3</li> <li>Environment: production | staging | dev</li> <li>Primary area: crawl | indexing | storage | api | replay | search | infra</li> <li>Owner:  <li>Start (UTC): YYYY-MM-DDTHH:MM:SSZ</li> <li>End (UTC): YYYY-MM-DDTHH:MM:SSZ (or ongoing)</li>"},{"location":"_templates/incident-template/#summary","title":"Summary","text":"<p>What happened in 2\u20135 sentences, written for someone who wasn\u2019t online during the incident.</p>"},{"location":"_templates/incident-template/#impact","title":"Impact","text":"<ul> <li>User-facing impact:</li> <li>Internal impact (ops burden, automation failures, etc):</li> <li>Data impact:</li> <li>Data loss: yes/no/unknown</li> <li>Data integrity risk: yes/no/unknown</li> <li>Recovery completeness: complete/partial/unknown</li> <li>Duration:</li> </ul>"},{"location":"_templates/incident-template/#detection","title":"Detection","text":"<ul> <li>How was it detected (alert, operator check, user report)?</li> <li>What signals were most useful (commands/metrics/logs)?</li> </ul>"},{"location":"_templates/incident-template/#decision-log-optional-but-recommended-for-sev0sev1","title":"Decision log (optional but recommended for sev0/sev1)","text":"<p>Record key decisions and why they were made (especially if they trade off data integrity vs speed).</p> <ul> <li>YYYY-MM-DDTHH:MM:SSZ \u2014 Decision:  (why: , risks: )"},{"location":"_templates/incident-template/#timeline-utc","title":"Timeline (UTC)","text":"<p>Keep this as a chronological log. Prefer timestamps.</p> <ul> <li>YYYY-MM-DDTHH:MM:SSZ \u2014  <li>YYYY-MM-DDTHH:MM:SSZ \u2014"},{"location":"_templates/incident-template/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger:</li> <li>Underlying cause(s):</li> </ul>"},{"location":"_templates/incident-template/#contributing-factors","title":"Contributing factors","text":"<ul> <li>What made this worse or harder to debug?</li> </ul>"},{"location":"_templates/incident-template/#resolution-recovery","title":"Resolution / Recovery","text":"<p>Describe the recovery steps taken, in the order performed, with commands if helpful.</p>"},{"location":"_templates/incident-template/#post-incident-verification","title":"Post-incident verification","text":"<p>What you did to confirm we\u2019re actually healthy (and not just \u201crunning\u201d).</p> <ul> <li>Public surface checks:</li> <li>Worker/job health checks:</li> <li>Storage/mount checks (if relevant):</li> <li>Integrity checks (if relevant):</li> </ul>"},{"location":"_templates/incident-template/#public-communication-optional-do-this-when-it-changes-user-expectations","title":"Public communication (optional; do this when it changes user expectations)","text":"<p>Keep this public-safe (no sensitive incident details).</p> <ul> <li>Public status update (where/when):</li> <li>Changelog entry (date/link):</li> <li>Public summary (2\u20135 sentences):</li> </ul>"},{"location":"_templates/incident-template/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li> <li>"},{"location":"_templates/incident-template/#action-items-todos","title":"Action items (TODOs)","text":"<p>Make these specific, small, and verifiable. Link to issues/PRs/roadmaps if they exist.</p> <ul> <li>  (owner=, priority=, due=) <li>  (owner=, priority=, due=)"},{"location":"_templates/incident-template/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>What can be automated safely?</li> <li>What should stay manual (risk/false positives)?</li> </ul>"},{"location":"_templates/incident-template/#references-artifacts","title":"References / Artifacts","text":"<ul> <li><code>./scripts/vps-crawl-status.sh</code> snapshot(s):</li> <li>Relevant log path(s):</li> <li>Dashboard link(s) / metric names:</li> <li>Related playbooks/runbooks:</li> </ul>"},{"location":"_templates/mentions-log-template/","title":"Mentions Log (template)","text":"<p>Purpose: A lightweight, public-safe log of mentions, links, or citations. Populate this only with information that is already public and permitted.</p> <p>Do NOT add private emails or outreach notes.</p> <p>Canonical log (real entries live here):</p> <ul> <li><code>mentions-log.md</code></li> </ul>"},{"location":"_templates/mentions-log-template/#mentions-log","title":"Mentions log","text":"Date (UTC) Organization / Outlet Link Context Permission to name? YYYY-MM-DD Example University Library https://example.org/resource Added to digital scholarship resources Yes"},{"location":"_templates/mentions-log-template/#notes","title":"Notes","text":"<ul> <li>Only include public links or citations.</li> <li>Keep descriptions factual and short.</li> <li>If permission is unclear, use \"Pending\" and do not name the organization in   public-facing copy until confirmed.</li> </ul>"},{"location":"_templates/ops-ui-friction-log-template/","title":"Ops UI friction log (template)","text":"<p>Goal: decide whether a bespoke admin/ops UI is worth building by capturing real operator pain over time.</p> <p>Guideline:</p> <ul> <li>Use Grafana dashboards + existing JSON admin endpoints first.</li> <li>Only build a bespoke UI if it clearly reduces recurring toil.</li> </ul>"},{"location":"_templates/ops-ui-friction-log-template/#entry","title":"Entry","text":"<p>Date (UTC):</p> <p>Operator:</p> <p>Context:</p> <ul> <li>What were you trying to do? (triage, investigate outage, verify deploy, investigate search quality, etc.)</li> <li>What triggered it? (alert, user report, scheduled check, curiosity)</li> </ul> <p>What worked well:</p> <ul> <li>What was quick/easy? (dashboard answered it, existing playbook, one command)</li> </ul> <p>Friction / pain:</p> <ul> <li>What took longer than it should?</li> <li>Did you need SSH for more than port-forwarding?</li> <li>Did you need to manually handle tokens/headers?</li> <li>Did you need to \u201chunt\u201d for the right endpoint/query?</li> </ul> <p>Impact:</p> <ul> <li>Time spent (rough): <code>X minutes</code></li> <li>Frequency: one-off / weekly / daily / during incidents</li> <li>Risk: low / medium / high (chance of operator mistake)</li> </ul> <p>Workaround used (today):</p> <ul> <li>Command(s) / link(s) / dashboard(s):</li> </ul> <p>Proposed improvement:</p> <ul> <li>Dashboard improvement? (new panel/table/link)</li> <li>Script improvement? (new helper, safer defaults)</li> <li>Doc/playbook improvement?</li> <li>Is a bespoke UI actually required? Why?</li> </ul> <p>Decision signal:</p> <ul> <li>If this happened again, would a bespoke UI save meaningful time?</li> </ul>"},{"location":"_templates/playbook-template/","title":"Playbook:  (operators) <p>Purpose: one sentence on what this playbook achieves.</p> <p>This is a short, task-oriented checklist. Keep it procedural, public-safe, and low-toil.</p>","text":""},{"location":"_templates/playbook-template/#when-to-use","title":"When to use","text":"<ul> <li> <li>"},{"location":"_templates/playbook-template/#preconditions-access","title":"Preconditions / access","text":"<ul> <li>Environment: production | staging | local</li> <li>Required access: haadmin; <code>sudo</code> required?&gt; <li>Required inputs:"},{"location":"_templates/playbook-template/#safety-guardrails","title":"Safety / guardrails","text":"<ul> <li>What could go wrong?</li> <li>What should you not do during this procedure?</li> <li>Any caps/cooldowns/sentinels that must be in place?</li> </ul>"},{"location":"_templates/playbook-template/#steps","title":"Steps","text":"<p>1)  (what it changes) 2)  (what it changes) 3) &lt;\u2026&gt;"},{"location":"_templates/playbook-template/#verification-done-criteria","title":"Verification (\u201cdone\u201d criteria)","text":"<ul> <li> <li>"},{"location":"_templates/playbook-template/#rollback-recovery-if-needed","title":"Rollback / recovery (if needed)","text":"<ul> <li>"},{"location":"_templates/playbook-template/#references","title":"References","text":"<ul> <li>Canonical runbook/checklist: </li> <li>Related playbooks: </li> <li>Relevant incident note(s): </li> </ul>"},{"location":"_templates/restore-test-log-template/","title":"Restore Test Log (template)","text":"<p>Use this template to record quarterly restore tests. Keep it public-safe: no secrets, credentials, or internal IPs.</p>"},{"location":"_templates/restore-test-log-template/#restore-test-record","title":"Restore test record","text":"<ul> <li>Date (UTC):</li> <li>Operator:</li> <li>Backup source used: (e.g., latest nightly dump, date, location)</li> <li>Restore target: (local temp DB / staging host)</li> <li>Restore method: (command summary)</li> <li>Schema check: (<code>alembic current</code> output)</li> <li>API checks: (<code>/api/health</code>, <code>/api/stats</code>, <code>/api/sources</code>)</li> <li>Result: Pass / Fail</li> <li>Notes / anomalies:</li> <li>Follow-up actions:</li> </ul>"},{"location":"_templates/runbook-template/","title":"Runbook:  (operators) <p>Purpose: one paragraph describing what this runbook covers and when it is the canonical reference.</p>","text":""},{"location":"_templates/runbook-template/#scope","title":"Scope","text":"<ul> <li>Environment(s): production | staging | dev</li> <li>Audience: operator | developer | both</li> <li>Non-goals: what this runbook explicitly does not cover</li> </ul>"},{"location":"_templates/runbook-template/#preconditions","title":"Preconditions","text":"<ul> <li>Required access (Tailscale, SSH user, <code>sudo</code>, secrets location)</li> <li>Required inputs (env vars, paths, hostnames, domains)</li> <li>Required dependencies (packages, services)</li> </ul>"},{"location":"_templates/runbook-template/#architecture-topology-short","title":"Architecture / topology (short)","text":"<ul> <li>Components involved (API, worker, DB, reverse proxy, storage)</li> <li>Network posture (public ports vs loopback-only vs tailnet-only)</li> <li>Data paths (where state and artifacts live)</li> </ul>"},{"location":"_templates/runbook-template/#procedure","title":"Procedure","text":""},{"location":"_templates/runbook-template/#1","title":"1)  <pre><code>&lt;command&gt;\n</code></pre> <p>What this changes:</p> <ul> <li>","text":""},{"location":"_templates/runbook-template/#2","title":"2)  <pre><code>&lt;command&gt;\n</code></pre> <p>What this changes:</p> <ul> <li>","text":""},{"location":"_templates/runbook-template/#verification-done-criteria","title":"Verification (\u201cdone\u201d criteria)","text":"<ul> <li>Public surface:  <li>Internal health:  <li>Drift / policy checks:"},{"location":"_templates/runbook-template/#rollback-recovery","title":"Rollback / recovery","text":"<ul> <li>Safe rollback strategy (fast path)</li> <li>What to avoid (data integrity risks)</li> </ul>"},{"location":"_templates/runbook-template/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Common failures and the first 1\u20133 commands to triage</li> <li>Pointers to deeper playbooks and incident response</li> </ul>"},{"location":"_templates/runbook-template/#references","title":"References","text":"<ul> <li>Related playbooks: </li> <li>Related checklists: </li> <li>Related incident notes: </li> </ul>"},{"location":"datasets-external/","title":"Datasets","text":"<p>The HealthArchive datasets live in a separate repository:</p> <ul> <li>Repo: https://github.com/jerdaw/healtharchive-datasets</li> <li>Releases: https://github.com/jerdaw/healtharchive-datasets/releases</li> </ul> <p>This backend docs portal links out to the canonical datasets docs; it does not mirror them.</p> <p>Related backend docs:</p> <ul> <li>Dataset release runbook (ops): <code>../operations/dataset-release-runbook.md</code></li> <li>Export integrity contract: <code>../operations/export-integrity-contract.md</code></li> </ul>"},{"location":"decisions/","title":"Decision records (ADR-lite)","text":"<p>This folder contains decision records for high-stakes choices that affect:</p> <ul> <li>security posture,</li> <li>privacy / data handling,</li> <li>public vs private surfaces,</li> <li>operational invariants (what must remain true over time).</li> </ul> <p>Goal: make important choices legible and durable so they don\u2019t get lost in chat history, PR threads, or implicit \u201ctribal knowledge\u201d.</p> <p>Related:</p> <ul> <li>Documentation policy and doc taxonomy: <code>../documentation-guidelines.md</code></li> <li>Public/private boundaries: <code>../operations/observability-and-private-stats.md</code></li> <li>Data handling and retention: <code>../operations/data-handling-retention.md</code></li> <li>Production invariants (drift policy): <code>../operations/baseline-drift.md</code></li> </ul>"},{"location":"decisions/#what-goes-here-examples","title":"What goes here (examples)","text":"<ul> <li>Decisions that change public attack surface (e.g., making an endpoint public/private).</li> <li>Decisions that change what data is collected or retained (especially anything user-related).</li> <li>Decisions that change operational safety rails (automation posture, caps, sentinels).</li> <li>Decisions that change reproducibility guarantees (exports, dataset release immutability).</li> </ul>"},{"location":"decisions/#what-does-not-go-here","title":"What does not go here","text":"<ul> <li>Backlog items and implementation steps (use <code>../roadmaps/</code>).</li> <li>Incident timelines and recovery notes (use <code>../operations/incidents/</code>).</li> <li>Routine ops logs (restore tests, adoption signals; use <code>/srv/healtharchive/ops/...</code>).</li> </ul>"},{"location":"decisions/#naming","title":"Naming","text":"<p>One file per decision:</p> <ul> <li><code>YYYY-MM-DD-short-title.md</code> (UTC date the decision is made/accepted)</li> </ul> <p>If multiple decisions occur on one day, add a suffix:</p> <ul> <li><code>YYYY-MM-DD-short-title-a.md</code>, <code>...-b.md</code></li> </ul>"},{"location":"decisions/#how-to-create-a-new-decision-record","title":"How to create a new decision record","text":"<p>1) Copy the template: <code>../_templates/decision-template.md</code> 2) Fill Context + Decision first. 3) Record alternatives briefly (what you didn\u2019t do, and why). 4) Link to supporting artifacts (PRs, incident notes, runbooks, issues). 5) Mark status as <code>accepted</code> once you commit to it.</p> <p>If a decision changes later, create a new decision record and mark the old one as <code>superseded</code> (link both directions).</p>"},{"location":"decisions/#decision-records","title":"Decision records","text":"<ul> <li><code>2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage.md</code></li> <li><code>2026-01-23-annual-crawl-throughput-and-artifacts.md</code></li> <li><code>2026-01-19-annual-crawl-resiliency-and-queue-order.md</code></li> <li><code>2026-01-19-ops-first-monitoring.md</code></li> <li><code>2026-01-18-search-ranking-v3.md</code></li> <li><code>2026-01-09-public-incident-disclosure-posture.md</code></li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/","title":"Decision: Public incident disclosure posture (Option B for now) (2026-01-09)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#context","title":"Context","text":"<ul> <li>HealthArchive is early in its operational lifecycle and will likely see a higher rate of sev0/sev1 incidents while reliability work is still being built out.</li> <li>We want to maintain transparency without creating constant public \u201cincident noise\u201d that trains users to ignore updates.</li> <li>We already capture internal, public-safe incident notes for operational learning and repeatability under <code>docs/operations/incidents/</code>.</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#decision","title":"Decision","text":"<ul> <li>We will use Option B by default:</li> <li>publish a public-safe note (in <code>/changelog</code> and/or <code>/status</code>) only when an incident changes user expectations:<ul> <li>user-visible outage or major degradation,</li> <li>credible integrity risk,</li> <li>security posture change,</li> <li>public policy/governance change.</li> </ul> </li> <li>We will still write internal incident notes when appropriate (per <code>docs/operations/incidents/README.md</code>).</li> <li>We will revisit moving to Option A (always publish a public-safe note for sev0/sev1) once operations are demonstrably stable over multiple full campaign cycles.</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#rationale","title":"Rationale","text":"<ul> <li>Option B preserves transparency for incidents that matter to user trust and interpretation of the archive.</li> <li>It avoids turning the public changelog/status into a high-volume incident feed during an early, fast-changing period.</li> <li>Internal incident notes remain the \u201cfull fidelity\u201d learning system and can still drive follow-ups and hardening work.</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Option A (always publish public-safe notes for sev0/sev1)</li> <li>Rejected for now due to likely high volume during stabilization; risk of public update fatigue.</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#positive","title":"Positive","text":"<ul> <li>Public reporting remains high-signal and user-relevant.</li> <li>Internal learning remains intact (incident notes + follow-ups).</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#negative-risks","title":"Negative / risks","text":"<ul> <li>Some operator-relevant incidents may not be visible publicly, even if they were sev0/sev1 internally.</li> <li>Mitigation: treat \u201cchanges user expectations\u201d as the trigger, not severity alone, and err on the side of communicating when unsure.</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Incident templates and severity rubric should reflect the \u201cOption B\u201d trigger:</li> <li><code>docs/operations/incidents/incident-template.md</code></li> <li><code>docs/operations/incidents/severity.md</code></li> <li>Ops cadence includes routine doc maintenance so these rules don\u2019t drift:</li> <li><code>docs/operations/ops-cadence-checklist.md</code></li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#references","title":"References","text":"<ul> <li>Incident notes process: <code>../operations/incidents/README.md</code></li> <li>Changelog process: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md</li> <li>Future roadmap note: <code>../roadmaps/roadmap.md</code></li> </ul>"},{"location":"decisions/2026-01-18-search-ranking-v3/","title":"ADR: Search ranking v3 with is_archived column and enhanced signals","text":"<p>Date: 2026-01-18</p> <p>Status: Accepted</p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#context","title":"Context","text":"<p>Search quality is central to HealthArchive's value proposition - researchers need to discover relevant captures efficiently. The v2 ranking system (introduced in 2025) provided query-mode sensitive blending but had two key limitations:</p> <ol> <li> <p>Archived content detection was heuristic-only - We detected archived pages via title/snippet text patterns, causing false positives/negatives and making the signal unstable across content updates.</p> </li> <li> <p>Missing modern ranking signals - No recency preference for broad queries, no exact title matching boost, and fixed BM25 weights limited relevance tuning.</p> </li> </ol> <p>The roadmap <code>docs/roadmaps/2026-01-03-search-ranking-and-snippets-v3.md</code> defined a phased implementation plan to address these gaps while maintaining single-VPS constraints.</p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#decision","title":"Decision","text":"<p>We implemented search ranking v3 with three major enhancements:</p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#1-stable-archived-detection-via-database-column","title":"1. Stable Archived Detection via Database Column","text":"<p>Change: Added <code>Snapshot.is_archived</code> (nullable boolean) populated at index-time.</p> <p>Rationale: - Moves archived detection from query-time heuristics to index-time computation - Database column is stable across snippet updates and query variations - Nullable design allows graceful handling during migration (fallback to heuristics for NULL values)</p> <p>Implementation: - Alembic migration: <code>alembic/versions/0013_snapshot_is_archived.py</code> - Detection logic: <code>src/ha_backend/indexing/text_extraction.py::detect_is_archived()</code> - Ranking integration: <code>src/ha_backend/api/routes_public.py::build_archived_penalty()</code></p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#2-enhanced-text-extraction-for-better-fts-and-snippets","title":"2. Enhanced Text Extraction for Better FTS and Snippets","text":"<p>Changes: - ARIA role pruning (<code>role=navigation</code>, <code>role=banner</code>, etc.) - Content root scoring (prefer <code>&lt;main&gt;</code>, <code>&lt;article&gt;</code> over generic <code>&lt;div&gt;</code>) - Boilerplate phrase filtering (\"Skip to content\", cookies banners) - Extended FTS content to ~4KB (up from ~280 char snippets)</p> <p>Rationale: - Removes navigation boilerplate that pollutes snippets - Improves FTS match quality by indexing actual page content - 4KB limit balances index size vs. content coverage</p> <p>Implementation: - Text extraction: <code>src/ha_backend/indexing/text_extraction.py</code> - FTS integration: <code>src/ha_backend/search.py::build_search_vector()</code> - Pipeline: <code>src/ha_backend/indexing/pipeline.py</code></p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#3-additional-ranking-signals","title":"3. Additional Ranking Signals","text":"<p>New signals in v3:</p> <ol> <li>Recency boost (broad/mixed queries only):</li> <li>Formula: <code>coef * ln(1 + 365 / days_ago)</code></li> <li>Broad: 0.15, Mixed: 0.08, Specific: 0.0</li> <li> <p>Rationale: Recent content is more valuable for broad exploratory queries</p> </li> <li> <p>Title exact-match boost:</p> </li> <li>Bonus when query appears as substring in title</li> <li>Broad: +0.35, Mixed: +0.30, Specific: +0.25</li> <li> <p>Rationale: Stronger signal than token matching; indicates highly relevant pages</p> </li> <li> <p>BM25 weight tuning (ts_rank weights):</p> </li> <li>Increased title weight (A): 1.0 \u2192 1.2 for broad queries</li> <li>Reduced URL weight (D): 0.1 \u2192 0.05</li> <li>Rationale: Titles are more reliable than URL tokens for relevance</li> </ol> <p>Implementation: - Config: <code>src/ha_backend/search_ranking.py</code> - API wiring: <code>src/ha_backend/api/routes_public.py</code></p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#consequences","title":"Consequences","text":"<p>Positive: - \u2705 Stable archived detection reduces query-time variability - \u2705 Cleaner snippets improve user experience - \u2705 Recency boost helps with broad queries (\"covid\" prefers recent guidance) - \u2705 Title exact-match strongly signals relevance - \u2705 Database-backed <code>is_archived</code> enables analytics and filters</p> <p>Neutral: - \u26a0\ufe0f Nullable <code>is_archived</code> requires migration planning (run Alembic + backfill on production) - \u26a0\ufe0f Evaluation required before making v3 default (<code>HA_SEARCH_RANKING_VERSION=v3</code>)</p> <p>Negative: - \u274c Slight index storage increase due to 4KB FTS content (acceptable given single-VPS storage capacity)</p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>Opt-in via <code>ranking=v3</code> query parameter or <code>HA_SEARCH_RANKING_VERSION=v3</code> environment variable</li> <li>V2 remains default until evaluation completes</li> <li>All 234 tests pass (28 new tests for v3 functionality)</li> <li>Evaluation tooling updated: <code>scripts/search-eval-capture.sh</code> supports <code>--ranking v3</code></li> </ul>"},{"location":"decisions/2026-01-18-search-ranking-v3/#references","title":"References","text":"<ul> <li>Roadmap: <code>docs/roadmaps/2026-01-03-search-ranking-and-snippets-v3.md</code></li> <li>Migration: <code>alembic/versions/0013_snapshot_is_archived.py</code></li> <li>Tests: <code>tests/test_text_extraction_v3.py</code>, <code>tests/test_ranking_v3_signals.py</code></li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/","title":"Decision: Annual crawl resiliency defaults and deterministic queue order (2026-01-19)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#context","title":"Context","text":"<ul> <li>HealthArchive\u2019s annual campaign crawls are completeness-first (full-fidelity backups), not \u201cbest effort within a page cap\u201d.</li> <li>A 2026 annual crawl incident showed multiple failure modes that increased operator load and reduced progress:</li> <li>\u201cNoisy but progressing\u201d sites (notably <code>canada.ca</code>) hit low adaptive timeout thresholds, causing repeated restarts and long backoff delays.</li> <li>The single-worker queue pick order was ambiguous when multiple jobs were enqueued with identical <code>queued_at</code>, leading to non-deterministic job selection and starvation.</li> <li>Invalid crawler CLI args (e.g., unsupported Zimit flags) caused immediate job failure and retry churn.</li> <li>Constraints:</li> <li>Production currently uses a single worker loop and a small VPS.</li> <li>We want safe-by-default automation and predictable operations.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#decision","title":"Decision","text":"<ul> <li>For annual campaign jobs, we will use resiliency-oriented defaults that tolerate \u201cnoisy but progressing\u201d crawls:</li> <li><code>max_container_restarts &gt;= 20</code></li> <li><code>error_threshold_timeout = 50</code></li> <li><code>error_threshold_http = 50</code></li> <li><code>backoff_delay_minutes = 2</code></li> <li><code>ha-backend schedule-annual</code> will stagger per-source <code>queued_at</code> timestamps so the single-worker pick order is deterministic (hc \u2192 phac \u2192 cihr).</li> <li>We will treat invalid CLI / config failures (e.g., \u201cunrecognized arguments\u201d) as <code>infra_error_config</code> so the worker does not churn retry budgets.</li> <li>Annual campaign policy remains: no page/depth caps. Use scope rules to bound crawls, not limits that risk incompleteness.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#rationale","title":"Rationale","text":"<ul> <li>Long annual crawls inevitably hit timeouts and transient network/protocol issues; restart thrash and long backoffs reduce throughput and increase operator intervention.</li> <li>Deterministic <code>queued_at</code> ordering makes operations predictable and prevents \u201cqueue tie\u201d ambiguity in a single-worker environment.</li> <li>Classifying invalid CLI args as configuration errors surfaces the real problem quickly and avoids burning retries on a doomed job.</li> <li>Scope rules preserve the completeness-first mission without relying on early-termination caps.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Keep low default thresholds + long backoff \u2014 rejected: causes restart thrash and slows progress on long crawls.</li> <li>Add page/depth caps to guarantee completion \u2014 rejected: conflicts with completeness-first archival goals and can silently truncate captures.</li> <li>Add more workers/parallelism immediately \u2014 deferred: increases ops surface and resource needs; can be revisited once baseline stability is proven.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#positive","title":"Positive","text":"<ul> <li>Fewer \u201cthrash loops\u201d and shorter recovery delays for long annual crawls.</li> <li>Predictable queue order reduces starvation and makes on-call behavior easier to reason about.</li> <li>Faster identification of invalid crawler argument/config mistakes.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#negative-risks","title":"Negative / risks","text":"<ul> <li>Higher thresholds may delay intervention for truly stuck crawls; mitigation is improved metrics (state file + restart counters) and stalled/progress-age alerts.</li> <li>More container restarts increase resource usage; mitigated by restart caps and monitoring.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Code defaults:</li> <li>Annual job defaults include the new tool options (<code>src/ha_backend/job_registry.py</code>).</li> <li><code>schedule-annual</code> sets distinct <code>queued_at</code> per source (<code>src/ha_backend/cli.py</code>).</li> <li>Automation:</li> <li>Auto-recover enforces annual minima when recovering jobs (<code>scripts/vps-crawl-auto-recover.py</code>).</li> <li>Observability:</li> <li>Metrics exporter emits <code>.archive_state.json</code> health + restart counters (<code>scripts/vps-crawl-metrics-textfile.py</code>).</li> <li>Tests:</li> <li><code>tests/test_cli_schedule_annual.py</code></li> <li><code>tests/test_ops_crawl_auto_recover_tool_options.py</code></li> <li><code>tests/test_jobs_persistent.py</code></li> <li><code>tests/test_ops_crawl_metrics_textfile_state.py</code></li> </ul> <p>Rollback:</p> <ul> <li>Revert the defaults and ordering logic in the above files; redeploy and restart the worker.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#references","title":"References","text":"<ul> <li>Related canonical docs:</li> <li><code>docs/operations/annual-campaign.md</code></li> <li><code>docs/reference/archive-tool.md</code></li> <li><code>docs/architecture.md</code></li> <li>Related scripts:</li> <li><code>scripts/vps-crawl-status.sh</code></li> <li><code>scripts/vps-crawl-auto-recover.py</code></li> <li><code>scripts/vps-crawl-metrics-textfile.py</code></li> </ul>"},{"location":"decisions/2026-01-19-ops-first-monitoring/","title":"2026-01-19: Ops-First Monitoring Strategy (Textfile Collectors)","text":""},{"location":"decisions/2026-01-19-ops-first-monitoring/#context","title":"Context","text":"<p>The annual crawl campaign (2026) required deep visibility into the crawling process (job 6, 7, etc.), which runs as a Docker container managed by <code>healtharchive-worker</code>. We needed to know:</p> <ol> <li>Is the crawl actually writing pages? (Progress monitoring)</li> <li>Is the SSHFS mount stable? (Infrastructure health)</li> <li>Is independent indexing starting after the crawl finishes? (Pipeline integrity)</li> </ol> <p>Existing Prometheus exporters (<code>node_exporter</code>) give system-level metrics but lack application-specific context for these batch jobs.</p>"},{"location":"decisions/2026-01-19-ops-first-monitoring/#decision","title":"Decision","text":"<p>We decided to implement an \"Ops-First\" monitoring strategy using the Prometheus Node Exporter Textfile Collector pattern, driven by simple Systemd timers and Python scripts.</p>"},{"location":"decisions/2026-01-19-ops-first-monitoring/#key-components","title":"Key Components","text":"<ol> <li>Script-Driven Metrics: A dedicated script (<code>scripts/vps-crawl-metrics-textfile.py</code>) that queries the DB and probes filesystem state (logs, mounts) to generate <code>.prom</code> files.</li> <li>Systemd Timers: Instead of a long-running daemon, we use <code>healtharchive-crawl-metrics-textfile.timer</code> to run the script every minute. This avoids memory leaks and makes the monitoring itself robust and stateless.</li> <li>State-File Coupling: The crawler (<code>archive_tool</code>) writes a <code>.archive_state.json</code> file. The monitoring script consumes this. This decoupling means the monitor doesn't need to query the Docker container directly.</li> </ol>"},{"location":"decisions/2026-01-19-ops-first-monitoring/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-01-19-ops-first-monitoring/#positive","title":"Positive","text":"<ul> <li>Simplicity: No new long-running services to manage.</li> <li>Robustness: If the monitor crashes, systemd restarts it next minute.</li> <li>Decoupling: Monitoring logic is separate from core crawler logic.</li> </ul>"},{"location":"decisions/2026-01-19-ops-first-monitoring/#negative","title":"Negative","text":"<ul> <li>Latency: Metrics are updated minutely, not real-time (acceptable for long-running crawls).</li> <li>Disk I/O: Constant reading of logs/state files (mitigated by <code>tail</code> logic).</li> </ul>"},{"location":"decisions/2026-01-19-ops-first-monitoring/#status","title":"Status","text":"<p>Accepted and Implemented (Phase 4 &amp; 6 of Hardening Mission).</p>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/","title":"Decision: Annual crawl throughput and WARC-first artifacts (2026-01-23)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#context","title":"Context","text":"<ul> <li>HealthArchive\u2019s annual campaign is completeness-first within explicit scope boundaries and search-first for readiness.</li> <li>Production runs on a single VPS (Hetzner <code>cx33</code>: 4 vCPU / 8GB RAM / 80GB SSD) with optional StorageBox for cold storage.</li> <li>The backend indexes WARCs into <code>Snapshot</code> rows; it does not read <code>.zim</code> files. Building ZIMs during the campaign adds wall-clock time and failure surface without improving \u201csearch readiness\u201d.</li> <li>Large, browser-driven crawls (notably <code>canada.ca</code>) benefit from modest parallelism and a larger container <code>/dev/shm</code> to reduce timeouts/stalls and avoid restart churn.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#decision","title":"Decision","text":"<ul> <li>Annual campaign jobs will default to modest crawl parallelism on the single VPS:</li> <li><code>tool_options.initial_workers = 2</code></li> <li><code>tool_options.docker_shm_size = \"1g\"</code></li> <li><code>tool_options.stall_timeout_minutes = 60</code> for <code>canada.ca</code> sources</li> <li>Annual campaign jobs will default to WARC-first artifacts:</li> <li><code>tool_options.skip_final_build = True</code> to skip optional <code>.zim</code> generation during the campaign.</li> <li>Shared-host <code>canada.ca</code> sources will default to querystring-averse scope rules for content paths to reduce duplicate/trap-like expansions while preserving completeness within intended boundaries.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#rationale","title":"Rationale","text":"<ul> <li>Using <code>2</code> workers on a 4 vCPU host is a conservative way to improve throughput without introducing multi-job concurrency complexity.</li> <li>Increasing container <code>/dev/shm</code> is a low-risk stability improvement for browser-driven crawls.</li> <li>Skipping ZIM generation keeps the critical path focused on: crawl WARCs \u2192 index \u2192 searchable. ZIMs can be generated later as a secondary artifact if desired.</li> <li>Excluding querystring/fragment variants for <code>canada.ca</code> content paths reduces duplicate work and the risk of trap-like URL expansions without relying on page/depth caps.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Keep <code>initial_workers=1</code> \u2014 rejected: underutilizes the host and increases the likelihood a single slow URL dominates wall-clock progress.</li> <li>Build <code>.zim</code> during the annual campaign \u2014 rejected: increases time-to-search-readiness and adds an extra failure surface; backend does not require ZIMs.</li> <li>Add page/depth caps \u2014 rejected: conflicts with completeness-first goals and risks silent truncation.</li> <li>Run multiple sources concurrently \u2014 deferred: increases ops surface area and complicates resource contention on a small VPS.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#positive","title":"Positive","text":"<ul> <li>Faster wall-clock progress on the annual campaign without changing \u201cwhat is in scope\u201d.</li> <li>Reduced time lost to stalls/restarts on browser-driven crawls.</li> <li>Clear separation between \u201csearch readiness\u201d (WARCs indexed) and optional offline artifacts (ZIMs).</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#negative-risks","title":"Negative / risks","text":"<ul> <li>Slightly higher load on target sites due to parallelism; mitigation is modest concurrency and monitoring.</li> <li>Excluding querystring variants can omit some non-canonical pages; mitigation is explicit scope review if a source relies on query-driven content.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Rollout is via:</li> <li><code>ha_backend.job_registry</code> defaults for annual sources.</li> <li><code>archive_tool</code> support for <code>--skip-final-build</code> and <code>--docker-shm-size</code>.</li> <li>Verify with:</li> <li>crawl metrics (progress age, stalled flag, restart rate),</li> <li>successful indexing immediately after crawl completion,</li> <li>spot-check that <code>canada.ca</code> scope still matches canonical content URLs and continues to capture referenced assets.</li> </ul> <p>Rollback:</p> <ul> <li>Set annual defaults back to <code>initial_workers=1</code>, remove <code>docker_shm_size</code>, and set <code>skip_final_build=false</code>.</li> <li>Redeploy backend and restart the worker.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#references","title":"References","text":"<ul> <li>Related canonical docs:</li> <li><code>docs/operations/annual-campaign.md</code></li> <li><code>docs/reference/archive-tool.md</code></li> <li><code>docs/architecture.md</code></li> <li>Related monitoring:</li> <li><code>docs/operations/monitoring-and-alerting.md</code></li> <li>Related prior decision:</li> <li><code>docs/decisions/2026-01-19-annual-crawl-resiliency-and-queue-order.md</code></li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/","title":"Decision: Single-VPS ops automation guardrails for crawl + storage recovery (2026-01-24)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#context","title":"Context","text":"<ul> <li>The single-VPS production deployment experienced a failure mode where Storage Box / FUSE-backed paths became stale and raised <code>OSError: [Errno 107] Transport endpoint is not connected</code>, which led to:</li> <li>a retry storm (tight re-pick loop on a fast-failing <code>retryable</code> job), and</li> <li>periods where crawl progress stopped until manual operator intervention.</li> <li>We need automation that improves resilience without increasing the chance of data loss or corrupting in-flight crawl outputs.</li> <li>Constraints:</li> <li>single host, limited ops capacity, completeness-first crawl policy</li> <li>automation must be safe-by-default and easy to disable instantly</li> <li>observability must be strong enough to debug incidents without log-diving</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#decision","title":"Decision","text":"<ul> <li>We will implement conservative, sentinel-gated watchdog automation for crawl/storage recovery on the single VPS:</li> <li>bounded and rate-limited,</li> <li>idempotent where possible,</li> <li>heavily biased toward \u201cskip\u201d instead of risky actions,</li> <li>and emitting Prometheus textfile metrics for alerting and forensics.</li> <li>We will provide an operator-friendly, detached job execution path for re-running a specific crawl job without keeping SSH sessions open.</li> <li>We will keep documentation English-only (docs portal policy) and capture operational facts and follow-ups in incident notes + implemented roadmaps.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#rationale","title":"Rationale","text":"<ul> <li>Sentinel-gated timers and strong rate limits reduce the risk of automation doing harm during partial outages or deploys.</li> <li>Textfile metrics (node_exporter) make it possible to alert and to diagnose \u201cstuck but not down\u201d failure modes quickly.</li> <li>Detached job execution via transient systemd units prevents operator error and reduces the need for long-lived interactive sessions.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Add more aggressive auto-recovery (always unmount/remount immediately).</li> <li>Rejected: too risky when a crawl container may be writing; potential for partial writes or frontier loss.</li> <li>Add new infrastructure (multi-host, queue workers, object storage).</li> <li>Rejected for now: out of scope for immediate single-VPS stability improvements.</li> <li>Leave recovery fully manual.</li> <li>Rejected: does not meet operational goals; increases toil and time-to-recovery.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#positive","title":"Positive","text":"<ul> <li>Retry storms are mitigated and stale hot paths can be repaired earlier (including for queued/retryable jobs).</li> <li>Operators can re-run jobs without \u201ckeeping a terminal open\u201d.</li> <li>Alerts can be based on stable metrics instead of ad-hoc log greps.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#negative-risks","title":"Negative / risks","text":"<ul> <li>Automation adds moving parts; misconfiguration could cause unnecessary churn or flapping if safety caps are removed.</li> <li>Some recovery steps remain intentionally manual when they intersect with in-flight writes; this trades faster auto-recovery for safety.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Verify watchdog scripts in dry-run modes and via unit tests.</li> <li>On the VPS, enable the timers only after verifying sentinel files exist and <code>vps-crawl-status.sh</code> shows healthy metrics.</li> <li>Rollback: remove sentinel files and disable timers; revert to manual playbooks.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#references","title":"References","text":"<ul> <li>Related canonical docs:</li> <li><code>docs/operations/monitoring-and-alerting.md</code></li> <li><code>docs/deployment/systemd/README.md</code></li> <li>Related incident notes:</li> <li><code>docs/operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop.md</code></li> <li>Related implemented roadmap:</li> <li><code>docs/roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening.md</code></li> </ul>"},{"location":"deployment/","title":"Deployment docs","text":""},{"location":"deployment/#start-here","title":"Start Here","text":"<p>Deploying to production? - Main: Production Runbook \u2014 Current production setup (Hetzner VPS) - Config: Configuration \u2014 Cross-repo env vars - Checklist: Hosting Checklist \u2014 DNS, CORS, Vercel</p> <p>Quick reference: | Task | Documentation | |------|---------------| | Deploy to VPS | Production Runbook | | Configure environment | Configuration | | Setup systemd services | Systemd Units | | Rollback search changes | Search Rollout |</p>"},{"location":"deployment/#all-deployment-documentation","title":"All Deployment Documentation","text":"<ul> <li>Current production runbook: <code>production-single-vps.md</code></li> <li>Includes the recommended deploy helper: <code>scripts/vps-deploy.sh</code></li> <li>Runbook template (for new runbooks): <code>../_templates/runbook-template.md</code></li> <li>Systemd unit templates (annual scheduling, worker priority, replay reconcile): <code>systemd/README.md</code></li> <li>Search ranking rollout: <code>search-rollout.md</code></li> <li>Deployment checklist / Vercel wiring: <code>hosting-and-live-server-to-dos.md</code></li> <li>Cross\u2011repo env vars + host matrix: <code>environments-and-configuration.md</code></li> <li>Generic checklists:</li> <li><code>production-rollout-checklist.md</code></li> <li><code>staging-rollout-checklist.md</code> (optional future)</li> </ul>"},{"location":"deployment/disaster-recovery/","title":"Disaster Recovery Runbook","text":"<p>Last Updated: 2026-01-18 Status: Active</p>"},{"location":"deployment/disaster-recovery/#recovery-objectives","title":"Recovery Objectives","text":"<p>In the context of HealthArchive, these objectives define our boundaries for data loss and downtime during a major failure.</p> <ul> <li>RPO (Recovery Point Objective): The maximum age of files that must be recovered from backup storage for operations to resume. It defines our \"data loss tolerance.\"</li> <li>RTO (Recovery Time Objective): The maximum duration of time within which service must be restored after a disaster. It defines our \"downtime tolerance.\"</li> <li>MTTR (Mean Time To Recovery): The average time taken to repair a failed component and return it to service.</li> </ul>"},{"location":"deployment/disaster-recovery/#rpo-recovery-point-objective","title":"RPO (Recovery Point Objective)","text":"<p>Target: 24 hours</p> <p>Rationale: - We perform nightly backups of the database and configuration. - Crawl data (WARCs) is tiered to storage regularly. - Up to 24 hours of data loss (recent crawls, user actions) is considered acceptable for the current service criticality level (research access, no real-time critical operational dependencies). Data can often be re-crawled.</p>"},{"location":"deployment/disaster-recovery/#rto-recovery-time-objective","title":"RTO (Recovery Time Objective)","text":"<p>Target: 8 hours</p> <p>Rationale: - Recovery involves manual provisioning of a new VPS, installing dependencies, and restoring from backup. - This timeframe allows a single operator to perform these steps during a standard workday.</p>"},{"location":"deployment/disaster-recovery/#mttr-mean-time-to-recovery","title":"MTTR (Mean Time To Recovery)","text":"<p>Target: 4 hours</p> <p>Rationale: - For partial failures (e.g., service restart, database recovery without full VPS loss), we aim to restore service within 4 hours.</p>"},{"location":"deployment/disaster-recovery/#when-to-revisit","title":"When to Revisit","text":"<p>These targets should be reviewed: - Annually: During the full DR drill. - Service Changes: If the service criticality increases (e.g., adding real-time users). - Architecture Changes: If moving from a single VPS to a multi-node/HA setup. - Scale Changes: If the dataset size grows significantly enough to impact restoration times.</p>"},{"location":"deployment/disaster-recovery/#scenarios","title":"Scenarios","text":""},{"location":"deployment/disaster-recovery/#scenario-a-complete-vps-loss-nas-backup-available","title":"Scenario A: Complete VPS loss (NAS backup available)","text":"<p>Most likely DR scenario. Requires provisioning new VPS and restoring from offsite NAS backup.</p>"},{"location":"deployment/disaster-recovery/#scenario-b-database-corruption","title":"Scenario B: Database corruption","text":"<p>Restoration from local or NAS <code>pg_dump</code>.</p>"},{"location":"deployment/disaster-recovery/#scenario-c-storage-failure","title":"Scenario C: Storage failure","text":"<p>Recovery of WARC files from tiered storage or accepted data loss.</p>"},{"location":"deployment/disaster-recovery/#procedures","title":"Procedures","text":""},{"location":"deployment/disaster-recovery/#1-vps-complete-restoration-scenario-a","title":"1. VPS Complete Restoration (Scenario A)","text":"<p>Prerequisites: - Access to Hetzner Cloud Console. - Access to Synology NAS (via physical access or alternative network if Tailscale is down). - SSH key for <code>haadmin</code> available locally.</p>"},{"location":"deployment/disaster-recovery/#step-1-provision-new-vps","title":"Step 1: Provision New VPS","text":"<ol> <li> <p>Create Server: Follow the standard provisioning steps in Production Single VPS.</p> <ul> <li>Image: Ubuntu 24.04 LTS.</li> <li>Updates: <code>sudo apt update &amp;&amp; sudo apt upgrade -y</code>.</li> <li>User: Create <code>haadmin</code> user and harden SSH.</li> </ul> </li> <li> <p>Configure Networking:</p> <ul> <li>Set up Firewall rules (Allow 80/443, block 22 public, allow Tailscale UDP).</li> </ul> </li> </ol>"},{"location":"deployment/disaster-recovery/#step-2-install-base-dependencies","title":"Step 2: Install Base Dependencies","text":"<p>Run as <code>haadmin</code>: <pre><code>sudo apt install -y docker.io postgresql postgresql-contrib python3-venv python3-pip git curl build-essential pkg-config unzip\nsudo systemctl enable --now docker postgresql\n</code></pre></p>"},{"location":"deployment/disaster-recovery/#step-3-re-join-tailscale","title":"Step 3: Re-join Tailscale","text":"<ol> <li>Install Tailscale: <code>curl -fsSL https://tailscale.com/install.sh | sh</code>.</li> <li>Authenticate: <code>sudo tailscale up --ssh</code>.<ul> <li>Note: If possible, reuse the old IP/hostname from the admin console to simplify ACLs, or update ACLs to trust the new node.</li> </ul> </li> </ol>"},{"location":"deployment/disaster-recovery/#step-4-prepare-directories","title":"Step 4: Prepare Directories","text":"<pre><code>sudo groupadd --system healtharchive\nsudo mkdir -p /srv/healtharchive/{jobs,backups,ops}\nsudo chown -R haadmin:haadmin /srv/healtharchive/jobs\nsudo chown root:healtharchive /srv/healtharchive/backups /srv/healtharchive/ops\nsudo chmod 2770 /srv/healtharchive/backups /srv/healtharchive/ops\n</code></pre>"},{"location":"deployment/disaster-recovery/#step-5-retrieve-backup-from-nas","title":"Step 5: Retrieve Backup from NAS","text":"<p>If Tailscale is up on both ends: 1.  SSH to NAS: <code>ssh user@nas-ip</code>. 2.  Rsync backup to new VPS:     <pre><code>rsync -av /volume1/nobak/healtharchive/backups/db/latest.dump haadmin@new-vps-ip:/srv/healtharchive/backups/\n</code></pre> Alternatively, pull from VPS: <pre><code>scp user@nas-ip:/path/to/backup.dump /srv/healtharchive/backups/latest.dump\n</code></pre></p>"},{"location":"deployment/disaster-recovery/#step-6-restore-database","title":"Step 6: Restore Database","text":"<ol> <li>Create DB and User:     <pre><code>sudo -u postgres psql -c \"CREATE USER healtharchive WITH PASSWORD '&lt;password_from_backup_env&gt;';\"\nsudo -u postgres psql -c \"CREATE DATABASE healtharchive OWNER healtharchive;\"\n</code></pre></li> <li>Restore Schema and Data:     <pre><code>sudo -u postgres pg_restore -d healtharchive /srv/healtharchive/backups/latest.dump\n</code></pre></li> </ol>"},{"location":"deployment/disaster-recovery/#step-7-restore-application","title":"Step 7: Restore Application","text":"<ol> <li>Clone Repository:     <pre><code>git clone https://github.com/jerdaw/healtharchive-backend.git /opt/healtharchive-backend\ncd /opt/healtharchive-backend\npython3 -m venv .venv\n./.venv/bin/pip install -e \".[dev]\" \"psycopg[binary]\"\n</code></pre></li> <li>Restore Configuration:<ul> <li>Restore <code>/etc/healtharchive/backend.env</code> from your distinct secure offsite storage (e.g., password manager notes). Do not lose this file.</li> <li>If needed, regenerate the <code>ADMIN_TOKEN</code>.</li> </ul> </li> </ol>"},{"location":"deployment/disaster-recovery/#step-8-re-mount-storage-restore-warcs","title":"Step 8: Re-mount Storage / Restore WARCs","text":"<ul> <li>Mount the Storage Box (tiered storage) to <code>/srv/healtharchive/storagebox</code> using <code>sshfs</code> (see <code>production-single-vps.md</code>).</li> <li>If local WARCs were lost (<code>/srv/healtharchive/jobs</code>), you have two options:<ol> <li>Rescan: If files exist on Storage Box, re-import headers (slow).</li> <li>Empty Start: Start with empty local jobs; historical data remains on Storage Box/index.</li> </ol> </li> </ul>"},{"location":"deployment/disaster-recovery/#2-database-intact-restoration-scenario-b","title":"2. Database Intact Restoration (Scenario B)","text":"<p>Use this procedure when the VPS is running but the database is corrupted or dropped.</p> <p>Prerequisites: - Backup file available (local or NAS). - PostgreSQL service is running.</p>"},{"location":"deployment/disaster-recovery/#step-1-locate-backup","title":"Step 1: Locate Backup","text":"<ul> <li>Format: <code>pg_dump -Fc</code> (custom format, compressed).</li> <li>Local: <code>/srv/healtharchive/backups/</code><ul> <li>Naming: <code>healtharchive_&lt;timestamp&gt;.dump</code></li> <li>Retention: 14 days.</li> </ul> </li> <li>NAS: <code>/volume1/nobak/healtharchive/backups/db/</code> (needs retrieval)<ul> <li>Retention: Long-term/Permanent.</li> </ul> </li> </ul>"},{"location":"deployment/disaster-recovery/#step-2-restore-database","title":"Step 2: Restore Database","text":"<p>Warning: This will overwrite the current database state.</p> <ol> <li> <p>Drop and Recreate: <pre><code>sudo -u postgres dropdb --if-exists healtharchive_restored\nsudo -u postgres createdb healtharchive_restored\n</code></pre></p> </li> <li> <p>Restore from Dump: <pre><code># Replace &lt;backup_file&gt; with actual filename\nsudo -u postgres pg_restore -d healtharchive_restored -Fc /path/to/backup.dump\n</code></pre></p> </li> <li> <p>Verify Restoration:     Check that tables are populated:     <pre><code>sudo -u postgres psql -d healtharchive_restored -c \"SELECT count(*) FROM snapshots;\"\n</code></pre></p> </li> <li> <p>Swap Databases:     Stop services to preventing locking:     <pre><code>sudo systemctl stop healtharchive-api healtharchive-worker\n</code></pre></p> <p>Swap: <pre><code>sudo -u postgres psql -c \"ALTER DATABASE healtharchive RENAME TO healtharchive_old;\"\nsudo -u postgres psql -c \"ALTER DATABASE healtharchive_restored RENAME TO healtharchive;\"\n</code></pre></p> </li> <li> <p>Restart Services: <pre><code>sudo systemctl start healtharchive-api healtharchive-worker\n</code></pre></p> </li> </ol>"},{"location":"deployment/disaster-recovery/#step-3-integrity-verification","title":"Step 3: Integrity Verification","text":"<ul> <li>Row Counts: Compare <code>SELECT count(*) FROM snapshots</code> with expected values.</li> <li>Recent Data: Check for the most recent captures <code>SELECT * FROM snapshots ORDER BY id DESC LIMIT 5;</code>.</li> <li>Foreign Keys: <code>pg_restore</code> would have failed on constraint violations, but check application logs for ORM errors.</li> <li>Orphaned Records: Ensure core relations are intact:   <pre><code>-- Check for snapshots without sources\nSELECT count(*) FROM snapshots WHERE source_id NOT IN (SELECT id FROM sources);\n</code></pre></li> </ul>"},{"location":"deployment/disaster-recovery/#step-4-partial-restoration-advanced","title":"Step 4: Partial Restoration (Advanced)","text":"<ul> <li>Specific Table: Use <code>pg_restore -t &lt;tablename&gt;</code> to restore only one table to a temp DB, then copy data.</li> <li>Verify on Separate Server: For high-stakes restorations, perform the restoration on a development or temporary VPS first to verify integrity before swapping production.</li> <li>Point-in-Time: Requires WAL archiving (currently not enabled; rely on nightly dumps).</li> </ul>"},{"location":"deployment/disaster-recovery/#3-archive-root-recovery-scenario-c","title":"3. Archive Root Recovery (Scenario C)","text":"<p>Use this procedure when WARC files or the archive storage structure is compromised.</p> <p>Archive Root Structure: <pre><code>/srv/healtharchive/jobs/\n\u251c\u2500\u2500 &lt;source_slug&gt;-&lt;year&gt;-&lt;month&gt;/  # Job Output Directories\n\u2502   \u251c\u2500\u2500 warcs/                     # Stable WARC files\n\u2502   \u2502   \u251c\u2500\u2500 manifest.json          # Mapping of source -&gt; stable filenames\n\u2502   \u2502   \u2514\u2500\u2500 warc-000001.warc.gz\n\u2502   \u251c\u2500\u2500 provenance/                # Metadata preservation\n\u2502   \u2502   \u2514\u2500\u2500 archive_state.json\n\u2502   \u2514\u2500\u2500 logs/\n\u2514\u2500\u2500 tiered/                        # Mount point for cold storage (Storage Box)\n</code></pre></p>"},{"location":"deployment/disaster-recovery/#recovery-scenarios","title":"Recovery Scenarios","text":"<p>Case 1: Local WARCs lost (e.g., accidental deletion), Tiered storage intact This is the most common recovery case. 1.  Check Tiered Storage: Verify header-only WARCs or full files exist in <code>/srv/healtharchive/storagebox</code>.     2.  Re-import Headers/WARCs (Slow but safe):         If the database is intact, you don't need the local WARCs effectively immediately for the site to work, but the Replay service will fail for those snapshots.         To restore replayability, copy the WARCs back from tiered storage:         <pre><code># Example: Restore specific job\nrsync -av /srv/healtharchive/storagebox/jobs/hc-2026-01/ /srv/healtharchive/jobs/hc-2026-01/\n</code></pre>     3.  Verify against Manifest: <pre><code># Check that all files in manifest exist and have correct sizes\ncat /srv/healtharchive/jobs/hc-2026-01/warcs/manifest.json | jq .records\n</code></pre></p> <p>Case 2: Tiered storage unavailable, Local intact 1.  Run in Degraded Mode: Operations can continue using local WARCs. 2.  Disable Tiering: Stop the tiering cron job/timer to prevent errors. 3.  Restore Connection: Troubleshoot <code>sshfs</code> mount or Storage Box availability. 4.  Re-enable Tiering: Once fixed, the system will resume tiering new WARCs.</p> <p>Case 3: All copies lost (Catastrophic) 1.  Accept Data Loss: Crawl data is gone. 2.  Clean Database: You may need to truncate <code>snapshots</code> table if it references missing files, or mark them as lost. 3.  Re-crawl: Trigger new manual crawls for critical sources.</p>"},{"location":"deployment/disaster-recovery/#integrity-verification","title":"Integrity Verification","text":"<ol> <li>WARC Validation: <pre><code># Validate a single WARC file\nwarcio validate /path/to/file.warc.gz\n</code></pre></li> <li>Database Consistency:     Ensure database records point to existing files (custom script required).</li> </ol>"},{"location":"deployment/disaster-recovery/#re-tiering-and-consolidation-procedure","title":"Re-tiering and Consolidation Procedure","text":"<p>If tiered storage was wiped and replaced, or if you need to stabilize newly crawled data:</p> <ol> <li>Consolidate WARCs: Ensure files are moved from <code>.tmp*</code> to stable <code>warcs/</code> folders and manifests are updated.     <pre><code># Run as haadmin in .venv\nha-backend consolidate-warcs --id &lt;JOB_ID&gt;\n</code></pre></li> <li>Verify Local Integrity: Ensure local WARCs match their manifest and are valid.</li> <li>Force Tiering: Run the tiering command manually to re-upload everything:     <pre><code># Run as haadmin in .venv\nha-backend tier-warcs --force --dry-run  # Check first\nha-backend tier-warcs --force            # Execute\n</code></pre></li> <li>Verify Tiered Copies: Check that the files on the Storage Box match the local stable WARCs.</li> </ol>"},{"location":"deployment/disaster-recovery/#4-service-startup-sequence","title":"4. Service Startup Sequence","text":"<p>Order is critical:</p> <ol> <li>Database: <code>sudo systemctl start postgresql</code><ul> <li>Health Check: <code>sudo systemctl status postgresql</code> or <code>pg_isready</code></li> <li>Failure: Check disk space (<code>df -h</code>) and logs (<code>journalctl -u postgresql</code>).</li> </ul> </li> <li>API: <code>sudo systemctl start healtharchive-api</code><ul> <li>Health Check: <code>curl http://localhost:8001/api/health</code></li> <li>Failure: Check <code>/etc/healtharchive/backend.env</code> and <code>journalctl -u healtharchive-api -n 100</code>.</li> </ul> </li> <li>Worker: <code>sudo systemctl start healtharchive-worker</code><ul> <li>Health Check: <code>sudo systemctl status healtharchive-worker</code> (Check logs for \"Worker started\").</li> <li>Failure: Check database connectivity and logs.</li> </ul> </li> <li>Replay (Optional): Start pywb if configured.<ul> <li>Health Check: <code>curl http://localhost:8080</code> (or configured port).</li> </ul> </li> <li>Reverse Proxy: <code>sudo systemctl start caddy</code><ul> <li>Health Check: <code>sudo systemctl status caddy</code></li> <li>Failure: <code>sudo caddy validate --config /etc/caddy/Caddyfile</code>.</li> </ul> </li> </ol>"},{"location":"deployment/disaster-recovery/#5-verification-checklist","title":"5. Verification Checklist","text":"<p>Run these checks immediately after startup:</p> <ul> <li> Database Connectivity: <code>sudo -u postgres psql -d healtharchive -c 'SELECT count(*) FROM sources;'</code> (Should &gt; 0)</li> <li> API Health: <code>curl http://localhost:8001/api/health</code> -&gt; <code>{\"status\":\"ok\"}</code></li> <li> Public Endpoint (HTTPS): <code>curl -I https://api.healtharchive.ca/api/health</code> (Verify TLS works)</li> <li> Search Index: Query a known term via the frontend or API.</li> <li> Worker Health: Check logs for \"Worker started\" and no immediate crashes.</li> <li> Snapshot Viewing: Visit a known snapshot URL (e.g., the smoke test snapshot ID 1).</li> <li> Monitoring Reconnected: Confirm that Healthchecks.io, Prometheus, and external uptime monitors are receiving signals from the new VPS.</li> </ul>"},{"location":"deployment/disaster-recovery/#dr-drills","title":"DR Drills","text":"<p>Regular testing ensures that these procedures remain effective and that operators are familiar with the recovery process.</p>"},{"location":"deployment/disaster-recovery/#schedule","title":"Schedule","text":"Drill Type Frequency Next Due Owner Scope Tabletop Quarterly Q1 2026 Operator Review procedure, check credentials, identify gaps. Partial Restore Quarterly Q1 2026 Operator Restore database summary/integrity check on local dev machine. Full DR Annual 2026-06 Operator Full recovery from backup to a fresh VPS."},{"location":"deployment/disaster-recovery/#procedures_1","title":"Procedures","text":""},{"location":"deployment/disaster-recovery/#1-tabletop-drill","title":"1. Tabletop Drill","text":"<p>Objective: Verify documentation accuracy and credential availability without interacting with production.</p> <ol> <li>Read-Through: Walk through the \"Complete VPS Restoration (Scenario A)\" procedure step-by-step.</li> <li>Credential Check: Verify you can locate/access:<ul> <li>Hetzner Cloud Console password/2FA.</li> <li>Synology NAS SSH keys.</li> <li>Encrypted backup of <code>/etc/healtharchive/backend.env</code>.</li> <li>Domain DNS controls (Namecheap).</li> </ul> </li> <li>Success Criteria:<ul> <li>All restoration steps are understood and commands are valid.</li> <li>All required credentials are confirmed as accessible and current.</li> </ul> </li> <li>Documentation &amp; Follow-up:<ul> <li>Fix any broken links, outdated commands, or unclear instructions found during the read-through.</li> <li>Record findings in the Results Log (see below).</li> </ul> </li> </ol>"},{"location":"deployment/disaster-recovery/#2-partial-restoration-drill","title":"2. Partial Restoration Drill","text":"<p>Objective: Verify backup integrity and database restorability.</p> <ol> <li>Retrieve Backup: Download the latest actual <code>healtharchive_&lt;ts&gt;.dump</code> from the NAS or VPS.</li> <li>Local Restore:<ul> <li>Spin up a local Docker Postgres container or use a local dev DB.</li> <li>Run the Scenario B (Database Corruption) restoration steps against this local instance.</li> </ul> </li> <li>Success Criteria:<ul> <li><code>pg_restore</code> completes without fatal errors.</li> <li>Row counts for <code>snapshots</code> match or are within expected growth margins.</li> <li>Recent captures are present and readable.</li> </ul> </li> <li>Documentation &amp; Follow-up:<ul> <li>Record the size of the backup and restoration time in the Results Log.</li> <li>If corruption is found, investigate backup job logs and schedule an immediate re-run.</li> </ul> </li> <li>Cleanup: Delete the local test database and backup file.</li> </ol>"},{"location":"deployment/disaster-recovery/#3-full-dr-drill-annual","title":"3. Full DR Drill (Annual)","text":"<p>Objective: Prove total system recovery capability.</p> <p>Prerequisites: - Perform during low-traffic window (e.g., weekend). - Budget ~$5 for temporary VPS costs.</p> <p>Procedure: 1.  Provision: Create a new VPS (e.g., <code>dr-test-2026</code>) in Hetzner. DO NOT DELETE THE EXISTING PRODUCTION VPS. 2.  Execute Scenario A: Follow \"VPS Complete Restoration\" strictly.     - Modification: When restoring <code>backend.env</code>, change <code>HEALTHARCHIVE_PUBLIC_SITE_URL</code> to the temporary IP or a test subdomain to avoid DNS conflicts.     - Modification: Do not switch the main DNS (A record) unless you are intentionally testing failover (requires downtime). 3.  Verify &amp; Success Criteria:     - Run the complete \"Verification Checklist\" on the new host; all checks must pass.     - Verify you can pull a WARC file from tiered storage.     - Total restoration time is within the 8-hour RTO. 4.  Documentation &amp; Follow-up:     - Record total time to recovery (RTO metric) and any blockers in the Results Log.     - Update the MTTR/RTO targets if they are consistently missed or easily exceeded. 5.  Teardown:     - Destroy the temporary VPS.     - Remove the temporary node from Tailscale.</p>"},{"location":"deployment/disaster-recovery/#results-log","title":"Results Log","text":"<p>Copy and paste this template to <code>docs/operations/dr-logs/&lt;YYYY-MM-DD&gt;-drill-report.md</code>:</p> <pre><code># DR Drill Report: &lt;Date&gt;\n\n**Drill Type:** (Tabletop / Partial / Full)\n**Operator:** &lt;Name&gt;\n**Time Started:** &lt;HH:MM UTC&gt;\n**Time Finished:** &lt;HH:MM UTC&gt;\n**Total Duration:** &lt;Minutes&gt;\n\n## Outcome\n- [ ] Success (All objectives met)\n- [ ] Partial Success (Objectives met with issues)\n- [ ] Failure (Could not complete recovery)\n\n## Metric\n- **RTO Achieved:** N/A (or actual time if Full Drill)\n- **Backup Age:** &lt;Hours since last backup&gt; (RPO check)\n\n## Issues Encountered\n1. Issue description...\n\n## Documentation Updates Required\n- [ ] Update section X.Y...\n</code></pre>"},{"location":"deployment/environments-and-configuration/","title":"Environments and configuration (frontend + backend)","text":"<p>This document is the canonical cross-repo reference for how the backend (<code>healtharchive-backend</code>) and frontend (<code>healtharchive-frontend</code>) are wired together across environments.</p> <p>The root <code>ENVIRONMENTS.md</code> is a pointer to this file to avoid duplication.</p> <p>It is useful when:</p> <ul> <li>Setting or auditing environment variables (Vercel + backend host).</li> <li>Double\u2011checking that frontend hosts, backend hosts, and backend CORS settings   line up.</li> </ul> <p>For deeper operational details, see:</p> <ul> <li><code>production-single-vps.md</code> (current production runbook)</li> <li><code>hosting-and-live-server-to-dos.md</code> (high-level deployment checklist)</li> <li><code>../operations/monitoring-and-ci-checklist.md</code> (uptime/monitoring guidance)</li> <li><code>../operations/baseline-drift.md</code> (production drift checks: policy vs observed)</li> <li>Frontend docs: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/implementation-guide.md</li> <li>Frontend verification: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md</li> </ul>"},{"location":"deployment/environments-and-configuration/#1-environments-at-a-glance","title":"1) Environments at a glance","text":""},{"location":"deployment/environments-and-configuration/#what-exists-today","title":"What exists today","text":"<ul> <li>Single backend API: <code>https://api.healtharchive.ca</code></li> <li>No separate staging backend (by design)</li> <li>Backend CORS allowlist is intentionally strict:</li> <li><code>https://healtharchive.ca</code></li> <li><code>https://www.healtharchive.ca</code></li> <li><code>https://healtharchive.vercel.app</code></li> <li><code>https://replay.healtharchive.ca</code> (for the optional replay banner and direct replay UX)</li> </ul> <p>Expected limitation (by design):</p> <ul> <li>Branch preview URLs like <code>https://healtharchive-git-...vercel.app</code> may fall   back to demo mode until we explicitly allow those origins (CORS).</li> </ul>"},{"location":"deployment/environments-and-configuration/#11-validate-production-wiring-recommended","title":"1.1 Validate production wiring (recommended)","text":"<p>On the production VPS, run the baseline drift check in live mode:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/check_baseline_drift.py --mode live\n</code></pre> <p>This validates:</p> <ul> <li>env vars are set as expected (including CORS allowlist),</li> <li>HSTS is configured and observed,</li> <li>admin endpoints are protected,</li> <li>CORS headers are actually returned for the allowed origins.</li> </ul>"},{"location":"deployment/environments-and-configuration/#matrix","title":"Matrix","text":"Environment Frontend (browser origin) Backend API base Notes Local dev <code>http://localhost:3000</code> <code>http://127.0.0.1:8001</code> Local dev flow. Vercel project domain <code>https://healtharchive.vercel.app</code> <code>https://api.healtharchive.ca</code> Allowed by CORS; useful as a stable \u201cnon-custom-domain\u201d URL. Production <code>https://healtharchive.ca</code> / <code>https://www.healtharchive.ca</code> <code>https://api.healtharchive.ca</code> Primary public site. Branch previews (Vercel) <code>https://healtharchive-git-...vercel.app</code> <code>https://api.healtharchive.ca</code> May fall back to demo mode due to strict CORS. <p>Optional future:</p> Environment Frontend (browser origin) Backend API base Notes Staging API (optional) Preview URLs or a dedicated staging frontend <code>https://api-staging.healtharchive.ca</code> Only if you decide you want a separate staging backend later."},{"location":"deployment/environments-and-configuration/#2-backend-configuration-healtharchive-backend","title":"2) Backend configuration (healtharchive-backend)","text":"<p>All backend env vars are read by:</p> <ul> <li><code>src/ha_backend/config.py</code></li> <li><code>src/ha_backend/api/deps.py</code></li> <li>Search ranking selection is controlled by <code>HA_SEARCH_RANKING_VERSION</code> (and can be overridden per-request with <code>ranking=v1|v2</code> on <code>/api/search</code>).</li> </ul>"},{"location":"deployment/environments-and-configuration/#21-local-development-typical","title":"2.1 Local development (typical)","text":"<p>Example shell setup (or via <code>.env.example</code> \u2192 <code>.env</code>, git-ignored):</p> <pre><code>export HEALTHARCHIVE_ENV=development\nexport HEALTHARCHIVE_DATABASE_URL=sqlite:///$(pwd)/.dev-healtharchive.db\nexport HEALTHARCHIVE_ARCHIVE_ROOT=$(pwd)/.dev-archive-root\nexport HEALTHARCHIVE_ZIMIT_DOCKER_IMAGE=ghcr.io/openzim/zimit  # optional override (pin by tag or digest)\nexport HEALTHARCHIVE_ADMIN_TOKEN=localdev-admin\nexport HEALTHARCHIVE_LOG_LEVEL=DEBUG\nexport HA_SEARCH_RANKING_VERSION=v2\nexport HA_PAGES_FASTPATH=1\nexport HEALTHARCHIVE_REPLAY_BASE_URL=http://127.0.0.1:8090\nexport HEALTHARCHIVE_REPLAY_PREVIEW_DIR=$(pwd)/.dev-replay-previews\nexport HEALTHARCHIVE_EXPORTS_ENABLED=1\nexport HEALTHARCHIVE_EXPORTS_DEFAULT_LIMIT=1000\nexport HEALTHARCHIVE_EXPORTS_MAX_LIMIT=10000\n</code></pre>"},{"location":"deployment/environments-and-configuration/#22-production-current","title":"2.2 Production (current)","text":"<p>On the production backend host (systemd env file / Docker env / PaaS env):</p> <pre><code>export HEALTHARCHIVE_ENV=production\nexport HEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://healtharchive:&lt;DB_PASSWORD&gt;@127.0.0.1:5432/healtharchive\nexport HEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs\nexport HEALTHARCHIVE_ZIMIT_DOCKER_IMAGE=ghcr.io/openzim/zimit@sha256:&lt;PINNED_DIGEST&gt;\nexport HEALTHARCHIVE_ADMIN_TOKEN=&lt;LONG_RANDOM_SECRET&gt;\nexport HEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.ca,https://www.healtharchive.ca,https://healtharchive.vercel.app,https://replay.healtharchive.ca\nexport HEALTHARCHIVE_LOG_LEVEL=INFO\nexport HA_SEARCH_RANKING_VERSION=v2\nexport HA_PAGES_FASTPATH=1\nexport HEALTHARCHIVE_USAGE_METRICS_ENABLED=1\nexport HEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS=30\nexport HEALTHARCHIVE_CHANGE_TRACKING_ENABLED=1\nexport HEALTHARCHIVE_EXPORTS_ENABLED=1\nexport HEALTHARCHIVE_EXPORTS_DEFAULT_LIMIT=1000\nexport HEALTHARCHIVE_EXPORTS_MAX_LIMIT=10000\nexport HEALTHARCHIVE_PUBLIC_SITE_URL=https://healtharchive.ca\nexport HEALTHARCHIVE_REPLAY_BASE_URL=https://replay.healtharchive.ca\nexport HEALTHARCHIVE_REPLAY_PREVIEW_DIR=/srv/healtharchive/replay/previews\n</code></pre> <p>Notes:</p> <ul> <li><code>HEALTHARCHIVE_ADMIN_TOKEN</code> should be a long random secret stored in a secret   manager (e.g., Bitwarden + server env), never committed.</li> <li><code>HEALTHARCHIVE_ZIMIT_DOCKER_IMAGE</code> pins the crawler container image used by <code>archive_tool</code>.   Use a digest (<code>...@sha256:...</code>) in production to avoid upstream <code>latest</code> changes breaking crawls.</li> <li><code>HEALTHARCHIVE_REPLAY_BASE_URL</code> enables <code>browseUrl</code> fields in <code>/api/search</code>   and <code>/api/snapshot/{id}</code> so the frontend can embed the replay service.</li> <li><code>HEALTHARCHIVE_USAGE_METRICS_ENABLED</code> controls whether aggregated daily usage   counts are recorded; disable it for a metrics-free deployment.</li> <li><code>HEALTHARCHIVE_CHANGE_TRACKING_ENABLED</code> controls whether change tracking   endpoints/diff feeds are active (disable if you are not running the pipeline).</li> <li>Compare-live controls (public snapshot vs live diffs):</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_ENABLED</code> (default <code>1</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_TIMEOUT_SECONDS</code> (default <code>8</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_REDIRECTS</code> (default <code>4</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_BYTES</code> (default <code>2000000</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_ARCHIVE_BYTES</code> (default <code>2000000</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_RENDER_LINES</code> (default <code>5000</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_CONCURRENCY</code> (default <code>4</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_USER_AGENT</code> (default identifies HealthArchive).</li> <li>Indexing integrity (optional, Phase 4 safety rail):</li> <li><code>HEALTHARCHIVE_INDEX_WARC_VERIFY_LEVEL</code> (default <code>0</code>; allowed: <code>0|1|2</code>).</li> <li><code>HEALTHARCHIVE_INDEX_WARC_VERIFY_MAX_DECOMPRESSED_BYTES</code> (default unset; bounds Level 1 gzip checks per file).</li> <li><code>HEALTHARCHIVE_INDEX_WARC_VERIFY_MAX_RECORDS</code> (default unset; bounds Level 2 WARC iteration per file).</li> <li><code>HEALTHARCHIVE_PUBLIC_SITE_URL</code> sets the public base URL used in RSS links.</li> <li>In <code>production</code> (and <code>staging</code>), if the admin token is missing, admin/metrics   endpoints fail closed (HTTP 500) instead of being left open.</li> <li><code>HEALTHARCHIVE_CORS_ORIGINS</code> should be kept as narrow as possible; it controls   which browser origins can call public API routes.</li> <li>If you use the optional replay banner / direct replay UX, the replay origin   must also be allowed by CORS so the banner can call <code>/api/replay/resolve</code>.</li> </ul>"},{"location":"deployment/environments-and-configuration/#23-optional-staging-backend-future","title":"2.3 Optional: staging backend (future)","text":"<p>If you later add a separate staging backend, it should generally mirror production except for DB/archive root and CORS origins:</p> <pre><code>export HEALTHARCHIVE_ENV=staging\nexport HEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://healtharchive:&lt;DB_PASSWORD&gt;@127.0.0.1:5432/healtharchive_staging\nexport HEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs-staging\nexport HEALTHARCHIVE_ADMIN_TOKEN=&lt;LONG_RANDOM_SECRET&gt;\nexport HEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.vercel.app\nexport HEALTHARCHIVE_LOG_LEVEL=INFO\nexport HEALTHARCHIVE_USAGE_METRICS_ENABLED=1\nexport HEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS=30\nexport HEALTHARCHIVE_CHANGE_TRACKING_ENABLED=1\nexport HEALTHARCHIVE_EXPORTS_ENABLED=1\nexport HEALTHARCHIVE_EXPORTS_DEFAULT_LIMIT=1000\nexport HEALTHARCHIVE_EXPORTS_MAX_LIMIT=10000\nexport HEALTHARCHIVE_PUBLIC_SITE_URL=https://healtharchive.ca\n</code></pre>"},{"location":"deployment/environments-and-configuration/#3-frontend-configuration-healtharchive-frontend","title":"3) Frontend configuration (healtharchive-frontend)","text":"<p>The frontend reads env vars at build time.</p>"},{"location":"deployment/environments-and-configuration/#31-local-development","title":"3.1 Local development","text":"<p>Frontend repo <code>.env.local</code> (git-ignored):</p> <pre><code>NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8001\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=true\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=true\nNEXT_PUBLIC_SHOW_API_BASE_HINT=true\n</code></pre>"},{"location":"deployment/environments-and-configuration/#32-vercel-production-env","title":"3.2 Vercel Production env","text":"<p>In Vercel \u2192 Settings \u2192 Environment Variables \u2192 Production:</p> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api.healtharchive.ca\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=false\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=false\nNEXT_PUBLIC_SHOW_API_BASE_HINT=false\n</code></pre>"},{"location":"deployment/environments-and-configuration/#33-vercel-preview-env","title":"3.3 Vercel Preview env","text":"<p>In Vercel \u2192 Settings \u2192 Environment Variables \u2192 Preview:</p> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api.healtharchive.ca\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=true\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=true\nNEXT_PUBLIC_SHOW_API_BASE_HINT=true\n</code></pre> <p>Note:</p> <ul> <li>Even with the Preview env var set, branch preview URLs may still fall back to   demo mode unless the backend CORS allowlist includes those preview origins.</li> </ul>"},{"location":"deployment/environments-and-configuration/#4-security-notes-secrets-cors","title":"4) Security notes (secrets + CORS)","text":"<ul> <li>Never commit secrets:</li> <li>No real <code>HEALTHARCHIVE_ADMIN_TOKEN</code>, DB passwords, Healthchecks URLs, etc.</li> <li>Use placeholders in docs and store real values in Bitwarden + server/Vercel     env settings.</li> <li>CORS is a security control:</li> <li>Tight allowlists reduce accidental exposure of browser-accessible APIs.</li> <li>If you loosen CORS to include branch previews, do it deliberately and     document the tradeoff.</li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/","title":"Hosting &amp; Live Server TODOs (Backend + Frontend)","text":"<p>This document tracks the remaining infrastructure / hosting steps needed to run HealthArchive.ca with a fully wired frontend + backend in production (and optionally add a staging environment later).</p> <p>Nothing in here requires code changes \u2013 it is all environment configuration, DNS, and manual verification.</p> <p>Note: The current production deployment is a single Hetzner VPS using Tailscale-only SSH, Caddy TLS, and nightly DB backups with an NAS pull. See <code>production-single-vps.md</code> for the exact runbook that was implemented.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#0-quick-index-of-remoteonly-tasks","title":"0. Quick index of \u201cremote\u2011only\u201d tasks","text":"<p>Use this as a map of everything that must be done outside your local dev environment (i.e., on live servers, in Vercel, or in the GitHub UI). Each item links to the detailed checklist later in this file.</p> <ul> <li>On the backend server (production) \u2013 see \u00a72 and \u00a74:</li> <li> Provision a Postgres DB and set <code>HEALTHARCHIVE_DATABASE_URL</code>.</li> <li> Choose and provision storage for <code>HEALTHARCHIVE_ARCHIVE_ROOT</code>.</li> <li> Configure <code>HEALTHARCHIVE_ADMIN_TOKEN</code> and <code>HEALTHARCHIVE_CORS_ORIGINS</code>.</li> <li> Reload/restart the backend service with the new env vars.</li> <li> Verify <code>/api/health</code>, <code>/api/sources</code>, <code>/api/search</code>, and CORS headers         over HTTPS.</li> <li> Ensure HTTPS is enforced (HTTP\u2192HTTPS redirect) and HSTS is enabled for         <code>api.healtharchive.ca</code> (and <code>api-staging.healtharchive.ca</code> only if you         later create a staging API).</li> <li> <p> Configure DNS for <code>api.healtharchive.ca</code> (and optionally         <code>api-staging.healtharchive.ca</code> if you later create a staging API)         pointing at the backend.</p> </li> <li> <p>In Vercel for the frontend \u2013 see \u00a73 and \u00a75:</p> </li> <li> Ensure the <code>healtharchive-frontend</code> GitHub repo is connected to a         Vercel project.</li> <li> Set <code>NEXT_PUBLIC_API_BASE_URL</code> for Production and Preview         environments.</li> <li> Configure diagnostics flags         (<code>NEXT_PUBLIC_SHOW_API_HEALTH_BANNER</code>,         <code>NEXT_PUBLIC_LOG_API_HEALTH_FAILURE</code>,         <code>NEXT_PUBLIC_SHOW_API_BASE_HINT</code>) per environment.</li> <li> <p> Trigger deployments and run the browser\u2011side smoke checks on         <code>/archive</code>, <code>/archive/browse-by-source</code>, and <code>/snapshot/[id]</code>.</p> </li> <li> <p>In GitHub for both repos \u2013 see \u00a77:</p> </li> <li> Commit and push CI workflows:         <code>.github/workflows/backend-ci.yml</code> and         https://github.com/jerdaw/healtharchive-frontend/blob/main/.github/workflows/frontend-ci.yml</li> <li> Enable Actions in the GitHub UI if prompted.</li> <li> Configure branch protection on <code>main</code> to require the CI checks before         merging.</li> </ul> <p>You can tick off these high\u2011level items as you go, using the later sections for the exact commands and UI steps.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#1-decide-canonical-urls-onetime-decision","title":"1. Decide canonical URLs (one\u2011time decision)","text":"<p>Before configuring env vars, confirm the URLs you want to use:</p> <ul> <li>Frontend \u2013 production</li> <li><code>https://healtharchive.ca</code></li> <li> <p><code>https://www.healtharchive.ca</code></p> </li> <li> <p>Frontend \u2013 preview</p> </li> <li><code>https://healtharchive.vercel.app</code> (Vercel default)</li> <li> <p>plus any branch\u2011preview URLs Vercel creates</p> </li> <li> <p>Backend \u2013 production API (current choice: single API for everything)</p> </li> <li> <p><code>https://api.healtharchive.ca</code> (used by both Preview and Production frontends)</p> </li> <li> <p>Backend \u2013 staging API (optional future)</p> </li> <li><code>https://api-staging.healtharchive.ca</code> (only if you later decide you want one)</li> </ul> <p>Once you\u2019re happy with those hostnames, the remaining steps in this document assume that naming. Substitute your actual choices as needed.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#2-backend-configuration-cors-env","title":"2. Backend configuration (CORS + env)","text":"<p>The backend already supports CORS and uses environment variables for its DB and archive root. Production/staging configuration is about setting the right env vars in the host environment and restarting the service.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#21-environment-variables-to-set","title":"2.1. Environment variables to set","text":"<p>On each backend deployment (systemd unit, Docker container, or PaaS app), configure the following environment variables on the remote host (not just in your local shell). Typical flow:</p> <ol> <li>SSH into the server or open your cloud provider\u2019s \u201cenvironment variables\u201d    UI for the backend app.</li> <li>Add/update the variables below.</li> <li> <p>Restart the backend service (see \u00a72.2).</p> </li> <li> <p><code>HEALTHARCHIVE_DATABASE_URL</code></p> </li> <li>Points at your production/staging DB (Postgres recommended).</li> <li> <p>Example:</p> <pre><code>export HEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://user:pass@db-host:5432/healtharchive\n</code></pre> </li> <li> <p><code>HEALTHARCHIVE_ENV</code></p> </li> <li>High\u2011level environment hint used by admin auth.</li> <li>Recommended values:<ul> <li><code>development</code> (or unset) for local dev.</li> <li><code>staging</code> for staging hosts.</li> <li><code>production</code> for production hosts.</li> </ul> </li> <li> <p>When <code>HEALTHARCHIVE_ENV</code> is <code>staging</code> or <code>production</code> and     <code>HEALTHARCHIVE_ADMIN_TOKEN</code> is unset, admin and metrics endpoints fail     closed with HTTP 500 instead of being left open.</p> </li> <li> <p><code>HEALTHARCHIVE_ARCHIVE_ROOT</code></p> </li> <li>Root directory where crawl jobs and WARCs will be written.</li> <li> <p>Must be on a filesystem with enough space and backups appropriate for     your risk tolerance.</p> <pre><code>export HEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs\n</code></pre> </li> <li> <p><code>HEALTHARCHIVE_ADMIN_TOKEN</code></p> </li> <li>Token required for <code>/api/admin/*</code> and <code>/metrics</code> when set.</li> <li> <p>Should be a strong random string, stored only in secure places (not     committed to git).</p> <pre><code>export HEALTHARCHIVE_ADMIN_TOKEN=\"some-long-random-string\"\n</code></pre> </li> <li> <p><code>HEALTHARCHIVE_CORS_ORIGINS</code></p> </li> <li>Critical for frontend integration.</li> <li>Comma\u2011separated list of frontend origins allowed to call the public API.</li> <li>When set, overrides the built\u2011in defaults.</li> </ol> <p>Production example (frontend at <code>healtharchive.ca</code>):</p> <pre><code>export HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.ca,https://www.healtharchive.ca\"\n</code></pre> <p>Staging example (frontend at <code>healtharchive.vercel.app</code>):</p> <pre><code>export HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.vercel.app\"\n</code></pre> <p>Optional local dev access to prod/staging API:</p> <pre><code>export HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.ca,https://www.healtharchive.ca,http://localhost:3000\"\n# or with staging:\nexport HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.vercel.app,http://localhost:3000\"\n</code></pre>"},{"location":"deployment/hosting-and-live-server-to-dos/#22-apply-config-and-restart-services","title":"2.2. Apply config and restart services","text":"<p>How you do this depends on your hosting stack:</p> <ul> <li>systemd unit:</li> <li>Add env vars to the unit file (<code>Environment=</code> lines) or a drop\u2011in     <code>EnvironmentFile=/etc/default/healtharchive-backend</code>.</li> <li> <p>Reload + restart:     <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart healtharchive-backend.service\n</code></pre></p> </li> <li> <p>Docker / Docker Compose:</p> </li> <li>Add env vars under <code>environment:</code> in your compose file or <code>docker run</code>     command.</li> <li> <p>Recreate containers:     <pre><code>docker compose up -d --force-recreate backend\n</code></pre></p> </li> <li> <p>PaaS (Render, Fly.io, Heroku, etc.):</p> </li> <li>Use the provider\u2019s UI/CLI to set env vars.</li> <li>Trigger a deployment or restart.</li> </ul> <p>In staging and production you will typically run two backend processes:</p> <ul> <li>An API process (FastAPI + uvicorn) that serves <code>/api/**</code> and <code>/metrics</code>.</li> <li>A worker process (<code>ha-backend start-worker --poll-interval 30</code>) that   continuously processes queued jobs.</li> </ul> <p>Both processes must see the same <code>HEALTHARCHIVE_DATABASE_URL</code>, <code>HEALTHARCHIVE_ARCHIVE_ROOT</code>, and related env vars from \u00a72.1 so they share jobs and archive output consistently.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#23-backend-smoke-checks-stagingprod","title":"2.3. Backend smoke checks (staging/prod)","text":"<p>From a machine that can reach the backend host:</p> <ol> <li>API health</li> </ol> <pre><code>curl -i \"https://api.healtharchive.ca/api/health\"\n</code></pre> <p>Check:    - HTTP 200.    - JSON body like:      <pre><code>{\"status\":\"ok\",\"checks\":{\"db\":\"ok\",\"jobs\":{...},\"snapshots\":{\"total\":...}}}\n</code></pre></p> <ol> <li> <p>CORS headers</p> </li> <li> <p>Call the API with a fake <code>Origin</code> header matching your frontend:      <pre><code>curl -i \\\n  -H \"Origin: https://healtharchive.ca\" \\\n  \"https://api.healtharchive.ca/api/health\"\n</code></pre></p> </li> <li> <p>Response should include:      <pre><code>Access-Control-Allow-Origin: https://healtharchive.ca\nVary: Origin\n</code></pre></p> </li> <li> <p>Basic public routes</p> </li> <li> <p>Verify:      <pre><code>curl -i \"https://api.healtharchive.ca/api/sources\"\ncurl -i \"https://api.healtharchive.ca/api/search?page=1&amp;pageSize=10\"\n</code></pre></p> </li> <li> <p>Expect HTTP 200, JSON bodies, and CORS headers.</p> </li> <li> <p>Security headers</p> </li> <li> <p>Confirm that security-related headers are present on responses:</p> <pre><code>curl -i \"https://api.healtharchive.ca/api/health\" | sed -n '1,20p'\n</code></pre> </li> <li> <p>Look for:</p> <ul> <li><code>X-Content-Type-Options: nosniff</code></li> <li><code>Referrer-Policy: strict-origin-when-cross-origin</code></li> <li><code>X-Frame-Options: SAMEORIGIN</code></li> <li><code>Permissions-Policy: geolocation=(), microphone=(), camera=()</code></li> </ul> </li> </ol>"},{"location":"deployment/hosting-and-live-server-to-dos/#24-archive-storage-retention","title":"2.4. Archive storage &amp; retention","text":"<p>The <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> directory is where crawl jobs and WARCs live. In staging and production you should treat it as persistent, non\u2011ephemeral storage and have a basic retention plan.</p> <p>Checklist for each non\u2011dev environment:</p> <ul> <li> Place <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> on a filesystem that:</li> <li>Is not ephemeral (survives VM/container restarts).</li> <li>Has enough capacity for expected WARCs and logs.</li> <li>Has a backup or snapshot policy appropriate for your risk tolerance.</li> <li> Decide whether this path is:</li> <li>Backed up regularly (if you want WARCs as part of a disaster\u2011recovery     story), or</li> <li>Treated as \u201cbest\u2011effort cache\u201d (if you rely on ZIMs/exports or other     secondary storage).</li> <li> Decide when it is safe to delete temporary crawl artifacts:</li> <li>Only once jobs are <code>indexed</code> or <code>index_failed</code> and you have verified any     desired ZIMs/exports.</li> <li>Use the <code>ha-backend cleanup-job --id JOB_ID --mode temp</code> command for this     cleanup; it removes <code>.tmp*</code> directories and <code>.archive_state.json</code> but     leaves the main job directory and any final ZIMs.</li> <li>If you are using replay (pywb) for a job, do not run <code>cleanup-job --mode temp</code>     for that job \u2014 replay depends on the WARCs remaining on disk.</li> <li>If replay is enabled globally (<code>HEALTHARCHIVE_REPLAY_BASE_URL</code> is set),     <code>cleanup-job --mode temp</code> will refuse unless you pass <code>--force</code>. Treat     <code>--force</code> as an emergency override (it can break replay by deleting WARCs).</li> <li> For larger deployments, consider:</li> <li>Keeping a simple inventory of jobs (via <code>/api/admin/jobs</code> and metrics) so     you know roughly how many indexed jobs you have and how big <code>jobs/</code> is.</li> <li>Periodically reviewing <code>cleanup_status</code> via <code>/metrics</code>     (<code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"temp_cleaned\"}</code>)     to ensure temp artifacts are being pruned over time.</li> </ul> <p>For local development it is sufficient to keep <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> inside the repo (e.g. <code>&lt;./.dev-archive-root&gt;</code>) and delete it manually when you want a clean slate.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#3-frontend-configuration-vercel-env-vars","title":"3. Frontend configuration (Vercel env vars)","text":"<p>The Next.js app reads <code>NEXT_PUBLIC_API_BASE_URL</code> at build time and uses it for all backend requests. It must be set separately for each environment in Vercel.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#31-production-env-vars-vercel","title":"3.1. Production env vars (Vercel)","text":"<p>In the Vercel dashboard for the <code>healtharchive-frontend</code> project:</p> <ol> <li>Log in to https://vercel.com with the GitHub account that owns    <code>healtharchive-frontend</code>.</li> <li>From the Vercel dashboard, click the healtharchive-frontend project.</li> <li>Go to Settings \u2192 Environment Variables.</li> <li>Under Production, add:</li> </ol> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api.healtharchive.ca\n</code></pre> <ol> <li>(Optional, but recommended) keep diagnostics off in production:</li> </ol> <pre><code>NEXT_PUBLIC_SHOW_API_HEALTH_BANNER=false\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=false\nNEXT_PUBLIC_SHOW_API_BASE_HINT=false\n</code></pre> <ol> <li>Trigger a new deployment of the <code>main</code> branch:</li> <li>Either click Deploy for the latest <code>main</code> commit in Vercel, or push a      new commit to <code>main</code> so Vercel automatically builds and deploys.</li> </ol>"},{"location":"deployment/hosting-and-live-server-to-dos/#32-preview-env-vars-vercel","title":"3.2. Preview env vars (Vercel)","text":"<p>Still in Vercel:</p> <ol> <li>In the same Settings \u2192 Environment Variables screen, switch to the    Preview tab.</li> <li>Under Preview environment variables, add:</li> </ol> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api.healtharchive.ca\n</code></pre> <ol> <li>Enable diagnostics to make issues more obvious:</li> </ol> <pre><code>NEXT_PUBLIC_SHOW_API_HEALTH_BANNER=true\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=true\nNEXT_PUBLIC_SHOW_API_BASE_HINT=true\n</code></pre> <ol> <li>Deploy a preview build (push a commit to a non-<code>main</code> branch) and note the    preview URL Vercel creates.</li> </ol> <p>Expected limitation (by design): because the backend uses a strict CORS    allowlist, branch preview URLs like <code>https://healtharchive-git-...vercel.app</code>    may fall back to demo mode until you explicitly allow those origins.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#33-local-development-env-already-mostly-done","title":"3.3. Local development env (already mostly done)","text":"<p>In the frontend repo <code>.env.local</code> (not committed):</p> <pre><code>NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8001\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=true\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=true\nNEXT_PUBLIC_SHOW_API_BASE_HINT=true\n</code></pre> <p>This is the template for your local dev; Vercel envs for Preview/Production should mirror the same shape but with different API URLs and diagnostics typically disabled in production.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#4-dns-todos","title":"4. DNS TODOs","text":"<p>Ensure DNS records are in place for the backend hosts.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#41-production-api-dns","title":"4.1. Production API DNS","text":"<ul> <li>In your DNS provider\u2019s UI (e.g., Namecheap, Cloudflare, Route 53), locate the   zone for <code>healtharchive.ca</code>.</li> <li>Create a record for <code>api.healtharchive.ca</code>:</li> <li>If the backend is on a VM with a fixed IP:<ul> <li>Add an <code>A</code> record (and <code>AAAA</code> for IPv6 if applicable) pointing to the   backend server IP.</li> </ul> </li> <li>If the backend is behind a load balancer or PaaS:<ul> <li>Add a <code>CNAME</code> pointing at the provider hostname (e.g.,   <code>your-app.region.cloudprovider.com</code>).</li> </ul> </li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/#42-staging-api-dns-optional","title":"4.2. Staging API DNS (optional)","text":"<ul> <li>If you want a separate staging backend, create <code>api-staging.healtharchive.ca</code>   in the same DNS zone:</li> <li>Use an <code>A</code>/<code>AAAA</code> record (for a separate staging VM) or a <code>CNAME</code> (for a     staging app/load balancer) pointing at the staging backend host.</li> </ul> <p>After DNS is configured:</p> <ul> <li>Verify with:   <pre><code>dig +short api.healtharchive.ca\ndig +short api-staging.healtharchive.ca\n</code></pre></li> <li>Then run the API health curl commands in \u00a72.3 against the HTTPS URLs.</li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/#43-tls-https-and-hsts","title":"4.3. TLS / HTTPS and HSTS","text":"<ul> <li>Terminate TLS (HTTPS) for <code>api.healtharchive.ca</code> (and   <code>api-staging.healtharchive.ca</code> if applicable) at your reverse proxy or load   balancer:</li> <li>Use Let's Encrypt or a managed certificate.</li> <li>Configure HTTP\u2192HTTPS redirects for all HTTP traffic.</li> <li>Add an <code>Strict-Transport-Security</code> header on HTTPS responses to enforce   long-lived HTTPS in browsers. For example, in Nginx:</li> </ul> <pre><code>add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n</code></pre> <ul> <li>After enabling HSTS, verify with:</li> </ul> <pre><code>curl -i \"https://api.healtharchive.ca/api/health\" | grep -i strict-transport-security\n</code></pre>"},{"location":"deployment/hosting-and-live-server-to-dos/#5-endtoend-smoke-checklist-stagingprod","title":"5. End\u2011to\u2011end smoke checklist (staging/prod)","text":"<p>Once backend env vars, Vercel env vars, and DNS are in place:</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#51-from-the-frontend-domain","title":"5.1. From the frontend domain","text":"<p>On production (<code>https://healtharchive.ca</code>) and the Vercel domain (<code>https://healtharchive.vercel.app</code>):</p> <ol> <li>Visit <code>/archive</code>:</li> <li>With backend up:<ul> <li>Filters header should show <code>Filters (live API)</code>.</li> <li>If the DB has snapshots, you\u2019ll see real data (no demo fallback notice).</li> </ul> </li> <li> <p>If the backend is unreachable:</p> <ul> <li>Filters header changes to <code>Filters (demo dataset fallback)</code>.</li> <li>Demo records appear instead of live data.</li> <li>A small \u201cBackend unreachable\u201d banner may appear when diagnostics are enabled.</li> </ul> </li> <li> <p>Try filtering:</p> </li> <li>Choose a source, e.g. <code>source=hc</code>.</li> <li>URL updates with <code>?source=hc</code>.</li> <li> <p>Results list changes accordingly (when live snapshots exist).</p> </li> <li> <p>Navigate to <code>/archive/browse-by-source</code>:</p> </li> <li>With backend up:<ul> <li>Cards should show real record counts from <code>/api/sources</code>.</li> </ul> </li> <li> <p>If the backend is unreachable:</p> <ul> <li>\u201cBackend unavailable\u201d callout appears and demo summaries are shown.</li> </ul> </li> <li> <p>Open a snapshot detail page <code>/snapshot/[id]</code>:</p> </li> <li>For a real backend snapshot ID:<ul> <li>Metadata (title, source, date, language, URL) is from <code>/api/snapshot/{id}</code>.</li> <li>\u201cOpen raw snapshot\u201d ultimately points at <code>https://api\u2026/api/snapshots/raw/{id}</code>    on the API host (the frontend prefixes the <code>rawSnapshotUrl</code> path from    the API with <code>NEXT_PUBLIC_API_BASE_URL</code>).</li> </ul> </li> <li>For a demo snapshot ID:<ul> <li>Metadata comes from the bundled demo dataset, and the iframe points    into <code>/demo-archive/**</code>.</li> </ul> </li> </ol>"},{"location":"deployment/hosting-and-live-server-to-dos/#52-console-diagnostics-preview","title":"5.2. Console diagnostics (Preview)","text":"<p>On a Preview deployment, with diagnostics enabled:</p> <ul> <li>Open <code>/archive</code> and check the browser console:</li> <li>You should see something like:     <pre><code>[healtharchive] API base URL (from NEXT_PUBLIC_API_BASE_URL or default): https://api.healtharchive.ca\n</code></pre></li> <li>If the base URL is wrong or the API is unreachable, the health banner and     warning logs will make it obvious.</li> </ul> <p>Production deployments typically keep diagnostics turned off, so you may not see these console logs even when everything is wired correctly.</p> <p>This document should be revisited and checked off as each environment (local, production, and optional future staging) is brought fully online.</p> <p>For a more detailed staging rollout, see:</p> <ul> <li><code>staging-rollout-checklist.md</code></li> </ul> <p>For a more detailed production rollout, see:</p> <ul> <li><code>production-rollout-checklist.md</code></li> </ul> <p>For a more detailed Preview/Production verification of CSP, headers, CORS, and the snapshot viewer iframe behavior, see:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md</li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/#53-monitoring-uptime-checks-optional-but-recommended","title":"5.3. Monitoring &amp; uptime checks (optional but recommended)","text":"<ul> <li>Configure an external uptime monitor (e.g., UptimeRobot, healthchecks.io, or   your cloud provider) to poll:</li> <li><code>https://api.healtharchive.ca/api/health</code> (backend health).</li> <li><code>https://healtharchive.ca/archive</code> (frontend &amp; backend integration).</li> <li>Configure alerts (email/Slack/etc.) for repeated failures or slow responses.</li> <li>If you deploy Prometheus or a similar system, scrape   <code>https://api.healtharchive.ca/metrics</code> and build dashboards/alerts for:</li> <li><code>healtharchive_jobs_total{status=\"failed\"}</code> \u2013 job failures.</li> <li><code>healtharchive_snapshots_total</code> \u2013 sudden jumps in snapshot count.</li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/#6-admin-operator-access-todos","title":"6. Admin / operator access TODOs","text":"<ul> <li> Configure <code>HEALTHARCHIVE_ADMIN_TOKEN</code> in every non\u2011dev environment:</li> <li>Set a long, random value via your hosting platform\u2019s secret manager.</li> <li>Do not commit the token to the repo or to any checked\u2011in <code>.env</code> file.</li> <li> Verify that <code>/api/admin/*</code> and <code>/metrics</code> require the token:</li> <li>Without headers:     <pre><code>curl -i \"https://api.healtharchive.ca/api/admin/jobs\"\ncurl -i \"https://api.healtharchive.ca/metrics\"\n</code></pre>     Expect <code>403 Forbidden</code> when the token is configured.</li> <li>With token:     <pre><code>curl -i \\\n  -H \"Authorization: Bearer $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  \"https://api.healtharchive.ca/api/admin/jobs\"\ncurl -i \\\n  -H \"Authorization: Bearer $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  \"https://api.healtharchive.ca/metrics\"\n</code></pre>     Expect <code>200 OK</code>.</li> <li> Decide how operators will call admin APIs:</li> <li>Short\u2011term: direct <code>curl</code>/CLI usage with the token exported in the shell.</li> <li>Longer\u2011term (optional): a separate admin console (e.g.,     <code>https://admin.healtharchive.ca</code>) that runs in a trusted environment and     never exposes <code>HEALTHARCHIVE_ADMIN_TOKEN</code> to browser JavaScript.</li> <li> If you later add an admin console:</li> <li>Protect it behind SSO, VPN, or other strong authentication.</li> <li>Avoid linking it from the public site navigation.</li> <li>Exclude admin URLs from search indexing (robots.txt and/or <code>&lt;meta&gt;</code> tags).</li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/#7-github-actions-branch-protection-todos","title":"7. GitHub Actions &amp; branch protection TODOs","text":"<p>Continuous integration is wired via workflow files in each repo, but it only becomes effective once you commit/push them and (optionally) protect branches.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#61-enable-and-verify-github-actions","title":"6.1. Enable and verify GitHub Actions","text":"<p>For each repo (<code>healtharchive-backend</code> and <code>healtharchive-frontend</code>):</p> <ol> <li> <p>Ensure the workflow files are present (already true in this repo) and    enabled in the GitHub UI:</p> </li> <li> <p>Navigate to the repository on https://github.com.</p> </li> <li>Click the Actions tab.</li> <li> <p>If GitHub shows a banner like \u201cWorkflows are disabled for this fork,\u201d      click Enable workflows.</p> </li> <li> <p>Push a test commit or re\u2011run the latest workflow to verify that a run is    triggered for branch <code>main</code> and for pull requests:</p> </li> <li> <p>Backend CI should:</p> <ul> <li>Run <code>make check</code>.</li> </ul> </li> <li>Frontend CI should:<ul> <li>Install deps via <code>npm ci</code>.</li> <li>Run <code>npm run check</code>.</li> </ul> </li> </ol>"},{"location":"deployment/hosting-and-live-server-to-dos/#62-configure-branch-protection-optional-but-recommended","title":"6.2. Configure branch protection (optional but recommended)","text":"<p>To prevent merging changes that break tests or linting:</p> <ol> <li>For each GitHub repo, open the repository page and go to    Settings \u2192 Branches.</li> <li> <p>Under Branch protection rules, click Add rule (or edit an existing    rule) and set:</p> </li> <li> <p>Branch name pattern: <code>main</code></p> </li> <li>Enable Require a pull request before merging (tune review settings as      you prefer).</li> <li>Enable Require status checks to pass before merging and select the CI      workflows:<ul> <li>In the backend repo, select the check corresponding to    <code>.github/workflows/backend-ci.yml</code> (e.g., <code>Backend CI</code>).</li> <li>In the frontend repo, select the check corresponding to    https://github.com/jerdaw/healtharchive-frontend/blob/main/.github/workflows/frontend-ci.yml (e.g., <code>Frontend CI</code>).</li> </ul> </li> <li> <p>Optionally enable Include administrators so even admin users must      wait for green CI.</p> </li> <li> <p>Click Create or Save changes to persist the rule.</p> </li> </ol> <p>After this, any PR targeting <code>main</code> will need green CI checks before it can be merged, ensuring that:</p> <ul> <li>Backend changes don\u2019t break the pytest suite.</li> <li>Frontend changes don\u2019t break linting or Vitest tests.</li> </ul>"},{"location":"deployment/pages-table-rollout/","title":"Pages table rollout (browse performance + capture counts)","text":"<p>The backend can optionally maintain a <code>pages</code> table that materializes a per\u2011source \u201cpage\u201d concept (grouped by <code>normalized_url_group</code>) from the raw <code>snapshots</code> table.</p> <p>Important: this is metadata only. It does not modify WARCs, does not delete snapshots, and does not affect replay fidelity.</p>"},{"location":"deployment/pages-table-rollout/#what-it-improves","title":"What it improves","text":"<ul> <li>Browse performance for <code>GET /api/search?view=pages</code> when there is no   search query (and no date range). This avoids expensive window functions over   the entire <code>snapshots</code> table.</li> <li>Adds <code>pageSnapshotsCount</code> to page-browse results so the frontend can show   \u201cCaptures N\u201d.</li> </ul> <p>Keyword searches (<code>q=...</code>) and date-range filters still run directly against <code>snapshots</code> to keep correctness predictable.</p>"},{"location":"deployment/pages-table-rollout/#rollout-steps-production","title":"Rollout steps (production)","text":"<p>1) Apply migrations:</p> <pre><code>./.venv/bin/alembic upgrade head\n</code></pre> <p>2) Backfill the table once:</p> <pre><code>./.venv/bin/ha-backend rebuild-pages --truncate\n</code></pre> <p>Notes:</p> <ul> <li>On Postgres, the CLI may print <code>upserted unknown</code> because the DB driver often   does not report an accurate <code>rowcount</code> for large <code>INSERT ... SELECT</code> statements.   Use the verification steps below (or <code>SELECT count(*) FROM pages;</code>) to confirm   it worked.</li> <li>For large datasets this can take a while; run it in <code>tmux</code> or off-peak.</li> <li>The worker will keep the table updated for newly indexed jobs (incremental   rebuilds happen after indexing).</li> </ul>"},{"location":"deployment/pages-table-rollout/#verification","title":"Verification","text":"<p>1) Confirm pages browse includes capture counts:</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?view=pages&amp;pageSize=1\" | python3 -m json.tool | head\n</code></pre> <p>You should see <code>pageSnapshotsCount</code> as an integer (not <code>null</code>) on results.</p> <p>2) Confirm metrics (admin token required):</p> <pre><code>curl -s https://api.healtharchive.ca/metrics \\\n  -H \"Authorization: Bearer $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  | grep -E \"healtharchive_pages_(table_present|total|fastpath_enabled)|healtharchive_search_mode_total\\\\{mode=\\\\\\\"pages_fastpath\\\\\\\"\\\\}\"\n</code></pre>"},{"location":"deployment/pages-table-rollout/#rollback-safety-valve","title":"Rollback / safety valve","text":"<p>If anything looks suspicious in production (for example: browse ordering or unexpected results), you can disable the fast path without touching data:</p> <p>1) Set <code>HA_PAGES_FASTPATH=0</code> in <code>/etc/healtharchive/backend.env</code> 2) Restart only the API process:</p> <pre><code>sudo systemctl restart healtharchive-api\n</code></pre> <p>This forces <code>view=pages</code> browsing to fall back to snapshot-based grouping.</p>"},{"location":"deployment/production-rollout-checklist/","title":"Production rollout checklist \u2013 backend + frontend","text":"<p>This file is a step\u2011by\u2011step checklist for bringing the production environment online, based on the same patterns used for staging.</p> <p>Assumptions:</p> <ul> <li>Production API host: <code>https://api.healtharchive.ca</code></li> <li>Production frontend: <code>https://healtharchive.ca</code> and <code>https://www.healtharchive.ca</code></li> <li>Code from <code>main</code> in both repos is what you intend to deploy.</li> </ul> <p>Everything here happens on:</p> <ul> <li>The production backend host (VM/container/PaaS).</li> <li>Vercel (for the frontend).</li> <li>GitHub (for CI/branch protection).</li> </ul> <p>Nothing in this file requires changes to your local dev environment.</p> <p>For background, see:</p> <ul> <li><code>hosting-and-live-server-to-dos.md</code></li> <li><code>environments-and-configuration.md</code></li> <li><code>production-single-vps.md</code> (current production runbook)</li> <li><code>staging-rollout-checklist.md</code> (optional future)</li> </ul>"},{"location":"deployment/production-rollout-checklist/#1-backend-production-environment","title":"1. Backend production environment","text":""},{"location":"deployment/production-rollout-checklist/#11-set-env-vars-on-the-production-backend-host","title":"1.1 Set env vars on the production backend host","text":"<p>On the production host (VM/container/PaaS):</p> <ol> <li>Decide where you want production jobs and WARCs to live, e.g.:</li> </ol> <pre><code>/srv/healtharchive/jobs\n</code></pre> <ol> <li>Configure env vars for the backend app (via systemd env file, Docker env,    or PaaS UI). Typical values:</li> </ol> <pre><code>export HEALTHARCHIVE_ENV=production\nexport HEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://user:pass@db-host:5432/healtharchive\nexport HEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs\nexport HEALTHARCHIVE_ADMIN_TOKEN=\"&lt;prod-long-random-secret&gt;\"\nexport HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.ca,https://www.healtharchive.ca\"\nexport HEALTHARCHIVE_LOG_LEVEL=INFO\n</code></pre> <p>Adjust the DB URL and archive root to match your actual production    infrastructure. <code>HEALTHARCHIVE_CORS_ORIGINS</code> should be as narrow as    possible in production: usually just the public frontend origins.</p>"},{"location":"deployment/production-rollout-checklist/#12-run-migrations-and-seed-sources","title":"1.2 Run migrations and seed sources","text":"<p>From a checkout of <code>healtharchive-backend</code> at the deployed revision on the production host:</p> <pre><code>cd /path/to/healtharchive-backend\n\n# Activate venv or ensure dependencies are installed\nalembic upgrade head\nha-backend seed-sources\n</code></pre> <p>This:</p> <ul> <li>Applies all Alembic migrations to the production DB.</li> <li>Ensures baseline <code>Source</code> rows exist (idempotent).</li> </ul> <p>If you have deployed the link-signal schema (tables <code>snapshot_outlinks</code> and <code>page_signals</code>), recompute page signals (includes <code>pagerank</code> when present):</p> <pre><code>ha-backend recompute-page-signals\n</code></pre>"},{"location":"deployment/production-rollout-checklist/#13-start-api-worker-processes","title":"1.3 Start API + worker processes","text":"<p>Configure your process manager to run:</p> <ul> <li>API:</li> </ul> <pre><code>uvicorn ha_backend.api:app --host 0.0.0.0 --port 8001\n</code></pre> <ul> <li>Worker:</li> </ul> <pre><code>ha-backend start-worker --poll-interval 30\n</code></pre> <p>Both processes must see the same <code>HEALTHARCHIVE_*</code> env vars from 1.1.</p> <p>Optional (recommended): enable the blended search ranking by default:</p> <pre><code>export HA_SEARCH_RANKING_VERSION=v2\n</code></pre> <p>Rollback is instant: set <code>HA_SEARCH_RANKING_VERSION=v1</code> and restart the API process.</p>"},{"location":"deployment/production-rollout-checklist/#14-dns-and-tls-for-the-api","title":"1.4 DNS and TLS for the API","text":"<p>In your DNS provider (e.g. Namecheap, Cloudflare, Route 53):</p> <ol> <li>Create/verify records for <code>api.healtharchive.ca</code>:</li> <li><code>A</code> / <code>AAAA</code> pointing at the backend host IP, or</li> <li> <p><code>CNAME</code> pointing at a load balancer / PaaS hostname.</p> </li> <li> <p>Ensure TLS is terminated correctly:</p> </li> <li>Use Let\u2019s Encrypt or a managed certificate for <code>api.healtharchive.ca</code>.</li> <li>Configure HTTP\u2192HTTPS redirects.</li> <li> <p>Add an HSTS header at the reverse proxy/load balancer layer, e.g.:</p> <pre><code>add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n</code></pre> </li> <li> <p>Quick checks from your own machine:</p> </li> </ol> <pre><code>curl -i \"https://api.healtharchive.ca/api/health\"\n\ncurl -i \\\n  -H \"Origin: https://healtharchive.ca\" \\\n  \"https://api.healtharchive.ca/api/health\"\n</code></pre> <p>Verify:</p> <ul> <li>HTTP 200 and <code>\"status\":\"ok\"</code> in the JSON body.</li> <li><code>Access-Control-Allow-Origin: https://healtharchive.ca</code> and <code>Vary: Origin</code>.</li> </ul>"},{"location":"deployment/production-rollout-checklist/#15-seed-initial-production-snapshots","title":"1.5 Seed initial production snapshots","text":"<p>How you seed production is a policy choice; some options:</p> <ul> <li>Use a few small, controlled crawls driven by the worker:</li> <li><code>ha-backend create-job --source hc</code></li> <li><code>ha-backend create-job --source phac</code></li> <li>Let the worker process these jobs and attempt indexing.</li> <li>Use a synthetic WARC snapshot (same pattern as staging) for a minimal   initial smoke test.</li> </ul> <p>At minimum, create one snapshot and note its ID <code>N_prod</code> so you can test the viewer end\u2011to\u2011end:</p> <pre><code>curl -i \"https://api.healtharchive.ca/api/snapshot/&lt;N_prod&gt;\"\ncurl -i \"https://api.healtharchive.ca/api/snapshots/raw/&lt;N_prod&gt;\"\n</code></pre>"},{"location":"deployment/production-rollout-checklist/#2-frontend-production-configuration-vercel","title":"2. Frontend production configuration (Vercel)","text":""},{"location":"deployment/production-rollout-checklist/#21-production-env-vars-vercel","title":"2.1 Production env vars (Vercel)","text":"<p>In the Vercel project for <code>healtharchive-frontend</code>:</p> <ol> <li>Go to Settings \u2192 Environment Variables \u2192 Production.</li> <li>Set:</li> </ol> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api.healtharchive.ca\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=false\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=false\nNEXT_PUBLIC_SHOW_API_BASE_HINT=false\n</code></pre> <ol> <li>Save and trigger a new Production deployment (by pushing to <code>main</code> or    clicking Redeploy for the latest <code>main</code> commit).</li> </ol>"},{"location":"deployment/production-rollout-checklist/#22-frontend-domains","title":"2.2 Frontend domains","text":"<p>In Vercel + DNS:</p> <ul> <li>Ensure:</li> <li><code>healtharchive.ca</code> and <code>www.healtharchive.ca</code> are pointed at Vercel.</li> <li>Any old records (e.g., GitHub Pages IPs) have been removed.</li> </ul> <p>Once the production deployment completes, visiting <code>https://healtharchive.ca</code> should show the live frontend.</p>"},{"location":"deployment/production-rollout-checklist/#3-production-verification-browser","title":"3. Production verification (browser)","text":"<p>With the production backend and frontend deployed:</p>"},{"location":"deployment/production-rollout-checklist/#31-archive-pages","title":"3.1 Archive pages","text":"<ol> <li>Visit:</li> </ol> <pre><code>https://healtharchive.ca/archive\n</code></pre> <ol> <li>Verify:</li> <li>The filters header shows <code>Filters (live API)</code> when the backend is up.</li> <li> <p>If the DB has snapshots, results reflect real data (no demo fallback      notice).</p> </li> <li> <p>Visit:</p> </li> </ol> <pre><code>https://healtharchive.ca/archive/browse-by-source\n</code></pre> <ul> <li>Cards should show real counts from <code>/api/sources</code>.</li> </ul>"},{"location":"deployment/production-rollout-checklist/#32-snapshot-viewer","title":"3.2 Snapshot viewer","text":"<ol> <li>Visit the production snapshot using <code>N_prod</code> from \u00a71.5:</li> </ol> <pre><code>https://healtharchive.ca/snapshot/&lt;N_prod&gt;\n</code></pre> <ol> <li>Confirm:</li> <li>Metadata (title, source, date, language, URL) matches      <code>/api/snapshot/&lt;N_prod&gt;</code>.</li> <li>\u201cOpen raw snapshot\u201d opens <code>https://api.healtharchive.ca/api/snapshots/raw/&lt;N_prod&gt;</code>.</li> <li> <p>The embedded iframe loads the same URL and renders the HTML.</p> </li> <li> <p>In DevTools \u2192 Network:</p> </li> <li>Confirm the iframe request goes to <code>api.healtharchive.ca</code>.</li> <li>Confirm security headers match staging expectations (no      <code>X-Frame-Options</code> on the raw snapshot route; other headers present).</li> </ol>"},{"location":"deployment/production-rollout-checklist/#4-monitoring-ci-signoff","title":"4. Monitoring &amp; CI sign\u2011off","text":"<p>Once production is healthy, tie this back to:</p> <ul> <li>Monitoring &amp; uptime (see <code>hosting-and-live-server-to-dos.md</code> \u00a75.3):</li> <li>Configure uptime checks for:<ul> <li><code>https://api.healtharchive.ca/api/health</code></li> <li><code>https://healtharchive.ca/archive</code></li> </ul> </li> <li> <p>If you have Prometheus or similar, scrape:</p> <ul> <li><code>https://api.healtharchive.ca/metrics</code></li> <li>Build alerts on:</li> <li><code>healtharchive_jobs_total{status=\"failed\"}</code></li> <li><code>healtharchive_snapshots_total</code></li> <li><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"temp_cleaned\"}</code></li> </ul> </li> <li> <p>CI &amp; branch protection (see <code>hosting-and-live-server-to-dos.md</code> \u00a77):</p> </li> <li>Ensure GitHub Actions workflows are enabled and passing.</li> <li>Configure branch protection on <code>main</code> to require CI checks before merging.</li> </ul> <p>With this in place, <code>main</code> deploys cleanly to production, and you have health and metrics coverage for both the API and the frontend.</p>"},{"location":"deployment/production-single-vps/","title":"HealthArchive.ca \u2013 Production on a Single VPS (Hetzner + Tailscale)","text":"<p>This is the record of the current production deployment. It is a single VPS that runs Postgres, the API, the worker, Caddy (TLS), and all archive storage. SSH is private-only via Tailscale; the public internet only sees ports <code>80/443</code>.</p> <p>Use this as the canonical runbook for rebuilding the stack, auditing it, or explaining it to new operators.</p> <p>For recovery from total failure, see the Disaster Recovery Runbook.</p>"},{"location":"deployment/production-single-vps/#1-hosting-topology","title":"1) Hosting / topology","text":"<ul> <li>Provider / size: Hetzner Cloud, <code>cx33</code> (Cost-Optimized, 4 vCPU / 8GB RAM / 80GB SSD)</li> <li>Region: Nuremberg (cost-optimized not available in US-East at the time)</li> <li>Public services: <code>api.healtharchive.ca</code> on 80/443 via Caddy</li> <li>Replay (optional): <code>replay.healtharchive.ca</code> via Caddy \u2192 pywb (see <code>deployment/replay-service-pywb.md</code>)</li> <li>Private-only: SSH on Tailscale (<code>tailscale0</code>), no public port 22</li> <li>Storage:</li> <li><code>/srv/healtharchive/jobs</code> \u2013 archive root (WARCs / job outputs)</li> <li><code>/srv/healtharchive/backups</code> \u2013 DB dumps</li> <li>(Optional) StorageBox mount (cold storage / tiering; not a crawl hot-path)</li> <li>Database: Local Postgres on the VPS</li> <li>Monitoring/alerts:</li> <li>Healthchecks.io pings for DB backup success/failure</li> <li>Healthchecks.io pings for disk-usage threshold</li> <li>(External uptime checks recommended: <code>/api/health</code> and <code>/archive</code>)</li> <li>Backups: Nightly <code>pg_dump -Fc</code> \u2192 <code>/srv/healtharchive/backups</code>, retained 14 days</li> <li>Offsite copy: Synology NAS pulls backups over Tailscale via rsync/SSH</li> </ul>"},{"location":"deployment/production-single-vps/#2-provision-os-hardening-hetzner","title":"2) Provision &amp; OS hardening (Hetzner)","text":"<p>1) Create server:    - Type: Cost-Optimized, x86, <code>cx33</code>    - Region: Nuremberg    - OS: Ubuntu 24.04 LTS    - Attach SSH public key; no password login 2) Hetzner Cloud Firewall (final state):    - Allow TCP 80, 443 (anywhere)    - Allow UDP 41641 (anywhere) for Tailscale    - No public TCP 22 3) OS setup:    - Create <code>haadmin</code> (sudo), disable root SSH login, disable SSH passwords    - Enable <code>unattended-upgrades</code>    - UFW: allow 80/443, allow 22 only on <code>tailscale0</code>, allow 41641/udp</p>"},{"location":"deployment/production-single-vps/#3-runtime-dependencies","title":"3) Runtime dependencies","text":"<p>On the VPS (as <code>haadmin</code>):</p> <pre><code>sudo apt update\nsudo apt -y install docker.io \\\n  postgresql postgresql-contrib \\\n  python3 python3-venv python3-pip \\\n  git curl build-essential pkg-config unzip\nsudo systemctl enable --now docker postgresql\n</code></pre>"},{"location":"deployment/production-single-vps/#swap-recommended-on-cx33","title":"Swap (recommended on cx33)","text":"<p>Annual crawls are long-running and browser-driven; having a small swap file helps avoid OOM-driven churn and reduces time lost to restarts.</p> <p>Recommended on <code>cx33</code> (8GB RAM): add a <code>4G</code> swapfile on the local SSD:</p> <pre><code>sudo fallocate -l 4G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\necho '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab\nswapon --show\n</code></pre> <p>Notes:</p> <ul> <li>Docker Compose is optional for this stack. On Ubuntu 24.04, the packaged   Compose plugin is often <code>docker-compose-v2</code> (not <code>docker-compose-plugin</code>):</li> </ul> <pre><code>sudo apt -y install docker-compose-v2\ndocker compose version\n</code></pre> <p>Directories:</p> <pre><code>sudo groupadd --system healtharchive 2&gt;/dev/null || true\nsudo mkdir -p /srv/healtharchive/jobs /srv/healtharchive/backups /srv/healtharchive/ops\nsudo chown -R haadmin:haadmin /srv/healtharchive/jobs\nsudo chown root:healtharchive /srv/healtharchive/backups\nsudo chmod 2770 /srv/healtharchive/backups\nsudo chown root:healtharchive /srv/healtharchive/ops\nsudo chmod 2770 /srv/healtharchive/ops\n</code></pre> <p>Ops directories (public-safe logs + artifacts):</p> <ul> <li><code>root:healtharchive</code> ownership + <code>2770</code> perms is intentional:</li> <li><code>root</code> owns the directory tree</li> <li>operators (e.g., <code>haadmin</code>) write via the <code>healtharchive</code> group</li> <li>the setgid bit keeps group ownership consistent on new files/dirs</li> </ul> <p>Create the standard subdirectories:</p> <pre><code>sudo mkdir -p \\\n  /srv/healtharchive/ops/baseline \\\n  /srv/healtharchive/ops/restore-tests \\\n  /srv/healtharchive/ops/adoption \\\n  /srv/healtharchive/ops/search-eval\nsudo chown -R root:healtharchive /srv/healtharchive/ops\nsudo chmod 2770 /srv/healtharchive/ops /srv/healtharchive/ops/*\n</code></pre> <p>Postgres:</p> <pre><code>sudo -u postgres psql -c \"CREATE USER healtharchive WITH PASSWORD '&lt;DB_PASSWORD&gt;';\"\nsudo -u postgres psql -c \"CREATE DATABASE healtharchive OWNER healtharchive;\"\n</code></pre>"},{"location":"deployment/production-single-vps/#4-backend-deploy-api-worker-systemd","title":"4) Backend deploy (API + worker, systemd)","text":"<p>Clone + venv:</p> <pre><code>sudo mkdir -p /opt &amp;&amp; sudo chown haadmin:haadmin /opt\ngit clone https://github.com/jerdaw/healtharchive-backend.git /opt/healtharchive-backend\ncd /opt/healtharchive-backend\npython3 -m venv .venv\n./.venv/bin/pip install --upgrade pip\n./.venv/bin/pip install -e \".[dev]\" \"psycopg[binary]\"\n</code></pre> <p>Env file (root-owned, group-readable):</p> <pre><code>sudo groupadd --system healtharchive 2&gt;/dev/null || true\nsudo usermod -aG healtharchive haadmin\nsudo install -d -m 750 -o root -g healtharchive /etc/healtharchive\nsudo tee /etc/healtharchive/backend.env &gt;/dev/null &lt;&lt;'EOF'\nHEALTHARCHIVE_ENV=production\nHEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://healtharchive:&lt;DB_PASSWORD&gt;@127.0.0.1:5432/healtharchive\n# Keep the crawl hot-path on the local SSD for throughput; use the StorageBox only for cold storage/tiering.\nHEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs\nHEALTHARCHIVE_ADMIN_TOKEN=&lt;LONG_RANDOM_TOKEN&gt;\nHEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.ca,https://www.healtharchive.ca,https://healtharchive.vercel.app,https://replay.healtharchive.ca\nHEALTHARCHIVE_LOG_LEVEL=INFO\nHA_SEARCH_RANKING_VERSION=v2\nHA_PAGES_FASTPATH=1\n\n# Optional: aggregated, privacy-preserving usage metrics (daily counts only).\n# Drives the public reporting pages (`/status` and `/impact`) via `GET /api/usage`.\nHEALTHARCHIVE_USAGE_METRICS_ENABLED=1\nHEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS=30\n\n# Optional: change tracking + diff feeds.\nHEALTHARCHIVE_CHANGE_TRACKING_ENABLED=1\n\n# Optional: compare-live (snapshot vs current live page).\n# Defaults are safe, but you can tune these if needed.\nHEALTHARCHIVE_COMPARE_LIVE_ENABLED=1\nHEALTHARCHIVE_COMPARE_LIVE_TIMEOUT_SECONDS=8\nHEALTHARCHIVE_COMPARE_LIVE_MAX_REDIRECTS=4\nHEALTHARCHIVE_COMPARE_LIVE_MAX_BYTES=2000000\nHEALTHARCHIVE_COMPARE_LIVE_MAX_ARCHIVE_BYTES=2000000\nHEALTHARCHIVE_COMPARE_LIVE_MAX_RENDER_LINES=5000\nHEALTHARCHIVE_COMPARE_LIVE_MAX_CONCURRENCY=4\n# HEALTHARCHIVE_COMPARE_LIVE_USER_AGENT=HealthArchiveCompareLive/1.0 (+https://healtharchive.ca)\n\n# Optional: research exports.\n# Controls the public metadata export endpoints under `/api/exports`.\nHEALTHARCHIVE_EXPORTS_ENABLED=1\nHEALTHARCHIVE_EXPORTS_DEFAULT_LIMIT=1000\nHEALTHARCHIVE_EXPORTS_MAX_LIMIT=10000\n\n# Public site base URL for RSS feed links and public compare URLs.\nHEALTHARCHIVE_PUBLIC_SITE_URL=https://healtharchive.ca\n\n# Optional: replay integration (pywb). Enables `browseUrl` fields in the public API.\n# HEALTHARCHIVE_REPLAY_BASE_URL=https://replay.healtharchive.ca\n\n# Optional: cached replay preview images (homepage thumbnails for /archive cards).\n# HEALTHARCHIVE_REPLAY_PREVIEW_DIR=/srv/healtharchive/replay/previews\nEOF\nsudo chown root:healtharchive /etc/healtharchive/backend.env\nsudo chmod 640 /etc/healtharchive/backend.env\n</code></pre> <p>Migrate + seed:</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n./.venv/bin/alembic upgrade head\n./.venv/bin/ha-backend seed-sources\n./.venv/bin/ha-backend recompute-page-signals\n./.venv/bin/ha-backend rebuild-pages --truncate\n</code></pre> <p>Systemd services:</p> <ul> <li>API: <code>/etc/systemd/system/healtharchive-api.service</code></li> <li><code>ExecStart=/opt/healtharchive-backend/.venv/bin/uvicorn ha_backend.api:app --host 127.0.0.1 --port 8001</code></li> <li><code>EnvironmentFile=/etc/healtharchive/backend.env</code></li> <li>Worker: <code>/etc/systemd/system/healtharchive-worker.service</code></li> <li><code>ExecStart=/opt/healtharchive-backend/.venv/bin/ha-backend start-worker --poll-interval 30</code></li> </ul> <p>Optional systemd automation (recommended):</p> <ul> <li>Install/update systemd unit templates from this repo:</li> <li><code>./scripts/vps-install-systemd-units.sh --apply</code></li> <li>Templates + enablement steps: <code>deployment/systemd/README.md</code></li> <li>Baseline drift check timer (weekly; low-risk, recommended):</li> <li>Templates + enablement steps: <code>deployment/systemd/README.md</code></li> <li>Annual scheduling timer (Jan 01 UTC) + worker priority drop-in:</li> <li>Templates + install steps: <code>deployment/systemd/README.md</code></li> <li>Replay reconciliation timer (pywb indexing; capped, optional):</li> <li>Templates + install steps: <code>deployment/systemd/README.md</code></li> <li>Change tracking timer (edition-aware diffs; capped):</li> <li>Templates + install steps: <code>deployment/systemd/README.md</code></li> <li>Annual search verification capture (optional; safe):</li> <li>Templates + enablement steps: <code>deployment/systemd/README.md</code></li> </ul> <p>Enable + start:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable --now healtharchive-api healtharchive-worker\ncurl -i http://127.0.0.1:8001/api/health\n</code></pre> <p>Routine deploys (after initial install):</p> <pre><code>cd /opt/healtharchive-backend\n\n# Dry-run (prints actions):\n./scripts/vps-deploy.sh\n\n# Deploy latest main (fast-forward only):\n./scripts/vps-deploy.sh --apply\n\n# Deploy pinned commit:\n./scripts/vps-deploy.sh --apply --ref &lt;GIT_SHA&gt;\n</code></pre> <p>Notes:</p> <ul> <li>The deploy script runs a baseline drift check by default to catch   misconfiguration (filesystem perms, systemd enablement, env allowlists, etc.).</li> <li>Artifacts are written to: <code>/srv/healtharchive/ops/baseline/</code></li> <li>You can skip in emergencies: <code>./scripts/vps-deploy.sh --apply --skip-baseline-drift</code></li> <li>To include live HTTPS checks (HSTS, CORS headers, admin/metrics auth): <code>./scripts/vps-deploy.sh --apply --baseline-mode live</code></li> <li>If you update systemd unit templates in the repo, you can apply them during deploy:</li> <li><code>./scripts/vps-deploy.sh --apply --install-systemd-units</code></li> <li>If you update Prometheus alert rules, you can apply them during deploy:</li> <li><code>./scripts/vps-deploy.sh --apply --apply-alerting</code></li> <li>Requires alerting to be configured (webhook secret present at <code>/etc/healtharchive/observability/alertmanager_webhook_url</code>).</li> <li>The baseline policy (desired state) is versioned in git at:   <code>docs/operations/production-baseline-policy.toml</code></li> <li>The deploy script runs a public-surface smoke verify by default (public API + frontend + replay + usage):</li> <li><code>./scripts/verify_public_surface.py</code> (defaults to <code>https://api.healtharchive.ca</code> and <code>https://www.healtharchive.ca</code>)</li> <li>You can skip in emergencies: <code>./scripts/vps-deploy.sh --apply --skip-public-surface-verify</code></li> </ul>"},{"location":"deployment/production-single-vps/#41-observability-prometheus-grafana-operator-only","title":"4.1) Observability (Prometheus + Grafana; operator-only)","text":"<p>This is the private ops stack:</p> <ul> <li>Prometheus collects metrics (backend + host + Postgres exporters).</li> <li>Grafana shows dashboards (\u201cprivate stats\u201d).</li> <li>Alertmanager sends alerts to one operator channel (via the webhook relay).</li> </ul> <p>The important safety rule:</p> <ul> <li>These services bind to <code>127.0.0.1</code> on the VPS (loopback-only) and are accessed over the tailnet (Tailscale) using an SSH port-forward.</li> <li>Do not add Caddy vhosts for Prometheus/Grafana (keep them off the public internet).</li> </ul> <p>Install flow (VPS):</p> <ul> <li>Follow the observability playbooks under <code>docs/operations/playbooks/</code>:</li> <li><code>docs/operations/playbooks/observability-bootstrap.md</code></li> <li><code>docs/operations/playbooks/observability-exporters.md</code></li> <li><code>docs/operations/playbooks/observability-prometheus.md</code></li> <li><code>docs/operations/playbooks/observability-grafana.md</code></li> <li><code>docs/operations/playbooks/observability-dashboards.md</code></li> <li><code>docs/operations/playbooks/observability-alerting.md</code></li> <li><code>docs/operations/playbooks/observability-maintenance.md</code></li> </ul> <p>Where things live (VPS):</p> <ul> <li>Secrets (never commit): <code>/etc/healtharchive/observability/</code></li> <li>Prometheus config: <code>/etc/prometheus/prometheus.yml</code> and <code>/etc/prometheus/rules/</code></li> <li>Alertmanager config: <code>/etc/prometheus/alertmanager.yml</code></li> <li>Grafana dashboards provisioning: <code>/etc/grafana/provisioning/dashboards/healtharchive.yaml</code></li> <li>Public-safe dashboard JSON + ops artifacts: <code>/srv/healtharchive/ops/observability/</code></li> </ul> <p>Access from your laptop (via tailnet-only SSH):</p> <pre><code># Tunnel Grafana + Prometheus + admin proxy to your local machine.\n# Keep this terminal open.\nssh -N \\\n  -L 3000:127.0.0.1:3000 \\\n  -L 9090:127.0.0.1:9090 \\\n  -L 8002:127.0.0.1:8002 \\\n  haadmin@&lt;vps-tailscale-ip&gt;\n</code></pre> <p>Then open on your laptop:</p> <ul> <li>Grafana: <code>http://127.0.0.1:3000/</code></li> <li>Prometheus UI (optional): <code>http://127.0.0.1:9090/</code></li> <li>Admin proxy (operator triage; browser-friendly): <code>http://127.0.0.1:8002/</code></li> </ul> <p>Restart services (VPS):</p> <pre><code>sudo systemctl restart \\\n  prometheus \\\n  prometheus-alertmanager \\\n  prometheus-node-exporter \\\n  prometheus-postgres-exporter \\\n  grafana-server \\\n  healtharchive-pushover-relay \\\n  healtharchive-admin-proxy\n</code></pre>"},{"location":"deployment/production-single-vps/#42-storage-box-sshfs-stale-mount-failures-errno-107","title":"4.2) Storage Box / <code>sshfs</code> stale mount failures (Errno 107)","text":"<p>HealthArchive uses a Storage Box (via <code>sshfs</code>) as a cold tier in production (WARC tiering).</p> <p>Important failure mode:</p> <ul> <li>A mount can appear \u201cpresent\u201d but be stale/unreadable, causing:</li> <li><code>OSError: [Errno 107] Transport endpoint is not connected</code></li> </ul> <p>This can break:</p> <ul> <li>crawl progress metrics,</li> <li>archive job output dirs under <code>/srv/healtharchive/jobs/**</code>,</li> <li>and the worker/job lifecycle.</li> </ul> <p>Fast triage:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-crawl-status.sh --year \"$(date -u +%Y)\"\nls -la /srv/healtharchive/storagebox &gt;/dev/null &amp;&amp; echo \"OK: storagebox readable\" || echo \"BAD: storagebox unreadable\"\nmount | rg '/srv/healtharchive/jobs/|/srv/healtharchive/storagebox'\n</code></pre> <p>Recovery playbook:</p> <ul> <li><code>../operations/playbooks/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"deployment/production-single-vps/#5-https-dns-caddy","title":"5) HTTPS + DNS (Caddy)","text":"<p>1) DNS (Namecheap): <code>A api.healtharchive.ca -&gt; &lt;VPS_PUBLIC_IP&gt;</code> 2) Install Caddy: <code>sudo apt -y install caddy</code> 3) Caddyfile: <code>/etc/caddy/Caddyfile</code></p> <pre><code>api.healtharchive.ca {\n  header Strict-Transport-Security \"max-age=31536000\"\n  reverse_proxy 127.0.0.1:8001\n}\n</code></pre> <p>4) Validate + reload:</p> <pre><code>sudo caddy fmt --overwrite /etc/caddy/Caddyfile\nsudo caddy validate --config /etc/caddy/Caddyfile\nsudo systemctl reload caddy\n</code></pre> <p>Verify:</p> <pre><code>curl -i https://api.healtharchive.ca/api/health\n</code></pre>"},{"location":"deployment/production-single-vps/#51-optional-replay-service-pywb","title":"5.1) Optional: replay service (pywb)","text":"<p>Full-fidelity browsing (CSS/JS/images) requires a replay engine. If you want \u201cclick links and stay inside the archived backup\u201d, deploy pywb behind Caddy:</p> <ul> <li>Runbook: <code>deployment/replay-service-pywb.md</code></li> </ul> <p>Operational warning:</p> <ul> <li><code>ha-backend cleanup-job --mode temp</code> removes temp dirs including WARCs.   Replay depends on WARCs staying on disk, so do not run cleanup for any job   you intend to keep replayable.   If replay is enabled globally (<code>HEALTHARCHIVE_REPLAY_BASE_URL</code> is set),   <code>cleanup-job --mode temp</code> will refuse unless you pass <code>--force</code>.</li> </ul> <p>Optional UX improvement:</p> <ul> <li>If <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code> is configured, the API can serve cached   PNG \u201chomepage previews\u201d used by the frontend on <code>/archive</code>.   See <code>deployment/replay-service-pywb.md</code> (\u201cCached source preview images\u201d) for   the generation command.</li> </ul>"},{"location":"deployment/production-single-vps/#6-tailscale-sshprivate-access-only","title":"6) Tailscale (SSH/private access only)","text":"<ul> <li>Installed on VPS, NAS, and admin workstation.</li> <li>VPS Tailscale IP: <code>100.x.y.z</code> (example)</li> <li>SSH only allowed on <code>tailscale0</code> in UFW; public port 22 blocked at Hetzner.</li> <li>Hetzner firewall adds UDP 41641 for better Tailscale connectivity.</li> <li>Recommended: disable Tailscale key expiry for the VPS and NAS devices in the   Tailscale admin UI so access does not silently expire.</li> </ul> <p>Usage:</p> <pre><code>ssh -i ~/.ssh/healtharchive_hetzner haadmin@100.x.y.z\n</code></pre> <p>Public SSH: - Expected to fail: <code>ssh haadmin@api.healtharchive.ca</code> (closed).</p>"},{"location":"deployment/production-single-vps/#7-backups-nas-pull-rsync-over-tailscale","title":"7) Backups + NAS pull (rsync over Tailscale)","text":"<p>VPS backup user: - <code>habackup</code> user with NAS public key in <code>/home/habackup/.ssh/authorized_keys</code></p> <p>Backup script: <code>/usr/local/bin/healtharchive-db-backup</code> - <code>pg_dump -Fc</code> to <code>/srv/healtharchive/backups/healtharchive_&lt;ts&gt;.dump</code> - 14-day retention - Healthchecks <code>/start</code>/<code>/fail</code>/success pings (see \u00a78)</p> <p>Systemd: - <code>/etc/systemd/system/healtharchive-db-backup.service</code> - <code>/etc/systemd/system/healtharchive-db-backup.timer</code> (daily ~03:30 UTC, randomized delay)</p> <p>NAS pull: - NAS key: <code>~/.ssh/ha_backup_nas</code> (no passphrase) - SSH config alias on NAS:</p> <pre><code>Host ha-vps\n  HostName 100.x.y.z\n  User habackup\n  IdentityFile ~/.ssh/ha_backup_nas\n  IdentitiesOnly yes\n  StrictHostKeyChecking accept-new\n</code></pre> <ul> <li>Rsync command (used manually + DSM scheduled task):</li> </ul> <pre><code>rsync -av --delete ha-vps:/srv/healtharchive/backups/ /volume1/nobak/healtharchive/backups/db/\n</code></pre>"},{"location":"deployment/production-single-vps/#8-healthchecksio-backup-disk","title":"8) Healthchecks.io (backup + disk)","text":"<p>Secrets file: <code>/etc/healtharchive/healthchecks.env</code> (mode 600)</p> <p>Notes:</p> <ul> <li>This env file may also be used by the newer systemd unit templates under   <code>docs/deployment/systemd/</code> (which use <code>HEALTHARCHIVE_HC_PING_*</code> variable names).   It is OK to keep both sets of variables in the same file.</li> <li>Avoid placeholder values like <code>https://hc-ping.com/&lt;uuid&gt;</code> in this file if you   ever <code>source</code> it from bash; the <code>&lt;</code>/<code>&gt;</code> characters can break shell parsing.</li> </ul> <pre><code>HC_DB_BACKUP_URL=https://hc-ping.com/UUID_HERE\nHC_DISK_URL=https://hc-ping.com/UUID_HERE\nHC_DISK_THRESHOLD=80\n</code></pre> <p>Disk check: - Script: <code>/usr/local/bin/healtharchive-disk-check</code> - Service/Timer: <code>healtharchive-disk-check.service</code> / <code>healtharchive-disk-check.timer</code> (hourly) - Pings success; sends <code>/fail</code> if <code>/</code> or <code>/srv/healtharchive</code> exceeds 80%.</p>"},{"location":"deployment/production-single-vps/#9-synthetic-snapshot-for-smoke-testing","title":"9) Synthetic snapshot for smoke testing","text":"<p>Created a minimal WARC + Snapshot for smoke checks: - WARC: <code>/srv/healtharchive/jobs/manual-warcs/viewer-test.warc.gz</code> - Snapshot ID: <code>1</code> - Raw: <code>https://api.healtharchive.ca/api/snapshots/raw/1</code> - Viewer: <code>https://www.healtharchive.ca/snapshot/1</code></p> <p>Use this to verify end-to-end viewer behavior after deploys.</p>"},{"location":"deployment/production-single-vps/#10-restore-drill-completed","title":"10) Restore drill (completed)","text":"<p>Procedure:</p> <pre><code>latest=\"$(ls -t /srv/healtharchive/backups/healtharchive_*.dump | head -n 1)\"\nsudo -u postgres dropdb --if-exists healtharchive_restore_test\nsudo -u postgres createdb healtharchive_restore_test\nsudo -u postgres pg_restore --no-owner --no-acl -d healtharchive_restore_test &lt; \"$latest\"\nsudo -u postgres psql -d healtharchive_restore_test -c \"select count(*) from snapshots;\"\nsudo -u postgres dropdb healtharchive_restore_test\n</code></pre> <p>Result: restore succeeded, <code>snapshots</code> contained 1 row (the synthetic test snapshot).</p>"},{"location":"deployment/production-single-vps/#11-external-uptime-checks-recommended","title":"11) External uptime checks (recommended)","text":"<p>Configure an external monitor (e.g., UptimeRobot) for: - <code>https://api.healtharchive.ca/api/health</code> - <code>https://www.healtharchive.ca/archive</code> - (Optional) <code>https://replay.healtharchive.ca/</code> (if replay is enabled/in use)</p> <p>Note: some providers use <code>HEAD</code> by default; the backend supports <code>HEAD /api/health</code>.</p> <p>For a more detailed, step-by-step checklist (including branch protection / CI enforcement), see:</p> <ul> <li><code>../operations/monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"deployment/production-single-vps/#12-current-known-defaultsassumptions-2025-12","title":"12) Current known defaults/assumptions (2025-12)","text":"<ul> <li>CORS allowlist: <code>https://healtharchive.ca</code>, <code>https://www.healtharchive.ca</code>, <code>https://healtharchive.vercel.app</code></li> <li>Vercel envs set to use <code>https://api.healtharchive.ca</code> for both Preview and Production</li> <li>No staging backend; Preview and Production frontends point to the same API</li> <li>Public SSH closed; Tailscale required for admin/backup access</li> </ul>"},{"location":"deployment/replay-service-pywb/","title":"HealthArchive \u2013 Replay Service (pywb) runbook","text":"<p>This document covers setting up full\u2011fidelity web replay (HTML + CSS/JS/images/fonts) for HealthArchive using a dedicated pywb service behind Caddy.</p> <p>It is intentionally written so a future operator can follow it without needing additional context.</p>"},{"location":"deployment/replay-service-pywb/#0-what-this-is-and-is-not","title":"0) What this is (and is not)","text":"<p>What this provides</p> <ul> <li>A replay origin: <code>https://replay.healtharchive.ca</code></li> <li>Wayback\u2011style replay from the project\u2019s WARC files</li> <li>Natural browsing: links stay inside replay, and captured assets (CSS/JS/images) load from the archive when present</li> </ul> <p>What this does not provide</p> <ul> <li>Guaranteed completeness. If a page depends on third\u2011party CSS/JS/images that were   not captured into the WARCs, those assets will still be missing at replay time.</li> <li>A custom replay UI in pywb itself. HealthArchive provides the primary browsing   experience via the frontend wrapper pages (see \u201cBackend wiring\u201d below).</li> </ul>"},{"location":"deployment/replay-service-pywb/#1-core-decisions-contract","title":"1) Core decisions (contract)","text":""},{"location":"deployment/replay-service-pywb/#11-collections-are-per-archivejob","title":"1.1 Collections are per ArchiveJob","text":"<ul> <li>Each <code>ArchiveJob</code> becomes a replay \u201cedition\u201d.</li> <li> <p>Collection name is stable and mechanical:</p> </li> <li> <p><code>job-&lt;job_id&gt;</code> (example: <code>job-1</code>)</p> </li> </ul> <p>This makes it easy to generate replay URLs from DB data later.</p>"},{"location":"deployment/replay-service-pywb/#12-replay-url-format","title":"1.2 Replay URL format","text":"<p>We will use pywb\u2019s standard collection routing.</p> <p>Replay latest capture (most common):</p> <pre><code>https://replay.healtharchive.ca/&lt;collection&gt;/&lt;original_url&gt;\n</code></pre> <p>List captures (\u201ccalendar\u201d / capture list UI):</p> <pre><code>https://replay.healtharchive.ca/&lt;collection&gt;/*/&lt;original_url&gt;\n</code></pre> <p>Replay closest capture to a timestamp (14-digit UTC <code>YYYYMMDDhhmmss</code>):</p> <pre><code>https://replay.healtharchive.ca/&lt;collection&gt;/&lt;timestamp&gt;/&lt;original_url&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;collection&gt;</code> is <code>job-&lt;job_id&gt;</code></li> <li><code>&lt;timestamp&gt;</code> is UTC in <code>YYYYMMDDhhmmss</code> (14 digits)</li> </ul> <p>Example (latest capture):</p> <pre><code>https://replay.healtharchive.ca/job-1/https://www.canada.ca/en/health-canada.html\n</code></pre> <p>Note: HealthArchive\u2019s public API generates timestamp-locked replay URLs for snapshots (the <code>&lt;timestamp&gt;</code> form) so the viewer stays anchored to the capture time as you navigate within the backup.</p>"},{"location":"deployment/replay-service-pywb/#13-retention-warning-replay-depends-on-warcs-staying-on-disk","title":"1.3 Retention warning: replay depends on WARCs staying on disk","text":"<p>Replay reads from the WARC files referenced by each job.</p> <p>Important: <code>ha-backend cleanup-job --mode temp</code> currently removes archive_tool temp dirs including WARCs (see <code>src/ha_backend/cli.py:cmd_cleanup_job</code>).</p> <p>If you run cleanup on a replayable job, replay will break.</p> <p>Operational rule for now: do not run <code>cleanup-job --mode temp</code> for any job you want replayable.</p> <p>When replay is enabled (backend env var <code>HEALTHARCHIVE_REPLAY_BASE_URL</code> is set), <code>cleanup-job --mode temp</code> will refuse to run unless you pass <code>--force</code>.</p> <p>This rule is repeated in:</p> <ul> <li><code>docs/deployment/production-single-vps.md</code></li> <li><code>docs/deployment/hosting-and-live-server-to-dos.md</code></li> <li><code>docs/development/live-testing.md</code></li> </ul>"},{"location":"deployment/replay-service-pywb/#2-dns-tls","title":"2) DNS + TLS","text":"<p>Create DNS:</p> <ul> <li><code>A replay.healtharchive.ca -&gt; &lt;VPS_PUBLIC_IP&gt;</code></li> </ul> <p>TLS is handled by Caddy automatically once the site block exists.</p> <p>Before continuing, SSH to the VPS as your admin user (typically over Tailscale):</p> <pre><code>ssh -i ~/.ssh/healtharchive_hetzner haadmin@&lt;VPS_TAILSCALE_IP&gt;\n</code></pre>"},{"location":"deployment/replay-service-pywb/#3-vps-directory-layout","title":"3) VPS directory layout","text":"<p>On the VPS:</p> <ul> <li>WARCs/job outputs already live under:</li> <li><code>/srv/healtharchive/jobs</code></li> <li>Replay service state (config, collections, indexes) will live under:</li> <li><code>/srv/healtharchive/replay</code></li> </ul> <p>Create directories:</p> <pre><code>sudo mkdir -p /srv/healtharchive/replay\nsudo mkdir -p /srv/healtharchive/replay/collections\n</code></pre> <p>Create a dedicated system user for the replay volume:</p> <pre><code>sudo adduser --system --no-create-home --ingroup healtharchive hareplay\n</code></pre> <p>Recommended perms (important):</p> <pre><code>sudo chown -R hareplay:healtharchive /srv/healtharchive/replay\nsudo chmod 2770 /srv/healtharchive/replay /srv/healtharchive/replay/collections\n</code></pre> <p>Why the <code>hareplay</code> ownership matters:</p> <ul> <li>Your WARC files are typically <code>640</code> and group-owned by <code>healtharchive</code>.</li> <li>The pywb container is hardened with <code>--cap-drop=ALL</code>, which means \u201croot\u201d in   the container cannot bypass Unix permissions (no <code>CAP_DAC_OVERRIDE</code>).</li> <li>We will also run the container as the <code>hareplay</code> UID/GID explicitly (below),   so pywb can:</li> <li>write its indexes under <code>/webarchive</code> (owned by <code>hareplay:healtharchive</code>)</li> <li>read group-readable WARCs under <code>/warcs</code> (group <code>healtharchive</code>)</li> </ul>"},{"location":"deployment/replay-service-pywb/#4-pywb-container-deployment-systemd-docker","title":"4) pywb container deployment (systemd + Docker)","text":"<p>We run pywb only on localhost (Caddy is the public edge).</p>"},{"location":"deployment/replay-service-pywb/#41-create-pywb-config","title":"4.1 Create pywb config","text":"<p>Create <code>/srv/healtharchive/replay/config.yaml</code>:</p> <pre><code>debug: false\n\n# We embed replay inside a HealthArchive wrapper UI later; disable pywb\u2019s framed\n# replay chrome so the page itself renders \u201cas captured\u201d.\nframed_replay: false\n\n# Prefer stable URLs once a capture is resolved.\nredirect_to_exact: true\n\n# Optional: expose an aggregate across all on-disk collections at `/all/...`.\n# (This is not required for per-job collections like `/job-1/...`.)\n# collections:\n#   all: $all\n</code></pre>"},{"location":"deployment/replay-service-pywb/#42-create-systemd-service","title":"4.2 Create systemd service","text":"<p>Create <code>/etc/systemd/system/healtharchive-replay.service</code>:</p> <pre><code>[Unit]\nDescription=HealthArchive replay (pywb)\nAfter=network.target docker.service\nRequires=docker.service\n\n[Service]\nType=simple\nRestart=always\nRestartSec=3\n\n# Safety: start clean\nExecStartPre=-/usr/bin/docker rm -f healtharchive-replay\nExecStartPre=/usr/bin/docker pull webrecorder/pywb:2.9.1\n\n# Run on localhost only; Caddy terminates TLS publicly.\nExecStart=/usr/bin/docker run --rm --name healtharchive-replay \\\n  -p 127.0.0.1:8090:8080 \\\n  --user &lt;HAREPLAY_UID&gt;:&lt;HEALTHARCHIVE_GID&gt; \\\n  --cap-drop=ALL \\\n  --security-opt no-new-privileges:true \\\n  -v /srv/healtharchive/replay:/webarchive:rw \\\n  -v /srv/healtharchive/jobs:/warcs:ro,rshared \\\n  webrecorder/pywb:2.9.1\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Notes:</p> <ul> <li><code>HAREPLAY_UID</code> comes from <code>id -u hareplay</code> (often <code>110</code>).</li> <li><code>HEALTHARCHIVE_GID</code> comes from <code>getent group healtharchive</code> (3rd <code>:</code>-separated field).</li> <li>We run as <code>hareplay:healtharchive</code> to avoid the container needing to   <code>useradd</code>/<code>su</code> internally (which fails when <code>--cap-drop=ALL</code> removes   <code>CAP_SETUID</code>/<code>CAP_SETGID</code>).</li> <li>The <code>rshared</code> bind propagation on <code>/srv/healtharchive/jobs</code> helps pywb see   nested mounts under that tree (e.g., Storage Box tiering bind mounts) without   requiring a container restart after mount repairs.</li> </ul> <p>Enable + start:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable --now healtharchive-replay.service\nsudo systemctl status healtharchive-replay.service --no-pager\n</code></pre> <p>Local check (on the VPS):</p> <pre><code>curl -I http://127.0.0.1:8090/ | head\n</code></pre> <p>If <code>wb-manager reindex</code> fails with <code>Permission denied</code>:</p> <ul> <li>Double-check:</li> <li><code>/srv/healtharchive/replay</code> is owned by <code>hareplay:healtharchive</code> (not <code>root:healtharchive</code>)</li> <li>the systemd unit runs with <code>--user &lt;hareplay_uid&gt;:&lt;healtharchive_gid&gt;</code></li> </ul> <p>Then restart:</p> <pre><code>sudo chown -R hareplay:healtharchive /srv/healtharchive/replay\nsudo systemctl restart healtharchive-replay.service\n</code></pre>"},{"location":"deployment/replay-service-pywb/#5-caddy-config-public-https","title":"5) Caddy config (public HTTPS)","text":"<p>Edit <code>/etc/caddy/Caddyfile</code> and add:</p> <pre><code>replay.healtharchive.ca {\n  encode zstd gzip\n\n  # Replay needs to be embeddable by the HealthArchive frontend.\n  # (The frontend wrapper provides the visible banner/controls.)\n  header {\n    -X-Frame-Options\n    Content-Security-Policy \"frame-ancestors https://healtharchive.ca https://www.healtharchive.ca\"\n  }\n\n  reverse_proxy 127.0.0.1:8090\n}\n</code></pre> <p>Validate + reload:</p> <pre><code>sudo caddy fmt --overwrite /etc/caddy/Caddyfile\nsudo caddy validate --config /etc/caddy/Caddyfile\nsudo systemctl reload caddy\n</code></pre> <p>Public check (from your laptop):</p> <pre><code>curl -I https://replay.healtharchive.ca/ | head\n</code></pre>"},{"location":"deployment/replay-service-pywb/#51-optional-healtharchive-banner-on-direct-replay-pages","title":"5.1) Optional: HealthArchive banner on direct replay pages","text":"<p>The HealthArchive frontend provides the primary replay UX via <code>/snapshot/&lt;id&gt;</code> and <code>/browse/&lt;id&gt;</code> (header, navigation, disclaimers). Users may still open <code>replay.healtharchive.ca</code> directly in a new tab.</p> <p>To reduce confusion, you can inject a small HealthArchive banner into pywb\u2019s non-framed replay HTML using pywb\u2019s <code>custom_banner.html</code> hook.</p> <p>Implementation notes:</p> <ul> <li>The banner is inserted only for non-framed replay (our default).</li> <li>When replay is embedded in an iframe, the banner collapses to a minimal UI   (View diff + Details + Hide) to avoid duplicating the HealthArchive wrapper header.</li> <li>When embedded, the script also emits lightweight <code>postMessage</code> events with   the current replay URL/timestamp so the HealthArchive frontend can support   edition switching while you browse.</li> <li>Users can dismiss it via the Hide button (stored in <code>localStorage</code> on the   replay origin).</li> </ul> <p>Deploy on the VPS:</p> <pre><code>sudo mkdir -p /srv/healtharchive/replay/templates\nsudo install -o hareplay -g healtharchive -m 0640 \\\n  /opt/healtharchive-backend/docs/deployment/pywb/custom_banner.html \\\n  /srv/healtharchive/replay/templates/custom_banner.html\n\nsudo systemctl restart healtharchive-replay.service\n</code></pre> <p>Notes:</p> <ul> <li>The banner script calls the HealthArchive public API from the replay origin   (for example, <code>GET /api/replay/resolve</code>) to resolve the snapshot ID and build   the correct \u201cback to snapshot\u201d and compare links. Ensure the backend CORS   allowlist includes <code>https://replay.healtharchive.ca</code> when the banner is   enabled.</li> <li>The direct-replay banner is a compact sticky top bar: title, capture date,   original URL, an always-visible disclaimer line, and action links (View diff,   Details, All snapshots, Raw HTML, Metadata JSON, Cite, Report issue, Hide).   \u201cAll snapshots\u201d opens a right-aligned popover list.</li> <li>On small screens, the banner stacks the action links below the \u201c\u2190 HealthArchive.ca\u201d   button to avoid overlap; when you scroll, it transitions into a more compact   mode that hides the metadata/disclaimer line.</li> <li>The \u201c\u2190 HealthArchive.ca\u201d button returns to the HealthArchive page the user came   from when possible (the frontend passes an explicit return path in the replay   URL fragment). If no return path is available, it falls back to the archive   search page for the current original URL.</li> <li>The banner uses <code>XMLHttpRequest</code> with pywb\u2019s wombat opt-out (<code>xhr._no_rewrite = true</code>)   so API requests are not replay-rewritten. Ensure CORS allows the   <code>X-Pywb-Requested-With</code> header from <code>https://replay.healtharchive.ca</code>.</li> <li>Production expects the public API to be reachable at <code>https://api.&lt;apex&gt;</code> (for   example, <code>https://api.healtharchive.ca</code>). If your deployment instead proxies   <code>/api</code> on the frontend origin, ensure the banner\u2019s API base candidates are   still valid for your hostnames.</li> <li>If you deploy the backend using <code>./scripts/vps-deploy.sh --apply --restart-replay</code>,   the deploy helper will also install <code>custom_banner.html</code> and restart the replay   service as part of that run (single-VPS setup).</li> </ul> <p>Note: the banner can be disabled for screenshot generation by adding a fragment:</p> <pre><code>...#ha_nobanner=1\n</code></pre>"},{"location":"deployment/replay-service-pywb/#6-create-a-collection-and-index-a-jobs-warcs-no-copying","title":"6) Create a collection and index a job\u2019s WARCs (no copying)","text":"<p>pywb\u2019s <code>wb-manager</code> requires WARC files to exist in the collection\u2019s <code>archive/</code> directory. We avoid duplicating data by placing symlinks to the real WARC files (mounted read-only at <code>/warcs</code> in the container).</p>"},{"location":"deployment/replay-service-pywb/#60-recommended-use-the-backend-cli-one-command-per-job","title":"6.0 Recommended: use the backend CLI (one command per job)","text":"<p>If the backend and pywb run on the same VPS, you can make a job replayable via:</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend replay-index-job --id 1\n</code></pre> <p>Dry-run (prints actions without changes):</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend replay-index-job --id 1 --dry-run\n</code></pre>"},{"location":"deployment/replay-service-pywb/#61-initialize-collection-for-job-1","title":"6.1 Initialize collection for job 1","text":"<pre><code>sudo docker exec healtharchive-replay wb-manager init job-1\n</code></pre>"},{"location":"deployment/replay-service-pywb/#62-link-job-1-warcs-into-the-collection","title":"6.2 Link job 1 WARCs into the collection","text":"<p>1) Determine the job output directory:</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend show-job --id 1\n</code></pre> <p>2) Find WARCs under that output directory:</p> <pre><code>OUTPUT_DIR=\"/srv/healtharchive/jobs/imports/legacy-hc-2025-04-21\"  # example; replace\nfind \"$OUTPUT_DIR\" -type f -name '*.warc.gz' | sort &gt; /tmp/job-1-warcs.txt\nwc -l /tmp/job-1-warcs.txt\n</code></pre> <p>3) Convert host paths \u2192 container paths (because we mount <code>/srv/healtharchive/jobs</code> as <code>/warcs</code>):</p> <pre><code>sed 's#^/srv/healtharchive/jobs#\\/warcs#' /tmp/job-1-warcs.txt &gt; /tmp/job-1-warcs.container.txt\n</code></pre> <p>4) Create symlinks in the collection archive directory (prefixing with a stable counter to avoid name collisions):</p> <pre><code>COLL_ARCHIVE_DIR=\"/srv/healtharchive/replay/collections/job-1/archive\"\nsudo mkdir -p \"$COLL_ARCHIVE_DIR\"\n\nnl -ba /tmp/job-1-warcs.container.txt | while read -r n p; do\n  printf -v linkname \"warc-%06d.warc.gz\" \"$n\"\n  sudo ln -sf \"$p\" \"$COLL_ARCHIVE_DIR/$linkname\"\ndone\n</code></pre> <p>Note: the symlink targets are container paths under <code>/warcs/...</code>, so they may appear \u201cbroken\u201d when inspected on the host. They will resolve correctly inside the container because <code>/srv/healtharchive/jobs</code> is mounted as <code>/warcs</code>.</p> <p>5) Index:</p> <pre><code>sudo docker exec healtharchive-replay wb-manager reindex job-1\n</code></pre>"},{"location":"deployment/replay-service-pywb/#63-verify-replay-works","title":"6.3 Verify replay works","text":"<p>Pick a known URL in the job (example HC homepage):</p> <pre><code>curl -I \"https://replay.healtharchive.ca/job-1/https://www.canada.ca/en/health-canada.html\" | head\n</code></pre> <p>In a browser:</p> <ul> <li>Open the same URL and click around.</li> <li>Confirm CSS/images load and links stay under <code>replay.healtharchive.ca/job-1/...</code>.</li> </ul>"},{"location":"deployment/replay-service-pywb/#64-repeat-for-another-job-example-cihr","title":"6.4 Repeat for another job (example: CIHR)","text":"<p>Once the CIHR legacy WARCs are imported and indexed as an <code>ArchiveJob</code> (see <code>docs/operations/legacy-crawl-imports.md</code>), repeat the same steps with that job ID:</p> <ul> <li>Recommended:</li> <li><code>ha-backend replay-index-job --id &lt;id&gt;</code></li> <li><code>wb-manager init job-&lt;id&gt;</code></li> <li>Symlink that job\u2019s WARCs into <code>/srv/healtharchive/replay/collections/job-&lt;id&gt;/archive/</code></li> <li><code>wb-manager reindex job-&lt;id&gt;</code></li> <li>Verify: <code>https://replay.healtharchive.ca/job-&lt;id&gt;/&lt;some captured url&gt;/</code></li> </ul>"},{"location":"deployment/replay-service-pywb/#7-troubleshooting","title":"7) Troubleshooting","text":"<ul> <li>Blank pages / missing styling: the asset was not captured into the WARC set, or the page uses live third\u2011party resources not archived.</li> <li>Replay 404 but snapshot exists in DB: the job\u2019s WARCs were not linked+indexed into pywb (or you ran <code>cleanup-job</code> and deleted WARCs).</li> <li>Replay UI shows \u201cAll-time (0 captures)\u201d: that exact URL (including scheme + host, eg <code>www.</code> vs non-<code>www</code>) likely isn\u2019t present in the WARC set. Confirm via <code>/&lt;collection&gt;/cdx?url=...</code> and try host/scheme variants.</li> <li>Iframe blocked: check <code>frame-ancestors</code> header on <code>replay.healtharchive.ca</code> and ensure you removed <code>X-Frame-Options</code>.</li> <li>Service crash-loop with <code>groupadd/useradd</code> and <code>su: Authentication failure</code>: the container entrypoint is trying to create/switch users, but <code>--cap-drop=ALL</code> removes the capabilities needed. Fix by running the container as the host UID/GID directly via <code>--user &lt;hareplay_uid&gt;:&lt;healtharchive_gid&gt;</code>.</li> </ul>"},{"location":"deployment/replay-service-pywb/#8-backend-wiring-optional-but-recommended","title":"8) Backend wiring (optional, but recommended)","text":"<p>If you want the HealthArchive frontend to embed replay by default, configure the backend to emit a <code>browseUrl</code> for each snapshot.</p> <p>On the VPS (backend host), set in <code>/etc/healtharchive/backend.env</code>:</p> <pre><code>HEALTHARCHIVE_REPLAY_BASE_URL=https://replay.healtharchive.ca\n</code></pre> <p>Then restart the backend service.</p>"},{"location":"deployment/replay-service-pywb/#81-edition-switching-v2-preserve-current-page-across-backups","title":"8.1 Edition switching (v2: \u201cpreserve current page across backups\u201d)","text":"<p>HealthArchive supports switching \u201ceditions\u201d (jobs) while keeping you on the same original URL when possible.</p> <p>This is implemented as:</p> <ul> <li><code>GET /api/sources/{sourceCode}/editions</code></li> <li>lists replayable jobs (editions) for the source, including each job\u2019s     <code>entryBrowseUrl</code> (a good fallback when a specific page wasn\u2019t captured).</li> <li><code>POST /api/replay/resolve</code></li> <li>input: <code>{ \"jobId\": &lt;id&gt;, \"url\": \"&lt;original_url&gt;\", \"timestamp14\": \"YYYYMMDDhhmmss\" | null }</code></li> <li>output: a best-effort <code>browseUrl</code> for the selected job if a capture exists     (or <code>found=false</code> when it does not).</li> </ul> <p>The frontend relies on lightweight <code>postMessage</code> events emitted by the replay banner template (see \u201cOptional: HealthArchive banner on direct replay pages\u201d above) to learn the current original URL while the user clicks around inside replay.</p> <p>Frontend-side details and verification are documented in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md</li> </ul> <p>Frontend verification (recommended):</p> <ul> <li>See https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md for the end-to-end   checks that confirm:</li> <li>snapshot pages embed replay correctly, and</li> <li><code>/browse/&lt;snapshotId&gt;</code> provides a full-screen browsing wrapper with a     persistent HealthArchive banner above the replay iframe.</li> </ul>"},{"location":"deployment/replay-service-pywb/#9-cached-source-preview-images-optional-recommended","title":"9) Cached source preview images (optional, recommended)","text":"<p>The frontend <code>/archive</code> page can show a lightweight \u201chomepage preview\u201d tile for each source\u2019s latest replayable backup.</p> <p>To avoid rendering live iframes on every page load, these previews are served as cached static images generated out-of-band.</p>"},{"location":"deployment/replay-service-pywb/#91-configure-preview-directory-vps","title":"9.1 Configure preview directory (VPS)","text":"<p>Choose a directory on the VPS:</p> <ul> <li>Recommended: <code>/srv/healtharchive/replay/previews</code></li> </ul> <p>Create it with the same ownership model as the replay volume:</p> <pre><code>sudo mkdir -p /srv/healtharchive/replay/previews\nsudo chown -R hareplay:healtharchive /srv/healtharchive/replay/previews\nsudo chmod 2770 /srv/healtharchive/replay/previews\n</code></pre> <p>In <code>/etc/healtharchive/backend.env</code>, set:</p> <pre><code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR=/srv/healtharchive/replay/previews\n</code></pre> <p>Then restart the API:</p> <pre><code>sudo systemctl restart healtharchive-api\n</code></pre>"},{"location":"deployment/replay-service-pywb/#92-generate-previews-vps","title":"9.2 Generate previews (VPS)","text":"<p>Generate (or refresh) previews for all sources with:</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend replay-generate-previews\n</code></pre> <p>This uses a Playwright container to screenshot each source\u2019s <code>entryBrowseUrl</code> (with <code>#ha_nobanner=1</code> so the pywb banner is not captured).</p> <p>Note: The generator caches Playwright\u2019s Node.js dependencies under <code>&lt;preview_dir_parent&gt;/.preview-node/</code>. If you point <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code> at a path inside your repo for local testing, ensure <code>.preview-node/</code> is ignored by git (it is in <code>.gitignore</code>).</p>"},{"location":"deployment/replay-service-pywb/#93-verify-previews-are-available","title":"9.3 Verify previews are available","text":"<p>1) Confirm <code>/api/sources</code> advertises <code>entryPreviewUrl</code> where available:</p> <pre><code>curl -s https://api.healtharchive.ca/api/sources | python3 -m json.tool | rg entryPreviewUrl\n</code></pre> <p>2) Confirm an individual preview serves as an image:</p> <pre><code>curl -I \"https://api.healtharchive.ca/api/sources/hc/preview?jobId=1\" | head\n</code></pre>"},{"location":"deployment/search-rollout/","title":"Search ranking rollout (v2 default)","text":"<p>This is the recommended rollout procedure for enabling the blended search ranking (v2) in production.</p> <p>Decision: use v2 by default via <code>HA_SEARCH_RANKING_VERSION=v2</code>.</p> <p>Rationale (given current project goals/resources): - We\u2019re intentionally staying on Postgres FTS + lightweight heuristics (no separate search service). - v2 materially improves broad-query \u201chub\u201d discovery using signals we already compute (<code>page_signals</code>, <code>snapshot_outlinks</code>). - Rollback is instant and low-risk (flip one env var + restart API).</p>"},{"location":"deployment/search-rollout/#0-preconditions","title":"0) Preconditions","text":"<ul> <li>You have already deployed the authority schema (tables <code>snapshot_outlinks</code> and <code>page_signals</code>) and populated outlinks for your jobs.</li> <li>If <code>snapshot_outlinks</code> is empty, v2 won\u2019t get the link-graph benefits.</li> <li>You have the admin token available (for <code>/api/admin/search-debug</code> verification).</li> </ul>"},{"location":"deployment/search-rollout/#1-recommended-production-rollout-steps-single-vps","title":"1) Recommended production rollout steps (single VPS)","text":"<p>On the VPS (as <code>haadmin</code>) in the backend repo checkout (e.g. <code>/opt/healtharchive-backend</code>):</p> <p>1) Pull the new backend revision. 2) Apply migrations:    - <code>./.venv/bin/alembic upgrade head</code> 3) Recompute link signals (populates <code>inlink_count</code>, <code>outlink_count</code>, and <code>pagerank</code> when present):    - <code>./.venv/bin/ha-backend recompute-page-signals</code> 4) Enable v2 by default:    - Edit <code>/etc/healtharchive/backend.env</code> and set <code>HA_SEARCH_RANKING_VERSION=v2</code> 5) Restart only the API process (worker does not need the ranking env var):    - <code>sudo systemctl restart healtharchive-api</code></p> <p>Notes: - <code>ha-backend recompute-page-signals</code> can take a while on large graphs; run it in <code>tmux</code> and consider off-peak hours. - If you have not yet backfilled outlinks for existing WARCs, do that first (per job):   - <code>./.venv/bin/ha-backend backfill-outlinks --job-id &lt;JOB_ID&gt; --update-signals</code>   - Then run <code>./.venv/bin/ha-backend recompute-page-signals</code> once after the backfills finish.</p>"},{"location":"deployment/search-rollout/#2-verification-checklist-production","title":"2) Verification checklist (production)","text":"<p>1) Health:    - <code>curl -s https://api.healtharchive.ca/api/health | python3 -m json.tool</code> 2) Search v2 is active by default:    - <code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;view=pages&amp;sort=relevance&amp;pageSize=20\" | python3 -m json.tool | head</code> 3) Compare explicitly:    - <code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;view=pages&amp;sort=relevance&amp;pageSize=20&amp;ranking=v1\" | python3 -m json.tool | head</code>    - <code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;view=pages&amp;sort=relevance&amp;pageSize=20&amp;ranking=v2\" | python3 -m json.tool | head</code> 4) Debug a ranking decision (admin token required):    - <code>curl -s \"https://api.healtharchive.ca/api/admin/search-debug?q=covid&amp;view=pages&amp;sort=relevance&amp;ranking=v2&amp;pageSize=10\" -H \"X-Admin-Token: $HEALTHARCHIVE_ADMIN_TOKEN\" | python3 -m json.tool</code></p>"},{"location":"deployment/search-rollout/#3-capture-diff-recommended-smoke-eval","title":"3) Capture + diff (recommended \u201csmoke eval\u201d)","text":"<p>From any machine (your laptop is fine):</p> <p>1) Capture:    - <code>./scripts/search-eval-capture.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval --page-size 20 --ranking v1</code>    - <code>./scripts/search-eval-capture.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval --page-size 20 --ranking v2</code> 2) Diff:    - <code>python ./scripts/search-eval-diff.py --a /tmp/ha-search-eval/&lt;TS_V1&gt; --b /tmp/ha-search-eval/&lt;TS_V2&gt; --top 20</code></p> <p>Or run capture+diff in one command:</p> <ul> <li><code>./scripts/search-eval-run.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval</code></li> </ul>"},{"location":"deployment/search-rollout/#4-rollback-plan-fast","title":"4) Rollback plan (fast)","text":"<p>If something looks off in production:</p> <p>1) Set <code>HA_SEARCH_RANKING_VERSION=v1</code> in <code>/etc/healtharchive/backend.env</code> 2) <code>sudo systemctl restart healtharchive-api</code></p> <p>This reverts default behavior immediately without touching data/migrations. You can still test v2 per-request with <code>&amp;ranking=v2</code> while investigating.</p>"},{"location":"deployment/staging-rollout-checklist/","title":"Staging rollout checklist \u2013 backend + frontend","text":"<p>This file turns the higher\u2011level hosting notes into a step\u2011by\u2011step checklist for bringing a staging environment online. It assumes:</p> <ul> <li>Staging API host: <code>https://api-staging.healtharchive.ca</code></li> <li>Staging frontend/preview: <code>https://healtharchive.vercel.app</code> +   branch\u2011specific preview URLs.</li> <li>Code from <code>main</code> in both repos has been deployed to the staging host /   Vercel.</li> </ul> <p>It does not require or describe changes on your local machine, beyond pushing commits; all steps here are meant to be performed on the staging host and in Vercel / GitHub.</p> <p>Note: the current production deployment intentionally runs without a separate staging backend. If you are following the single\u2011VPS production runbook, you can skip this checklist unless/until you decide to add staging.</p> <p>For background, see:</p> <ul> <li><code>hosting-and-live-server-to-dos.md</code></li> <li><code>environments-and-configuration.md</code></li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md</li> </ul>"},{"location":"deployment/staging-rollout-checklist/#1-backend-staging-environment","title":"1. Backend staging environment","text":""},{"location":"deployment/staging-rollout-checklist/#11-set-env-vars-on-the-staging-backend-host","title":"1.1 Set env vars on the staging backend host","text":"<p>On the staging host (VM/container/PaaS):</p> <ol> <li>Decide where you want jobs and WARCs to live, e.g.:</li> </ol> <pre><code>/srv/healtharchive/jobs-staging\n</code></pre> <ol> <li>Configure env vars for the backend app (via systemd env file, Docker env,    or PaaS UI). Typical values:</li> </ol> <pre><code>export HEALTHARCHIVE_ENV=staging\nexport HEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://user:pass@db-host:5432/healtharchive_staging\nexport HEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs-staging\nexport HEALTHARCHIVE_ADMIN_TOKEN=\"&lt;staging-long-random-secret&gt;\"\nexport HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.vercel.app\"\nexport HEALTHARCHIVE_LOG_LEVEL=INFO\n</code></pre> <p>Adjust the DB URL, archive root, and CORS origins to match your actual    staging infrastructure. If you want specific branch preview URLs to call    the API directly, add them to <code>HEALTHARCHIVE_CORS_ORIGINS</code> as a    comma\u2011separated list.</p>"},{"location":"deployment/staging-rollout-checklist/#12-run-migrations-and-seed-sources","title":"1.2 Run migrations and seed sources","text":"<p>From a checkout of <code>healtharchive-backend</code> at the deployed revision on the staging host:</p> <pre><code>cd /path/to/healtharchive-backend\n\n# Activate venv or ensure dependencies are installed\nalembic upgrade head\nha-backend seed-sources\n</code></pre> <p>This:</p> <ul> <li>Applies all Alembic migrations to the staging DB.</li> <li>Ensures baseline <code>Source</code> rows for <code>hc</code> and <code>phac</code> exist (idempotent).</li> </ul>"},{"location":"deployment/staging-rollout-checklist/#13-start-api-worker-processes","title":"1.3 Start API + worker processes","text":"<p>Configure your process manager (systemd, Docker Compose, PaaS) to run:</p> <ul> <li>API:</li> </ul> <pre><code>uvicorn ha_backend.api:app --host 0.0.0.0 --port 8001\n</code></pre> <ul> <li>Worker:</li> </ul> <pre><code>ha-backend start-worker --poll-interval 30\n</code></pre> <p>Both processes must see the same <code>HEALTHARCHIVE_*</code> env vars from 1.1.</p>"},{"location":"deployment/staging-rollout-checklist/#14-create-at-least-one-staging-snapshot","title":"1.4 Create at least one staging snapshot","text":"<p>For basic end\u2011to\u2011end testing, you can start with a tiny synthetic WARC + one <code>Snapshot</code> row, using the recipe from the local live\u2011testing guide. On the staging host:</p> <ol> <li> <p>Ensure <code>HEALTHARCHIVE_DATABASE_URL</code> and <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> are set    (as in 1.1).</p> </li> <li> <p>Run the synthetic WARC script from    <code>healtharchive-backend/docs/development/live-testing.md</code> \u00a76.1 (\u201cHappy\u2011path    viewer using a synthetic WARC\u201d). It will:</p> </li> <li> <p>Create a small WARC file under <code>HEALTHARCHIVE_ARCHIVE_ROOT</code>.</p> </li> <li>Insert a <code>Snapshot</code> row in the staging DB.</li> <li> <p>Print:</p> <pre><code>SNAPSHOT_ID &lt;N&gt;\n</code></pre> </li> <li> <p>Record this <code>N</code> as your canonical staging snapshot ID for smoke tests.</p> </li> <li> <p>Quick check (from your own machine):</p> </li> </ol> <pre><code>curl -i \"https://api-staging.healtharchive.ca/api/snapshot/&lt;N&gt;\"\ncurl -i \"https://api-staging.healtharchive.ca/api/snapshots/raw/&lt;N&gt;\"\n</code></pre> <ul> <li>Both should return HTTP 200 with sensible data/HTML.</li> </ul>"},{"location":"deployment/staging-rollout-checklist/#15-api-health-and-cors-checks","title":"1.5 API health and CORS checks","text":"<p>From your own terminal (not on the staging host):</p> <pre><code>curl -i \"https://api-staging.healtharchive.ca/api/health\"\n\ncurl -i \\\n  -H \"Origin: https://healtharchive.vercel.app\" \\\n  \"https://api-staging.healtharchive.ca/api/health\"\n</code></pre> <p>Verify:</p> <ul> <li>HTTP 200 and <code>\"status\":\"ok\"</code> in the JSON body.</li> <li><code>Access-Control-Allow-Origin: https://healtharchive.vercel.app</code></li> <li><code>Vary: Origin</code></li> </ul>"},{"location":"deployment/staging-rollout-checklist/#2-frontend-staging-configuration-vercel-preview","title":"2. Frontend staging configuration (Vercel Preview)","text":""},{"location":"deployment/staging-rollout-checklist/#21-preview-env-vars","title":"2.1 Preview env vars","text":"<p>In the Vercel project for <code>healtharchive-frontend</code>:</p> <ol> <li>Go to Settings \u2192 Environment Variables \u2192 Preview.</li> <li>Set:</li> </ol> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api-staging.healtharchive.ca\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=true\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=true\nNEXT_PUBLIC_SHOW_API_BASE_HINT=true\n</code></pre> <ol> <li>Save and trigger a new Preview deployment (e.g. by pushing a commit to    the staging branch or redeploying the latest preview).</li> </ol>"},{"location":"deployment/staging-rollout-checklist/#22-grab-the-preview-url","title":"2.2 Grab the preview URL","text":"<p>After the build completes:</p> <ol> <li>Open the Vercel project\u2019s Deployments tab.</li> <li>Click the latest Preview deployment.</li> <li>Copy its URL, e.g.:</li> </ol> <pre><code>https://healtharchive-git-staging-&lt;hash&gt;.vercel.app\n</code></pre> <p>You will use this URL for the verification steps below.</p>"},{"location":"deployment/staging-rollout-checklist/#3-staging-verification-browser","title":"3. Staging verification (browser)","text":"<p>The following steps mirror https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md but framed as a checklist.</p>"},{"location":"deployment/staging-rollout-checklist/#31-frontend-security-headers-csp","title":"3.1 Frontend security headers &amp; CSP","text":"<ol> <li>Open the preview <code>/archive</code> route in a browser:</li> </ol> <pre><code>https://healtharchive-git-staging-&lt;hash&gt;.vercel.app/archive\n</code></pre> <ol> <li>In DevTools \u2192 Network:</li> <li>Select the main document request (<code>/archive</code>).</li> <li>Under Response headers, confirm:<ul> <li><code>Referrer-Policy: strict-origin-when-cross-origin</code></li> <li><code>X-Content-Type-Options: nosniff</code></li> <li><code>X-Frame-Options: SAMEORIGIN</code></li> <li><code>Permissions-Policy: geolocation=(), microphone=(), camera=()</code></li> <li><code>Content-Security-Policy-Report-Only: ...</code> with the expected    <code>connect-src</code> and <code>frame-src</code> entries for    <code>https://api.healtharchive.ca</code> and <code>https://api-staging.healtharchive.ca</code>.</li> </ul> </li> </ol>"},{"location":"deployment/staging-rollout-checklist/#32-backend-headers-cors-via-frontend","title":"3.2 Backend headers &amp; CORS (via frontend)","text":"<ol> <li>Still on <code>/archive</code>, filter the Network tab for requests to    <code>https://api-staging.healtharchive.ca</code>.</li> <li>Inspect <code>GET /api/health</code> and <code>GET /api/search?...</code>:</li> <li>Confirm HTTP 200 + JSON body.</li> <li>Confirm headers:<ul> <li><code>X-Content-Type-Options</code>, <code>Referrer-Policy</code>, <code>X-Frame-Options</code>,    <code>Permissions-Policy</code>.</li> <li><code>Access-Control-Allow-Origin</code> equal to the preview URL.</li> <li><code>Vary: Origin</code>.</li> </ul> </li> </ol>"},{"location":"deployment/staging-rollout-checklist/#33-snapshot-viewer-iframe","title":"3.3 Snapshot viewer iframe","text":"<ol> <li>Navigate to the staging snapshot using the ID from \u00a71.4:</li> </ol> <pre><code>https://healtharchive-git-staging-&lt;hash&gt;.vercel.app/snapshot/&lt;N&gt;\n</code></pre> <ol> <li>In DevTools \u2192 Elements:</li> <li>Locate the <code>&lt;iframe&gt;</code> in the snapshot viewer.</li> <li> <p>Confirm:</p> <ul> <li><code>src</code> is    <code>https://api-staging.healtharchive.ca/api/snapshots/raw/&lt;N&gt;</code> (i.e.    <code>NEXT_PUBLIC_API_BASE_URL</code> + the <code>rawSnapshotUrl</code> path).</li> <li><code>sandbox=\"allow-same-origin allow-scripts\"</code> is present.</li> </ul> </li> <li> <p>In Network:</p> </li> <li>Click the iframe request (<code>GET /api/snapshots/raw/&lt;N&gt;</code>).</li> <li>Confirm:<ul> <li>The request goes to <code>api-staging.healtharchive.ca</code>.</li> <li>Security headers match <code>/api/health</code>, except that    <code>X-Frame-Options</code> is intentionally omitted on this route so the    snapshot can be embedded.</li> </ul> </li> </ol>"},{"location":"deployment/staging-rollout-checklist/#34-console-diagnostics","title":"3.4 Console diagnostics","text":"<ol> <li>On <code>/archive</code>, open the browser console.</li> <li>Confirm you see a log similar to:</li> </ol> <pre><code>[healtharchive] API base URL (from NEXT_PUBLIC_API_BASE_URL or default): https://api-staging.healtharchive.ca\n</code></pre> <ol> <li>If the backend is unreachable or misconfigured, confirm:</li> <li>The health banner appears (when enabled).</li> <li><code>NEXT_PUBLIC_LOG_API_HEALTH_FAILURE</code> causes an appropriate warning.</li> </ol>"},{"location":"deployment/staging-rollout-checklist/#4-staging-sign-off","title":"4. Staging sign-off","text":"<p>Once the steps above pass, you can:</p> <ul> <li>Mark the staging\u2011related items in <code>hosting-and-live-server-to-dos.md</code> as   complete for the staging environment.</li> <li>Use the same patterns (with different env vars and hosts) when bringing   production online.</li> </ul>"},{"location":"deployment/systemd/","title":"Systemd unit templates (single VPS)","text":"<p>These files are templates meant to be copied onto the production VPS under <code>/etc/systemd/system/</code>.</p> <p>They implement:</p> <ul> <li>Annual scheduling timer (Jan 01 UTC)</li> <li>Worker priority lowering during campaign (always-on, low-risk)</li> <li>Storage Box mount (sshfs) for cold WARC storage (optional but recommended for tiering)</li> <li>WARC tiering bind mounts (Storage Box -&gt; canonical paths) (optional; for tiny-SSD setups)</li> <li>Replay reconciliation timer (pywb indexing; capped)</li> <li>Change tracking timer (edition-aware diffs; capped)</li> <li>Baseline drift check timer (policy vs observed; detects config drift)</li> <li>Public surface verification timer (public API + frontend; deeper than uptime checks)</li> <li>Optional \"timer ran\" pings (Healthchecks-style)</li> <li>Annual search verification capture (optional, safe)</li> </ul> <p>Assumptions (adjust paths/user if your VPS differs):</p> <ul> <li>Repo is deployed at: <code>/opt/healtharchive-backend</code></li> <li>Venv exists at: <code>/opt/healtharchive-backend/.venv</code></li> <li>Backend env file: <code>/etc/healtharchive/backend.env</code></li> <li>Backend system user: <code>haadmin</code></li> </ul>"},{"location":"deployment/systemd/#files","title":"Files","text":"<ul> <li><code>healtharchive-schedule-annual.service</code></li> <li>Apply mode: enqueues annual jobs (<code>--apply</code>) for the current UTC year.</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/automation-enabled</code>.</li> <li><code>RefuseManualStart=yes</code> to prevent accidental <code>systemctl start</code> while the     worker is running.</li> <li><code>healtharchive-schedule-annual.timer</code></li> <li>Runs at <code>*-01-01 00:05:00 UTC</code></li> <li><code>Persistent=true</code> (runs on next boot if missed)</li> <li><code>healtharchive-schedule-annual-dry-run.service</code></li> <li>Safe validation service (no DB writes).</li> <li><code>healtharchive-worker.service.override.conf</code></li> <li>Drop-in that lowers worker CPU/IO priority to keep the API responsive.</li> <li><code>healtharchive-replay-reconcile.service</code></li> <li>Apply mode: runs <code>ha-backend replay-reconcile --apply --max-jobs 1</code>.</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/replay-automation-enabled</code>.</li> <li>Uses a lock file under <code>/srv/healtharchive/replay/.locks/</code> to prevent concurrent runs.</li> <li><code>healtharchive-replay-reconcile.timer</code></li> <li>Daily at <code>*-*-* 02:30:00 UTC</code></li> <li><code>Persistent=true</code> (runs on next boot if missed)</li> <li><code>healtharchive-replay-reconcile-dry-run.service</code></li> <li>Safe validation service (no docker exec, no filesystem writes beyond the lock file dir).</li> <li><code>healtharchive-change-tracking.service</code></li> <li>Apply mode: runs <code>ha-backend compute-changes</code> (edition-aware diffs).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/change-tracking-enabled</code>.</li> <li><code>healtharchive-change-tracking.timer</code></li> <li>Daily at <code>*-*-* 03:40:00 UTC</code></li> <li><code>Persistent=true</code> (runs on next boot if missed)</li> <li><code>healtharchive-change-tracking-dry-run.service</code></li> <li>Safe validation service (no DB writes; reports how many diffs would be computed).</li> <li><code>scripts/systemd-healthchecks-wrapper.sh</code></li> <li>Helper for optional Healthchecks-style pings without embedding ping URLs in unit files.</li> <li><code>healtharchive-annual-search-verify.service</code></li> <li>Runs <code>scripts/annual-search-verify.sh</code> daily, but captures once per year (idempotent).</li> <li><code>healtharchive-annual-search-verify.timer</code></li> <li>Daily timer for <code>healtharchive-annual-search-verify.service</code>.</li> <li><code>healtharchive-coverage-guardrails.service</code> + <code>.timer</code></li> <li>Writes coverage regression guardrails to the node_exporter textfile collector.</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/coverage-guardrails-enabled</code>.</li> <li><code>healtharchive-replay-smoke.service</code> + <code>.timer</code></li> <li>Runs replay smoke tests against the latest indexed job per source (node_exporter textfile).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/replay-smoke-enabled</code>.</li> <li><code>healtharchive-cleanup-automation.service</code> + <code>.timer</code></li> <li>Cleans indexed jobs using safe <code>temp-nonwarc</code> mode (keeps WARCs).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/cleanup-automation-enabled</code>.</li> <li><code>healtharchive-baseline-drift-check.service</code></li> <li>Runs <code>scripts/check_baseline_drift.py</code> (policy vs observed; writes artifacts under <code>/srv/healtharchive/ops/baseline/</code>).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/baseline-drift-enabled</code>.</li> <li><code>healtharchive-baseline-drift-check.timer</code></li> <li>Weekly timer for <code>healtharchive-baseline-drift-check.service</code>.</li> <li><code>healtharchive-public-surface-verify.service</code></li> <li>Runs <code>scripts/verify_public_surface.py</code> (public API + frontend; includes changes/RSS and partner pages).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/public-verify-enabled</code>.</li> <li>Intended as a deeper \u201csynthetic check\u201d than external uptime monitors.</li> <li><code>healtharchive-public-surface-verify.timer</code></li> <li>Daily timer for <code>healtharchive-public-surface-verify.service</code>.</li> <li><code>healtharchive-tiering-metrics.service</code> + <code>.timer</code></li> <li>Writes a small set of tiering health metrics to the node_exporter textfile collector.</li> <li>Used to alert on Storage Box / tiering failures without needing a systemd collector.</li> <li>Prereq: node_exporter must run with <code>--collector.textfile.directory=/var/lib/node_exporter/textfile_collector</code>     (configured by <code>scripts/vps-install-observability-exporters.sh</code>).</li> <li><code>healtharchive-crawl-metrics.service</code> + <code>.timer</code></li> <li>Writes per-job crawl progress/stall metrics (based on crawlStatus logs) to the node_exporter textfile collector.</li> <li>Used to alert on stalled crawls without manual log tailing.</li> <li>Prereq: node_exporter textfile collector is enabled (same as tiering metrics).</li> <li><code>healtharchive-crawl-auto-recover.service</code> + <code>.timer</code></li> <li>Optional automation to recover stalled crawl jobs by marking stale running jobs as retryable (and restarting the worker when needed).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/crawl-auto-recover-enabled</code>.</li> <li>Disabled by default; enable only after you\u2019re comfortable with the thresholds/caps in <code>scripts/vps-crawl-auto-recover.py</code>.</li> <li><code>healtharchive-worker-auto-start.service</code> + <code>.timer</code></li> <li>Optional automation to ensure the worker is running when it should be (jobs pending + storage OK).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/worker-auto-start-enabled</code>.</li> <li>Conservative by default; prefers a \u201cdo nothing\u201d skip over unsafe starts.</li> <li><code>healtharchive-storage-hotpath-auto-recover.service</code> + <code>.timer</code></li> <li>Optional automation to recover stale/unreadable hot paths caused by <code>sshfs</code>/FUSE mount failures (Errno 107).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/storage-hotpath-auto-recover-enabled</code>.</li> <li>Disabled by default; enable only after dry-run validation and only if you\u2019re comfortable with the safety caps in <code>scripts/vps-storage-hotpath-auto-recover.py</code>.</li> <li><code>healtharchive-storagebox-sshfs.service</code></li> <li>Mounts a Hetzner Storage Box at <code>/srv/healtharchive/storagebox</code> via <code>sshfs</code>.</li> <li>Reads configuration from <code>/etc/healtharchive/storagebox.env</code>.</li> <li>Intended for tiered WARC storage on small SSD hosts.</li> <li><code>healtharchive-warc-tiering.service</code></li> <li>Applies bind mounts from <code>/etc/healtharchive/warc-tiering.binds</code> so canonical     archive paths under <code>/srv/healtharchive/jobs/**</code> resolve to Storage Box data.</li> <li>Runs before the API/worker/replay services start.</li> <li><code>healtharchive-annual-output-tiering.service</code></li> <li>After annual jobs are enqueued, bind-mounts each annual job output_dir onto the Storage Box tier.</li> <li>Triggered via <code>OnSuccess=</code> in <code>healtharchive-schedule-annual.service</code> (template).</li> <li><code>healtharchive-annual-campaign-sentinel.service</code> + <code>.timer</code></li> <li>Runs a \u201cday-of\u201d annual readiness gate automatically: preflight + annual-status + tiering checks.</li> <li>Writes a small Prometheus textfile metric so Alertmanager can notify on failures.</li> </ul>"},{"location":"deployment/systemd/#recommended-enablement-guidance","title":"Recommended enablement guidance","text":"<p>These timers are safe-by-default and gated by sentinel files. Enable only what matches your operational readiness.</p> <ul> <li>Change tracking (<code>healtharchive-change-tracking.timer</code>)</li> <li>Recommended to enable once the <code>snapshot_changes</code> table exists and a dry     run succeeds without errors.</li> <li>Annual scheduling (<code>healtharchive-schedule-annual.timer</code>)</li> <li>Enable only after an annual dry-run succeeds and storage headroom is     confirmed.</li> <li>Replay reconcile (<code>healtharchive-replay-reconcile.timer</code>)</li> <li>Enable only if replay is enabled and stable.</li> <li>Annual search verification (<code>healtharchive-annual-search-verify.timer</code>)</li> <li>Optional; safe to enable if you want a yearly search QA artifact.</li> <li>Coverage guardrails (<code>healtharchive-coverage-guardrails.timer</code>)</li> <li>Recommended once you have at least two annual editions indexed.</li> <li>Replay smoke tests (<code>healtharchive-replay-smoke.timer</code>)</li> <li>Enable only if replay is enabled and stable.</li> <li>Cleanup automation (<code>healtharchive-cleanup-automation.timer</code>)</li> <li>Optional; keep caps conservative and review first dry-run.</li> <li>Baseline drift check (<code>healtharchive-baseline-drift-check.timer</code>)</li> <li>Recommended; low-risk and catches \u201csilent\u201d ops drift.</li> <li>Storage hot-path auto-recover (<code>healtharchive-storage-hotpath-auto-recover.timer</code>)</li> <li>Dangerous if misconfigured; only enable after you\u2019ve validated Phase 1 alerts and run the watchdog in dry-run mode.</li> <li>The unit is gated by a venv presence check and the watchdog skips runs while the deploy lock is held (to avoid flapping during active deploys).</li> <li>Worker auto-start watchdog (<code>healtharchive-worker-auto-start.timer</code>)</li> <li>Recommended once you\u2019re confident in the single-VPS production automation stack.</li> <li>The unit is sentinel-gated and refuses to start the worker if the Storage Box mount is unreadable or if the DB indicates a <code>status=running</code> job while the worker is down.</li> </ul> <p>If a timer is enabled, also ensure its sentinel file exists under <code>/etc/healtharchive/</code> (see the enablement sections below).</p>"},{"location":"deployment/systemd/#install-update-on-the-vps","title":"Install / update on the VPS","text":"<p>Preferred (one command; installs templates + worker priority drop-in):</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker\n</code></pre>"},{"location":"deployment/systemd/#run-a-crawl-job-detached-optional","title":"Run a crawl job detached (optional)","text":"<p>If you need to run a specific DB-backed crawl job manually (for debugging or recovery), prefer launching it as a transient systemd unit so your SSH session doesn\u2019t need to stay open:</p> <pre><code>sudo /opt/healtharchive-backend/scripts/vps-run-db-job-detached.py --id 7 --retry-first\n</code></pre> <p>Follow the printed <code>journalctl -u &lt;unit&gt;.service -f</code> command to tail logs.</p> <p>If you are using WARC tiering with a Storage Box, also create these files on the VPS:</p> <ul> <li><code>/etc/healtharchive/storagebox.env</code></li> <li>Configuration consumed by <code>healtharchive-storagebox-sshfs.service</code>.</li> <li><code>/etc/healtharchive/warc-tiering.binds</code></li> <li>Bind mount manifest consumed by <code>healtharchive-warc-tiering.service</code>.</li> </ul> <p>See: <code>docs/operations/playbooks/warc-storage-tiering.md</code>.</p> <p>Before enabling timers that write artifacts under <code>/srv/healtharchive/ops/</code>, ensure the ops directories exist with the expected permissions:</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-bootstrap-ops-dirs.sh\n</code></pre> <p>Manual install (equivalent):</p> <p>Copy unit files:</p> <pre><code>sudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-schedule-annual.service \\\n  /etc/systemd/system/healtharchive-schedule-annual.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-schedule-annual.timer \\\n  /etc/systemd/system/healtharchive-schedule-annual.timer\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-schedule-annual-dry-run.service \\\n  /etc/systemd/system/healtharchive-schedule-annual-dry-run.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-replay-reconcile.service \\\n  /etc/systemd/system/healtharchive-replay-reconcile.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-replay-reconcile.timer \\\n  /etc/systemd/system/healtharchive-replay-reconcile.timer\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-replay-reconcile-dry-run.service \\\n  /etc/systemd/system/healtharchive-replay-reconcile-dry-run.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-change-tracking.service \\\n  /etc/systemd/system/healtharchive-change-tracking.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-change-tracking.timer \\\n  /etc/systemd/system/healtharchive-change-tracking.timer\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-change-tracking-dry-run.service \\\n  /etc/systemd/system/healtharchive-change-tracking-dry-run.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-annual-search-verify.service \\\n  /etc/systemd/system/healtharchive-annual-search-verify.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-annual-search-verify.timer \\\n  /etc/systemd/system/healtharchive-annual-search-verify.timer\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-baseline-drift-check.service \\\n  /etc/systemd/system/healtharchive-baseline-drift-check.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-baseline-drift-check.timer \\\n  /etc/systemd/system/healtharchive-baseline-drift-check.timer\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-public-surface-verify.service \\\n  /etc/systemd/system/healtharchive-public-surface-verify.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-public-surface-verify.timer \\\n  /etc/systemd/system/healtharchive-public-surface-verify.timer\n</code></pre> <p>Install the worker priority drop-in:</p> <pre><code>sudo install -d -m 0755 -o root -g root /etc/systemd/system/healtharchive-worker.service.d\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-worker.service.override.conf \\\n  /etc/systemd/system/healtharchive-worker.service.d/override.conf\n</code></pre> <p>Reload systemd:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre> <p>Restart worker to pick up priority changes:</p> <pre><code>sudo systemctl restart healtharchive-worker\n</code></pre> <p>Verify the priority values:</p> <pre><code>systemctl show healtharchive-worker -p Nice -p IOSchedulingClass -p IOSchedulingPriority\n</code></pre>"},{"location":"deployment/systemd/#optional-timer-ran-pings-healthchecks-style","title":"Optional: \"timer ran\" pings (Healthchecks-style)","text":"<p>This repo does not commit ping URLs. If you want \"did it run?\" checks, create a root-owned env file on the VPS:</p> <pre><code>sudo install -d -m 0755 -o root -g root /etc/healtharchive\n# Only create the file if missing (don't clobber existing values like HC_DB_BACKUP_URL).\nsudo test -f /etc/healtharchive/healthchecks.env || sudo install -m 0600 -o root -g root /dev/null /etc/healtharchive/healthchecks.env\nsudo chown root:root /etc/healtharchive/healthchecks.env\nsudo chmod 0600 /etc/healtharchive/healthchecks.env\n</code></pre> <p>Edit <code>/etc/healtharchive/healthchecks.env</code> and set (examples):</p> <pre><code># You do NOT need to set every variable listed here.\n# Only set the variables for Healthchecks checks you have actually created.\n# If a variable is missing/empty, the service still runs; it just won't ping.\n#\n# Note: the Healthchecks \"Name\" can be anything; it does not need to match the\n# env var name. The env var is just how systemd finds the ping URL.\nHEALTHARCHIVE_HC_PING_REPLAY_RECONCILE=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_SCHEDULE_ANNUAL=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_ANNUAL_SENTINEL=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_CHANGE_TRACKING=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_BASELINE_DRIFT=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_PUBLIC_VERIFY=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_ANNUAL_SEARCH_VERIFY=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_COVERAGE_GUARDRAILS=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_REPLAY_SMOKE=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION=https://hc-ping.com/UUID_HERE\n</code></pre> <p>Notes:</p> <ul> <li>The unit templates use <code>EnvironmentFile=-/etc/healtharchive/healthchecks.env</code>   so the file is optional.</li> <li>If you also use the legacy Healthchecks-based scripts described in   <code>../production-single-vps.md</code> (DB backup + disk check), keep their variables   in the same file too (<code>HC_DB_BACKUP_URL</code>, <code>HC_DISK_URL</code>, <code>HC_DISK_THRESHOLD</code>).</li> <li>If set, services will best-effort ping:</li> <li><code>&lt;url&gt;/start</code> at the beginning</li> <li><code>&lt;url&gt;</code> on success</li> <li><code>&lt;url&gt;/fail</code> on failure</li> <li>Ping failures do not fail the service.</li> </ul>"},{"location":"deployment/systemd/#audit-healthchecks-alignment-safe","title":"Audit Healthchecks alignment (safe)","text":"<p>This script compares:</p> <ul> <li>What ping env vars are set in <code>/etc/healtharchive/healthchecks.env</code></li> <li>What ping vars are referenced by installed systemd unit files (via <code>--ping-var ...</code>)</li> <li>Which timers exist (for manual cross-check with Healthchecks \u201clast ping\u201d timestamps)</li> </ul> <p>Run on the VPS:</p> <pre><code>cd /opt/healtharchive-backend\nsudo -u haadmin python3 ./scripts/verify_healthchecks_alignment.py\n</code></pre> <p>If it reports \u201creferenced but unset\u201d, you either:</p> <ul> <li>Intentionally have pings disabled for those timers (OK), or</li> <li>Should create the missing checks in Healthchecks and add the missing env vars.</li> </ul> <p>If it reports \u201cset but unused\u201d, you likely have a stale env var (remove it) or the unit that used to reference it was removed/renamed.</p>"},{"location":"deployment/systemd/#validate-the-annual-scheduler-safe","title":"Validate the annual scheduler (safe)","text":"<p>This dry-run service exercises DB connectivity + scheduler output without creating jobs:</p> <pre><code>sudo systemctl start healtharchive-schedule-annual-dry-run.service\nsudo journalctl -u healtharchive-schedule-annual-dry-run.service -n 200 --no-pager\n</code></pre> <p>Do not run <code>healtharchive-schedule-annual.service</code> manually in production; it enqueues jobs and the worker may start crawling immediately.</p>"},{"location":"deployment/systemd/#validate-replay-reconciliation-safe","title":"Validate replay reconciliation (safe)","text":"<p>This dry-run service exercises DB connectivity + filesystem drift detection without running any docker exec commands:</p> <pre><code>sudo systemctl start healtharchive-replay-reconcile-dry-run.service\nsudo journalctl -u healtharchive-replay-reconcile-dry-run.service -n 200 --no-pager\n</code></pre>"},{"location":"deployment/systemd/#validate-change-tracking-safe","title":"Validate change tracking (safe)","text":"<p>This dry-run service exercises DB connectivity and reports how many diffs would be computed:</p> <pre><code>sudo systemctl start healtharchive-change-tracking-dry-run.service\nsudo journalctl -u healtharchive-change-tracking-dry-run.service -n 200 --no-pager\n</code></pre> <p>If you see an error like <code>relation \"snapshot_changes\" does not exist</code>, apply migrations first (idempotent):</p> <pre><code>cd /opt/healtharchive-backend\nsudo -u haadmin /opt/healtharchive-backend/.venv/bin/alembic upgrade head\n</code></pre>"},{"location":"deployment/systemd/#enable-automation-jan-01","title":"Enable automation (Jan 01)","text":"<p>Create the automation sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/automation-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-schedule-annual.timer\nsystemctl list-timers | rg healtharchive-schedule-annual || systemctl list-timers | grep healtharchive-schedule-annual\n</code></pre> <p>Note: do not <code>enable</code> the <code>.service</code> units directly; only the <code>.timer</code> should be enabled.</p>"},{"location":"deployment/systemd/#enable-replay-reconciliation-automation-optional","title":"Enable replay reconciliation automation (optional)","text":"<p>Create the replay automation sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/replay-automation-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-replay-reconcile.timer\nsystemctl list-timers | rg healtharchive-replay-reconcile || systemctl list-timers | grep healtharchive-replay-reconcile\n</code></pre> <p>Note: by default, the timer only reconciles replay indexing. Preview image generation is intentionally left manual/capped until you decide it\u2019s stable enough to automate.</p>"},{"location":"deployment/systemd/#enable-change-tracking-automation-optional","title":"Enable change tracking automation (optional)","text":"<p>Create the change tracking sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/change-tracking-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-change-tracking.timer\nsystemctl list-timers | rg healtharchive-change-tracking || systemctl list-timers | grep healtharchive-change-tracking\n</code></pre>"},{"location":"deployment/systemd/#enable-annual-search-verification-capture-optional","title":"Enable annual search verification capture (optional)","text":"<p>This captures golden-query <code>/api/search</code> JSON once per year after the annual campaign becomes search-ready.</p> <p>The service is idempotent:</p> <ul> <li>If the campaign isn't ready, it exits 0 (no failure spam).</li> <li>If artifacts already exist for the current year/run-id, it exits 0.</li> </ul> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-annual-search-verify.timer\nsystemctl list-timers | rg healtharchive-annual-search-verify || systemctl list-timers | grep healtharchive-annual-search-verify\n</code></pre> <p>Artifacts default to:</p> <ul> <li><code>/srv/healtharchive/ops/search-eval/&lt;year&gt;/final/</code></li> </ul> <p>To force a re-run for the current year, delete that directory and run the service once.</p>"},{"location":"deployment/systemd/#enable-coverage-guardrails-optional","title":"Enable coverage guardrails (optional)","text":"<p>This emits daily metrics comparing the latest indexed annual job to the prior year per source.</p> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/coverage-guardrails-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-coverage-guardrails.timer\nsystemctl list-timers | rg healtharchive-coverage-guardrails || systemctl list-timers | grep healtharchive-coverage-guardrails\n</code></pre>"},{"location":"deployment/systemd/#enable-replay-smoke-tests-optional","title":"Enable replay smoke tests (optional)","text":"<p>This runs lightweight replay checks against the latest indexed job per source.</p> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/replay-smoke-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-replay-smoke.timer\nsystemctl list-timers | rg healtharchive-replay-smoke || systemctl list-timers | grep healtharchive-replay-smoke\n</code></pre>"},{"location":"deployment/systemd/#enable-cleanup-automation-optional","title":"Enable cleanup automation (optional)","text":"<p>This runs safe <code>temp-nonwarc</code> cleanup for older indexed jobs (keeps WARCs).</p> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/cleanup-automation-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-cleanup-automation.timer\nsystemctl list-timers | rg healtharchive-cleanup-automation || systemctl list-timers | grep healtharchive-cleanup-automation\n</code></pre>"},{"location":"deployment/systemd/#enable-storage-hot-path-auto-recovery-optional-high-impact","title":"Enable storage hot-path auto-recovery (optional; high impact)","text":"<p>This automation attempts conservative self-healing for the specific failure class:</p> <ul> <li><code>OSError: [Errno 107] Transport endpoint is not connected</code></li> </ul> <p>It can unmount stale hot paths and re-apply tiering. It will only stop the worker when either:</p> <ul> <li>a running job output directory is detected as stale (Errno 107), or</li> <li>there are no running jobs (to prevent races while repairing mountpoints for the next jobs).</li> </ul> <p>It also probes the output dirs of the next queued/retryable jobs to prevent infra-error retry storms (a stale mountpoint for a retryable job should be repaired before the worker selects it).</p> <p>After successful mount recovery it restarts replay (best-effort) so replay sees a clean view of <code>/srv/healtharchive/jobs</code>.</p> <p>Keep it disabled by default and enable only after:</p> <ul> <li>Phase 1 alerting/metrics are working (you have visibility),</li> <li>you have validated the watchdog in dry-run mode first.</li> </ul> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/storage-hotpath-auto-recover-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-storage-hotpath-auto-recover.timer\nsystemctl list-timers | rg healtharchive-storage-hotpath-auto-recover || systemctl list-timers | grep healtharchive-storage-hotpath-auto-recover\n</code></pre> <p>Rollback:</p> <pre><code>sudo systemctl disable --now healtharchive-storage-hotpath-auto-recover.timer\nsudo rm -f /etc/healtharchive/storage-hotpath-auto-recover-enabled\n</code></pre> <p>The watchdog writes state under:</p> <ul> <li><code>/srv/healtharchive/ops/watchdog/storage-hotpath-auto-recover.json</code></li> </ul> <p>and emits node_exporter textfile metrics via:</p> <ul> <li><code>healtharchive_storage_hotpath_auto_recover.prom</code></li> </ul>"},{"location":"deployment/systemd/#enable-worker-auto-start-watchdog-optional-conservative","title":"Enable worker auto-start watchdog (optional; conservative)","text":"<p>This automation exists to prevent \u201ceverything stopped\u201d failures where the system is healthy enough to run, but <code>healtharchive-worker.service</code> is down and jobs are pending.</p> <p>It will only start the worker when all of these are true:</p> <ul> <li>the worker unit is inactive,</li> <li>there are pending crawl jobs (<code>status in (queued, retryable)</code>),</li> <li>the Storage Box mount is readable,</li> <li>the deploy lock is not present (or is stale),</li> <li>and there are no DB jobs in <code>status=running</code> (conservative safety gate).</li> </ul> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/worker-auto-start-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-worker-auto-start.timer\nsystemctl list-timers | rg healtharchive-worker-auto-start || systemctl list-timers | grep healtharchive-worker-auto-start\n</code></pre> <p>Rollback:</p> <pre><code>sudo systemctl disable --now healtharchive-worker-auto-start.timer\nsudo rm -f /etc/healtharchive/worker-auto-start-enabled\n</code></pre> <p>The watchdog writes state under:</p> <ul> <li><code>/srv/healtharchive/ops/watchdog/worker-auto-start.json</code></li> </ul> <p>and emits node_exporter textfile metrics via:</p> <ul> <li><code>healtharchive_worker_auto_start.prom</code></li> </ul>"},{"location":"deployment/systemd/#enable-baseline-drift-checks-recommended","title":"Enable baseline drift checks (recommended)","text":"<p>Baseline drift checks validate that production still matches the project\u2019s expected invariants (security posture, perms, unit enablement).</p> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/baseline-drift-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-baseline-drift-check.timer\nsystemctl list-timers | rg healtharchive-baseline-drift-check || systemctl list-timers | grep healtharchive-baseline-drift-check\n</code></pre> <p>Artifacts are written under:</p> <ul> <li><code>/srv/healtharchive/ops/baseline/</code></li> </ul> <p>If the drift check fails, inspect:</p> <ul> <li><code>/srv/healtharchive/ops/baseline/drift-report-latest.txt</code></li> <li><code>journalctl -u healtharchive-baseline-drift-check.service --no-pager -l</code></li> </ul>"},{"location":"deployment/systemd/#enable-public-surface-verification-optional-recommended","title":"Enable public surface verification (optional, recommended)","text":"<p>This is a deeper synthetic check than external uptime monitors. It validates:</p> <ul> <li>public API health, sources, search, snapshot detail and raw HTML</li> <li>replay browse URL (unless skipped)</li> <li>exports manifest and export endpoint HEADs</li> <li>changes feed + RSS</li> <li>key frontend pages, including <code>/brief</code>, <code>/cite</code>, <code>/methods</code>, and <code>/governance</code></li> </ul> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/public-verify-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-public-surface-verify.timer\nsystemctl list-timers | rg healtharchive-public-surface-verify || systemctl list-timers | grep healtharchive-public-surface-verify\n</code></pre>"},{"location":"deployment/systemd/#rollback-disable-quickly","title":"Rollback / disable quickly","text":"<ul> <li>Disable timer immediately:</li> </ul> <pre><code>sudo systemctl disable --now healtharchive-schedule-annual.timer\n</code></pre> <ul> <li>Disable all scheduling automation immediately:</li> </ul> <pre><code>sudo rm -f /etc/healtharchive/automation-enabled\n</code></pre> <ul> <li>Disable replay reconciliation automation immediately:</li> </ul> <pre><code>sudo systemctl disable --now healtharchive-replay-reconcile.timer\nsudo rm -f /etc/healtharchive/replay-automation-enabled\n</code></pre> <ul> <li>Disable annual search verification automation immediately:</li> </ul> <pre><code>sudo systemctl disable --now healtharchive-annual-search-verify.timer\n</code></pre> <ul> <li>Remove the worker priority override:</li> </ul> <pre><code>sudo rm -f /etc/systemd/system/healtharchive-worker.service.d/override.conf\nsudo systemctl daemon-reload\nsudo systemctl restart healtharchive-worker\n</code></pre>"},{"location":"development/","title":"Development docs","text":""},{"location":"development/#start-here","title":"Start Here","text":"<p>New developer? - Setup: Dev Environment Setup \u2014 Local setup guide - Test: Live Testing \u2014 Local testing workflows - Contribute: Testing Guidelines \u2014 Test conventions - Architecture: Architecture \u2014 How the code works</p> <p>Quick reference: | Task | Documentation | |------|---------------| | Run backend locally | Live Testing | | Run tests | Testing Guidelines | | Understand architecture | Architecture | | Deploy changes | Change to Production |</p>"},{"location":"development/#all-development-documentation","title":"All Development Documentation","text":"<ul> <li>Local testing flows (recommended): <code>live-testing.md</code></li> <li>Local + VPS setup (recommended): <code>dev-environment-setup.md</code></li> <li>Backend testing conventions: <code>testing-guidelines.md</code></li> <li>Development playbooks (task workflows): <code>playbooks/README.md</code></li> </ul>"},{"location":"development/#code-annotations-demo","title":"Code Annotations (Demo)","text":"<p>This project uses MkDocs Material code annotations to provide inline context for complex configurations:</p> <pre><code># Example docker-compose.yml\nservices:\n  api:\n    image: healtharchive-backend:latest\n    ports:\n      - \"8001:8001\" # (1)\n    environment:\n      - HEALTHARCHIVE_DATABASE_URL=sqlite:///data.db # (2)\n</code></pre> <ol> <li>Standard FastAPI port for local development.</li> <li>Default SQLite path inside the container.</li> </ol>"},{"location":"development/dev-environment-setup/","title":"Developer environment setup (local + VPS)","text":"<p>This document answers two questions:</p> <p>1) How to set up a local dev environment for HealthArchive (backend + frontend). 2) Where to run which commands (your dev machine vs the production VPS).</p> <p>For full backend live-testing flows, see <code>live-testing.md</code>.</p>"},{"location":"development/dev-environment-setup/#repo-layout","title":"Repo layout","text":"<p>HealthArchive currently lives as multiple repos:</p> <ul> <li>Backend: https://github.com/jerdaw/healtharchive-backend (local dir: <code>healtharchive-backend/</code>)</li> <li>Frontend: https://github.com/jerdaw/healtharchive-frontend (local dir: <code>healtharchive-frontend/</code>)</li> <li>Datasets: https://github.com/jerdaw/healtharchive-datasets (local dir: <code>healtharchive-datasets/</code>)</li> </ul> <p>Many developers keep them in one folder and use the mono-repo root <code>Makefile</code> to run checks across repos.</p>"},{"location":"development/dev-environment-setup/#local-machine-setup-recommended","title":"Local machine setup (recommended)","text":""},{"location":"development/dev-environment-setup/#0-prereqs","title":"0) Prereqs","text":"<ul> <li><code>git</code></li> <li><code>python3</code> (match <code>healtharchive-backend</code> requirements)</li> <li><code>node</code> (match frontend <code>engines.node</code>: https://github.com/jerdaw/healtharchive-frontend/blob/main/package.json)</li> <li><code>make</code></li> </ul> <p>Recommended:</p> <ul> <li><code>pipx</code> (for global Python tools like <code>pre-commit</code>)</li> <li>Docker (only needed for end-to-end crawling runs)</li> </ul>"},{"location":"development/dev-environment-setup/#1-backend-setup-local","title":"1) Backend setup (local)","text":"<p>From <code>healtharchive-backend/</code>:</p> <pre><code>make venv\nmake check\n</code></pre> <p>Then follow <code>docs/development/live-testing.md</code> for running the API locally, running worker flows, and Docker-based crawling tests.</p>"},{"location":"development/dev-environment-setup/#2-frontend-setup-local","title":"2) Frontend setup (local)","text":"<p>From <code>healtharchive-frontend/</code>:</p> <pre><code>npm ci\nnpm run check\n</code></pre>"},{"location":"development/dev-environment-setup/#3-local-guardrails-recommended-for-solo-fast-direct-to-main","title":"3) Local guardrails (recommended for solo-fast direct-to-main)","text":"<p>If you\u2019re moving fast and pushing directly to <code>main</code>, you want local guardrails that reduce \u201coops I forgot to run checks\u201d mistakes.</p>"},{"location":"development/dev-environment-setup/#one-command-check-workspace-root","title":"One-command check (workspace root)","text":"<p>From the mono-repo root:</p> <pre><code>make check\n</code></pre> <p>This runs:</p> <ul> <li>backend <code>make check</code></li> <li>frontend <code>pre-commit run --all-files</code> + <code>npm run check</code></li> <li>datasets checks</li> </ul>"},{"location":"development/dev-environment-setup/#pre-push-hooks-recommended","title":"Pre-push hooks (recommended)","text":"<p>These run automatically on <code>git push</code>:</p> <ul> <li>Backend (runs <code>make check</code>; set <code>HA_PRE_PUSH_FULL=1</code> to run <code>make check-full</code>):</li> <li><code>scripts/install-pre-push-hook.sh</code></li> <li>Frontend (runs <code>pre-commit</code> + <code>npm run check</code>):</li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/scripts/install-pre-push-hook.sh</li> </ul> <p>Install them on your dev machine:</p> <pre><code>cd healtharchive-backend &amp;&amp; ./scripts/install-pre-push-hook.sh\ncd ../healtharchive-frontend &amp;&amp; ./scripts/install-pre-push-hook.sh\n</code></pre> <p>Bypass once if needed (emergency only):</p> <ul> <li><code>git push --no-verify</code></li> <li>or set <code>HA_SKIP_PRE_PUSH=1</code></li> </ul>"},{"location":"development/dev-environment-setup/#pre-commit-recommended","title":"Pre-commit (recommended)","text":"<p>Both repos include <code>.pre-commit-config.yaml</code>.</p> <ul> <li>Install once: <code>pipx install pre-commit</code></li> <li>Enable \u201crun on commit\u201d inside each repo:</li> <li><code>pre-commit install</code></li> </ul>"},{"location":"development/dev-environment-setup/#vps-usage-production","title":"VPS usage (production)","text":""},{"location":"development/dev-environment-setup/#what-runs-on-the-vps","title":"What runs on the VPS","text":"<p>Run these on the production VPS (typically from <code>/opt/healtharchive-backend</code>):</p> <ul> <li>Deploy + restart services:</li> <li><code>./scripts/vps-deploy.sh --apply</code></li> <li>Production verification gates:</li> <li><code>./scripts/check_baseline_drift.py --mode live</code></li> <li><code>./scripts/verify_public_surface.py</code></li> <li>Ops bootstrap / automation helpers (recommended):</li> <li>one-time: <code>sudo ./scripts/vps-bootstrap-ops-dirs.sh</code></li> <li>install/update systemd templates: <code>sudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker</code></li> <li>verify timers/sentinels: <code>./scripts/verify_ops_automation.sh</code></li> </ul> <p>Recommended deploy flow (single command):</p> <pre><code>./scripts/vps-deploy.sh --apply --baseline-mode live\n</code></pre> <p>Note: systemd timer enablement is explicit and gated by sentinel files under <code>/etc/healtharchive/</code>. For enable/rollback steps, see <code>../deployment/systemd/README.md</code>.</p>"},{"location":"development/dev-environment-setup/#what-should-not-run-on-the-vps","title":"What should not run on the VPS","text":"<p>These are local-developer guardrails and should run on your dev machine:</p> <ul> <li><code>make check</code> (workspace root)</li> <li><code>healtharchive-backend/scripts/install-pre-push-hook.sh</code> (this repo)</li> <li>Frontend hook installer: https://github.com/jerdaw/healtharchive-frontend/blob/main/scripts/install-pre-push-hook.sh</li> </ul> <p>Reason: hooks install into <code>.git/hooks/</code> for the repo you\u2019re pushing from (your laptop/workstation), not on the server.</p>"},{"location":"development/live-testing/","title":"HealthArchive Backend \u2013 Live Testing Guide","text":"<p>This document describes a practical, incremental way to live\u2011test the <code>healtharchive-backend</code> in a local development environment, starting with the smallest checks and working up to more realistic scenarios.</p> <p>It assumes you are working from the repo root and are comfortable with a terminal and Python tooling.</p>"},{"location":"development/live-testing/#0-onetime-setup","title":"0. One\u2011time setup","text":""},{"location":"development/live-testing/#01-create-a-virtualenv-and-install-the-backend","title":"0.1 Create a virtualenv and install the backend","text":"<pre><code>make venv\n# or (manual):\n# python -m venv .venv\n# source .venv/bin/activate\n# pip install -e \".[dev]\"\n</code></pre> <p>This provides:</p> <ul> <li><code>ha-backend</code> \u2013 backend CLI</li> <li><code>archive-tool</code> \u2013 crawler CLI implemented by the in-repo <code>archive_tool</code> package (uses Docker + Zimit)</li> </ul>"},{"location":"development/live-testing/#02-configure-environment-variables","title":"0.2 Configure environment variables","text":"<p>Use a local SQLite DB and archive root so you never touch production paths:</p> <pre><code>export HEALTHARCHIVE_DATABASE_URL=sqlite:///$(pwd)/.dev-healtharchive.db\nexport HEALTHARCHIVE_ARCHIVE_ROOT=$(pwd)/.dev-archive-root\nexport HEALTHARCHIVE_ADMIN_TOKEN=localdev-admin  # optional but recommended\n# Optional: set CORS origins if your frontend runs on a non-default host\n# (defaults already include http://localhost:3000 and https://healtharchive.ca)\n# export HEALTHARCHIVE_CORS_ORIGINS=http://localhost:3000\n\n# Shortcut: copy the sample file and source it\n# cp .env.example .env\n# source .env\n</code></pre> <p>Run Alembic migrations once to create the schema:</p> <pre><code>alembic upgrade head\n</code></pre>"},{"location":"development/live-testing/#1-smallest-checks-no-docker-no-jobs","title":"1. Smallest checks (no Docker, no jobs)","text":"<p>Goal: prove the Python package and DB wiring work in isolation.</p>"},{"location":"development/live-testing/#11-run-the-test-suite","title":"1.1 Run the test suite","text":"<pre><code>make ci\n# or (full suite):\n# make check-full\n# or (tests only):\n# pytest -q\n</code></pre> <p>All tests should pass. (At time of writing, a 422 around <code>/api/admin/jobs/status-counts</code> was fixed so this now passes too.)</p>"},{"location":"development/live-testing/#12-check-db-connectivity","title":"1.2 Check DB connectivity","text":"<pre><code>ha-backend check-db\n</code></pre> <p>You should see:</p> <ul> <li>The <code>HEALTHARCHIVE_DATABASE_URL</code> you set.</li> <li>\u201cDatabase connection OK.\u201d</li> </ul>"},{"location":"development/live-testing/#13-check-environment-archive-root","title":"1.3 Check environment / archive root","text":"<pre><code>ha-backend check-env\n</code></pre> <p>Confirms:</p> <ul> <li><code>HEALTHARCHIVE_ARCHIVE_ROOT</code> exists and is writable.</li> <li>The configured <code>archive_tool</code> command is resolvable.</li> </ul>"},{"location":"development/live-testing/#2-apionly-live-smoke-tests-no-archive_tool-no-jobs","title":"2. API\u2011only live smoke tests (no archive_tool, no jobs)","text":"<p>Goal: run FastAPI + DB with an empty dataset.</p>"},{"location":"development/live-testing/#21-start-the-api","title":"2.1 Start the API","text":"<p>In one terminal (with <code>.venv</code> active and env vars set):</p> <pre><code>uvicorn ha_backend.api:app --reload --port 8001\n</code></pre>"},{"location":"development/live-testing/#22-hit-public-endpoints","title":"2.2 Hit public endpoints","text":"<p>From another terminal:</p> <pre><code>curl http://localhost:8001/api/health\ncurl http://localhost:8001/api/sources\ncurl \"http://localhost:8001/api/search?q=test\"\n</code></pre> <p>Expect:</p> <ul> <li><code>/api/health</code> \u2192 <code>{\"status\":\"ok\", ... \"db\":\"ok\" ...}</code>.</li> <li><code>/api/sources</code> \u2192 <code>[]</code> (no data yet).</li> <li><code>/api/search</code> \u2192 empty results, but HTTP 200.</li> </ul>"},{"location":"development/live-testing/#23-admin-endpoints","title":"2.3 Admin endpoints","text":"<p>With <code>HEALTHARCHIVE_ADMIN_TOKEN</code> unset (local dev only):</p> <pre><code>curl http://localhost:8001/api/admin/jobs\n</code></pre> <p>Admin routes are open (dev mode).</p> <p>With <code>HEALTHARCHIVE_ADMIN_TOKEN=localdev-admin</code> set when starting uvicorn:</p> <pre><code>curl -H \"Authorization: Bearer localdev-admin\" \\\n  http://localhost:8001/api/admin/jobs\n</code></pre> <p>Confirms admin auth + simple bearer token protection.</p>"},{"location":"development/live-testing/#24-admin-access-patterns-local-vs-stagingprod","title":"2.4 Admin access patterns (local vs staging/prod)","text":"<p>In local development it is acceptable to either leave <code>HEALTHARCHIVE_ADMIN_TOKEN</code> unset (open admin endpoints) or to use a simple token like <code>localdev-admin</code> as shown above.</p> <p>In staging and production you should always set a strong, random admin token and treat it as a secret:</p> <pre><code>export HEALTHARCHIVE_ADMIN_TOKEN=\"prod-admin-token-from-secret-store\"\nuvicorn ha_backend.api:app --host 0.0.0.0 --port 8001\n</code></pre> <p>From a trusted machine you can then verify access:</p> <ul> <li>Without a token (should be forbidden when the env var is set):</li> </ul> <pre><code>curl -i \"https://api.healtharchive.ca/api/admin/jobs\"\ncurl -i \"https://api.healtharchive.ca/metrics\"\n</code></pre> <ul> <li>With the correct token:</li> </ul> <pre><code>curl -i \\\n  -H \"Authorization: Bearer $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  \"https://api.healtharchive.ca/api/admin/jobs\"\n\ncurl -i \\\n  -H \"Authorization: Bearer $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  \"https://api.healtharchive.ca/metrics\"\n</code></pre> <p>In staging/prod you should call these endpoints only from operator tooling or monitoring systems (Prometheus, etc.), not from the public frontend.</p>"},{"location":"development/live-testing/#3-minimal-archive_tool-integration-sanity-only","title":"3. Minimal archive_tool integration (sanity only)","text":"<p>Goal: prove Docker + <code>archive_tool</code> wiring work without committing to long crawls.</p>"},{"location":"development/live-testing/#31-verify-archive_tool-docker","title":"3.1 Verify archive_tool &amp; Docker","text":"<pre><code>ha-backend check-archive-tool\n</code></pre> <p>This runs <code>archive-tool --help</code> via the configured command (by default <code>archive-tool</code>, which uses Docker).</p> <p>If this fails, fix Docker or PATH before proceeding.</p>"},{"location":"development/live-testing/#32-optional-direct-archive_tool-dry-run","title":"3.2 Optional: direct archive_tool dry run","text":"<p>From the repo root:</p> <pre><code>archive-tool --seeds https://example.org --name example --output-dir $(pwd)/.dev-archive-root/dry-run --dry-run\n</code></pre> <p>This is not required for backend work, but is a quick sanity check that the integrated crawler CLI works directly and that your configuration (seeds, output directory, workers, monitoring flags) is valid without actually starting Docker containers.</p>"},{"location":"development/live-testing/#4-small-dbbacked-job-pipeline-in-a-dev-sandbox","title":"4. Small DB\u2011backed job pipeline in a dev sandbox","text":"<p>Goal: run a single small job end\u2011to\u2011end (create \u2192 run \u2192 index) using the same flows the worker will use, with the important caveat that the current Zimit image may not leave WARCs accessible (see notes below).</p>"},{"location":"development/live-testing/#41-seed-sources","title":"4.1 Seed sources","text":"<pre><code>ha-backend seed-sources\n</code></pre> <p>This inserts baseline <code>Source</code> rows (e.g., <code>hc</code>, <code>phac</code>).</p> <pre><code>ha-backend list-jobs\n</code></pre> <p>Should still show no <code>ArchiveJob</code> rows initially.</p>"},{"location":"development/live-testing/#42-create-a-job","title":"4.2 Create a job","text":"<p>Start with Health Canada:</p> <pre><code>ha-backend create-job --source hc\n</code></pre> <p>Note the printed job ID (call it <code>JOB_ID</code>). At this point:</p> <ul> <li>A DB row exists with <code>status=\"queued\"</code>.</li> <li>An <code>output_dir</code> path under <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> is reserved.</li> </ul>"},{"location":"development/live-testing/#43-run-the-crawl-once","title":"4.3 Run the crawl once","text":"<pre><code>ha-backend run-db-job --id JOB_ID\n</code></pre> <p>This:</p> <ul> <li>Loads the DB row.</li> <li>Constructs an <code>archive_tool</code> command.</li> <li>Runs Docker + Zimit.</li> </ul> <p>It can take a minute or more depending on seeds and limits. If it fails:</p> <ul> <li>Inspect <code>ha-backend show-job --id JOB_ID</code> for <code>crawler_exit_code</code>,   <code>status</code>, and <code>output_dir</code>.</li> <li>Check logs under that <code>output_dir</code> with <code>ls</code> and <code>less</code>.</li> </ul> <p>Note: With the current Zimit image and defaults, small runs may still end with <code>FAILED_NO_WARCS</code> because no accessible WARCs are left under <code>/output/.tmp*</code>. This is a limitation of the current crawler image and does not block backend/API development (see section 6 for a controlled WARC test).</p>"},{"location":"development/live-testing/#44-index-the-job-best-effort","title":"4.4 Index the job (best effort)","text":"<p>Once a job has <code>status=\"completed\"</code>, you can attempt:</p> <pre><code>ha-backend index-job --id JOB_ID\n</code></pre> <p>This:</p> <ul> <li>Runs WARC discovery based on <code>job.output_dir</code>.</li> <li>Streams WARCs into <code>Snapshot</code> rows.</li> <li>Updates <code>warc_file_count</code> and <code>indexed_page_count</code>.</li> </ul> <p>If no WARCs are discovered, the job is marked <code>index_failed</code>. This is expected when the crawler leaves no accessible WARCs.</p>"},{"location":"development/live-testing/#45-verify-via-cli-and-api","title":"4.5 Verify via CLI and API","text":"<p>CLI:</p> <pre><code>ha-backend show-job --id JOB_ID\n</code></pre> <p>Look for:</p> <ul> <li><code>status=\"indexed\"</code> and <code>indexed_page_count &gt; 0</code> (ideal case), or</li> <li><code>status=\"index_failed\"</code> if no WARCs were found.</li> </ul> <p>API (with uvicorn running):</p> <pre><code>curl http://localhost:8001/api/sources\ncurl \"http://localhost:8001/api/search?q=health&amp;source=hc\"\n</code></pre> <p>If indexing succeeded, these will reflect real crawl data. In practice, for development we often use synthetic snapshots instead (see 6.2).</p>"},{"location":"development/live-testing/#5-worker-loop-tests-background-processing","title":"5. Worker loop tests (background processing)","text":"<p>Goal: test the long\u2011running worker process that automates job execution.</p>"},{"location":"development/live-testing/#51-queue-a-couple-of-jobs","title":"5.1 Queue a couple of jobs","text":"<pre><code>ha-backend create-job --source hc\nha-backend create-job --source phac\nha-backend list-jobs\n</code></pre> <p>You should see the new jobs in <code>status=\"queued\"</code>.</p>"},{"location":"development/live-testing/#52-run-worker-in-singlecycle-mode","title":"5.2 Run worker in single\u2011cycle mode","text":"<pre><code>ha-backend start-worker --once\n</code></pre> <p>The worker:</p> <ul> <li>Picks the oldest <code>queued</code>/<code>retryable</code> job.</li> <li>Runs <code>run_persistent_job(job_id)</code> (archive_tool).</li> <li>Immediately runs <code>index_job(job_id)</code>.</li> <li>Exits after one iteration.</li> </ul> <p>Check transitions:</p> <pre><code>ha-backend list-jobs\n</code></pre> <p>Statuses should move (e.g., <code>queued</code> \u2192 <code>completed</code>/<code>index_failed</code>).</p>"},{"location":"development/live-testing/#53-worker-loop-with-a-harmless-tool-command-optional","title":"5.3 Worker loop with a harmless tool command (optional)","text":"<p>For pure orchestration tests, point <code>archive_tool</code> at <code>echo</code>:</p> <pre><code>export HEALTHARCHIVE_TOOL_CMD=echo\n</code></pre> <p>Then:</p> <pre><code>ha-backend create-job --source hc\nha-backend start-worker --once\nha-backend list-jobs\n</code></pre> <p>You will see jobs flip from <code>queued</code> to <code>completed</code> (crawl RC 0) and then to <code>index_failed</code> (no WARCs), verifying the worker loop and status updates without touching Docker.</p>"},{"location":"development/live-testing/#6-raw-snapshot-viewer-tests","title":"6. Raw snapshot viewer tests","text":"<p>Goal: confirm WARC \u2192 HTML replay is functioning.</p> <p>There are two complementary approaches:</p>"},{"location":"development/live-testing/#61-happypath-viewer-using-a-synthetic-warc","title":"6.1 Happy\u2011path viewer using a synthetic WARC","text":"<p>You can create a tiny WARC file and a corresponding <code>Snapshot</code> in the DB:</p> <pre><code>python - &lt;&lt; 'PY'\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom io import BytesIO\n\nfrom warcio.warcwriter import WARCWriter\n\nfrom ha_backend.db import get_session\nfrom ha_backend.models import Source, Snapshot\n\nroot = Path(\".dev-archive-root\") / \"manual-warcs\"\nwarc_path = root / \"viewer-test.warc.gz\"\nurl = \"https://example.org/page\"\nhtml_body = \"&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hello from WARC&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\"\n\nroot.mkdir(parents=True, exist_ok=True)\nwith warc_path.open(\"wb\") as f:\n    writer = WARCWriter(f, gzip=True)\n    payload = BytesIO(\n        (\n            \"HTTP/1.1 200 OK\\\\r\\\\n\"\n            \"Content-Type: text/html; charset=utf-8\\\\r\\\\n\"\n            f\"Content-Length: {len(html_body.encode('utf-8'))}\\\\r\\\\n\"\n            \"\\\\r\\\\n\" +\n            html_body\n        ).encode(\"utf-8\")\n    )\n    record = writer.create_warc_record(\n        uri=url,\n        record_type=\"response\",\n        payload=payload,\n        warc_headers_dict={\"WARC-Date\": \"2025-01-01T12:00:00Z\"},\n    )\n    writer.write_record(record)\n    record_id = record.rec_headers.get_header(\"WARC-Record-ID\")\n\nwith get_session() as session:\n    src = session.query(Source).filter_by(code=\"test\").one_or_none()\n    if src is None:\n        src = Source(\n            code=\"test\",\n            name=\"Test Source\",\n            base_url=\"https://example.org\",\n            description=\"Test source for viewer\",\n            enabled=True,\n        )\n        session.add(src)\n        session.flush()\n\n    snap = Snapshot(\n        job_id=None,\n        source_id=src.id,\n        url=url,\n        normalized_url_group=url,\n        capture_timestamp=datetime(2025, 1, 1, 12, 0, tzinfo=timezone.utc),\n        mime_type=\"text/html\",\n        status_code=200,\n        title=\"Viewer Test Page\",\n        snippet=\"Hello from WARC\",\n        language=\"en\",\n        warc_path=str(warc_path),\n        warc_record_id=record_id,\n    )\n    session.add(snap)\n    session.flush()\n    print(\"SNAPSHOT_ID\", snap.id)\nPY\n</code></pre> <p>Note the printed <code>SNAPSHOT_ID</code> (for example <code>5</code>), then:</p> <pre><code>curl \"http://localhost:8001/api/snapshots/raw/5\"\n</code></pre> <p>You should see the HTML body with <code>\"Hello from WARC\"</code>, confirming that:</p> <ul> <li><code>warc_path</code> is valid.</li> <li><code>warc_record_id</code> is used for lookup.</li> <li><code>viewer.py</code> can reconstruct and return HTML.</li> </ul>"},{"location":"development/live-testing/#62-error-path-when-warcs-are-missing","title":"6.2 Error path when WARCs are missing","text":"<p>For snapshots whose <code>warc_path</code> points to a non\u2011existent file (e.g., your seeded dev snapshots), the route returns a meaningful error:</p> <pre><code>curl \"http://localhost:8001/api/snapshots/raw/1\"\n</code></pre> <p>Returns HTTP 404 with <code>{\"detail\":\"Underlying WARC file for this snapshot is missing\"}</code>.</p>"},{"location":"development/live-testing/#7-real-warc-indexing-advanced","title":"7. Real WARC indexing (advanced)","text":"<p>Goal: take a small real Zimit crawl, fix any permission issues, and index its WARCs into snapshots for use via the HTTP API.</p> <p>This section assumes you have already run a small crawl with something like:</p> <pre><code>ha-backend run-job \\\n  --name hc-dev-warcs \\\n  --seeds https://www.canada.ca/en/health-canada.html \\\n  --initial-workers 1 \\\n  --log-level INFO \\\n  -- \\\n  --pageLimit 5 \\\n  --depth 1\n</code></pre> <p>and have a job directory such as:</p> <pre><code>.dev-archive-root/20251210T013134Z__hc-dev-warcs\n</code></pre>"},{"location":"development/live-testing/#71-fix-permissions-on-the-temp-dir-if-needed","title":"7.1 Fix permissions on the temp dir (if needed)","text":"<p>Zimit may create <code>.tmp*</code> directories owned by <code>root</code>, which prevents the backend from reading WARCs. In the job directory:</p> <pre><code>cd .dev-archive-root/20251210T013134Z__hc-dev-warcs\nls -ld .tmp*\n</code></pre> <p>If you see <code>drwx------ root root ...</code>, fix ownership:</p> <pre><code>sudo chown -R $(id -u):$(id -g) .tmp*\n</code></pre> <p>Verify you can see WARCs:</p> <pre><code>find . -maxdepth 6 -type f \\( -name '*.warc' -o -name '*.warc.gz' \\) -print\n</code></pre> <p>You should see something like:</p> <pre><code>./.tmpXXXX/collections/crawl-.../archive/rec-...warc.gz\n</code></pre>"},{"location":"development/live-testing/#72-create-a-db-job-pointing-at-this-output_dir","title":"7.2 Create a DB job pointing at this output_dir","text":"<p>From the repo root:</p> <pre><code>python - &lt;&lt; 'PY'\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob, Source\n\njob_dir = Path(\".dev-archive-root/20251210T013134Z__hc-dev-warcs\").resolve()\n\nwith get_session() as session:\n    src = session.query(Source).filter_by(code=\"hc\").one()\n    now = datetime.now(timezone.utc)\n    job = ArchiveJob(\n        source_id=src.id,\n        name=\"hc-dev-warcs\",\n        output_dir=str(job_dir),\n        status=\"completed\",   # ready for indexing\n        queued_at=now,\n        started_at=now,\n        finished_at=now,\n    )\n    session.add(job)\n    session.flush()\n    print(\"JOB_ID\", job.id)\nPY\n</code></pre> <p>Note the printed <code>JOB_ID</code> (e.g. <code>11</code>).</p> <p>Alternative (CLI): you can now do the same with a helper command:</p> <pre><code>ha-backend register-job-dir \\\n  --source hc \\\n  --output-dir .dev-archive-root/20251210T013134Z__hc-dev-warcs \\\n  --name hc-dev-warcs\n</code></pre> <p>This creates a DB row in <code>status=\"completed\"</code> so it is ready for indexing.</p>"},{"location":"development/live-testing/#73-index-the-job-and-verify-via-api","title":"7.3 Index the job and verify via API","text":"<p>Index the WARCs:</p> <pre><code>ha-backend index-job --id JOB_ID\nha-backend show-job --id JOB_ID\n</code></pre> <p>You should see:</p> <ul> <li><code>status=\"indexed\"</code></li> <li><code>warc_file_count &gt; 0</code></li> <li><code>indexed_page_count &gt; 0</code></li> </ul> <p>With uvicorn running:</p> <pre><code>curl http://localhost:8001/api/sources\ncurl \"http://localhost:8001/api/search?q=health&amp;source=hc\"\n</code></pre> <p>You will see the real crawl snapshots alongside any synthetic dev data.</p>"},{"location":"development/live-testing/#8-admin-retry-and-cleanup-flows","title":"8. Admin, retry, and cleanup flows","text":"<p>Goal: exercise non\u2011happy\u2011path and maintenance commands.</p>"},{"location":"development/live-testing/#81-retry-jobs","title":"8.1 Retry jobs","text":"<p>If a job has <code>status=\"failed\"</code> or <code>status=\"index_failed\"</code>:</p> <pre><code>ha-backend retry-job --id JOB_ID\nha-backend show-job --id JOB_ID\n</code></pre> <p>Behavior:</p> <ul> <li><code>status=\"failed\"</code> \u2192 <code>status=\"retryable\"</code> (for another crawl).</li> <li><code>status=\"index_failed\"</code> \u2192 <code>status=\"completed\"</code> (allowing re\u2011indexing).</li> </ul>"},{"location":"development/live-testing/#82-cleanup-temp-dirs-and-state","title":"8.2 Cleanup temp dirs and state","text":"<p>Only allowed for <code>status in {\"indexed\", \"index_failed\"}</code>:</p> <pre><code>ha-backend cleanup-job --id JOB_ID --mode temp\nha-backend show-job --id JOB_ID\n</code></pre> <p>This:</p> <ul> <li>Locates temp dirs and <code>.archive_state.json</code> via <code>archive_tool.state</code>.</li> <li>Deletes <code>.tmp*</code> dirs and the state file.</li> <li>Sets:</li> <li><code>cleanup_status = \"temp_cleaned\"</code></li> <li><code>cleaned_at</code> to the cleanup time</li> <li><code>state_file_path = None</code></li> </ul> <p>Caution: This removes temp crawl artifacts (including WARCs) under <code>.tmp*</code> for that job. Only run it once you are satisfied with indexing and any ZIMs/exports.</p> <p>If you are using the replay service (pywb) to serve this job\u2019s WARCs, do not run <code>cleanup-job --mode temp</code> for that job \u2014 replay depends on the WARCs remaining on disk.</p> <p>If replay is enabled globally (<code>HEALTHARCHIVE_REPLAY_BASE_URL</code> is set), <code>cleanup-job --mode temp</code> will refuse unless you pass <code>--force</code>. Treat <code>--force</code> as an emergency override (it can break replay by deleting WARCs).</p>"},{"location":"development/live-testing/#9-metrics-and-observability","title":"9. Metrics and observability","text":"<p>Goal: validate Prometheus\u2011style metrics.</p> <p>With uvicorn running:</p> <pre><code>curl -H \"Authorization: Bearer localdev-admin\" \\\n  http://localhost:8001/metrics | head\n</code></pre> <p>Look for:</p> <ul> <li>Job status metrics:</li> </ul> <pre><code>healtharchive_jobs_total{status=\"failed\"} 6\nhealtharchive_jobs_total{status=\"indexed\"} 1\n...\n</code></pre> <ul> <li>Cleanup status metrics:</li> </ul> <pre><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"none\"} ...\nhealtharchive_jobs_cleanup_status_total{cleanup_status=\"temp_cleaned\"} ...\n</code></pre> <ul> <li>Snapshot metrics:</li> </ul> <pre><code>healtharchive_snapshots_total 5\nhealtharchive_snapshots_total{source=\"hc\"} 3\nhealtharchive_snapshots_total{source=\"test\"} 2\n</code></pre> <ul> <li>Page-level crawl metrics (best-effort from crawl logs):</li> </ul> <pre><code>healtharchive_jobs_pages_crawled_total 1234\nhealtharchive_jobs_pages_crawled_total{source=\"hc\"} 789\nhealtharchive_jobs_pages_failed_total 12\nhealtharchive_jobs_pages_failed_total{source=\"hc\"} 3\n</code></pre> <p>Counts should roughly match <code>ha-backend list-jobs</code>, <code>/api/sources</code> / <code>/api/search</code>, and the page counters shown in <code>/api/admin/jobs/{id}</code>.</p>"},{"location":"development/live-testing/#10-scaling-up-to-more-realistic-scenarios","title":"10. Scaling up to more realistic scenarios","text":"<p>Once the above is stable, you can incrementally increase realism:</p> <ul> <li>Multiple jobs with the worker running continuously.</li> </ul> <pre><code>ha-backend start-worker --poll-interval 30\n</code></pre> <p>In another terminal, periodically run <code>create-job</code> and watch statuses   transition through <code>queued \u2192 running \u2192 completed \u2192 indexed/index_failed</code>.</p> <ul> <li> <p>Postgres instead of SQLite by pointing   <code>HEALTHARCHIVE_DATABASE_URL</code> at a dev Postgres instance and re\u2011running   <code>alembic upgrade head</code>.</p> </li> <li> <p>Monitoring/adaptive options via <code>job_registry</code> overrides:</p> </li> <li> <p>Enable <code>enable_monitoring</code>, <code>enable_adaptive_workers</code>,     <code>enable_vpn_rotation</code> and confirm they affect archive_tool behavior.</p> </li> <li> <p>Frontend integration by running the separate <code>healtharchive-frontend</code>   against your local backend (<code>NEXT_PUBLIC_BACKEND_URL=http://localhost:8001</code>)   and exercising the full UI \u2192 API \u2192 DB \u2192 WARC stack.</p> </li> </ul> <p>Note on real WARCs: At time of writing, the default Zimit image may not leave WARCs in the expected <code>/output/.tmp*/collections/.../archive</code> path or may create temp directories with restrictive permissions. For backend/API and viewer development, using synthetic WARCs (as in 6.1) and seeded snapshots is sufficient. Integrating with live WARCs may require either adjusting Zimit options or updating WARC discovery to match the crawler\u2019s current layout and permissions.</p>"},{"location":"development/testing-guidelines/","title":"Backend testing guidelines (internal)","text":"<p>This doc describes the backend testing expectations and how to run checks locally.</p> <p>If you want step-by-step \u201crun the app and click it\u201d workflows, use:</p> <ul> <li><code>live-testing.md</code></li> </ul>"},{"location":"development/testing-guidelines/#what-ci-runs-recommended-locally","title":"What CI runs (recommended locally)","text":"<p>From the repo root:</p> <ul> <li><code>make check</code> (fast CI gate: format check, lint, typecheck, tests)</li> <li><code>make check-full</code> (optional: pre-commit, security scan, docs build/lint)</li> </ul> <p><code>make check</code> is intentionally kept low-friction so it can run constantly without blocking development. Use <code>make check-full</code> before deploys or when you want stricter validation.</p> <p>Notes:</p> <ul> <li><code>make check</code> runs <code>make test-fast</code> (a curated subset).</li> <li><code>make test-all</code> runs the full test suite.</li> </ul>"},{"location":"development/testing-guidelines/#end-to-end-smoke-public-surface","title":"End-to-end smoke (public surface)","text":"<p>CI also runs a fast end-to-end smoke check that starts the backend + frontend locally and verifies user-critical routes (no browser automation):</p> <ul> <li><code>./scripts/ci-e2e-smoke.sh --frontend-dir ../healtharchive-frontend</code></li> <li>If the frontend is already built (CI artifact), add: <code>--skip-frontend-build</code></li> </ul> <p>In CI, the smoke check is treated as a post-merge safety net (runs on <code>main</code> pushes / manual runs) rather than a PR gate.</p>"},{"location":"development/testing-guidelines/#running-subsets","title":"Running subsets","text":"<ul> <li>Unit tests: <code>pytest</code></li> <li>One test file: <code>pytest tests/test_something.py</code></li> <li>One test: <code>pytest -k some_keyword</code></li> <li>Lint + format: <code>ruff check .</code> and <code>ruff format --check .</code></li> <li>Type-check: <code>mypy src tests</code></li> </ul>"},{"location":"development/testing-guidelines/#writing-tests","title":"Writing tests","text":"<ul> <li>Put tests in <code>tests/</code> and prefer plain <code>pytest</code> tests (no custom harness).</li> <li>Keep tests deterministic:</li> <li>avoid real network calls</li> <li>avoid wall-clock dependencies</li> <li>avoid global state between tests</li> <li>If you add a new API route, add at least one test that exercises the route and asserts the key behavior.</li> <li>If you change DB behavior, prefer tests that set up a temporary DB using the existing test fixtures/patterns.</li> </ul>"},{"location":"development/testing-guidelines/#scope-what-belongs-in-tests-vs-scripts","title":"Scope (what belongs in tests vs scripts)","text":"<ul> <li>Application behavior belongs in <code>tests/</code>.</li> <li>VPS automation scripts under <code>scripts/</code> should stay simple and safe; when logic grows (parsing, policy evaluation), prefer moving that logic into a small Python module that can be tested.</li> </ul>"},{"location":"development/playbooks/","title":"Development playbooks (task-oriented)","text":"<p>These playbooks describe common developer workflows without duplicating deeper docs.</p> <ul> <li>Local environment setup: <code>../dev-environment-setup.md</code></li> <li>Local end-to-end testing flows: <code>../live-testing.md</code></li> <li>Change \u2192 production workflow (solo-fast): <code>change-to-production.md</code></li> </ul>"},{"location":"development/playbooks/change-to-production/","title":"Change \u2192 production workflow (solo-fast)","text":"<p>Goal: ship a change safely while keeping \u201cgreen main\u201d as the deploy gate.</p> <p>Canonical references:</p> <ul> <li>Docs guidelines: <code>../../documentation-guidelines.md</code></li> <li>Monitoring/CI gate: <code>../../operations/monitoring-and-ci-checklist.md</code></li> <li>Deploy playbook (VPS): <code>../../operations/playbooks/deploy-and-verify.md</code></li> </ul>"},{"location":"development/playbooks/change-to-production/#workflow","title":"Workflow","text":"<ol> <li>Make the change locally.</li> <li>Run checks:</li> <li><code>make check</code></li> <li>Commit and push.</li> <li>Wait for CI to pass on <code>main</code>.</li> <li>Deploy on the VPS using the deploy playbook.</li> </ol>"},{"location":"development/playbooks/change-to-production/#cross-repo-guardrails","title":"Cross-repo guardrails","text":"<ul> <li>If you add/change a user-facing frontend route that is part of the production \u201cpublic surface\u201d, update:</li> <li><code>scripts/verify_public_surface.py</code></li> <li>Frontend bilingual rules (in the frontend repo): https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/development/bilingual-dev-guide.md</li> </ul>"},{"location":"frontend-external/","title":"Frontend documentation","text":"<p>The HealthArchive frontend lives in the separate repository:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend</li> </ul> <p>This backend repo\u2019s documentation site includes a minimal \u201cFrontend\u201d section that links out to the canonical docs (we intentionally do not mirror frontend docs into this site).</p> <p>For the canonical, up-to-date frontend docs, see:</p> <ul> <li>Frontend overview: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/README.md</li> <li>I18n: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/i18n.md</li> <li>Implementation guide: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/implementation-guide.md</li> </ul>"},{"location":"frontend-external/i18n/","title":"Frontend i18n","text":"<p>Canonical frontend i18n documentation lives in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/i18n.md</li> </ul>"},{"location":"frontend-external/implementation-guide/","title":"Frontend implementation guide","text":"<p>Canonical frontend implementation documentation lives in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/implementation-guide.md</li> </ul>"},{"location":"meta/documentation-health/","title":"Documentation Health Metrics","text":"<p>This page tracks the health and coverage of HealthArchive documentation.</p> <p>Last Updated: Auto-generated on every docs build</p>"},{"location":"meta/documentation-health/#coverage-metrics","title":"Coverage Metrics","text":""},{"location":"meta/documentation-health/#navigation-coverage","title":"Navigation Coverage","text":"<p>Goal: Key documentation is discoverable via sidebar navigation</p> Category Files on Disk In Navigation Coverage Target Tutorials 4 4 100% 100% Operations 50+ 30+ 60%+ 50% Development 5 4 80% 80% Deployment 15+ 8 53% 60% Reference 5 5 100% 100% Explanation 10+ 8 80% 70% Playbooks 32 32 100% 100% Roadmaps 20+ 4 20% 20% Overall 123+ 74+ 60% 50% <p>Status: \u2705 Above target (60% &gt; 50%)</p> <p>Achievements: - All tutorials in navigation (4/4) - All critical playbooks accessible - Reference documentation complete - Production runbook directly accessible</p> <p>Remaining gaps: - Some historical roadmap documents (intentionally archived) - Some operational logs (reference-only)</p>"},{"location":"meta/documentation-health/#documentation-types-diataxis-framework","title":"Documentation Types (Di\u00e1taxis Framework)","text":""},{"location":"meta/documentation-health/#distribution-by-type","title":"Distribution by Type","text":"Type Count Percentage Target Status Tutorials (Learning) 4 3% 3-5% \u2705 At target How-To Guides (Tasks) 50+ 41% 40-50% \u2705 Within range Reference (Information) 10 8% 10-15% \u26a0\ufe0f Could add more Explanation (Understanding) 25+ 20% 15-25% \u2705 Within range Meta/Templates 10 8% 5-10% \u2705 Good Pointers 5 4% &lt;5% \u2705 Minimal <p>Status: \u2705 Well-balanced according to Di\u00e1taxis principles</p>"},{"location":"meta/documentation-health/#content-quality-indicators","title":"Content Quality Indicators","text":""},{"location":"meta/documentation-health/#documentation-completeness","title":"Documentation Completeness","text":"Indicator Status Notes Quick Start exists \u2705 Yes <code>quickstart.md</code> Architecture documented \u2705 Yes Comprehensive 1,314-line guide API documented \u2705 Yes OpenAPI spec + consumer guide Contribution guide \u2705 Yes Complete CONTRIBUTING.md Code of Conduct \u2705 Yes In CONTRIBUTING.md Deployment runbook \u2705 Yes <code>deployment/production-single-vps.md</code> Incident response \u2705 Yes <code>operations/playbooks/incident-response.md</code> Testing guidelines \u2705 Yes <code>development/testing-guidelines.md</code> <p>Score: 8/8 \u2705 Excellent</p>"},{"location":"meta/documentation-health/#freshness","title":"Freshness","text":""},{"location":"meta/documentation-health/#recently-updated-last-30-days","title":"Recently Updated (Last 30 Days)","text":"<p>Based on recent documentation improvements:</p> <ul> <li>\u2705 Navigation restructure (2026-01-18)</li> <li>\u2705 New tutorials added (3 tutorials)</li> <li>\u2705 API consumer guide created</li> <li>\u2705 Project hub enhanced</li> <li>\u2705 CONTRIBUTING.md updated</li> <li>\u2705 Reference section created</li> </ul> <p>Status: \u2705 Active maintenance</p>"},{"location":"meta/documentation-health/#stale-documentation-check","title":"Stale Documentation Check","text":"<p>Documents not updated in &gt;180 days: TBD (requires git analysis)</p> <p>Action: Review quarterly as part of Ops Cadence</p>"},{"location":"meta/documentation-health/#link-health","title":"Link Health","text":""},{"location":"meta/documentation-health/#internal-links","title":"Internal Links","text":"<p>Check script: <code>scripts/check_docs_references.py</code></p> <p>Run: <code>make docs-refs</code></p> <p>Last status: \u23f3 Run <code>make docs-refs</code> to check</p> <p>Expected: 0 broken internal links</p>"},{"location":"meta/documentation-health/#external-links","title":"External Links","text":"<p>Check tool: Lychee (GitHub Action)</p> <p>Last status: \u26a0\ufe0f Advisory only (doesn't fail build)</p> <p>Action: Review and fix broken external links quarterly</p>"},{"location":"meta/documentation-health/#accessibility","title":"Accessibility","text":""},{"location":"meta/documentation-health/#navigation-depth","title":"Navigation Depth","text":"Metric Value Target Status Max nav depth 4 levels \u22644 \u2705 Good Avg nav depth 2.5 levels 2-3 \u2705 Good Orphaned docs 49 &lt;30% \u2705 Below threshold"},{"location":"meta/documentation-health/#search-effectiveness","title":"Search Effectiveness","text":"<p>Features enabled: - \u2705 Search suggestions - \u2705 Search highlighting - \u2705 Tag-based search (new) - \u2705 Minimum search length: 2 chars - \u2705 Language: English</p> <p>Status: \u2705 Good search experience</p>"},{"location":"meta/documentation-health/#multi-repo-consistency","title":"Multi-Repo Consistency","text":""},{"location":"meta/documentation-health/#cross-repo-references","title":"Cross-Repo References","text":"Repo Documented Linked Status healtharchive-backend \u2705 \u2705 This repo healtharchive-frontend \u2705 \u2705 <code>frontend-external/</code> pointers healtharchive-datasets \u2705 \u2705 <code>datasets-external/</code> pointer <p>Linking standard: GitHub URLs (not workspace-relative)</p> <p>Status: \u2705 Consistent</p>"},{"location":"meta/documentation-health/#documentation-workflows","title":"Documentation Workflows","text":""},{"location":"meta/documentation-health/#build-process","title":"Build Process","text":"<p>Command: <code>make docs-build</code></p> <p>Steps: 1. Generate OpenAPI spec (<code>scripts/export_openapi.py</code>) 2. Generate AI context (<code>scripts/generate_llms_txt.py</code>) 3. Build MkDocs site 4. Run advisory checks (refs, coverage) 5. Link checking (Lychee)</p> <p>CI Status: \u2705 Auto-deploys to docs.healtharchive.ca</p>"},{"location":"meta/documentation-health/#validation-checks","title":"Validation Checks","text":"Check Command Status Reference validation <code>make docs-refs</code> \u23f3 Run to check Coverage reporting <code>make docs-coverage</code> \u23f3 Run to check Link checking Lychee (in CI) \u26a0\ufe0f Advisory Format/lint <code>make check-full</code> \u2705 Part of CI"},{"location":"meta/documentation-health/#templates","title":"Templates","text":""},{"location":"meta/documentation-health/#available-templates","title":"Available Templates","text":"<p>Located in <code>docs/_templates/</code>:</p> Template Purpose Usage Count <code>runbook-template.md</code> Deployment procedures 15+ runbooks <code>playbook-template.md</code> Operational tasks 32 playbooks <code>incident-template.md</code> Post-mortems 4 incidents <code>decision-template.md</code> ADR-lite records 1 decision <code>restore-test-log-template.md</code> Restore verification VPS logs <code>adoption-signals-log-template.md</code> Adoption tracking VPS logs <code>mentions-log-template.md</code> Mentions tracking VPS logs <code>ops-ui-friction-log-template.md</code> UX issues VPS logs <p>Status: \u2705 Well-used templates ensure consistency</p>"},{"location":"meta/documentation-health/#documentation-improvements-roadmap","title":"Documentation Improvements Roadmap","text":""},{"location":"meta/documentation-health/#completed-2026-01-18","title":"Completed (2026-01-18)","text":"<ul> <li>\u2705 Navigation restructure (Di\u00e1taxis framework)</li> <li>\u2705 Quick start guide</li> <li>\u2705 Tutorial trilogy (first contribution, architecture, debugging)</li> <li>\u2705 API consumer guide</li> <li>\u2705 Enhanced project hub</li> <li>\u2705 CONTRIBUTING.md</li> <li>\u2705 Reference section (data model, CLI, archive-tool)</li> <li>\u2705 Documentation health dashboard (this page)</li> <li>\u2705 Search optimization</li> <li>\u2705 Advanced navigation features</li> </ul>"},{"location":"meta/documentation-health/#planned-improvements","title":"Planned Improvements","text":"<p>Near-term (Next quarter): - [ ] Add more code examples to architecture docs - [ ] Create video walkthroughs for tutorials - [ ] Expand troubleshooting guides - [ ] Add more FAQ entries</p> <p>Medium-term (6 months): - [ ] Multi-format export (PDF, ePub) - [ ] Analytics integration (track popular pages) - [ ] Interactive diagrams (clickable Mermaid) - [ ] Versioned documentation (per release)</p> <p>Long-term (Future): - [ ] Multilingual documentation (French) - [ ] Documentation chatbot (AI-powered search) - [ ] Automated screenshot updates - [ ] Doc contribution gamification</p>"},{"location":"meta/documentation-health/#quality-assurance","title":"Quality Assurance","text":""},{"location":"meta/documentation-health/#documentation-review-checklist","title":"Documentation Review Checklist","text":"<p>For each new document:</p> <ul> <li> Follows appropriate template</li> <li> Uses clear, concise language</li> <li> Includes code examples (if applicable)</li> <li> Cross-referenced from related docs</li> <li> Added to mkdocs.yml navigation (if key doc)</li> <li> Links verified (<code>make docs-refs</code>)</li> <li> Preview checked (<code>make docs-serve</code>)</li> <li> Spell-checked</li> <li> Grammar-checked</li> <li> Technical accuracy verified</li> </ul>"},{"location":"meta/documentation-health/#quarterly-review","title":"Quarterly Review","text":"<p>Every 3 months, review:</p> <ol> <li>Freshness: Update stale docs (&gt;180 days)</li> <li>Accuracy: Verify technical details match current code</li> <li>Completeness: Check for new features needing docs</li> <li>Gaps: Identify missing documentation</li> <li>Feedback: Incorporate user feedback from issues/discussions</li> </ol> <p>Tracked in: Operations Cadence Checklist</p>"},{"location":"meta/documentation-health/#metrics-over-time","title":"Metrics Over Time","text":""},{"location":"meta/documentation-health/#historical-trends","title":"Historical Trends","text":"Date Total Docs In Nav Coverage Notable Changes 2026-01-17 121 23 19% Baseline before restructure 2026-01-18 123 74 60% Di\u00e1taxis restructure + new content <p>Trend: \u2b06\ufe0f Significant improvement (+41 percentage points)</p>"},{"location":"meta/documentation-health/#contributing-to-documentation","title":"Contributing to Documentation","text":""},{"location":"meta/documentation-health/#how-you-can-help","title":"How You Can Help","text":"<ul> <li>\ud83d\udc1b Report issues: Broken links, unclear instructions, typos</li> <li>\ud83d\udca1 Suggest improvements: Missing topics, better examples</li> <li>\u270f\ufe0f Fix typos: Small PRs welcome!</li> <li>\ud83d\udcdd Write new docs: Fill gaps in coverage</li> <li>\ud83c\udfa8 Improve diagrams: Enhance Mermaid diagrams</li> <li>\ud83d\udd0d Review PRs: Help review documentation changes</li> </ul> <p>See: CONTRIBUTING.md</p>"},{"location":"meta/documentation-health/#tools-infrastructure","title":"Tools &amp; Infrastructure","text":""},{"location":"meta/documentation-health/#documentation-stack","title":"Documentation Stack","text":"Component Technology Purpose Generator MkDocs Material Static site generation Markdown GitHub-flavored Content format Diagrams Mermaid Visual documentation API Docs OpenAPI + Swagger UI Interactive API reference Search MkDocs search plugin Full-text search Hosting GitHub Pages docs.healtharchive.ca CI/CD GitHub Actions Auto-build and deploy"},{"location":"meta/documentation-health/#key-configuration-files","title":"Key Configuration Files","text":"File Purpose <code>mkdocs.yml</code> MkDocs configuration <code>docs/_templates/</code> Document templates <code>scripts/export_openapi.py</code> Generate API spec <code>scripts/generate_llms_txt.py</code> Generate AI context <code>scripts/check_docs_references.py</code> Validate links <code>scripts/check_docs_coverage.py</code> Report coverage"},{"location":"meta/documentation-health/#resources","title":"Resources","text":""},{"location":"meta/documentation-health/#documentation-standards","title":"Documentation Standards","text":"<ul> <li>Documentation Guidelines - Project standards</li> <li>Di\u00e1taxis Framework - Documentation philosophy</li> <li>MkDocs Material - Theme documentation</li> <li>GitHub-Flavored Markdown - Markdown spec</li> </ul>"},{"location":"meta/documentation-health/#related-meta-docs","title":"Related Meta Docs","text":"<ul> <li>Documentation Process Audit - 2026-01-09 audit</li> <li>Documentation Guidelines - Standards and taxonomy</li> <li>Documentation Architecture Improvements - Implementation roadmap</li> </ul>"},{"location":"meta/documentation-health/#summary","title":"Summary","text":"<p>Overall Health: \u2705 Excellent</p> <ul> <li>60% navigation coverage (above 50% target)</li> <li>Well-balanced content types (Di\u00e1taxis-aligned)</li> <li>Complete core documentation (8/8 key docs)</li> <li>Active maintenance and improvement</li> <li>Good search and accessibility features</li> <li>Consistent multi-repo approach</li> </ul> <p>Recent Achievements: - Major restructure completed (2026-01-18) - 51 new docs added to navigation - 4 new tutorials created - Comprehensive reference section - Enhanced user experience</p> <p>Next Steps: - Monitor link health quarterly - Continue quarterly freshness reviews - Gather user feedback - Iterate on improvements</p> <p>Questions or suggestions? Open an issue or discussion on GitHub!</p>"},{"location":"operations/","title":"Operations Documentation","text":""},{"location":"operations/#start-here","title":"Start Here","text":"<p>New operator?</p> <ul> <li>First: Operator Responsibilities \u2014 Core duties</li> <li>Deploy: Deploy &amp; Verify \u2014 Deployment workflow</li> <li>Monitor: Monitoring Checklist \u2014 Monitoring setup</li> <li>Respond: Incident Response \u2014 When something breaks</li> </ul> <p>Quick reference:</p> Task Documentation Daily checks Ops Cadence Deploy changes Deploy &amp; Verify Investigate issues Incident Response Monitor health Monitoring Quarterly tasks Restore Test, Dataset Release"},{"location":"operations/#all-operational-documentation","title":"All Operational Documentation","text":"<ul> <li>Ops playbooks (task-oriented checklists): <code>playbooks/README.md</code></li> <li>Incident notes / postmortems (internal): <code>incidents/README.md</code></li> <li>Observability + private stats contract (public vs private boundaries): <code>observability-and-private-stats.md</code></li> <li>Annual capture campaign (scope + seeds): <code>annual-campaign.md</code></li> <li>Automation implementation plan (phased, production-only): <code>automation-implementation-plan.md</code></li> <li>Monitoring + uptime + CI checklist: <code>monitoring-and-ci-checklist.md</code></li> <li>Annual Crawl Alerting Strategy: <code>monitoring-and-alerting.md</code></li> <li>Agent handoff guidelines (internal rules): <code>agent-handoff-guidelines.md</code></li> <li>Claims registry (proof artifacts): <code>claims-registry.md</code></li> <li>Data handling &amp; retention (internal contract): <code>data-handling-retention.md</code></li> <li>Export integrity contract (manifest + immutability): <code>export-integrity-contract.md</code></li> <li>Automation verification rituals (timer checks): <code>automation-verification-rituals.md</code></li> <li>Dataset release runbook (verification checklist): <code>dataset-release-runbook.md</code></li> <li>Risk register (top risks + mitigations): <code>risk-register.md</code></li> <li>Ops cadence checklist (internal routine): <code>ops-cadence-checklist.md</code></li> <li>Ops UI friction log template (internal; ongoing): <code>../_templates/ops-ui-friction-log-template.md</code></li> <li>Growth constraints (storage + scope budgets): <code>growth-constraints.md</code></li> <li>Legacy crawl imports (historical import notes): <code>legacy-crawl-imports.md</code></li> <li>Restore test procedure (quarterly): <code>restore-test-procedure.md</code></li> <li>Restore test log template: <code>../_templates/restore-test-log-template.md</code></li> <li>Adoption signals log template (public-safe, quarterly): <code>../_templates/adoption-signals-log-template.md</code></li> <li>HealthArchive ops roadmap + todo (remaining tasks): <code>healtharchive-ops-roadmap.md</code></li> <li>Partner kit (brief + citation + screenshots): <code>partner-kit.md</code></li> <li>One-page brief (pointer to frontend public asset): <code>one-page-brief.md</code></li> <li>Citation handout (pointer to frontend public asset): <code>citation-handout.md</code></li> <li>Outreach templates (email copy): <code>outreach-templates.md</code></li> <li>Verification packet (verifier handoff): <code>verification-packet.md</code></li> <li>Mentions log (public-safe, link-only): <code>mentions-log.md</code></li> <li>Mentions log template (public-safe): <code>mentions-log-template.md</code></li> <li>Exports data dictionary (pointer to public asset): <code>exports-data-dictionary.md</code></li> <li>Methods note outline (poster/preprint scaffold): <code>methods-note-outline.md</code></li> <li>Search relevance evaluation (process + commands): <code>search-quality.md</code></li> <li>Golden queries + expected results (living checklist): <code>search-golden-queries.md</code></li> <li>Replay + preview automation plan (design + guardrails; includes <code>replay-reconcile</code>): <code>replay-and-preview-automation-plan.md</code></li> <li>Production baseline drift checks (policy + snapshot + compare): <code>baseline-drift.md</code></li> </ul>"},{"location":"operations/#mission-reports-logs","title":"Mission Reports &amp; Logs","text":"<ul> <li>2026-01-19: Annual Crawl Hardening Shipment</li> <li>2026-01-19: Investigation: Indexing Delay / Zero Indexed Pages</li> </ul>"},{"location":"operations/agent-handoff-guidelines/","title":"Agent Handoff Guidelines (internal)","text":"<p>This repo contains internal operations documentation. Keep everything public-safe:</p> <ul> <li>Do not include secrets, tokens, private emails, internal IPs, or private names.</li> <li>Prefer stable identifiers over prose (timer names, release tags, file paths, commit SHAs).</li> <li>When recording \u201cfirsts\u201d, treat them as historical facts and include stable IDs (e.g., release tag, log filename).</li> </ul> <p>Related docs:</p> <ul> <li><code>docs/operations/claims-registry.md</code></li> <li><code>docs/operations/data-handling-retention.md</code></li> <li><code>docs/operations/export-integrity-contract.md</code></li> <li><code>docs/operations/automation-verification-rituals.md</code></li> <li><code>docs/operations/dataset-release-runbook.md</code></li> <li><code>docs/operations/risk-register.md</code></li> <li><code>docs/operations/healtharchive-ops-roadmap.md</code> (todo list)</li> </ul>"},{"location":"operations/annual-campaign/","title":"Annual Capture Campaign (Jan 01 UTC) \u2014 Scope, Sources, Seeds","text":"<p>Status: approved (v1 scope) \u2014 single VPS, production-only.</p> <p>This document defines the canonical scope of HealthArchive\u2019s annual crawl campaign:</p> <ul> <li>Runs once per year on Jan 01 (UTC).</li> <li>Uses no page/depth limits (completeness and accuracy are the priority).</li> <li>Targets a small, stable set of sources to keep operations reliable on a     single VPS.</li> <li>Optimizes for getting the annual capture indexed and searchable as soon as     each crawl completes (replay + preview automation is explicitly secondary).</li> </ul> <p>This doc is intentionally focused on \u201cwhat we crawl\u201d and \u201cwhere we start\u201d. Implementation details (scheduler, timers, reconciler, monitoring) live in:</p> <ul> <li><code>automation-implementation-plan.md</code></li> </ul>"},{"location":"operations/annual-campaign/#1-goals-and-non-goals","title":"1) Goals and non-goals","text":""},{"location":"operations/annual-campaign/#goals","title":"Goals","text":"<ul> <li>Annual snapshot semantics: each source gets one \u201cannual edition\u201d per year,     labeled as Jan 01 (UTC) for that year.</li> <li>Completeness and accuracy: do not artificially cap depth/pages. Prefer     broad coverage of each source, even if crawls take days.</li> <li>Search-first readiness: once a crawl finishes, indexing should run next so     results become searchable as quickly as possible on production hardware.</li> <li>WARC-first pipeline: WARCs are the canonical input to search indexing; <code>.zim</code> outputs are optional artifacts and are not required for annual \u201cdone\u201d.</li> <li>Stable scope: only include sources we can realistically crawl and operate     with minimal ongoing tweaking.</li> </ul>"},{"location":"operations/annual-campaign/#non-goals-for-v1","title":"Non-goals (for v1)","text":"<ul> <li>Adding many new sources quickly (scope explosion).</li> <li>Achieving a literally simultaneous capture moment across all sources (single     VPS + limited concurrency makes this unrealistic).</li> <li>Automating cleanup/retention that could delete WARCs (explicitly deferred).</li> <li>Building a separate staging environment.</li> <li>Full-text indexing of non-HTML content (e.g. PDFs) \u2014 captured for completeness, but not searchable in v1.</li> </ul>"},{"location":"operations/annual-campaign/#2-canonical-sources-v1","title":"2) Canonical sources (v1)","text":"<p>For the initial annual campaign, we intentionally crawl only three sources:</p> <ol> <li>Health Canada (<code>hc</code>)</li> <li>Public Health Agency of Canada (<code>phac</code>)</li> <li>Canadian Institutes of Health Research (<code>cihr</code>)</li> </ol> <p>Rationale:</p> <ul> <li>These are core, high-value federal public health sources.</li> <li>They keep the campaign small enough to remain operable on a single VPS.</li> <li>They map cleanly to existing backend concepts (<code>Source</code>, <code>ArchiveJob</code>).</li> </ul>"},{"location":"operations/annual-campaign/#3-canonical-seeds-entry-urls","title":"3) Canonical seeds (entry URLs)","text":"<p>Seeds are the \u201centry points\u201d from which the crawler discovers pages.</p> <p>Important notes:</p> <ul> <li>Seeds must be stable and canonical (avoid ephemeral campaign pages).</li> <li>For bilingual sites, include both English and French entry points so     coverage does not depend on cross-link discovery.</li> <li>Seeds should be chosen to represent \u201cthe main hub\u201d of the source.</li> </ul>"},{"location":"operations/annual-campaign/#31-source-table","title":"3.1 Source table","text":"Code Source Primary host(s) English seed French seed <code>hc</code> Health Canada <code>www.canada.ca</code> <code>https://www.canada.ca/en/health-canada.html</code> <code>https://www.canada.ca/fr/sante-canada.html</code> <code>phac</code> Public Health Agency of Canada <code>www.canada.ca</code> <code>https://www.canada.ca/en/public-health.html</code> <code>https://www.canada.ca/fr/sante-publique.html</code> <code>cihr</code> CIHR <code>cihr-irsc.gc.ca</code> <code>https://cihr-irsc.gc.ca/e/193.html</code> <code>https://cihr-irsc.gc.ca/f/193.html</code>"},{"location":"operations/annual-campaign/#32-scope-boundary-notes-important","title":"3.2 Scope boundary notes (important)","text":"<p>These are policy decisions to prevent crawls from ballooning unexpectedly while still preserving completeness within each source:</p> <ul> <li>Primary scope boundary (required): each source has an explicit, mechanical     \u201cin-scope URL rule\u201d.</li> <li>Cross-domain assets: pages will reference external assets (fonts, JS,     images). Capturing all third-party assets is not required for search indexing.     If replay fidelity requires specific additional domains later, add them     explicitly and sparingly (do not allow arbitrary internet expansion).</li> <li>Canada.ca shared host: <code>hc</code> and <code>phac</code> both live on <code>www.canada.ca</code>.     We must scope by host + path allowlist (not \u201call of <code>www.canada.ca</code>\u201d).</li> </ul>"},{"location":"operations/annual-campaign/#33-in-scope-url-rules-mechanical-v1","title":"3.3 In-scope URL rules (mechanical, v1)","text":"<p>These rules define \u201cwhat counts as Health Canada / PHAC\u201d on a shared host.</p>"},{"location":"operations/annual-campaign/#health-canada-hc-wwwcanadaca","title":"Health Canada (<code>hc</code>) \u2014 <code>www.canada.ca</code>","text":"<p>In scope:</p> <ul> <li>Exactly:<ul> <li><code>https://www.canada.ca/en/health-canada.html</code></li> <li><code>https://www.canada.ca/fr/sante-canada.html</code></li> </ul> </li> <li>Any URL under these path prefixes:<ul> <li><code>https://www.canada.ca/en/health-canada/</code></li> <li><code>https://www.canada.ca/fr/sante-canada/</code></li> </ul> </li> <li>Any URL under these asset path prefixes (captured only when referenced by in-scope pages):<ul> <li><code>https://www.canada.ca/etc/designs/canada/wet-boew/</code></li> <li><code>https://www.canada.ca/content/dam/canada/sitemenu/</code></li> <li><code>https://www.canada.ca/content/dam/themes/health/</code></li> <li><code>https://www.canada.ca/content/dam/hc-sc/</code></li> </ul> </li> </ul> <p>Out of scope (examples):</p> <ul> <li><code>https://www.canada.ca/en/services/</code></li> <li><code>https://www.canada.ca/en/government/</code></li> <li>Any other <code>https://www.canada.ca/&lt;lang&gt;/...</code> that is not the hub page or under     the allowed prefixes above.</li> </ul>"},{"location":"operations/annual-campaign/#phac-phac-wwwcanadaca","title":"PHAC (<code>phac</code>) \u2014 <code>www.canada.ca</code>","text":"<p>In scope:</p> <ul> <li>Exactly:<ul> <li><code>https://www.canada.ca/en/public-health.html</code></li> <li><code>https://www.canada.ca/fr/sante-publique.html</code></li> </ul> </li> <li>Any URL under these path prefixes:<ul> <li><code>https://www.canada.ca/en/public-health/</code></li> <li><code>https://www.canada.ca/fr/sante-publique/</code></li> </ul> </li> <li>Any URL under these asset path prefixes (captured only when referenced by in-scope pages):<ul> <li><code>https://www.canada.ca/etc/designs/canada/wet-boew/</code></li> <li><code>https://www.canada.ca/content/dam/canada/sitemenu/</code></li> <li><code>https://www.canada.ca/content/dam/themes/health/</code></li> <li><code>https://www.canada.ca/content/dam/phac-aspc/</code></li> </ul> </li> </ul> <p>Out of scope (examples):</p> <ul> <li><code>https://www.canada.ca/en/services/</code></li> <li><code>https://www.canada.ca/en/government/</code></li> <li>Any other <code>https://www.canada.ca/&lt;lang&gt;/...</code> that is not the hub page or under     the allowed prefixes above.</li> </ul>"},{"location":"operations/annual-campaign/#cihr-cihr-cihr-irscgcca","title":"CIHR (<code>cihr</code>) \u2014 <code>cihr-irsc.gc.ca</code>","text":"<p>In scope:</p> <ul> <li>Any URL on host <code>cihr-irsc.gc.ca</code> (all paths), starting from the EN+FR seeds     above.</li> </ul> <p>Out of scope:</p> <ul> <li>Any other host.</li> </ul> <p>This is a \u201ccompleteness\u201d project, not an \u201cinfinite crawl\u201d project: completeness means \u201ccomplete within the intended source boundaries.\u201d</p>"},{"location":"operations/annual-campaign/#4-campaign-ordering-single-vps-reality","title":"4) Campaign ordering (single VPS reality)","text":"<p>With a single production VPS and limited parallelism, crawls and indexing will not complete simultaneously across sources.</p> <p>We still want the annual snapshot to feel like \u201cone moment in time\u201d, so we should order annual jobs to minimize the spread between the first and last job to finish indexing.</p> <p>Opinionated default ordering for v1:</p> <ol> <li><code>hc</code> (expected to be largest / slowest)</li> <li><code>phac</code></li> <li><code>cihr</code> (expected to be smallest / fastest)</li> </ol> <p>Rationale:</p> <ul> <li>Running the slowest job first reduces \u201cfinish-time spread\u201d across the set.</li> <li>A fixed ordering makes operations reproducible year over year.</li> </ul> <p>After the first annual campaign completes, revisit ordering based on real job durations and storage growth.</p> <p>Implementation note:</p> <ul> <li><code>ha-backend schedule-annual</code> staggers <code>queued_at</code> by a few seconds across   sources to make the single-worker pick order deterministic (hc \u2192 phac \u2192 cihr),   even when all jobs are enqueued in the same command invocation.</li> </ul>"},{"location":"operations/annual-campaign/#5-what-done-means-per-source","title":"5) What \u201cdone\u201d means (per source)","text":"<p>For each source\u2019s annual job:</p> <ol> <li>Job reaches <code>status=indexed</code> (searchable).</li> <li><code>/api/search</code> returns results for that source and year as expected.</li> </ol> <p>Replay and previews are \u201ceventually consistent\u201d follow-ups and are not part of the \u201csearch is ready\u201d definition for the annual campaign.</p>"},{"location":"operations/annual-campaign/#6-post-campaign-verification","title":"6) Post-campaign verification","text":"<p>Once all annual jobs are <code>indexed</code>, capture \u201csearch readiness\u201d evidence as a timestamped artifact directory:</p> <pre><code>YEAR=2026\nset -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend annual-status --year \"$YEAR\"\n./scripts/annual-search-verify.sh --year \"$YEAR\" --out-root /srv/healtharchive/ops/search-eval --base-url http://127.0.0.1:8001\n</code></pre> <p>This produces:</p> <ul> <li><code>annual-status.json</code> / <code>annual-status.txt</code> (campaign readiness evidence)</li> <li>captured <code>/api/search</code> JSON for the golden query set (for later diffing)</li> </ul>"},{"location":"operations/automation-implementation-plan/","title":"Automation Implementation Plan (Production-Only, Single VPS)","text":"<p>Status: active plan (implementation proceeds in phases).</p> <p>This document is the excruciatingly detailed, sequential implementation plan for HealthArchive automation, tailored to the current operating reality:</p> <ul> <li>One production VPS (no staging backend).</li> <li>Annual capture campaign runs on Jan 01 (UTC).</li> <li>Current annual sources: Health Canada (<code>hc</code>), PHAC (<code>phac</code>), CIHR (<code>cihr</code>).</li> <li>No page/depth caps (completeness/accuracy first).</li> <li>Top priority is making the annual snapshot searchable ASAP once crawls   complete; replay/previews are secondary and eventually consistent.</li> </ul> <p>This plan intentionally minimizes operational complexity:</p> <ul> <li>Every new automation starts as manual + dry-run, then graduates to a   systemd timer only once it is boring and predictable.</li> <li>Every automated action must be:</li> <li>idempotent,</li> <li>allowlistable,</li> <li>rate-limited,</li> <li>observable,</li> <li>and instantly disable-able.</li> </ul> <p>Canonical \u201cwhat we crawl\u201d:</p> <ul> <li><code>annual-campaign.md</code></li> </ul> <p>Related context:</p> <ul> <li>Monitoring/CI guidance: <code>monitoring-and-ci-checklist.md</code></li> <li>Replay/preview automation design: <code>replay-and-preview-automation-plan.md</code></li> <li>Production runbook: <code>../deployment/production-single-vps.md</code></li> </ul>"},{"location":"operations/automation-implementation-plan/#global-invariants-do-not-violate","title":"Global invariants (do not violate)","text":""},{"location":"operations/automation-implementation-plan/#safety","title":"Safety","text":"<ul> <li>Never run heavy automation on request paths. No crawl, indexing, replay   indexing, or screenshotting should be triggered by a public HTTP request.</li> <li>Never automate destructive cleanup of WARCs until retention is designed   and tested (see <code>replay-and-preview-automation-plan.md</code>).</li> <li>No secrets in repo or logs. Timers/services must read secrets from   root-owned env files on the VPS, and logs must not print their contents.</li> </ul>"},{"location":"operations/automation-implementation-plan/#idempotency-and-boundedness","title":"Idempotency and boundedness","text":"<ul> <li>Every scheduled unit must be safe to run multiple times (systemd timers with   <code>Persistent=true</code> can replay missed runs).</li> <li>Every automated loop must have:</li> <li>hard caps (jobs per run, previews per run),</li> <li>a lock (global and/or per-item),</li> <li>and clear refusal rules (disk low, dependency down).</li> </ul>"},{"location":"operations/automation-implementation-plan/#single-vps-discipline","title":"\u201cSingle VPS\u201d discipline","text":"<ul> <li>Treat the worker as a scarce resource. Avoid adding competing heavy   automation during the annual campaign.</li> <li>Prefer \u201cqueue work then let the worker run\u201d over spawning extra parallel   processes.</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-1-of-9-define-annual-scope-seeds-docs-only","title":"Step 1 (of 9) \u2014 Define annual scope + seeds (docs only)","text":"<p>Objective</p> <p>Lock the annual campaign\u2019s scope so automation is deterministic and auditable.</p> <p>Deliverables</p> <ul> <li><code>annual-campaign.md</code>:</li> <li>sources list (<code>hc</code>, <code>phac</code>, <code>cihr</code>),</li> <li>canonical seeds (EN+FR where applicable),</li> <li>scope boundary notes,</li> <li>recommended crawl ordering.</li> </ul> <p>No code and no infrastructure changes in this step.</p> <p>Acceptance criteria</p> <ul> <li>Operators can answer \u201cwhat will Jan 01 crawl?\u201d by pointing at a single file.</li> </ul> <p>Rollback</p> <ul> <li>N/A (docs only).</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-2-of-9-align-backend-registry-seeding-with-v1-sources-code","title":"Step 2 (of 9) \u2014 Align backend registry + seeding with v1 sources (code)","text":"<p>Objective</p> <p>Ensure the backend\u2019s canonical configuration matches the annual campaign:</p> <ul> <li><code>seed-sources</code> creates <code>hc</code>, <code>phac</code>, <code>cihr</code>.</li> <li>Job registry can create annual jobs for these sources consistently.</li> </ul> <p>Key decisions</p> <ul> <li>Registry remains the single source of truth for per-source defaults.</li> <li>We do not add page/depth limits to achieve \u201cfast campaigns\u201d; completeness   remains the priority.</li> <li><code>hc</code> and <code>phac</code> are sections of <code>www.canada.ca</code>, so their job configs must   enforce a path allowlist scope (as defined in <code>annual-campaign.md</code>) to   avoid crawling all of Canada.ca.</li> </ul> <p>Implementation steps</p> <ol> <li>Add/confirm <code>cihr</code> in source seeding (<code>ha-backend seed-sources</code>).</li> <li>Add a <code>SourceJobConfig</code> entry for <code>cihr</code> in <code>job_registry.py</code>:</li> <li>seeds from <code>annual-campaign.md</code></li> <li>conservative safety defaults (monitoring off by default unless you choose      otherwise)</li> <li>Update <code>hc</code> and <code>phac</code> seeds to match the canonical list (likely add FR    entry points if not already).</li> <li>Encode the <code>annual-campaign.md</code> in-scope URL rules into job configs:</li> <li>for <code>hc</code> and <code>phac</code>: host + path allowlist scope on <code>www.canada.ca</code></li> <li>for <code>cihr</code>: host scope on <code>cihr-irsc.gc.ca</code></li> <li>Ensure naming templates can represent annual campaigns (see Step 3).</li> </ol> <p>Tests</p> <ul> <li>Unit tests:</li> <li>seeding includes <code>cihr</code>,</li> <li>job creation for each source works,</li> <li>config JSON contains expected seeds and scope constraints.</li> </ul> <p>Acceptance criteria</p> <ul> <li>Running <code>seed-sources</code> on a fresh DB yields all three sources.</li> <li><code>create-job --source cihr</code> works locally and yields a job row with expected   defaults.</li> </ul> <p>Rollback</p> <ul> <li>Revert code changes; no DB migrations required if only seeding/registry   changes are made.</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-3-of-9-implement-annual-scheduler-cli-production-only-logic-dry-run-first","title":"Step 3 (of 9) \u2014 Implement annual scheduler CLI (production-only logic, dry-run first)","text":"<p>Objective</p> <p>Provide a single, safe command that enqueues the annual campaign jobs for a given year (Jan 01 UTC), exactly once.</p> <p>Proposed CLI</p> <ul> <li><code>ha-backend schedule-annual</code></li> </ul> <p>Flags (opinionated):</p> <ul> <li><code>--apply</code> (otherwise dry-run only)</li> <li><code>--year YYYY</code></li> <li>If omitted: allowed only when running on Jan 01 (UTC), in which     case it schedules the current UTC year.</li> <li><code>--sources hc phac cihr</code> (explicit allowlist; subset selection)</li> <li><code>--max-create-per-run N</code> (defaults to number of selected sources)</li> </ul> <p>Idempotency rules</p> <p>For each source in the allowlist:</p> <ul> <li>If a job exists for the same <code>campaign_year</code> (recorded in <code>ArchiveJob.config</code>)   \u2192 skip.</li> <li>If a job exists with the same would-be annual job name (e.g. <code>hc-20270101</code>)   \u2192 skip (prevents duplicates even if the job predates <code>campaign_year</code> metadata).</li> <li>If an \u201cactive\u201d job exists for that source (queued/running/completed/indexing   /index_failed/retryable) \u2192 skip and report why.</li> </ul> <p>Job labeling</p> <ul> <li>Job name must include the campaign date <code>YYYY0101</code> even if the scheduler runs   late (e.g. after reboot).</li> <li>Record metadata in <code>ArchiveJob.config</code> (no schema change):</li> <li><code>campaign_kind=\"annual\"</code></li> <li><code>campaign_year=YYYY</code></li> <li><code>campaign_date=\"YYYY-01-01\"</code></li> <li><code>campaign_date_utc=\"YYYY-01-01T00:00:00Z\"</code></li> <li><code>scheduler_version=\"v1\"</code></li> </ul> <p>Ordering</p> <ul> <li>Create jobs in the order defined in <code>annual-campaign.md</code> to make queue   processing predictable with a single worker.</li> </ul> <p>Tests</p> <ul> <li>Idempotency: second apply creates 0 jobs.</li> <li>Active-job skip: if a job is in progress, scheduler does not add another.</li> <li>Ordering: created jobs are in the expected order.</li> <li>Year labeling: job name/config reflect the specified year, not \u201cnow\u201d.</li> </ul> <p>Acceptance criteria</p> <ul> <li>Dry-run output is readable and complete (operator can review before applying).</li> <li>Apply mode creates exactly one job per selected source, unless prevented by   the idempotency/active-job guards or <code>--max-create-per-run</code>.</li> </ul> <p>Rollback</p> <ul> <li>If jobs were created incorrectly, use admin tooling to mark them failed or   delete rows only if you have a safe procedure (prefer \u201cleave rows, don\u2019t run   them\u201d over ad-hoc deletion).</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-4-of-9-add-annual-statusreporting-cli-operability","title":"Step 4 (of 9) \u2014 Add annual status/reporting CLI (operability)","text":"<p>Objective</p> <p>Make it trivial to answer:</p> <ul> <li>\u201cIs the annual snapshot searchable yet?\u201d</li> <li>\u201cWhich source is stuck, and where?\u201d</li> </ul> <p>Proposed CLI</p> <ul> <li><code>ha-backend annual-status --year YYYY [--json] [--sources ...]</code></li> </ul> <p>Reports per source:</p> <ul> <li>job id/name</li> <li>job status + timestamps</li> <li>retry_count</li> <li>indexed_page_count</li> <li>crawl/index exit codes if applicable</li> </ul> <p>Campaign-level summary:</p> <ul> <li>total sources, indexed count, failed count, in-progress count</li> <li>\u201cready for search\u201d boolean (all indexed)</li> </ul> <p>Implementation notes (v1)</p> <ul> <li>Uses <code>ArchiveJob.config</code> metadata written by <code>schedule-annual</code>:</li> <li><code>campaign_kind=\"annual\"</code></li> <li><code>campaign_year=YYYY</code></li> <li>Fallback: if metadata is missing, it will also consider the canonical annual   name format (e.g. <code>hc-20270101</code>).</li> <li>If no annual job is found for a source, the command will also surface the   most recent \u201cactive\u201d job for that source (queued/running/completed/indexing/   index_failed/retryable) to help explain why scheduling may have been skipped.</li> <li>If multiple annual candidates are found for a source/year, the command prints   an error for that source (operators must resolve duplicates).</li> </ul> <p>Acceptance criteria</p> <ul> <li>An operator can copy/paste the output into an incident note and it\u2019s   self-explanatory.</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-5-of-9-production-systemd-timer-for-jan-01-scheduling-infrastructure","title":"Step 5 (of 9) \u2014 Production systemd timer for Jan 01 scheduling (infrastructure)","text":"<p>Objective</p> <p>Run the annual scheduler automatically on Jan 01 UTC, reliably.</p> <p>systemd units (draft)</p> <p>Templates live in: <code>../deployment/systemd/</code></p> <ul> <li><code>healtharchive-schedule-annual.service</code> (apply)</li> <li>Runs: <code>ha-backend schedule-annual --apply --year &lt;UTC_YEAR&gt; --sources hc phac cihr</code><ul> <li><code>&lt;UTC_YEAR&gt;</code> is computed at runtime (<code>date -u +%Y</code>) so that   <code>Persistent=true</code> can safely run a missed activation after a reboot.</li> </ul> </li> <li>Uses <code>EnvironmentFile=/etc/healtharchive/backend.env</code></li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/automation-enabled</code></li> <li>Uses <code>RefuseManualStart=yes</code> to reduce accidental production scheduling.</li> <li><code>healtharchive-schedule-annual-dry-run.service</code> (safe validation)</li> <li>Runs the same scheduler without <code>--apply</code> (no DB writes).</li> <li><code>healtharchive-schedule-annual.timer</code></li> <li><code>OnCalendar=*-01-01 00:05:00 UTC</code></li> <li><code>Persistent=true</code></li> </ul> <p>Why <code>Persistent=true</code></p> <ul> <li>If the VPS reboots or the timer is disabled temporarily, systemd will run the   missed activation on the next boot/start, but the scheduler still labels jobs   as Jan 01 for the target year.</li> </ul> <p>Acceptance criteria</p> <ul> <li>Timer wiring is validated by running the dry-run service manually:   <code>systemctl start healtharchive-schedule-annual-dry-run.service</code></li> <li>Timer is enabled only after manual review.</li> </ul> <p>Rollback</p> <ul> <li>Disable timer: <code>systemctl disable --now healtharchive-schedule-annual.timer</code></li> <li>Remove <code>/etc/healtharchive/automation-enabled</code> to stop all automation quickly.</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-6-of-9-resource-policy-during-campaign-keep-site-up-without-safe-window","title":"Step 6 (of 9) \u2014 Resource policy during campaign (keep site up without \u201csafe window\u201d)","text":"<p>Objective</p> <p>Annual crawls may run for days. We want the public API and frontend to remain available even if performance is degraded.</p> <p>Approach</p> <ul> <li>Prefer systemd-level prioritization over complex in-app throttling.</li> </ul> <p>Actions:</p> <ul> <li>Ensure worker service runs with lower priority than API:</li> <li><code>Nice=5</code> or <code>Nice=10</code></li> <li>optionally <code>IOSchedulingClass=best-effort</code>, <code>IOSchedulingPriority=6</code></li> <li>Keep only one worker process unless you explicitly decide to accept more   contention for a tighter \u201csame moment\u201d capture.</li> </ul> <p>Implementation (v1)</p> <ul> <li>Use a systemd drop-in for the worker:</li> <li>template: <code>../deployment/systemd/healtharchive-worker.service.override.conf</code></li> <li>install path: <code>/etc/systemd/system/healtharchive-worker.service.d/override.conf</code></li> </ul> <p>Acceptance criteria</p> <ul> <li>API stays responsive (no sustained 5xx/timeouts attributable to worker load).</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-7-of-9-post-campaign-search-readiness-verification-light-automation","title":"Step 7 (of 9) \u2014 Post-campaign \u201csearch readiness\u201d verification (light automation)","text":"<p>Objective</p> <p>After all annual jobs are indexed, capture evidence that search is working and stable.</p> <p>Implementation (v1):</p> <ul> <li><code>scripts/search-eval-capture.sh</code> now supports <code>--run-id ID</code> so you can place   captures under a stable, year-tagged path (instead of a nested timestamp you   need to \u201cdiscover\u201d after the fact).</li> <li><code>scripts/annual-search-verify.sh</code> wraps the flow safely:</li> <li>runs <code>ha-backend annual-status --year YYYY --json</code>,</li> <li>refuses to capture unless <code>summary.readyForSearch=true</code> (unless you pass     <code>--allow-not-ready</code>),</li> <li>writes <code>annual-status.json</code>/<code>annual-status.txt</code> into the same capture dir as     the golden query responses,</li> <li>passes optional args through to <code>search-eval-capture.sh</code>.</li> </ul> <p>Recommended artifact layout on the VPS:</p> <ul> <li><code>/srv/healtharchive/ops/search-eval/&lt;year&gt;/&lt;run_id&gt;/</code></li> <li><code>annual-status.json</code></li> <li><code>annual-status.txt</code></li> <li><code>annual-search-verify.meta.txt</code></li> <li><code>meta.txt</code> (from <code>search-eval-capture.sh</code>)</li> <li><code>&lt;query&gt;.(pages|snapshots).json</code></li> </ul> <p>Operator command (production example):</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\ncd /opt/healtharchive-backend\n./scripts/annual-search-verify.sh --year 2026 --out-root /srv/healtharchive/ops/search-eval --base-url http://127.0.0.1:8001\n</code></pre> <p>Optional (Postgres, manual): consider running a one-time <code>VACUUM (ANALYZE)</code> after large ingestion completes. Do this manually, off-peak, and only if you\u2019re confident it won\u2019t starve IO for user traffic.</p> <p>Acceptance criteria</p> <ul> <li>You have a year-tagged artifact directory showing <code>annual-status</code> output plus   captured <code>/api/search</code> JSON for the golden query set.</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-8-of-9-replaypreview-reconciliation-after-search-is-stable","title":"Step 8 (of 9) \u2014 Replay/preview reconciliation (after search is stable)","text":"<p>Objective</p> <p>Make replay and previews converge to correct state without risking core search availability.</p> <p>Implementation (v1) (aligns with <code>replay-and-preview-automation-plan.md</code>):</p> <ul> <li>New ops command: <code>ha-backend replay-reconcile</code></li> <li>default mode is dry-run (safe): prints what it would do.</li> <li><code>--apply</code> performs the actions.</li> <li>global lock file prevents concurrent runs (default:     <code>/srv/healtharchive/replay/.locks/replay-reconcile.lock</code>).</li> <li>caps:<ul> <li><code>--max-jobs N</code> (default 1) limits replay indexing repairs per run.</li> <li>optional <code>--previews --max-previews N</code> (default 1) generates missing   preview images for <code>/archive</code> source cards (still capped).</li> </ul> </li> <li> <p>allowlists:</p> <ul> <li><code>--sources hc phac ...</code></li> <li><code>--job-id 123 456 ...</code></li> <li>optional <code>--campaign-year YYYY</code> for annual-only reconciliation.</li> </ul> </li> <li> <p>Replay indexing metadata:</p> </li> <li><code>ha-backend replay-index-job --id &lt;id&gt;</code> now writes a marker file under the     collection root: <code>replay-index.meta.json</code> (WARC count + hash + timestamps).</li> <li><code>replay-reconcile --verify-warc-hash</code> can use the marker to detect drift     (slower; optional).</li> </ul> <p>Staged rollout:</p> <ol> <li>Dry-run:</li> <li><code>ha-backend replay-reconcile --collections-dir /srv/healtharchive/replay/collections</code></li> <li>Apply for one job (manual allowlist):</li> <li><code>ha-backend replay-reconcile --apply --job-id &lt;JOB_ID&gt; --max-jobs 1</code></li> <li>Timer with caps (templates under <code>docs/deployment/systemd/</code>; disabled by default).</li> <li>Optional previews (still capped; failures are surfaced clearly).</li> </ol>"},{"location":"operations/automation-implementation-plan/#step-9-of-9-deployment-automation-low-cost-low-risk-first","title":"Step 9 (of 9) \u2014 Deployment automation (low-cost, low-risk first)","text":"<p>Objective</p> <p>Reduce operator error in backend deployments without introducing brittle GitHub\u2192VPS automation.</p> <p>Implementation (v1)</p> <ul> <li>A single-VPS deploy helper script now exists:</li> <li><code>scripts/vps-deploy.sh</code></li> <li>It is dry-run by default; use <code>--apply</code> to actually deploy.</li> <li>It supports:</li> <li>fast-forward deploys (<code>git pull --ff-only</code>), or pinned SHAs via <code>--ref</code></li> <li>dependency install (editable) + optional skip flags</li> <li>Alembic migrations (sources <code>/etc/healtharchive/backend.env</code> but does not print it)</li> <li>systemd restarts for API + worker</li> <li>a final <code>/api/health</code> check</li> <li>a deploy lock file to avoid concurrent deploys</li> </ul> <p>Operator usage (production):</p> <pre><code>cd /opt/healtharchive-backend\n\n# Dry-run:\n./scripts/vps-deploy.sh\n\n# Deploy latest main:\n./scripts/vps-deploy.sh --apply\n\n# Deploy pinned SHA:\n./scripts/vps-deploy.sh --apply --ref &lt;GIT_SHA&gt;\n</code></pre> <p>Future (optional, higher-risk without staging):</p> <ul> <li>GitHub Actions deployments can be considered later, but require secrets,   rollback discipline, and careful failure handling. For now, the recommended   posture is \u201cboring manual deploy with a single trusted script\u201d.</li> </ul>"},{"location":"operations/automation-verification-rituals/","title":"Automation Verification Rituals (internal)","text":"<p>Use these checks before claiming automation is \u201con\u201d.</p>"},{"location":"operations/automation-verification-rituals/#systemd-timers","title":"systemd timers","text":"<ul> <li>One-command posture check (recommended): <code>./scripts/verify_ops_automation.sh</code></li> <li>Diff-friendly JSON summary (optional): <code>./scripts/verify_ops_automation.sh --json | python3 -m json.tool</code></li> <li>JSON-only artifact (optional): <code>./scripts/verify_ops_automation.sh --json-only &gt; /srv/healtharchive/ops/automation/posture.json</code></li> <li>Strict checks (optional):</li> <li>all timers present: <code>./scripts/verify_ops_automation.sh --require-all-present</code></li> <li>all timers enabled: <code>./scripts/verify_ops_automation.sh --require-all-enabled</code></li> <li><code>systemctl is-enabled &lt;timer&gt;</code> (should be <code>enabled</code>)</li> <li>sentinel file exists under <code>/etc/healtharchive/*enabled</code></li> <li><code>systemctl list-timers --all | grep healtharchive-</code> (shows next/last run)</li> <li><code>journalctl -u &lt;service&gt; -n 200</code> (shows last run success)</li> </ul>"},{"location":"operations/automation-verification-rituals/#posture-snapshots-optional","title":"Posture snapshots (optional)","text":"<ul> <li>Keep dated JSON under <code>/srv/healtharchive/ops/automation/</code> so you can diff over time.</li> <li>If the directory is missing, run: <code>sudo ./scripts/vps-bootstrap-ops-dirs.sh</code> (idempotent).</li> <li>Diff examples:</li> <li><code>diff -u &lt;(python3 -m json.tool &lt; old.json) &lt;(python3 -m json.tool &lt; new.json)</code></li> </ul>"},{"location":"operations/automation-verification-rituals/#dataset-releases","title":"Dataset releases","text":"<ul> <li>Confirm GitHub Actions are enabled in <code>jerdaw/healtharchive-datasets</code></li> <li>Confirm a release exists for the expected quarter/date</li> <li>Download assets and verify: <code>sha256sum -c SHA256SUMS</code></li> </ul>"},{"location":"operations/automation-verification-rituals/#restore-tests","title":"Restore tests","text":"<ul> <li>Confirm a dated log file exists in <code>/srv/healtharchive/ops/restore-tests/</code></li> <li>Ensure it includes: backup source, schema check, API checks, pass/fail, follow-ups</li> </ul>"},{"location":"operations/baseline-drift/","title":"Production baseline drift checks (internal)","text":"<p>Goal: avoid \u201cconfiguration drift\u201d where production stops matching what the project expects (security posture, perms, service units, etc.).</p> <p>This is implemented as:</p> <p>1) Desired state (in git): <code>production-baseline-policy.toml</code> 2) Observed state (generated on the VPS): JSON snapshots written to <code>/srv/healtharchive/ops/baseline/</code> 3) Drift check: compares observed vs policy and fails on required mismatches</p>"},{"location":"operations/baseline-drift/#files","title":"Files","text":"<ul> <li>Policy (edit in git): <code>production-baseline-policy.toml</code></li> <li>Snapshot generator: <code>../../scripts/baseline_snapshot.py</code></li> <li>Drift checker: <code>../../scripts/check_baseline_drift.py</code></li> </ul>"},{"location":"operations/baseline-drift/#one-shot-usage-recommended-after-any-production-change","title":"One-shot usage (recommended after any production change)","text":"<p>On the VPS (as <code>haadmin</code>):</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/check_baseline_drift.py --mode live\n</code></pre> <p>This writes:</p> <ul> <li><code>observed-&lt;timestamp&gt;.json</code> (machine-readable)</li> <li><code>drift-report-&lt;timestamp&gt;.txt</code> (human-readable)</li> <li>plus <code>observed-latest.json</code> and <code>drift-report-latest.txt</code></li> </ul> <p>All files live under <code>/srv/healtharchive/ops/baseline/</code>.</p>"},{"location":"operations/baseline-drift/#local-only-mode-no-network-dependency","title":"\u201cLocal only\u201d mode (no network dependency)","text":"<p>Use local-only mode when you want checks that don\u2019t depend on DNS/TLS/external routing:</p> <pre><code>./scripts/check_baseline_drift.py --mode local\n</code></pre> <p>In <code>local</code> mode:</p> <ul> <li>HSTS is validated by parsing <code>/etc/caddy/Caddyfile</code> for the API site block.</li> <li>Admin endpoint checks are skipped (warn-only).</li> </ul>"},{"location":"operations/baseline-drift/#optional-weekly-drift-timer-systemd","title":"Optional: weekly drift timer (systemd)","text":"<p>If you want drift checks to run automatically (not just during deploys), this repo includes a weekly systemd timer:</p> <ul> <li>Templates: <code>docs/deployment/systemd/healtharchive-baseline-drift-check.*</code></li> <li>Installer helper (VPS): <code>scripts/vps-install-systemd-units.sh --apply</code></li> <li>Enablement steps: <code>docs/deployment/systemd/README.md</code></li> </ul>"},{"location":"operations/baseline-drift/#cors-validation","title":"CORS validation","text":"<p>The policy enforces a strict production allowlist (no extra origins) via <code>HEALTHARCHIVE_CORS_ORIGINS</code>.</p> <ul> <li><code>--mode local</code> validates the env file value (CSV set comparison).</li> <li><code>--mode live</code> additionally probes the API with an <code>Origin:</code> header and checks   real <code>Access-Control-Allow-Origin</code> behavior.</li> </ul>"},{"location":"operations/baseline-drift/#replay-usage-invariants","title":"Replay + usage invariants","text":"<p>The policy can also pin \u201cpublic UX\u201d toggles that affect what users see:</p> <ul> <li><code>HEALTHARCHIVE_REPLAY_BASE_URL</code> and <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code> (replay browse URLs + previews)</li> <li><code>HEALTHARCHIVE_USAGE_METRICS_ENABLED</code> and <code>HEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS</code> (public <code>/status</code> + <code>/impact</code>)</li> </ul>"},{"location":"operations/baseline-drift/#when-to-update-policy","title":"When to update policy","text":"<p>Update <code>production-baseline-policy.toml</code> only when you intentionally change production invariants:</p> <ul> <li>URL strategy (adding staging, changing canonical domains)</li> <li>security posture (HSTS policy, admin auth policy)</li> <li>directory layout / ownership model</li> <li>systemd service names or enablement expectations</li> </ul> <p>Avoid adding \u201cthings that change often\u201d to policy (package versions, job counts, etc.).</p>"},{"location":"operations/citation-handout/","title":"HealthArchive.ca - Citation handout (pointer)","text":"<p>Canonical public copy lives in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/public/partner-kit/healtharchive-citation.md</li> <li>Live page: https://www.healtharchive.ca/cite</li> </ul> <p>If you need to update the handout, edit the frontend asset and deploy the frontend.</p>"},{"location":"operations/claims-registry/","title":"Claims Registry (internal)","text":"<p>Use this to back any reliability/automation/privacy/reproducibility claims with proof artifacts.</p> <ul> <li>Claim: Quarterly dataset releases run automatically (metadata-only).</li> <li>Evidence:<ul> <li>GitHub Releases: <code>https://github.com/jerdaw/healtharchive-datasets/releases</code> (tags <code>healtharchive-dataset-YYYY-MM-DD</code>)</li> <li>Release assets: <code>manifest.json</code> + <code>SHA256SUMS</code> + <code>healtharchive-*.jsonl.gz</code></li> <li>Workflow: <code>jerdaw/healtharchive-datasets</code> \u2192 Actions \u2192 \u201cPublish dataset release\u201d</li> </ul> </li> <li>Cadence: quarterly (Jan/Apr/Jul/Oct)</li> <li>Recorded in: dataset repo Releases + <code>/srv/healtharchive/ops/adoption/</code></li> <li>Claim: Dataset releases are integrity-verifiable.</li> <li>Evidence:<ul> <li>Download assets into one directory and run <code>sha256sum -c SHA256SUMS</code></li> <li><code>manifest.json</code> includes artifact SHA256s and row counts</li> </ul> </li> <li>Cadence: per release</li> <li>Recorded in: release assets + <code>/srv/healtharchive/ops/adoption/</code></li> <li>Claim: Change tracking is computed on schedule.</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-change-tracking.timer</code> (plus sentinel <code>/etc/healtharchive/change-tracking-enabled</code>)</li> <li>logs: <code>journalctl -u healtharchive-change-tracking.service</code></li> <li>public surface: <code>/changes</code> + <code>/api/changes</code></li> </ul> </li> <li>Cadence: daily</li> <li>Recorded in: journald (+ optional Healthchecks ping)</li> <li>Claim: Replay indexes are reconciled on schedule (when replay enabled).</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-replay-reconcile.timer</code> (plus sentinel <code>/etc/healtharchive/replay-automation-enabled</code>)</li> <li>logs: <code>journalctl -u healtharchive-replay-reconcile.service</code></li> </ul> </li> <li>Cadence: daily</li> <li>Recorded in: journald (+ optional Healthchecks ping)</li> <li>Claim: Annual search verification artifacts are captured (when enabled).</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-annual-search-verify.timer</code> (plus sentinel <code>/etc/healtharchive/automation-enabled</code>)</li> <li>artifacts: <code>/srv/healtharchive/ops/search-eval/&lt;year&gt;/final/</code></li> </ul> </li> <li>Cadence: daily timer, captures once per year when ready</li> <li>Recorded in: ops artifacts (+ optional Healthchecks ping)</li> <li>Claim: Coverage guardrails run for annual editions (when enabled).</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-coverage-guardrails.timer</code> (plus sentinel <code>/etc/healtharchive/coverage-guardrails-enabled</code>)</li> <li>metrics: <code>healtharchive_coverage_*</code> in node_exporter textfile collector</li> </ul> </li> <li>Cadence: daily</li> <li>Recorded in: metrics (+ optional Healthchecks ping)</li> <li>Claim: Replay smoke tests run for latest indexed jobs (when enabled).</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-replay-smoke.timer</code> (plus sentinel <code>/etc/healtharchive/replay-smoke-enabled</code>)</li> <li>metrics: <code>healtharchive_replay_smoke_*</code> in node_exporter textfile collector</li> </ul> </li> <li>Cadence: daily</li> <li>Recorded in: metrics (+ optional Healthchecks ping)</li> <li>Claim: Cleanup automation runs safely on indexed jobs (when enabled).</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-cleanup-automation.timer</code> (plus sentinel <code>/etc/healtharchive/cleanup-automation-enabled</code>)</li> <li>logs: <code>journalctl -u healtharchive-cleanup-automation.service</code></li> </ul> </li> <li>Cadence: weekly</li> <li>Recorded in: journald (+ optional Healthchecks ping)</li> <li>Claim: Annual campaign scheduling is automated and gated.</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-schedule-annual.timer</code> (plus sentinel <code>/etc/healtharchive/automation-enabled</code>)</li> <li>logs: <code>journalctl -u healtharchive-schedule-annual.service</code></li> </ul> </li> <li>Cadence: annually (Jan 01 UTC)</li> <li>Recorded in: journald</li> <li>Claim: Quarterly restore tests are performed (backups are usable).</li> <li>Evidence:<ul> <li>restore-test logs: <code>/srv/healtharchive/ops/restore-tests/restore-test-YYYY-MM-DD.md</code></li> <li>procedure reference: <code>docs/operations/restore-test-procedure.md</code></li> </ul> </li> <li>Cadence: quarterly</li> <li>Recorded in: <code>/srv/healtharchive/ops/restore-tests/</code></li> <li>Claim: Public usage metrics are privacy-preserving and aggregated.</li> <li>Evidence:<ul> <li>DB table: <code>usage_metrics</code> (aggregated daily counts only)</li> <li>API: <code>GET /api/usage</code> (windowed aggregates; no personal identifiers)</li> </ul> </li> <li>Cadence: daily aggregation; public reporting window is configurable</li> <li>Recorded in: DB + <code>/api/usage</code></li> </ul>"},{"location":"operations/data-handling-retention/","title":"Data Handling &amp; Retention (internal)","text":"<p>Prevent accidental collection/retention creep and PHI risk. Keep notes public-safe.</p>"},{"location":"operations/data-handling-retention/#issue-reports-post-apireports","title":"Issue reports (<code>POST /api/reports</code>)","text":"<p>Stored in DB table <code>issue_reports</code> with fields:</p> <ul> <li><code>category</code>, <code>description</code> (free text)</li> <li>optional <code>reporter_email</code>, <code>snapshot_id</code>, <code>original_url</code>, <code>page_url</code></li> <li><code>status</code>, <code>internal_notes</code></li> </ul> <p>Policy:</p> <ul> <li>Public UI must warn users not to submit personal health information.</li> <li>Admin views are operator-only; never expose reports in public UI.</li> <li>If a report includes PHI, do not copy it into other systems/logs; redact/delete and record a public-safe note.</li> <li>Retention: keep minimal; retain only what\u2019s needed to resolve the issue.</li> </ul>"},{"location":"operations/data-handling-retention/#usage-metrics-get-apiusage","title":"Usage metrics (<code>GET /api/usage</code>)","text":"<ul> <li>Stored in DB table <code>usage_metrics</code>: <code>metric_date</code>, <code>event</code>, <code>count</code>.</li> <li>Aggregated daily counts only (no IPs, no user IDs).</li> <li>Public API returns a rolling window (<code>HEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS</code>).</li> </ul>"},{"location":"operations/data-handling-retention/#backups","title":"Backups","text":"<ul> <li>Postgres dumps (custom-format) are stored on the VPS (see <code>docs/deployment/production-single-vps.md</code>).</li> <li>Treat dumps as sensitive; they may contain report text/emails and should not be shared publicly.</li> </ul>"},{"location":"operations/data-handling-retention/#serverapplication-logs","title":"Server/application logs","text":"<ul> <li>journald and web server logs may include IPs and request paths.</li> <li>Treat logs as sensitive; do not paste raw logs into public issues or git.</li> </ul>"},{"location":"operations/data-handling-retention/#ops-logs-public-safe","title":"Ops logs (public-safe)","text":"<ul> <li>Restore tests: <code>/srv/healtharchive/ops/restore-tests/</code> (public-safe Markdown entries only).</li> <li>Adoption signals: <code>/srv/healtharchive/ops/adoption/</code> (public-safe; quarterly; links + aggregates only).</li> <li>Mentions log: <code>mentions-log.md</code> (public-safe, link-only; no private contact details).</li> </ul>"},{"location":"operations/dataset-release-runbook/","title":"Dataset Release Runbook (internal)","text":"<p>This release is normally hands-off (GitHub Actions). Use this checklist for verification or recovery.</p>"},{"location":"operations/dataset-release-runbook/#checklist","title":"Checklist","text":"<p>1) Check <code>https://github.com/jerdaw/healtharchive-datasets/releases</code> for the latest tag. 2) Download all assets to one directory; run <code>sha256sum -c SHA256SUMS</code>. 3) Inspect <code>manifest.json</code> for <code>truncated=false</code> and plausible row counts. 4) Record a quarterly entry in <code>/srv/healtharchive/ops/adoption/</code> (links + aggregates only).</p> <p>Notes:</p> <ul> <li>The datasets publish workflow also validates the release bundle before publishing:</li> <li><code>manifest.json</code> required fields + invariants (including <code>truncated=false</code>)</li> <li>Checksums match both <code>manifest.json</code> and <code>SHA256SUMS</code></li> </ul>"},{"location":"operations/dataset-release-runbook/#if-a-release-is-missing","title":"If a release is missing","text":"<ul> <li>Manually run the Publish dataset release workflow in GitHub Actions.</li> <li>Confirm it creates a tag <code>healtharchive-dataset-YYYY-MM-DD</code> and uploads assets.</li> </ul>"},{"location":"operations/escalation-procedures/","title":"Escalation Procedures","text":"<p>Last Updated: 2026-01-18 Status: Active</p> <p>This document defines how to categorize, escalate, and respond to incidents affecting the HealthArchive production environment.</p>"},{"location":"operations/escalation-procedures/#1-severity-levels","title":"1. Severity Levels","text":"<p>We categorize incidents into four levels based on impact and urgency.</p> Level Definition Response Time Actions Sev0 Critical Outage / Data LossSystem is totally unusable, or confirmed data loss is occurring. Immediate 1. Stop all non-recovery work.2. Notify stakeholders (if any).3. Initiate Disaster Recovery. Sev1 Major DegradationCore features (Search, API) are broken or extremely slow. User impact is high. &lt; 1 Hour 1. Engage Primary On-Call.2. Investigate immediately.3. Deploy hotfix or rollback. Sev2 Partial DegradationSecondary features (e.g., Replay) broken, or performance issues with workarounds. &lt; 4 Hours 1. Log incident.2. Investigate within business hours.3. Schedule fix for next release window. Sev3 Minor IssueTrivial bugs, cosmetic issues, or single-page failures. No broad user impact. &lt; 24 Hours 1. Log ticket/issue.2. Prioritize in normal development backlog."},{"location":"operations/escalation-procedures/#2-escalation-path","title":"2. Escalation Path","text":""},{"location":"operations/escalation-procedures/#current-state-single-operator","title":"Current State: Single Operator","text":"<p>In the current single-maintainer topology, the escalation path is flat.</p> <ol> <li>Primary: Operator (You) - Responsible for all triage and resolution.</li> <li>Backup: None (Bus factor = 1).<ul> <li>Mitigation: Comprehensive Runbooks and Disaster Recovery docs to allow a skilled third party to recover the system using \"Break-Glass\" credentials if the primary operator is incapacitated.</li> </ul> </li> </ol>"},{"location":"operations/escalation-procedures/#future-state-multi-operator","title":"Future State: Multi-Operator","text":"<p>When the team grows, follow this hierarchy:</p> <ol> <li>Level 1 (On-Call): Triage, immediate mitigation, and initial investigation.</li> <li>Level 2 (Secondary/Backup): Deep dive debugging, code fixes, and complex recovery.</li> <li>Level 3 (Project Lead): Strategic decisions (e.g., data loss acceptance, major architecture rollback).</li> </ol>"},{"location":"operations/escalation-procedures/#3-dri-assignments-directly-responsible-individuals","title":"3. DRI Assignments (Directly Responsible Individuals)","text":"<p>Since we largely operate as a single unit, the Operator is the DRI for all areas. This matrix serves as a template for future delegation.</p> Area DRI Responsibilities Backend API Operator FastAPI availability, performance, response correctness. Worker / Crawls Operator Job scheduling, zimit/warcio execution, tiering to storage. Database Operator PostgreSQL uptime, backup verification, schema migrations. Storage / WARC Operator Disk space management, Storage Box connectivity, manifest integrity. Replay Service Operator <code>pywb</code> availability and indexing health. Infrastructure Operator VPS provisioning, OS updates, systemd maintenance, Tailscale."},{"location":"operations/escalation-procedures/#4-contact-information-storage","title":"4. Contact Information Storage","text":"<p>For security reasons, do not store phone numbers or sensitive access codes in this git repository.</p>"},{"location":"operations/escalation-procedures/#production-contact-list","title":"Production Contact list","text":"<p>Store a secure, read-only file on the production VPS for emergency reference:</p> <ul> <li>Path: <code>/etc/healtharchive/contacts.env</code></li> <li>Permissions: <code>600</code> (root/owner only)</li> <li>Format: Key-Value pairs</li> </ul> <pre><code># Example content for /etc/healtharchive/contacts.env\nOPERATOR_PHONE=\"+1-555-0100\"\nOPERATOR_EMAIL=\"admin@healtharchive.ca\"\nSECONDARY_CONTACT_PHONE=\"+1-555-0101\" # Backup contact (if any)\nHETZNER_SUPPORT_PIN=\"12345\"\nNAMECHEAP_SUPPORT_PIN=\"67890\"\n</code></pre>"},{"location":"operations/escalation-procedures/#personal-backup","title":"Personal Backup","text":"<p>Mirror this information in your password manager (e.g., 1Password, Bitwarden) under a secure note titled \"HealthArchive Emergency Contacts\".</p>"},{"location":"operations/escalation-procedures/#5-break-glass-procedures","title":"5. Break-Glass Procedures","text":"<p>Quick-reference steps for common critical failures where normal access or services are blocked.</p>"},{"location":"operations/escalation-procedures/#a-api-unresponsive-http-502503timeout","title":"A. API Unresponsive (HTTP 502/503/Timeout)","text":"<ol> <li>Access: SSH to VPS via Tailscale (<code>ssh haadmin@100.x.y.z</code>).</li> <li>Status: Check if the service is running.     <pre><code>username@host:~$ systemctl status healtharchive-api\n</code></pre></li> <li>Logs: specific error messages?     <pre><code>username@host:~$ journalctl -u healtharchive-api -n 100\n</code></pre></li> <li>Action: Restart the service.     <pre><code>username@host:~$ sudo systemctl restart healtharchive-api\n</code></pre></li> <li>Escalation: If restart fails or immediately crashes, check Database connectivity (see B).</li> </ol>"},{"location":"operations/escalation-procedures/#b-database-unreachable","title":"B. Database Unreachable","text":"<ol> <li>Status: Is Postgres running?     <pre><code>username@host:~$ systemctl status postgresql\n</code></pre></li> <li>Resources: Is disk full?     <pre><code>username@host:~$ df -h\n</code></pre></li> <li>Logs: <pre><code>username@host:~$ journalctl -u postgresql -n 100\n</code></pre></li> <li>Action: Restart Postgres.     <pre><code>username@host:~$ sudo systemctl restart postgresql\n</code></pre></li> <li>Escalation: If database won't start due to corruption, proceed to Disaster Recovery Scenario B.</li> </ol>"},{"location":"operations/escalation-procedures/#c-vps-unreachable-ssh-down","title":"C. VPS Unreachable (SSH Down)","text":"<ol> <li>Check Network: Try accessing via different Tailscale node or public IP (if SSH open/testing).</li> <li>Console Access: Log in to Hetzner Cloud Console &gt; Select Server &gt; Console.<ul> <li>This bypasses network/SSH config issues.</li> </ul> </li> <li>Reboot: Use the Hetzner \"Power\" menu to force a reboot ACPI or hard reset if the OS is frozen.</li> <li>Escalation: If the server is deleted or hardware failed, proceed to Disaster Recovery Scenario A.</li> </ol>"},{"location":"operations/escalation-procedures/#6-handoff-procedures","title":"6. Handoff Procedures","text":"<p>When transferring responsibility (e.g., vacation coverage):</p> <ol> <li>Sync: Verify current system health (dashboards, logs).</li> <li>Access: Confirm backup operator has valid SSH/Tailscale access.</li> <li>Docs: Ensure emergency contact info is accessible to the backup.</li> <li>Notify: Inform any stakeholders (if applicable) of the active operator change.</li> </ol>"},{"location":"operations/export-integrity-contract/","title":"Export Integrity Contract (internal)","text":"<p>Exports and dataset releases must be defensible and reproducible over time.</p>"},{"location":"operations/export-integrity-contract/#export-endpoints-ordering-pagination","title":"Export endpoints (ordering + pagination)","text":"<ul> <li><code>GET /api/exports/snapshots</code> is ordered by <code>snapshot_id</code> ascending and paginates via <code>afterId</code>.</li> <li><code>GET /api/exports/changes</code> is ordered by <code>change_id</code> ascending and paginates via <code>afterId</code>.</li> </ul>"},{"location":"operations/export-integrity-contract/#dataset-release-manifest-manifestjson","title":"Dataset release manifest (<code>manifest.json</code>)","text":"<p>Required fields:</p> <ul> <li><code>version</code> (schema version for the manifest itself)</li> <li><code>tag</code> (release tag)</li> <li><code>releasedAtUtc</code> (ISO-8601 UTC timestamp)</li> <li><code>apiBase</code> and <code>exportsManifest</code> (from <code>GET /api/exports</code>)</li> <li><code>artifacts.snapshots</code> and <code>artifacts.changes</code> including:</li> <li><code>rows</code>, <code>minId</code>, <code>maxId</code>, <code>requestsMade</code>, <code>limitPerRequest</code>, <code>truncated</code></li> <li><code>sha256</code> for each artifact</li> </ul> <p>Rules:</p> <ul> <li><code>SHA256SUMS</code> must match all listed files.</li> <li><code>truncated</code> should be <code>false</code> for both exports; if <code>true</code>, treat the release as incomplete and re-run.</li> </ul>"},{"location":"operations/export-integrity-contract/#immutability-corrections","title":"Immutability / corrections","text":"<ul> <li>Treat published dataset release tags as immutable research objects.</li> <li>If a correction is required, document it in release notes and prefer a new tag over silently rewriting history.</li> </ul>"},{"location":"operations/export-integrity-contract/#diff-recomputation-policy","title":"Diff recomputation policy","text":"<ul> <li>Change export rows include <code>diff_version</code> and <code>normalization_version</code>.</li> <li>If change tracking methodology changes, bump versions and document it (methods note/changelog) rather than silently rewriting history.</li> </ul>"},{"location":"operations/exports-data-dictionary/","title":"HealthArchive exports \u2014 data dictionary (pointer)","text":"<p>Canonical public copy lives in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/public/exports/healtharchive-data-dictionary.md</li> <li>Live page: https://www.healtharchive.ca/exports</li> </ul> <p>If you need to update the data dictionary, edit the frontend asset and deploy the frontend.</p>"},{"location":"operations/growth-constraints/","title":"Growth Constraints (internal policy)","text":"<p>Purpose: prevent slow scope creep from undermining reliability on a single-VPS architecture.</p> <p>These constraints are intentionally conservative defaults. Adjust only after a capacity review.</p>"},{"location":"operations/growth-constraints/#storage-budget","title":"Storage budget","text":"<ul> <li>Target: keep total disk usage under 70% of available space.</li> <li>Review threshold: 80% usage triggers a pause on new sources until cleanup or capacity planning is complete.</li> <li>Replay retention: if replay is enabled, WARCs must remain available. Use safe cleanup only (<code>cleanup-job --mode temp-nonwarc</code>).</li> </ul>"},{"location":"operations/growth-constraints/#source-cap-per-annual-edition","title":"Source cap per annual edition","text":"<ul> <li>Default cap: add no more than 2 new sources per annual edition.</li> <li>Any additions beyond the cap require:</li> <li>a successful dry-run capture,</li> <li>indexing verification,</li> <li>and a storage headroom check.</li> </ul>"},{"location":"operations/growth-constraints/#performance-budgets-initial-targets","title":"Performance budgets (initial targets)","text":"<p>These are targets, not guarantees. Use them to detect regressions.</p> <ul> <li>Search (view=pages): p95 response &lt; 2s for common queries.</li> <li>Snapshot detail: p95 response &lt; 1s for metadata payloads.</li> <li>Changes feed: p95 response &lt; 2s for edition-aware queries.</li> </ul> <p>If p95 exceeds targets for multiple weeks, pause new scope additions and prioritize performance fixes.</p>"},{"location":"operations/growth-constraints/#crawl-load-limits","title":"Crawl load limits","text":"<ul> <li>Keep crawler parallelism conservative during annual campaigns to avoid:</li> <li>starving the API,</li> <li>exceeding source rate limits,</li> <li>and filling storage too quickly.</li> <li>If capture jobs start to lag, reduce concurrency before expanding scope.</li> </ul>"},{"location":"operations/growth-constraints/#public-facing-summary-use-in-governance","title":"Public-facing summary (use in governance)","text":"<p>Reliability and provenance take priority over expanding coverage. Sources are added deliberately within storage and operational capacity, and the archive does not attempt to crawl the entire internet.</p>"},{"location":"operations/healtharchive-ops-roadmap/","title":"HealthArchive ops roadmap (internal)","text":"<p>This file tracks the current ops roadmap/todo items only. Keep it short and current.</p> <p>For historical roadmaps and upgrade context, see:</p> <ul> <li><code>docs/roadmaps/README.md</code> (backend repo)</li> </ul> <p>Keep the two synced copies of this file aligned:</p> <ul> <li>Backend repo: <code>docs/operations/healtharchive-ops-roadmap.md</code></li> <li>Optional local working copy (non-git): if you keep a separate ops checklist outside the repo, keep it in sync with this canonical file.</li> </ul>"},{"location":"operations/healtharchive-ops-roadmap/#recurring-ops-non-irl-ongoing","title":"Recurring ops (non-IRL, ongoing)","text":"<ul> <li>Quarterly: run a restore test and record a public-safe log entry in <code>/srv/healtharchive/ops/restore-tests/</code>.</li> <li>Quarterly: add an adoption signals entry in <code>/srv/healtharchive/ops/adoption/</code> (links + aggregates only).</li> <li>Quarterly: confirm dataset release exists and passes checksum verification (<code>sha256sum -c SHA256SUMS</code>).</li> <li>Quarterly: confirm core timers are enabled and succeeding (recommended: on the VPS run <code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/verify_ops_automation.sh</code>; then spot-check <code>journalctl -u &lt;service&gt;</code>).</li> <li>Quarterly: docs drift skim: re-read the production runbook + incident response and fix any drift you notice (keep docs matching reality).</li> </ul>"},{"location":"operations/healtharchive-ops-roadmap/#current-ops-tasks-implementation-already-exists-enableverify","title":"Current ops tasks (implementation already exists; enable/verify)","text":"<ul> <li>Verify the new Docker resource limit environment variables are set appropriately on VPS if defaults need adjustment:</li> <li><code>HEALTHARCHIVE_DOCKER_MEMORY_LIMIT</code> (default: 4g)</li> <li><code>HEALTHARCHIVE_DOCKER_CPU_LIMIT</code> (default: 1.5)</li> <li>For already-created annual jobs, decide whether to patch <code>ArchiveJob.config.tool_options</code> to adopt:</li> <li><code>skip_final_build=true</code></li> <li><code>docker_shm_size=\"1g\"</code></li> <li><code>stall_timeout_minutes=60</code> (canada.ca sources)</li> <li><code>initial_workers=2</code> (if you want it to take effect on retry/recovery)</li> <li>Verify the new alerts are firing correctly in Grafana:</li> <li><code>HealthArchiveCrawlRateSlow</code></li> <li><code>HealthArchiveInfraErrorsHigh</code></li> <li><code>HealthArchiveCrawlMetricsStale</code></li> </ul>"},{"location":"operations/healtharchive-ops-roadmap/#irl-external-validation-pending","title":"IRL / external validation (pending)","text":"<p>Track external validation/outreach work (partner, verifier, mentions/citations log) in:</p> <ul> <li><code>../roadmaps/roadmap.md</code></li> </ul>"},{"location":"operations/legacy-crawl-imports/","title":"Importing legacy crawls (from old <code>.zim</code> backups)","text":"<p>This doc explains how we imported existing crawl data (originally used to produce <code>.zim</code> files) into the HealthArchive backend so it shows up in:</p> <ul> <li><code>https://api.healtharchive.ca</code> (search, sources, viewer)</li> <li><code>https://www.healtharchive.ca/archive</code> (live results)</li> </ul>"},{"location":"operations/legacy-crawl-imports/#important-clarification-we-did-not-import-the-zim-files","title":"Important clarification: we did not \u201cimport the <code>.zim</code> files\u201d","text":"<p>The backend does not read <code>.zim</code> files today.</p> <p>Instead, we imported the WARC files (the raw web captures) that live next to those <code>.zim</code> outputs in the old crawl directories. The backend indexes WARCs into database rows (<code>Snapshot</code>) and the snapshot viewer replays archived HTML from those WARCs.</p> <p>If you want to keep the <code>.zim</code> files, store them as separate artifacts (NAS / object storage). They\u2019re useful for offline viewing, but they are not part of the backend\u2019s serving path.</p>"},{"location":"operations/legacy-crawl-imports/#terminology","title":"Terminology","text":"<ul> <li>Legacy crawl: a crawl run before this project\u2019s integrated backend existed.</li> <li>WARC: compressed web capture files (<code>*.warc.gz</code>).</li> <li>Import directory: a directory on the VPS under   <code>/srv/healtharchive/jobs/imports/&lt;import-name&gt;</code> that holds the legacy WARCs in   a layout the backend\u2019s WARC discovery can find.</li> <li>Register: create an <code>ArchiveJob</code> row pointing at an existing directory   (<code>ha-backend register-job-dir</code>).</li> <li>Index: parse WARCs and write <code>Snapshot</code> rows (<code>ha-backend index-job</code>).</li> </ul>"},{"location":"operations/legacy-crawl-imports/#prerequisites","title":"Prerequisites","text":"<ul> <li>Backend is already deployed on the VPS and can reach Postgres.</li> <li>You have an SSH path from your NAS to the VPS:</li> <li>We used Tailscale so the NAS can reach the VPS consistently even if home     IP changes.</li> <li>We used a dedicated NAS SSH key and a dedicated VPS user (<code>habackup</code>) for     file transfer.</li> <li>The backend env file exists on the VPS:</li> <li><code>/etc/healtharchive/backend.env</code> (mode <code>0600</code>, owned by <code>root:root</code>).</li> <li>The backend archive root exists:</li> <li><code>/srv/healtharchive/jobs</code></li> </ul>"},{"location":"operations/legacy-crawl-imports/#source-of-truth-locations-nas-vs-dev-vm-mount","title":"Source of truth locations (NAS vs dev VM mount)","text":"<p>Your Synology NAS stores the legacy crawl data at:</p> <ul> <li><code>/volume1/nobak/gov-health-archives/...</code></li> </ul> <p>Your Linux dev VM mounts that NAS path at:</p> <ul> <li><code>/mnt/nasd/nobak/gov-health-archives/...</code></li> </ul> <p>When writing instructions for \u201crun on NAS\u201d, use the <code>/volume1/...</code> path. When running on the dev VM, use the <code>/mnt/nasd/...</code> path.</p>"},{"location":"operations/legacy-crawl-imports/#step-by-step-importing-a-legacy-dataset","title":"Step-by-step: importing a legacy dataset","text":""},{"location":"operations/legacy-crawl-imports/#step-1-identify-the-warc-archive-directory-to-import","title":"Step 1 \u2014 Identify the WARC archive directory to import","text":"<p>In the legacy crawl output, find the directory that contains the WARCs. Example (Health Canada legacy crawl):</p> <ul> <li>NAS:</li> <li><code>/volume1/nobak/gov-health-archives/canada_ca_health_backup_2025-04-21/crawler_data/collections/canada_ca_health_crawl_2025-04/archive/</code></li> <li>Dev VM (same content, mounted):</li> <li><code>/mnt/nasd/nobak/gov-health-archives/canada_ca_health_backup_2025-04-21/crawler_data/collections/canada_ca_health_crawl_2025-04/archive/</code></li> </ul> <p>What you\u2019re looking for:</p> <ul> <li>many files like <code>rec-&lt;id&gt;-&lt;collection&gt;-&lt;timestamp&gt;-N.warc.gz</code></li> </ul>"},{"location":"operations/legacy-crawl-imports/#step-2-create-the-destination-import-directory-structure-on-the-vps","title":"Step 2 \u2014 Create the destination \u201cimport directory\u201d structure on the VPS","text":"<p>We created an import output directory that looks like an <code>archive_tool</code> output, because the backend\u2019s WARC discovery already knows how to find WARCs inside a job directory by looking for temp dirs like <code>.tmp-*</code> and <code>collections/*/archive</code>.</p> <p>Example destination (Health Canada legacy import):</p> <ul> <li><code>/srv/healtharchive/jobs/imports/legacy-hc-2025-04-21/.tmp-legacy/collections/crawl-legacy-hc-2025-04-21/archive/</code></li> </ul> <p>Example destination (CIHR legacy import):</p> <ul> <li><code>/srv/healtharchive/jobs/imports/legacy-cihr-2025-04/.tmp-legacy/collections/crawl-legacy-cihr-2025-04/archive/</code></li> </ul> <p>Notes:</p> <ul> <li>The exact collection directory name doesn\u2019t matter much; what matters is the   presence of:</li> <li>a <code>.tmp-*</code> directory (we used <code>.tmp-legacy</code>)</li> <li>a <code>collections/&lt;anything&gt;/archive/</code> directory inside it</li> <li>WARCs under that archive directory</li> <li>The backend log line you\u2019ll see during indexing is similar to:</li> <li>\u201cFallback found latest temp dir: \u2026/.tmp-legacy\u201d</li> </ul>"},{"location":"operations/legacy-crawl-imports/#step-3-transfer-the-warcs-from-nas-vps-via-rsync","title":"Step 3 \u2014 Transfer the WARCs from NAS \u2192 VPS via rsync","text":"<p>We ran <code>rsync</code> from the NAS, using SSH over Tailscale:</p> <pre><code>rsync -av --info=progress2 --partial --append-verify --bwlimit=5000 \\\n  -e \"ssh -i ~/.ssh/&lt;NAS_SSH_KEY&gt;\" \\\n  \"/volume1/nobak/gov-health-archives/&lt;LEGACY_PATH&gt;/archive/\" \\\n  \"habackup@&lt;VPS_TAILSCALE_IP&gt;:/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;/.tmp-legacy/collections/&lt;COLLECTION_NAME&gt;/archive/\"\n</code></pre> <p>Why these flags:</p> <ul> <li><code>--partial --append-verify</code>: safe-ish resume if the connection drops.</li> <li><code>--bwlimit</code>: avoids saturating your uplink.</li> </ul> <p>After transfer, verify on VPS:</p> <pre><code>sudo find \"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\" -name '*.warc.gz' | wc -l\nsudo du -sh \"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\"\n</code></pre> <p>Real example result (Health Canada legacy import):</p> <ul> <li><code>959</code> WARCs</li> <li><code>~26G</code> total</li> </ul>"},{"location":"operations/legacy-crawl-imports/#step-4-normalize-permissions-rsync-from-nas-can-create-unsafe-modes","title":"Step 4 \u2014 Normalize permissions (rsync from NAS can create unsafe modes)","text":"<p>The rsync upload preserved permissive modes (<code>777</code>) from the source, which is not what we want on the VPS.</p> <p>We normalized permissions on the VPS:</p> <pre><code>IMPORT_DIR=\"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\"\n\nsudo chown -R habackup:healtharchive \"$IMPORT_DIR\"\n\n# Directories: rwx for owner+group, setgid so new files inherit group\nsudo find \"$IMPORT_DIR\" -type d -exec chmod 2770 {} +\n\n# WARCs: readable by owner+group only\nsudo find \"$IMPORT_DIR\" -type f -name '*.warc.gz' -exec chmod 640 {} +\n</code></pre> <p>Why:</p> <ul> <li><code>healtharchive</code> group can be granted controlled read access.</li> <li>We avoid <code>777</code> and other \u201cworld writable\u201d mistakes.</li> </ul>"},{"location":"operations/legacy-crawl-imports/#optional-helper-one-command-to-normalize-register-index","title":"Optional helper: one command to normalize + register + index","text":"<p>This repo includes a helper script that wraps the \u201cVPS-side\u201d steps:</p> <ul> <li>permissions normalization (Step 4)</li> <li>register-job-dir (Step 5)</li> <li>index-job (Step 6)</li> </ul> <p>It is dry-run by default:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/import-legacy-crawl.sh --import-dir \"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\" --source &lt;SOURCE_CODE&gt;\n</code></pre> <p>To apply (real run):</p> <pre><code>./scripts/import-legacy-crawl.sh --apply --import-dir \"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\" --source &lt;SOURCE_CODE&gt; --job-name \"&lt;JOB_NAME&gt;\"\n</code></pre>"},{"location":"operations/legacy-crawl-imports/#step-5-register-the-directory-as-an-archivejob","title":"Step 5 \u2014 Register the directory as an ArchiveJob","text":"<p>Because <code>/etc/healtharchive/backend.env</code> is root-owned and not readable by normal users, we used <code>systemd-run</code> to run the CLI with <code>EnvironmentFile=/etc/healtharchive/backend.env</code>.</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend register-job-dir \\\n  --source &lt;SOURCE_CODE&gt; \\\n  --output-dir \"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\" \\\n  --name \"&lt;JOB_NAME&gt;\"\n</code></pre> <p>Example (Health Canada legacy import):</p> <ul> <li><code>--source hc</code></li> <li><code>--output-dir /srv/healtharchive/jobs/imports/legacy-hc-2025-04-21</code></li> <li><code>--name legacy-hc-2025-04-21</code></li> <li>Created <code>ArchiveJob</code> ID <code>1</code></li> </ul>"},{"location":"operations/legacy-crawl-imports/#step-6-index-warcs-into-snapshot-rows","title":"Step 6 \u2014 Index WARCs into Snapshot rows","text":"<p>Indexing is the expensive part. It scans WARCs and creates one <code>Snapshot</code> row per captured HTML page.</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend index-job \\\n  --id &lt;JOB_ID&gt;\n</code></pre> <p>Expected resource usage (real example: Health Canada legacy import):</p> <ul> <li><code>959</code> WARCs</li> <li><code>~2h 16m</code> wall time</li> <li><code>~1.9G</code> peak RAM</li> <li>CPU pegged near 100% during indexing</li> </ul> <p>You can watch progress indirectly via systemd:</p> <pre><code>sudo systemctl status run-uXXX.service --no-pager -l\n</code></pre>"},{"location":"operations/legacy-crawl-imports/#step-7-verify-the-import-worked-db-api-frontend","title":"Step 7 \u2014 Verify the import worked (DB + API + frontend)","text":"<p>On VPS (DB/job view):</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend show-job --id &lt;JOB_ID&gt;\n</code></pre> <p>On your laptop (public API):</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?page=1&amp;pageSize=10\"\ncurl -s \"https://api.healtharchive.ca/api/sources\"\n</code></pre> <p>In the browser (frontend):</p> <ul> <li><code>https://www.healtharchive.ca/archive</code> should show a large snapshot count and real results.</li> <li>Clicking \u201cView snapshot\u201d should open <code>https://www.healtharchive.ca/snapshot/&lt;id&gt;</code> and the embedded content should load from:</li> <li><code>https://api.healtharchive.ca/api/snapshots/raw/&lt;id&gt;</code></li> </ul> <p>Optional single command (VPS; requires CIHR to be imported/indexed if you ask for it):</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/verify_public_surface.py --require-source cihr\n</code></pre>"},{"location":"operations/legacy-crawl-imports/#known-gotcha-capture-dates-may-look-wrong-fix-requires-re-index","title":"Known gotcha: capture dates may look wrong (fix requires re-index)","text":"<p>If the indexer fails to parse <code>WARC-Date</code>, the backend may fall back to \u201cnow\u201d, making imported snapshots look like they were captured on import day.</p> <p>When this is fixed in code, you must re-run indexing for the job to update the stored capture timestamps.</p>"},{"location":"operations/legacy-crawl-imports/#what-we-imported-so-far-real-outcomes","title":"What we imported so far (real outcomes)","text":""},{"location":"operations/legacy-crawl-imports/#health-canada-legacy-import","title":"Health Canada legacy import","text":"<ul> <li>Source: legacy Health Canada crawl output (April 2025).</li> <li>Imported to VPS under:</li> <li><code>/srv/healtharchive/jobs/imports/legacy-hc-2025-04-21</code></li> <li>Result:</li> <li><code>ArchiveJob</code> ID <code>1</code></li> <li><code>959</code> WARCs</li> <li><code>123,656</code> snapshots indexed</li> <li>API + frontend show live results.</li> </ul>"},{"location":"operations/legacy-crawl-imports/#cihr-legacy-import-in-progress-next","title":"CIHR legacy import (in progress / next)","text":"<ul> <li>Source: legacy CIHR crawl output (April 2025).</li> <li>WARCs transferred to:</li> <li><code>/srv/healtharchive/jobs/imports/legacy-cihr-2025-04/.../archive/</code></li> <li>Next steps:</li> <li>normalize permissions</li> <li>register-job-dir with <code>--source cihr</code></li> <li>index-job</li> </ul>"},{"location":"operations/mentions-log/","title":"Mentions log (public-safe, link-only)","text":"<p>Purpose: a lightweight log of public mentions, links, or citations for HealthArchive.ca.</p> <p>Hard rules:</p> <ul> <li>Do not add private contact details (emails, phone numbers, private notes).</li> <li>If permission to name is unclear, do not name the organization here.</li> <li>Use <code>Pending</code> in the organization field until permission is explicit.</li> </ul>"},{"location":"operations/mentions-log/#confirmed-mentions-permission-to-name-yes","title":"Confirmed mentions (permission to name: Yes)","text":"Date (UTC) Organization / Outlet Link Context"},{"location":"operations/mentions-log/#unconfirmed-mentions-permission-pending-unknown","title":"Unconfirmed mentions (permission: Pending / Unknown)","text":"Date (UTC) Organization / Outlet Link Context Permission status"},{"location":"operations/mentions-log/#notes","title":"Notes","text":"<ul> <li>Keep descriptions factual and short.</li> <li>Prefer the most specific public link (the actual resources page that mentions HealthArchive).</li> </ul>"},{"location":"operations/methods-note-outline/","title":"Methods Note Outline (public-safe)","text":"<p>This outline is designed to become a poster, short preprint, or blog-style methods write-up that is:</p> <ul> <li>Descriptive (not interpretive).</li> <li>Explicitly not medical advice and not current guidance.</li> <li>Reproducibility-focused: \u201cwhat was published, when, and how we preserve it\u201d.</li> </ul> <p>It is intentionally \u201coutline-first\u201d so it can be quickly adapted to different venues without rewriting the project.</p>"},{"location":"operations/methods-note-outline/#working-title-options","title":"Working title options","text":"<ul> <li>HealthArchive.ca: A provenance-first archive of Canadian public health webpages</li> <li>Preserving temporal provenance in Canadian public health web guidance</li> <li>From snapshots to auditability: indexing and change tracking for public health webpages</li> </ul>"},{"location":"operations/methods-note-outline/#one-sentence-framing-use-everywhere","title":"One-sentence framing (use everywhere)","text":"<p>HealthArchive.ca preserves time-stamped snapshots of selected Canadian public health webpages so changes remain auditable, citable, and reproducible over time.</p>"},{"location":"operations/methods-note-outline/#abstract-structure","title":"Abstract (structure)","text":"<ul> <li>Background / problem: Public health web guidance and surveillance dashboards are \u201cliving documents\u201d that can change or disappear; this complicates reproducibility, journalism, and policy history.</li> <li>Objective: Build an independent, non-authoritative archive that makes historical versions discoverable and citable, with provenance labeling and descriptive change tracking.</li> <li>Methods: Automated capture \u2192 WARC storage \u2192 indexing into a searchable database \u2192 snapshot viewer + optional replay \u2192 edition-aware change tracking.</li> <li>Outputs: Public UI, metadata API, change feed, and metadata-only exports for research use.</li> <li>Limitations: Not authoritative, not current guidance, replay fidelity varies, scope intentionally constrained.</li> </ul>"},{"location":"operations/methods-note-outline/#introduction-what-why","title":"Introduction (what + why)","text":"<ul> <li>Web content in public health matters because it is used operationally (clinicians, journalists, researchers, public).</li> <li>Web guidance changes are normal, but they become hard to reconstruct after updates.</li> <li>Existing general-purpose web archives may not be optimized for:</li> <li>discoverability via structured search,</li> <li>consistent provenance labeling,</li> <li>edition-aware change tracking,</li> <li>research-ready exports and citation workflows.</li> </ul>"},{"location":"operations/methods-note-outline/#scope-and-safety-posture-non-negotiable","title":"Scope and safety posture (non-negotiable)","text":"<ul> <li>What it is: An independent archive of historical public webpages, designed for auditability and reproducibility.</li> <li>What it is not: A source of current guidance, a government site, or medical advice.</li> <li>Primary audiences: researchers, journalists, educators (secondary: clinicians/public with strong labeling).</li> <li>Privacy posture: no accounts, no PHI collection, minimal aggregated usage metrics only.</li> </ul>"},{"location":"operations/methods-note-outline/#system-overview-architecture","title":"System overview (architecture)","text":"<p>Describe at a high level (no sensitive infra details):</p> <ul> <li>Capture pipeline: browser-based crawling to standards-based WARC files.</li> <li>Storage: WARC files retained as the archival source of truth.</li> <li>Indexing: extract title/snippet/text signals into a relational database to enable fast search/browse.</li> <li>Serving: public API + Next.js frontend snapshot viewer; optional higher-fidelity replay.</li> <li>Edition model: annual \u201ceditions\u201d anchored to a capture campaign date (e.g., Jan 01 UTC) with occasional ad-hoc captures.</li> </ul>"},{"location":"operations/methods-note-outline/#capture-methodology-how-snapshots-are-created","title":"Capture methodology (how snapshots are created)","text":"<ul> <li>Seed URL sets and explicit include/exclude rules per source.</li> <li>Capture outputs recorded as WARCs (HTTP responses + timestamps and metadata).</li> <li>Operational constraints:</li> <li>scope boundaries to avoid \u201ccrawl everything\u201d failure modes,</li> <li>reliability &gt; breadth,</li> <li>respect for safe crawling practices (rate limiting, constrained seeds).</li> </ul>"},{"location":"operations/methods-note-outline/#indexing-and-discovery-how-users-find-things","title":"Indexing and discovery (how users find things)","text":"<ul> <li>Convert WARC records into \u201csnapshot\u201d rows with:</li> <li>capture timestamp (UTC),</li> <li>source attribution,</li> <li>original URL,</li> <li>stable snapshot permalink,</li> <li>language + status code where available.</li> <li>Provide:</li> <li>keyword search,</li> <li>source filtering,</li> <li>date range filtering,</li> <li>page-grouping (\u201clatest per page\u201d) vs \u201call captures\u201d views.</li> </ul>"},{"location":"operations/methods-note-outline/#provenance-labeling-how-users-avoid-misuse","title":"Provenance labeling (how users avoid misuse)","text":"<ul> <li>Snapshot pages and changes surfaces display:</li> <li>capture date/time (UTC),</li> <li>source name,</li> <li>original URL,</li> <li>\u201carchival snapshot\u201d warning / \u201cnot current guidance\u201d callout,</li> <li>links back to official sources.</li> </ul>"},{"location":"operations/methods-note-outline/#change-tracking-descriptive-diffing","title":"Change tracking (descriptive diffing)","text":"<p>Goal: make changes visible without interpreting meaning.</p> <ul> <li>Edition-aware change tracking: defaults to \u201cchanges between editions\u201d, not \u201crecent changes\u201d.</li> <li>Normalization: extract readable text and reduce boilerplate noise.</li> <li>Diffs: generate human-readable comparisons and a changes feed.</li> <li>Guardrails: no medical interpretation; the system reports \u201ctext changed\u201d and where.</li> </ul>"},{"location":"operations/methods-note-outline/#research-exports-metadata-only","title":"Research exports (metadata-only)","text":"<ul> <li>Public export manifest describes formats and limits.</li> <li>Exports include:</li> <li>snapshot metadata export (no raw HTML),</li> <li>change event export (no diff bodies).</li> <li>Intended uses:</li> <li>reproducible citations in papers/articles,</li> <li>quantitative analysis of guidance drift without redistributing full copyrighted page bodies.</li> </ul>"},{"location":"operations/methods-note-outline/#limitations-and-failure-modes-be-explicit","title":"Limitations and failure modes (be explicit)","text":"<ul> <li>Coverage gaps: not all pages captured; crawling can fail; scope is limited.</li> <li>Replay fidelity: some JS assets or third-party embeds may not replay.</li> <li>Temporal resolution: annual editions mean \u201cbetween-edition changes\u201d are not real-time updates.</li> <li>Non-authoritative: content is archival and may be outdated or superseded.</li> </ul>"},{"location":"operations/methods-note-outline/#ethics-governance-and-corrections","title":"Ethics, governance, and corrections","text":"<ul> <li>Governance pages define:</li> <li>inclusion criteria,</li> <li>correction workflow and response expectations,</li> <li>takedown/opt-out process,</li> <li>changelog discipline.</li> </ul>"},{"location":"operations/methods-note-outline/#results-descriptive-no-interpretation","title":"Results (descriptive; no interpretation)","text":"<p>Suggested content:</p> <ul> <li>Coverage counts (sources, snapshots, pages).</li> <li>Example: a single URL\u2019s timeline across editions.</li> <li>Example: change feed categories (new page, updated, removed/redirected).</li> <li>Usage signals (aggregated counts only; no personal identifiers).</li> </ul>"},{"location":"operations/methods-note-outline/#discussion-what-this-enables","title":"Discussion (what this enables)","text":"<ul> <li>Reproducibility for researchers (date-stamped citations).</li> <li>Accountability and auditability for journalists and educators.</li> <li>Public-interest infrastructure value without claiming authority.</li> </ul>"},{"location":"operations/methods-note-outline/#conclusion","title":"Conclusion","text":"<ul> <li>Reiterate that the contribution is infrastructure for temporal provenance and discoverability.</li> <li>State planned work at a high level:</li> <li>more sources within scope,</li> <li>improved diff quality,</li> <li>versioned dataset releases (if/when adopted).</li> </ul>"},{"location":"operations/methods-note-outline/#figures-tables-plan","title":"Figures / tables (plan)","text":"<p>1) Architecture diagram (capture \u2192 WARC \u2192 indexing \u2192 API/UI \u2192 exports). 2) Timeline figure for one URL (captures over time). 3) Coverage table (sources + first/last capture + snapshot counts). 4) Example change comparison screenshot (with \u201cdescriptive only\u201d labeling).</p>"},{"location":"operations/methods-note-outline/#appendix-links-public","title":"Appendix / links (public)","text":"<ul> <li>Project site: <code>https://healtharchive.ca</code></li> <li>Governance: <code>https://healtharchive.ca/governance</code></li> <li>Methods: <code>https://healtharchive.ca/methods</code></li> <li>Changes + compare: <code>https://healtharchive.ca/changes</code>, <code>https://healtharchive.ca/compare</code></li> <li>Exports: <code>https://healtharchive.ca/exports</code></li> <li>API manifest: <code>https://api.healtharchive.ca/api/exports</code></li> </ul>"},{"location":"operations/monitoring-and-alerting/","title":"Monitoring &amp; Alerting Strategy - Annual Crawl Campaign","text":"<p>Last Updated: 2026-01-27</p>"},{"location":"operations/monitoring-and-alerting/#overview","title":"Overview","text":"<p>This document defines the monitoring strategy for the HealthArchive annual crawl campaign. We use a combination of systemd timers, Python scripts, and Prometheus <code>node_exporter</code> textfile collectors to expose custom metrics about crawl health, restart stability, and progress.</p>"},{"location":"operations/monitoring-and-alerting/#metric-sources","title":"Metric Sources","text":"<p>Custom metrics are written to the node_exporter textfile collector directory:</p> <ul> <li><code>/var/lib/node_exporter/textfile_collector/</code></li> </ul> <p>Primary files (single-VPS annual campaign):</p> <ul> <li><code>healtharchive_crawl.prom</code></li> <li>Written by <code>scripts/vps-crawl-metrics-textfile.py</code></li> <li>Triggered every 1 minute by <code>healtharchive-crawl-metrics.timer</code></li> <li><code>healtharchive_storage_hotpath_auto_recover.prom</code></li> <li>Written by <code>scripts/vps-storage-hotpath-auto-recover.py</code></li> <li>Triggered every 1 minute by <code>healtharchive-storage-hotpath-auto-recover.timer</code> (sentinel-gated)</li> <li><code>healtharchive_worker_auto_start.prom</code></li> <li>Written by <code>scripts/vps-worker-auto-start.py</code></li> <li>Triggered every 2 minutes by <code>healtharchive-worker-auto-start.timer</code> (sentinel-gated)</li> </ul>"},{"location":"operations/monitoring-and-alerting/#key-metric-families","title":"Key Metric Families","text":"Metric Name Type Description <code>healtharchive_crawl_running_jobs</code> Gauge Count of currently active jobs in the DB. <code>healtharchive_worker_active</code> Gauge 1 = worker systemd unit is active. <code>healtharchive_jobs_pending_crawl</code> Gauge Count of jobs in <code>status in (queued, retryable)</code>. <code>healtharchive_jobs_infra_error_recent_total{minutes=\"10\"}</code> Gauge Count of jobs recently failing due to infra errors (windowed). <code>healtharchive_worker_should_be_running</code> Gauge 1 = pending crawl jobs exist and Storage Box mount is readable. <code>healtharchive_crawl_running_job_state_file_ok</code> Gauge 1 = <code>.archive_state.json</code> is readable and valid. 0 = Probe failed (SSHFS/Permissions issue). <code>healtharchive_crawl_running_job_container_restarts_done</code> Gauge Cumulative count of Zimit container restarts for the current job. <code>healtharchive_crawl_running_job_last_progress_age_seconds</code> Gauge Time since the last \"pages crawled\" increment in the logs. <code>healtharchive_crawl_running_job_stalled</code> Gauge 1 = Progress stalled &gt; 1 hour. <code>healtharchive_crawl_running_job_output_dir_ok</code> Gauge 1 = Output directory is accessible. <code>healtharchive_crawl_running_job_log_probe_ok</code> Gauge 1 = Combined log file is readable. <code>healtharchive_crawl_running_job_crawl_rate_ppm</code> Gauge Pages per minute crawl rate (from state file). <code>healtharchive_crawl_running_job_progress_known</code> Gauge 1 = Progress metrics available from state file. <code>healtharchive_crawl_metrics_timestamp_seconds</code> Gauge Unix timestamp when metrics were last written. <code>healtharchive_jobs_infra_error_recent_total{window=\"10m\"}</code> Gauge Count of jobs with infra errors in rolling window."},{"location":"operations/monitoring-and-alerting/#alerting-thresholds","title":"Alerting Thresholds","text":"<p>Alerts are defined in:</p> <ul> <li><code>ops/observability/alerting/healtharchive-alerts.yml</code></li> </ul>"},{"location":"operations/monitoring-and-alerting/#1-worker-availability-high-signal","title":"1) Worker availability (high-signal)","text":"<p>Alert: <code>HealthArchiveWorkerDownWhileJobsPending</code></p> <ul> <li>Threshold: <code>healtharchive_worker_should_be_running == 1 and healtharchive_worker_active == 0</code> for 10m.</li> <li>Meaning: There is pending crawl work and storage appears usable, but the worker service is down.</li> <li>Action: Check <code>healtharchive-worker.service</code> logs and Storage Box hot-path health. If automation is enabled, check <code>healtharchive-storage-hotpath-auto-recover.timer</code> + state.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#2-sshfsmount-stability","title":"2) SSHFS/Mount Stability","text":"<p>Alert: <code>HealthArchiveCrawlOutputDirUnreadable</code> (and related probe alerts)</p> <ul> <li>Threshold: <code>healtharchive_crawl_running_job_output_dir_ok == 0</code> for 2m.</li> <li>Meaning: A running crawl job cannot access its output directory. Errno 107 typically means a stale SSHFS/FUSE mount.</li> <li>Action: Follow the Storage Box stale mount recovery playbook and/or ensure hot-path auto-recover is enabled and succeeding.</li> </ul> <p>Alert: <code>HealthArchiveStorageHotpathStaleUnrecovered</code></p> <ul> <li>Threshold: <code>healtharchive_storage_hotpath_auto_recover_detected_targets &gt; 0</code> for 10m (when the automation is enabled).</li> <li>Meaning: Hot-path auto-recover still sees stale/unreadable paths after 10 minutes.</li> <li>Action: Inspect <code>/srv/healtharchive/ops/watchdog/storage-hotpath-auto-recover.json</code> and consider manual unmount + tiering re-apply.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#3-restart-stability","title":"3) Restart stability","text":"<p>Alert: <code>HealthArchiveCrawlContainerRestartsHigh</code></p> <ul> <li>Threshold: <code>healtharchive_crawl_running_job_container_restarts_done &gt;= 10</code> (for 15m).</li> <li>Meaning: The crawler is requiring many adaptive container restarts; this can be a normal resiliency mechanism, but sustained growth can indicate timeouts or I/O instability.</li> <li>Action: Review worker logs and combined logs around restarts; check for repeated timeouts on the same URL or storage errors.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#4-progress-stalls","title":"4) Progress Stalls","text":"<p>Alert: <code>HealthArchiveCrawlStalled</code></p> <ul> <li>Threshold: <code>healtharchive_crawl_running_job_stalled == 1</code> (for 30m).</li> <li>Meaning: The crawler is running but hasn't archived a new page in over an hour.</li> <li>Action: Check if the crawler is stuck on a massive PDF or looped trap.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#5-crawl-rate-throughput","title":"5) Crawl Rate (throughput)","text":"<p>Alert: <code>HealthArchiveCrawlRateSlow</code></p> <ul> <li>Threshold: <code>healtharchive_crawl_running_job_crawl_rate_ppm &lt; 5</code> (for 30m, when progress is known).</li> <li>Meaning: The crawler is running but archiving fewer than 5 pages per minute for an extended period.</li> <li>Action: Check for network issues, site rate limiting, or resource constraints. Consider adjusting worker count or Docker resource limits.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#6-infrastructure-errors","title":"6) Infrastructure Errors","text":"<p>Alert: <code>HealthArchiveInfraErrorsHigh</code></p> <ul> <li>Threshold: <code>healtharchive_jobs_infra_error_recent_total{window=\"10m\"} &gt;= 3</code> (for 5m).</li> <li>Meaning: Multiple jobs are failing due to infrastructure errors (errno 107 stale mount, permission denied, etc.) in a short window.</li> <li>Action: Check Storage Box mount health, run hot-path recovery, verify output directory permissions.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#7-metrics-freshness","title":"7) Metrics Freshness","text":"<p>Alert: <code>HealthArchiveCrawlMetricsStale</code></p> <ul> <li>Threshold: <code>(time() - healtharchive_crawl_metrics_timestamp_seconds) &gt; 600</code> (for 5m).</li> <li>Meaning: The crawl metrics textfile hasn't been updated in over 10 minutes.</li> <li>Action: Check if <code>healtharchive-crawl-metrics.timer</code> is running and <code>vps-crawl-metrics-textfile.py</code> is succeeding.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#indexing-monitoring","title":"Indexing Monitoring","text":"<p>Indexing runs after the crawl completes.</p> <ul> <li>Active Indexing: Check worker logs for <code>Indexing for job &lt;ID&gt; completed successfully</code>.</li> <li>Failure Detection: <code>healtharchive_job_crawl_status{status=\"completed\"}</code> AND <code>healtharchive_job_indexed_pages == 0</code> for &gt; 1 hour indicates a broken pipeline.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/","title":"Monitoring, uptime, and CI checklist","text":"<p>This file pulls together the ongoing operations aspects of the project:</p> <ul> <li>Uptime and health monitoring.</li> <li>Metrics and alerting.</li> <li>CI enforcement and branch protection.</li> </ul> <p>It is meant to complement:</p> <ul> <li><code>../deployment/hosting-and-live-server-to-dos.md</code></li> <li><code>../deployment/staging-rollout-checklist.md</code></li> <li><code>../deployment/production-rollout-checklist.md</code></li> <li><code>service-levels.md</code> \u2014 for SLO targets and commitments</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#0-implementation-steps-ci-external-monitoring","title":"0. Implementation steps (CI + external monitoring)","text":"<p>This section is a practical, sequential setup plan for enforcing CI and configuring external monitoring in the real world (GitHub + UptimeRobot, etc.).</p> <p>Important: most of this is not \u201ccode you deploy\u201d \u2014 it is configuration in:</p> <ul> <li>GitHub repository settings (branch protection)</li> <li>Your monitoring provider (UptimeRobot, Healthchecks, etc.)</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-0-baseline-audit-decisions-operator","title":"Step 0 \u2014 Baseline audit + decisions (operator)","text":"<p>Objective: avoid duplicate monitors, avoid alert noise, and avoid \u201cunknown settings drift\u201d.</p> <ol> <li>Inventory current external monitors (UptimeRobot, etc.):</li> <li>Monitor name</li> <li>URL</li> <li>Interval + timeout</li> <li>Alert contacts/routes</li> <li>Any keyword/body assertions</li> <li>Decide alert routing:</li> <li>Which alerts should page you vs. just email (recommended: only \u201csite down\u201d      pages; everything else emails).</li> <li>Decide the <code>main</code> branch policy:</li> <li>Mode A \u2014 Solo-fast (recommended for this project right now): direct pushes to <code>main</code>.<ul> <li>CI still runs on every push to <code>main</code>.</li> <li>Deploys are gated by \u201cgreen main\u201d + VPS verification steps (below).</li> </ul> </li> <li>Mode B \u2014 Multi-committer (defer until needed): PR-only merges into <code>main</code> with required      status checks + code owner review (track in <code>../roadmaps/roadmap.md</code>).</li> <li>Switch to Mode B when there is more than one regular committer, or when you want stricter      enforcement than \u201cgreen main + local hooks\u201d.</li> </ol> <p>Verification:</p> <ul> <li>You can point to a quick note (even in a personal doc) listing current   monitors + what each covers.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-1-verify-ci-runs-on-main-pushes-operator","title":"Step 1 \u2014 Verify CI runs on <code>main</code> pushes (operator)","text":"<p>Objective: ensure CI runs on pushes to <code>main</code> so you can treat \u201cgreen main\u201d as the deploy gate.</p> <ol> <li>Confirm GitHub Actions workflows are enabled:</li> <li>Repo \u2192 Actions \u2192 ensure workflows are enabled (not disabled by org/fork policy).</li> <li>Push a trivial commit to <code>main</code> (e.g. a doc tweak).</li> <li>Confirm the workflow runs and passes on that commit.</li> </ol> <p>Verification:</p> <ul> <li>GitHub Actions shows the backend CI workflow completing successfully on <code>main</code>.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#check-name-inventory-for-future-branch-protection","title":"Check name inventory (for future branch protection)","text":"<p>If/when you enable branch protection required checks, use the stable workflow/job check names shown in GitHub\u2019s UI. Avoid renaming workflow/job IDs after you start requiring them.</p> <p>As of 2026-01-17, the checks typically appear as:</p> <ul> <li>Backend repo:</li> <li><code>Backend CI / test</code> (recommended required PR gate)</li> <li><code>Backend CI / e2e-smoke</code> (post-merge / optional)</li> <li>Frontend repo: <code>Frontend CI / lint-and-test</code>, <code>Frontend CI / e2e-smoke</code></li> <li>Datasets repo: <code>Datasets CI / lint</code></li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-1b-end-to-end-smoke-checks-ci","title":"Step 1b \u2014 End-to-end smoke checks (CI)","text":"<p>Objective: catch regressions where the apps \u201cbuild\u201d but user\u2011critical paths fail at runtime.</p> <p>What the smoke does:</p> <ul> <li>Starts the backend locally (uvicorn) with a tiny seeded SQLite + WARC dataset.</li> <li>Builds and starts the frontend locally (<code>next start</code>) pointing at that backend.</li> <li>Runs <code>healtharchive-backend/scripts/verify_public_surface.py</code> against:</li> <li>Frontend: <code>/archive</code>, <code>/fr/archive</code>, <code>/snapshot/{id}</code>, <code>/fr/snapshot/{id}</code>, and other key pages</li> <li>API: <code>/api/health</code>, <code>/api/sources</code>, <code>/api/search</code>, <code>/api/snapshot/{id}</code>, <code>/api/usage</code>, <code>/api/exports</code>, <code>/api/changes</code></li> <li>Replay (pywb) is intentionally skipped in CI (<code>--skip-replay</code>).</li> <li>The verifier includes minimal \u201cnot just 200\u201d assertions:</li> <li><code>/archive</code> pages must include a stable Next.js marker (<code>/_next/static/</code>).</li> <li><code>/snapshot/{id}</code> pages must include the snapshot title returned by the API.</li> </ul> <p>Where it runs:</p> <ul> <li>Backend repo CI: <code>.github/workflows/backend-ci.yml</code> job <code>e2e-smoke</code></li> <li>Tests backend changes against latest frontend <code>main</code>.</li> <li>Runs on <code>main</code> pushes / manual runs (kept out of the PR gate to avoid blocking development on flaky cross-repo or environment issues).</li> <li>Frontend repo CI: https://github.com/jerdaw/healtharchive-frontend/blob/main/.github/workflows/frontend-ci.yml (job: <code>e2e-smoke</code>)</li> <li>Tests frontend changes against latest backend <code>main</code>.</li> <li>If cross-repo checkout fails (private repo), set a repo secret:</li> <li><code>HEALTHARCHIVE_CI_READ_TOKEN</code> (PAT with read access)</li> </ul> <p>Local reproduction (from the mono\u2011repo workspace where the repos are siblings):</p> <pre><code>cd healtharchive-frontend &amp;&amp; npm ci\ncd ../healtharchive-backend\nmake venv\n./scripts/ci-e2e-smoke.sh --frontend-dir ../healtharchive-frontend\n</code></pre> <p>On failure, the script prints the tail of the backend/frontend logs that it writes under:</p> <ul> <li><code>healtharchive-backend/.tmp/ci-e2e-smoke/</code></li> </ul> <p>CI also uploads the smoke logs as a GitHub Actions artifact on failure:</p> <ul> <li>Backend repo: <code>backend-e2e-smoke-artifacts</code></li> <li>Frontend repo: <code>frontend-e2e-smoke-artifacts</code></li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-2-solo-fast-deploy-gate-operator-recommended","title":"Step 2 \u2014 Solo-fast deploy gate (operator; recommended)","text":"<p>Objective: prevent broken deploys by only deploying when <code>main</code> is green.</p> <p>Workflow (recommended):</p> <ol> <li>Local guardrails (recommended while branch protections are relaxed):</li> <li>Run checks before you push:<ul> <li>From the mono-repo root: <code>make check</code></li> <li>Or per-repo:</li> <li><code>healtharchive-backend: make check</code></li> <li><code>healtharchive-frontend: npm run check</code></li> <li><code>healtharchive-datasets: make check</code></li> <li>Optional before deploys: <code>healtharchive-backend: make check-full</code></li> </ul> </li> <li>Optional but recommended: install pre-push hooks so you can't forget:<ul> <li>Backend: <code>scripts/install-pre-push-hook.sh</code> (set <code>HA_PRE_PUSH_FULL=1</code> for <code>make check-full</code>)</li> <li>Frontend: https://github.com/jerdaw/healtharchive-frontend/blob/main/scripts/install-pre-push-hook.sh</li> <li>Datasets: https://github.com/jerdaw/healtharchive-datasets/blob/main/scripts/install-pre-push-hook.sh</li> </ul> </li> <li>Push to <code>main</code>.</li> <li>Wait for GitHub Actions to go green on that commit.</li> <li>Deploy on the VPS:</li> <li>Recommended (one command): <code>./scripts/vps-deploy.sh --apply --baseline-mode live</code><ul> <li>Includes baseline drift + public-surface verify by default.</li> </ul> </li> <li>If you use a local alias like <code>dodeploy</code>, ensure you still run:<ul> <li><code>./scripts/check_baseline_drift.py --mode live</code></li> <li><code>./scripts/verify_public_surface.py</code></li> </ul> </li> </ol> <p>Verification:</p> <ul> <li>The VPS deploy completes and both verification scripts pass.</li> </ul> <p>Future (tighten later):</p> <ul> <li>When there are multiple committers or when you want stricter enforcement, switch to PR-only merges   and require the backend/frontend checks in branch protection (track in <code>../roadmaps/roadmap.md</code>).</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-3-external-uptime-monitoring-operator-uptimerobot-settings","title":"Step 3 \u2014 External uptime monitoring (operator; UptimeRobot settings)","text":"<p>Objective: catch real, user-visible failures with minimal noise.</p> <p>Recommended minimal monitor set (HTTP(s) checks):</p> <ol> <li>API health</li> <li>URL: <code>https://api.healtharchive.ca/api/health</code></li> <li>Expected: HTTP 200</li> <li>Interval: 1\u20135 minutes</li> <li>Note: backend supports <code>HEAD /api/health</code> for providers that default to <code>HEAD</code>.</li> <li>Frontend integration</li> <li>URL: <code>https://www.healtharchive.ca/archive</code></li> <li>Expected: HTTP 200</li> <li>Interval: 5 minutes</li> <li>Optional: keyword assertion (stable string that should always appear).</li> <li>Replay base URL (optional but recommended if you rely on replay)</li> <li>URL: <code>https://replay.healtharchive.ca/</code></li> <li>Expected: HTTP 200</li> <li>Interval: 5\u201310 minutes</li> </ol> <p>Optional, higher-signal replay monitoring (recommended later):</p> <ul> <li>After annual jobs exist and are replay-indexed, add 1\u20133 \u201cknown-good replay URL\u201d   monitors (one per source or one total) pointing at a stable capture inside a   <code>job-&lt;id&gt;</code> collection. Update them annually.</li> </ul> <p>Verification:</p> <ul> <li>Optional pre-flight from the VPS (or any machine with internet + <code>curl</code>):</li> <li><code>./scripts/smoke-external-monitors.sh</code></li> <li>All monitors show \u201cUp\u201d.</li> <li>Alerting routes work (optional test: intentionally break a monitor briefly).</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-4-timer-ran-monitoring-healthchecks-style-optional-but-recommended","title":"Step 4 \u2014 Timer ran monitoring (Healthchecks-style; optional but recommended)","text":"<p>Objective: get alerted if systemd timers stop running (even when the site is up).</p> <p>This is intentionally optional: you already have high-value uptime checks in Step 3, but \"timer ran\" alerts are useful for catching silent failures (timer disabled, unit failing, disk low refusal, etc.).</p> <p>Recommended checks to monitor:</p> <ul> <li>Baseline drift check (weekly):</li> <li><code>healtharchive-baseline-drift-check.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_BASELINE_DRIFT</code></li> <li>Public surface verification (daily):</li> <li><code>healtharchive-public-surface-verify.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_PUBLIC_VERIFY</code></li> <li>Replay reconcile (daily):</li> <li><code>healtharchive-replay-reconcile.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_REPLAY_RECONCILE</code></li> <li>Change tracking (daily):</li> <li><code>healtharchive-change-tracking.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_CHANGE_TRACKING</code></li> <li>Annual scheduler (yearly):</li> <li><code>healtharchive-schedule-annual.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_SCHEDULE_ANNUAL</code></li> <li>Annual search verify (daily, idempotent once per year):</li> <li><code>healtharchive-annual-search-verify.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_ANNUAL_SEARCH_VERIFY</code></li> <li>Coverage guardrails (daily):</li> <li><code>healtharchive-coverage-guardrails.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_COVERAGE_GUARDRAILS</code></li> <li>Replay smoke tests (daily):</li> <li><code>healtharchive-replay-smoke.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_REPLAY_SMOKE</code></li> <li>Cleanup automation (weekly):</li> <li><code>healtharchive-cleanup-automation.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION</code></li> </ul> <p>Note: avoid pinging high-frequency timers (e.g., crawl metrics, crawl auto-recover) to reduce noise.</p> <p>Implementation approach (VPS):</p> <ol> <li>Create a check in your Healthchecks provider for each timer you care about.</li> <li>Store ping URLs only on the VPS in a root-owned env file:</li> <li><code>/etc/healtharchive/healthchecks.env</code> (mode 0600, root:root)</li> <li>Note: this file may be shared across multiple automations; it is OK to keep both:<ul> <li>legacy <code>HC_*</code> variables (DB backup + disk check)</li> <li>newer <code>HEALTHARCHIVE_HC_PING_*</code> variables (systemd unit templates)</li> </ul> </li> <li>Ensure the installed systemd units source that env file:</li> <li><code>EnvironmentFile=-/etc/healtharchive/healthchecks.env</code></li> <li>Ensure the unit uses the wrapper so ping URLs never appear in unit files:</li> <li><code>/opt/healtharchive-backend/scripts/systemd-healthchecks-wrapper.sh</code></li> </ol> <p>Safety posture:</p> <ul> <li>Pinging is best-effort; ping failures do not fail jobs.</li> <li>Removing <code>/etc/healtharchive/healthchecks.env</code> disables pings immediately.</li> </ul> <p>Verification (VPS):</p> <ul> <li>Add a temporary ping URL for one service, then run:</li> <li><code>sudo systemctl start healtharchive-replay-reconcile-dry-run.service</code></li> <li>Confirm the check receives a ping.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-5-automated-post-campaign-search-verification-capture-optional","title":"Step 5 \u2014 Automated post-campaign search verification capture (optional)","text":"<p>Objective: once the annual campaign becomes search-ready, automatically capture golden-query <code>/api/search</code> JSON into a year-tagged directory for later diffing and audits.</p> <p>What gets captured (recommended minimal set):</p> <ul> <li><code>annual-status.json</code> and a human-readable <code>annual-status.txt</code></li> <li><code>meta.txt</code> (capture metadata)</li> <li><code>&lt;query&gt;.pages.json</code> + <code>&lt;query&gt;.snapshots.json</code> for your golden query list</li> </ul> <p>Implementation approach (VPS, systemd):</p> <ul> <li>This repo provides an optional daily timer that is idempotent:</li> <li>If the campaign is not ready, it exits 0 (no alert noise).</li> <li>If artifacts already exist for the current year/run-id, it exits 0.</li> </ul> <p>Install and enable:</p> <ol> <li>Copy templates onto the VPS (see <code>../deployment/systemd/README.md</code>):</li> <li><code>healtharchive-annual-search-verify.service</code></li> <li><code>healtharchive-annual-search-verify.timer</code></li> <li>Reload systemd:</li> <li><code>sudo systemctl daemon-reload</code></li> <li>Enable the timer:</li> <li><code>sudo systemctl enable --now healtharchive-annual-search-verify.timer</code></li> </ol> <p>Artifacts:</p> <ul> <li>Default location: <code>/srv/healtharchive/ops/search-eval/&lt;year&gt;/final/</code></li> <li>To force re-run for a year: delete that directory and re-run the service.</li> </ul> <p>Verification (VPS):</p> <ul> <li>Force-run once:</li> <li><code>sudo systemctl start healtharchive-annual-search-verify.service</code></li> <li>Confirm it either:</li> <li>exits 0 quickly (not ready), or</li> <li>creates artifacts under <code>/srv/healtharchive/ops/search-eval/&lt;year&gt;/final/</code>.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-6-optional-github-driven-deploys-cd-infrastructure-project","title":"Step 6 \u2014 Optional GitHub-driven deploys (CD) (infrastructure project)","text":"<p>Objective: reduce deploy mistakes without expanding the production attack surface.</p> <p>Recommended posture for this project (single VPS, no staging backend):</p> <ul> <li>Keep deployments manual on the VPS.</li> <li>Use the deploy helper script:</li> <li><code>scripts/vps-deploy.sh</code> (dry-run default; <code>--apply</code> to deploy)</li> </ul> <p>Rationale:</p> <ul> <li>Avoids storing production access secrets in GitHub.</li> <li>Avoids granting passwordless sudo/SSH access to GitHub Actions.</li> <li>Keeps the operational path \u201cboring\u201d and easy to reason about.</li> </ul> <p>Verification (VPS):</p> <ul> <li>Dry-run: <code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-deploy.sh</code></li> <li>Apply: <code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-deploy.sh --apply</code></li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#1-uptime-and-health-checks","title":"1. Uptime and health checks","text":""},{"location":"operations/monitoring-and-ci-checklist/#11-backend-health-endpoint","title":"1.1 Backend health endpoint","text":"<p>Primary health endpoint:</p> <ul> <li><code>GET https://api.healtharchive.ca/api/health</code></li> <li><code>HEAD https://api.healtharchive.ca/api/health</code> (supported; some uptime tools use <code>HEAD</code>)</li> </ul> <p>Some uptime providers issue <code>HEAD</code> requests by default. The backend supports <code>HEAD /api/health</code> so monitors may use either method.</p> <p>Expected behavior:</p> <ul> <li>HTTP 200</li> <li>JSON body like:</li> </ul> <pre><code>{\n  \"status\": \"ok\",\n  \"checks\": {\n    \"db\": \"ok\",\n    \"jobs\": {\n      \"queued\": 0,\n      \"indexed\": 5,\n      \"failed\": 0\n    },\n    \"snapshots\": {\n      \"total\": 123\n    }\n  }\n}\n</code></pre> <p>Suggested uptime monitor:</p> <ul> <li>Configure an external service (UptimeRobot, healthchecks.io, your cloud   provider) to poll:</li> <li><code>https://api.healtharchive.ca/api/health</code></li> <li>If you later add a separate staging API, also poll:</li> <li><code>https://api-staging.healtharchive.ca/api/health</code></li> <li>Alert on:</li> <li>5xx responses.</li> <li>Timeouts.</li> <li>Repeated failures over a short window.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#12-frontend-integration-check","title":"1.2 Frontend + integration check","text":"<p>To verify the frontend and backend integration:</p> <ul> <li><code>GET https://healtharchive.ca/archive</code></li> </ul> <p>Expected behavior:</p> <ul> <li>HTTP 200.</li> <li>Page renders with:</li> <li>Filters header showing <code>Filters (live API)</code> when backend is up.</li> <li>Real search results when snapshots exist.</li> </ul> <p>Suggested uptime monitor:</p> <ul> <li>Configure a separate check that:</li> <li>Downloads <code>https://healtharchive.ca/archive</code>.</li> <li>Optionally asserts presence of a known string in the body (e.g.     \u201cHealthArchive.ca\u201d or \u201cBrowse &amp; search demo snapshots\u201d).</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#13-replay-uptime-check-optional-but-recommended-if-replay-is-in-use","title":"1.3 Replay uptime check (optional but recommended if replay is in use)","text":"<p>To ensure replay is reachable:</p> <ul> <li><code>GET https://replay.healtharchive.ca/</code></li> </ul> <p>If you want a higher-signal check (recommended once you have stable annual jobs):</p> <ul> <li>Monitor a known-good replay URL inside a specific <code>job-&lt;id&gt;</code> collection:</li> <li><code>https://replay.healtharchive.ca/job-&lt;id&gt;/&lt;original_url&gt;</code></li> <li>Choose an original URL that is stable and low-cost to serve.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#2-metrics-and-alerting","title":"2. Metrics and alerting","text":""},{"location":"operations/monitoring-and-ci-checklist/#21-metrics-endpoint","title":"2.1 Metrics endpoint","text":"<p>Metrics are exposed at:</p> <ul> <li><code>GET https://api.healtharchive.ca/metrics</code></li> <li>If you later add a separate staging API:</li> <li><code>GET https://api-staging.healtharchive.ca/metrics</code></li> </ul> <p>This endpoint is protected by <code>HEALTHARCHIVE_ADMIN_TOKEN</code>. In Prometheus or a similar system, you will typically:</p> <ul> <li>Store the token in a secure place (e.g. Prometheus config / secret).</li> <li>Pass it via <code>Authorization: Bearer &lt;token&gt;</code> or <code>X-Admin-Token</code> header.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#22-key-metrics","title":"2.2 Key metrics","text":"<p>The metrics endpoint exposes, among others:</p> <ul> <li>Job counts:</li> </ul> <pre><code>healtharchive_jobs_total{status=\"queued\"} 1\nhealtharchive_jobs_total{status=\"indexed\"} 5\nhealtharchive_jobs_total{status=\"failed\"} 0\n</code></pre> <ul> <li>Cleanup status:</li> </ul> <pre><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"none\"} 10\nhealtharchive_jobs_cleanup_status_total{cleanup_status=\"temp_cleaned\"} 3\n</code></pre> <ul> <li>Snapshot counts:</li> </ul> <pre><code>healtharchive_snapshots_total 123\nhealtharchive_snapshots_total{source=\"hc\"} 80\nhealtharchive_snapshots_total{source=\"phac\"} 43\n</code></pre> <ul> <li>Page\u2011level crawl metrics:</li> </ul> <pre><code>healtharchive_jobs_pages_crawled_total 45678\nhealtharchive_jobs_pages_crawled_total{source=\"hc\"} 30000\nhealtharchive_jobs_pages_failed_total 120\nhealtharchive_jobs_pages_failed_total{source=\"hc\"} 30\n</code></pre> <ul> <li>Search metrics (per-process; reset on restart):</li> </ul> <pre><code>healtharchive_search_requests_total 123\nhealtharchive_search_errors_total 0\nhealtharchive_search_duration_seconds_bucket{le=\"0.3\"} 100\nhealtharchive_search_mode_total{mode=\"relevance_fts\"} 80\nhealtharchive_search_mode_total{mode=\"relevance_fallback\"} 25\nhealtharchive_search_mode_total{mode=\"relevance_fuzzy\"} 5\nhealtharchive_search_mode_total{mode=\"boolean\"} 2\nhealtharchive_search_mode_total{mode=\"url\"} 3\nhealtharchive_search_mode_total{mode=\"newest\"} 8\n</code></pre>"},{"location":"operations/monitoring-and-ci-checklist/#23-example-alert-ideas-prometheusstyle","title":"2.3 Example alert ideas (Prometheus\u2011style)","text":"<p>These are examples, not full rules, but can guide what you set up:</p> <ul> <li> <p>Crawl state file unhealthy:</p> </li> <li> <p>Alert if <code>healtharchive_crawl_running_job_state_file_ok==1</code> but     <code>healtharchive_crawl_running_job_state_parse_ok==0</code> for &gt;10m.</p> </li> <li> <p>Crawl stalled:</p> </li> <li> <p>Alert if <code>healtharchive_crawl_running_job_stalled==1</code> for &gt;30m.</p> </li> <li> <p>Crawl completed but indexing not starting:</p> </li> <li> <p>Alert if <code>healtharchive_indexing_pending_job_max_age_seconds</code> exceeds your SLA (e.g., &gt;1h).</p> </li> <li> <p>High job failure rate:</p> </li> <li> <p>Alert if <code>healtharchive_jobs_total{status=\"failed\"}</code> jumps unexpectedly     over a sliding window.</p> </li> <li> <p>No new snapshots over time:</p> </li> <li> <p>Alert if <code>increase(healtharchive_snapshots_total[24h]) == 0</code> while jobs     are being created, indicating indexing or crawl issues.</p> </li> <li> <p>Cleanup not happening:</p> </li> <li> <p>Alert if <code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"none\"}</code>     grows without bound while <code>temp_cleaned</code> remains flat.</p> </li> </ul> <p>Tune these based on actual volumes and acceptable thresholds.</p>"},{"location":"operations/monitoring-and-ci-checklist/#3-ci-and-branch-protection","title":"3. CI and branch protection","text":""},{"location":"operations/monitoring-and-ci-checklist/#31-github-actions-workflows","title":"3.1 GitHub Actions workflows","text":"<p>Workflows live at:</p> <ul> <li>Backend: <code>.github/workflows/backend-ci.yml</code></li> <li>Frontend: https://github.com/jerdaw/healtharchive-frontend/blob/main/.github/workflows/frontend-ci.yml</li> </ul> <p>Each should:</p> <ul> <li>Backend:</li> <li>Run <code>make check</code>.</li> <li>Frontend:</li> <li>Install deps via <code>npm ci</code>.</li> <li>Run <code>npm run check</code>.</li> <li>Optionally run <code>npm audit --audit-level=high</code>.</li> </ul> <p>Checklist:</p> <ul> <li> Ensure workflows are enabled in GitHub:</li> <li>Open the repo on GitHub.</li> <li>Go to Actions.</li> <li>If you see \u201cWorkflows are disabled for this fork\u201d, click Enable.</li> <li> Verify that pushing to <code>main</code> or opening a PR triggers the workflows.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#32-branch-protection-on-main","title":"3.2 Branch protection on <code>main</code>","text":"<p>In each repo:</p> <ol> <li>Go to Settings \u2192 Branches \u2192 Branch protection rules.</li> <li>Add or edit a rule for the <code>main</code> branch:</li> <li>Enable Require a pull request before merging.</li> <li>Enable Require status checks to pass before merging.</li> <li>Select:<ul> <li>The backend CI workflow for the backend repo.</li> <li>The frontend CI workflow for the frontend repo.</li> </ul> </li> <li>Enable Include administrators (recommended best practice).</li> <li>Enable Require review from Code Owners (recommended; requires <code>.github/CODEOWNERS</code>).</li> </ol> <p>This ensures:</p> <ul> <li>No changes reach <code>main</code> without passing tests and linting.</li> <li>Every <code>main</code> deploy (to staging/production) is backed by green CI.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#4-periodic-operations-review","title":"4. Periodic operations review","text":"<p>On a regular cadence (e.g. monthly or quarterly), review:</p> <ul> <li>Uptime logs:</li> <li>Are there recurring outages at specific times?</li> <li>Metrics:</li> <li>Are job failures spiking?</li> <li>Are snapshots growing at the expected rate?</li> <li>Is cleanup keeping up with new jobs?</li> <li>CI:</li> <li>Are workflows still running on all relevant branches?</li> <li>Do new checks or tooling need to be added?</li> </ul> <p>Recording a short \u201cops state\u201d note alongside these reviews will make future debugging and capacity planning much easier.</p>"},{"location":"operations/observability-and-private-stats/","title":"Observability + private stats (internal contract)","text":"<p>This document defines the public vs private boundaries for HealthArchive observability and \u201cprivate stats\u201d, with a bias toward:</p> <ul> <li>low maintenance / low toil</li> <li>privacy-preserving measurement (aggregate-only; no identifiers)</li> <li>no new public attack surface</li> </ul> <p>This is an internal ops document. Keep it public-safe (no secrets).</p>"},{"location":"operations/observability-and-private-stats/#1-definitions-public-vs-private-surfaces","title":"1) Definitions: public vs private surfaces","text":""},{"location":"operations/observability-and-private-stats/#public-surfaces-intentionally-public","title":"Public surfaces (intentionally public)","text":"<ul> <li>Frontend pages like <code>/status</code> and <code>/impact</code> (public reporting).</li> <li>Public API routes under <code>/api/**</code> (search, sources, snapshots, public usage window).</li> </ul>"},{"location":"operations/observability-and-private-stats/#private-surfaces-operator-only","title":"Private surfaces (operator-only)","text":"<ul> <li>Observability stack UIs (Grafana; optionally Prometheus UI).</li> <li>Admin endpoints:</li> <li><code>/api/admin/**</code></li> <li><code>/metrics</code> (Prometheus-style metrics)</li> </ul> <p>Rule: public web UI must never call or depend on <code>/api/admin/**</code> or <code>/metrics</code>.</p>"},{"location":"operations/observability-and-private-stats/#2-private-access-model-default","title":"2) Private access model (default)","text":"<p>Default approach: tailnet-only access, using Tailscale.</p> <ul> <li>Preferred: access Grafana via an SSH port-forward over Tailscale (simple, private, no Tailscale HTTPS certs required).</li> <li>Optional: use <code>tailscale serve</code> (tailnet-only HTTPS) if you want a shareable URL and you are OK with the tailnet hostname appearing in public certificate logs.</li> <li>Keep Prometheus UI loopback-only unless operators explicitly need it.</li> </ul> <p>Non-goals:</p> <ul> <li>No new public DNS records for ops tools.</li> <li>No Caddy vhosts for ops tools.</li> <li>No new public firewall openings.</li> </ul> <p>If an operator needs access without Tailscale, treat that as a deliberate security change and document it as a separate decision.</p>"},{"location":"operations/observability-and-private-stats/#21-host-footprint-dirs-secrets","title":"2.1 Host footprint (dirs + secrets)","text":"<p>These paths are conventions for operators and automation; they do not imply anything is public.</p>"},{"location":"operations/observability-and-private-stats/#ops-directories-public-safe-by-policy","title":"Ops directories (public-safe by policy)","text":"<ul> <li><code>/srv/healtharchive/ops/observability/</code></li> <li><code>dashboards/</code> \u2014 exported dashboard JSON, provisioning files (no secrets)</li> <li><code>alerting/</code> \u2014 public-safe alert rule templates/notes (no secrets)</li> <li><code>notes/</code> \u2014 public-safe operational notes</li> </ul> <p>Low-maintenance default:</p> <ul> <li>Keep Prometheus/Grafana data in distro defaults (typically <code>/var/lib/prometheus</code> and   <code>/var/lib/grafana</code>) unless you have a strong reason to relocate.</li> </ul>"},{"location":"operations/observability-and-private-stats/#secrets-root-owned-never-under-srvhealtharchiveops","title":"Secrets (root-owned; never under <code>/srv/healtharchive/ops/</code>)","text":"<ul> <li><code>/etc/healtharchive/observability/prometheus_backend_admin_token</code></li> <li><code>/etc/healtharchive/observability/grafana_admin_password</code></li> <li><code>/etc/healtharchive/observability/postgres_grafana_password</code></li> <li><code>/etc/healtharchive/observability/postgres_exporter.env</code></li> <li><code>/etc/healtharchive/observability/postgres_exporter_password</code></li> <li><code>/etc/healtharchive/observability/alertmanager_webhook_url</code> (routes alerts to one operator channel)</li> <li><code>/etc/healtharchive/observability/pushover_app_token</code> (optional; for Pushover relay)</li> <li><code>/etc/healtharchive/observability/pushover_user_key</code> (optional; for Pushover relay)</li> </ul> <p>Bootstrap helper (VPS only):</p> <ul> <li><code>scripts/vps-bootstrap-observability-scaffold.sh</code></li> <li><code>scripts/vps-install-observability-exporters.sh</code></li> <li><code>scripts/vps-install-observability-prometheus.sh</code></li> <li><code>scripts/vps-install-observability-grafana.sh</code></li> <li><code>scripts/vps-enable-tailscale-serve-grafana.sh</code></li> <li><code>scripts/vps-install-observability-dashboards.sh</code></li> <li><code>scripts/vps-install-observability-alerting.sh</code></li> <li><code>scripts/vps-install-observability-pushover-relay.sh</code> (optional; Alertmanager -&gt; Pushover)</li> <li><code>scripts/vps-install-ops-admin-proxy.sh</code> (optional; browser-friendly /api/admin/** + /metrics via SSH tunnel)</li> <li><code>scripts/vps-verify-observability.sh</code> (read-only; quick \u201cis everything up?\u201d check)</li> </ul>"},{"location":"operations/observability-and-private-stats/#3-data-collection-contract-privacy-preserving","title":"3) Data collection contract (privacy-preserving)","text":""},{"location":"operations/observability-and-private-stats/#31-what-we-collect-allowed","title":"3.1 What we collect (allowed)","text":"<ul> <li>Usage metrics: daily aggregate counters only.</li> <li>Storage: <code>usage_metrics(metric_date, event, count)</code></li> <li>No IPs, no user IDs, no per-request identifiers.</li> <li>Ops/service metrics: operational counts and totals needed to keep the service healthy.</li> <li>Examples: job status counts, snapshot totals, storage totals, search error rate.</li> </ul> <p>See also: <code>data-handling-retention.md</code> (retention + PHI risk notes).</p>"},{"location":"operations/observability-and-private-stats/#32-what-we-do-not-collect-explicitly-disallowed","title":"3.2 What we do not collect (explicitly disallowed)","text":"<ul> <li>Query strings / \u201ctop search terms\u201d.</li> <li>IP addresses, user agents, referrers stored into analytics tables.</li> <li>Per-user or per-session identifiers.</li> <li>High-cardinality dimensions (per-URL/page/path tracking).</li> <li>Third-party browser analytics scripts (unless a separate, explicit privacy/security decision is made).</li> </ul>"},{"location":"operations/observability-and-private-stats/#33-public-vs-private-usage-reporting","title":"3.3 Public vs private usage reporting","text":"<ul> <li>Public reporting can show only a curated subset of aggregate usage metrics.</li> <li>Private dashboards (Grafana) may show a broader set of aggregate events from the DB,   as long as they remain aggregate-only and public-safe.</li> </ul> <p>If adding new usage events:</p> <ul> <li>Keep the event set small and stable.</li> <li>Update docs and tests to ensure private-only events do not accidentally appear in public reporting.</li> </ul>"},{"location":"operations/observability-and-private-stats/#4-observability-architecture-intended-shape","title":"4) Observability architecture (intended shape)","text":""},{"location":"operations/observability-and-private-stats/#41-prometheus-scraping","title":"4.1 Prometheus scraping","text":"<ul> <li>Prometheus scrapes backend <code>GET /metrics</code> via loopback and includes the admin token.</li> <li>Exporters (node/postgres) are loopback- or tailnet-only and scraped by Prometheus.</li> <li>Retention must be capped (time and/or size) so Prometheus cannot fill disk.</li> </ul>"},{"location":"operations/observability-and-private-stats/#42-grafana-dashboards-private-stats-page","title":"4.2 Grafana dashboards (\u201cprivate stats page\u201d)","text":"<p>Grafana is the operator-facing surface.</p> <p>Data sources:</p> <ul> <li>Prometheus (time-series).</li> <li>Postgres (tables and long-window aggregates), using a dedicated read-only DB role.</li> </ul> <p>Sensitive tables:</p> <ul> <li>Treat <code>issue_reports</code> as sensitive (may contain free text and emails).</li> <li>Prefer redacted views for Grafana (counts + metadata only), and keep full text for explicit operator workflows.</li> </ul>"},{"location":"operations/observability-and-private-stats/#5-operational-invariants-must-remain-true","title":"5) Operational invariants (must remain true)","text":"<ul> <li>In production/staging, admin token must be configured and admin/metrics endpoints must not be public.</li> <li>Secrets must not be written under <code>/srv/healtharchive/ops/</code> (ops artifacts are public-safe by policy).</li> <li>Anything that changes public vs private boundaries must be documented as a deliberate decision.</li> </ul>"},{"location":"operations/observability-and-private-stats/#6-related-docs-canonical-references","title":"6) Related docs (canonical references)","text":"<ul> <li>Monitoring + CI checklist: <code>monitoring-and-ci-checklist.md</code></li> <li>Data handling &amp; retention: <code>data-handling-retention.md</code></li> <li>Production runbook: <code>../deployment/production-single-vps.md</code></li> <li>Ops playbooks index: <code>playbooks/README.md</code></li> </ul>"},{"location":"operations/one-page-brief/","title":"HealthArchive.ca - One-page brief (pointer)","text":"<p>Canonical public copy lives in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/public/partner-kit/healtharchive-brief.md</li> <li>Live page: https://www.healtharchive.ca/brief</li> </ul> <p>If you need to update the brief, edit the frontend asset and deploy the frontend.</p>"},{"location":"operations/ops-cadence-checklist/","title":"Ops Cadence Checklist (internal)","text":"<p>Purpose: make routine operations repeatable and low-friction so the project can be maintained without heroics.</p> <p>This checklist is intentionally short. If a task feels too heavy to do regularly, it should be moved to a longer cadence or automated safely.</p>"},{"location":"operations/ops-cadence-checklist/#every-deploy-always","title":"Every deploy (always)","text":"<ul> <li>Treat green <code>main</code> as the deploy gate (run local checks, push, wait for CI).</li> <li>Deploy using the VPS helper (safe deploy + verification):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-deploy.sh --apply --baseline-mode live</code></li> <li>Verify observability is still healthy (internal; loopback-only):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-verify-observability.sh</code></li> <li>Update docs if reality changed</li> <li>If you had to do manual steps not captured in a runbook/playbook, update the canonical doc(s) so the next deploy is repeatable.</li> <li>If the deploy script fails, don\u2019t retry blindly:</li> <li>read the drift report / verifier output</li> <li>fix the underlying mismatch (policy vs reality)</li> </ul> <p>Related docs:</p> <ul> <li>Deploy runbook: <code>../deployment/production-single-vps.md</code></li> <li>Verification/monitoring: <code>monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"operations/ops-cadence-checklist/#weekly-1015-minutes","title":"Weekly (10\u201315 minutes)","text":"<ul> <li>Observability sanity check</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-verify-observability.sh</code></li> <li>Service health</li> <li><code>curl -sS http://127.0.0.1:8001/api/health; echo</code></li> <li><code>sudo systemctl status healtharchive-api healtharchive-worker --no-pager -l</code></li> <li>Disk usage trend</li> <li><code>df -h /</code></li> <li>If <code>/srv/healtharchive</code> exists: <code>du -sh /srv/healtharchive/* | sort -h | tail -n 5</code></li> <li>Recent errors</li> <li><code>sudo journalctl -u healtharchive-api -n 200 --no-pager</code></li> <li><code>sudo journalctl -u healtharchive-worker -n 200 --no-pager</code></li> <li>Change tracking timer (if enabled)</li> <li><code>systemctl list-timers | rg healtharchive-change-tracking || systemctl list-timers | grep healtharchive-change-tracking</code></li> </ul>"},{"location":"operations/ops-cadence-checklist/#ongoing-automation-maintenance","title":"Ongoing automation maintenance","text":"<ul> <li>Keep systemd unit templates installed/updated on the VPS after repo updates:</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; sudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker</code></li> <li>Treat sentinel files under <code>/etc/healtharchive/</code> as the explicit on/off controls for automation.</li> <li>If you enable Healthchecks pings, keep ping URLs only in the root-owned VPS env file:</li> <li><code>/etc/healtharchive/healthchecks.env</code> (never commit ping URLs)</li> <li>If you use Healthchecks pings, periodically audit for drift (missing or stale checks):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; sudo -u haadmin python3 ./scripts/verify_healthchecks_alignment.py</code></li> <li>If you enable optional automations (coverage guardrails, replay smoke, cleanup), confirm their timers + sentinels are intentional.</li> </ul> <p>See: <code>../deployment/systemd/README.md</code></p>"},{"location":"operations/ops-cadence-checklist/#monthly-3060-minutes","title":"Monthly (30\u201360 minutes)","text":"<ul> <li>Reliability review (can be folded into the impact report)</li> <li>Note any incidents, slowdowns, or crawl failures.</li> <li>Confirm <code>/status</code> and <code>/impact</code> look reasonable and are current.</li> <li>Changelog update</li> <li>Add a short entry in <code>/changelog</code> reflecting meaningful updates (process: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md).</li> <li>Docs drift skim (10 minutes)</li> <li>Skim the production runbook + any playbooks you used recently; fix drift you notice.</li> <li>Search quality spot-check (lightweight)</li> <li>Run a few common queries on <code>/archive</code> and ensure results look plausible.</li> <li>Automation sanity check</li> <li>Verify timers are enabled only where intended.</li> </ul>"},{"location":"operations/ops-cadence-checklist/#quarterly-12-hours","title":"Quarterly (1\u20132 hours)","text":"<ul> <li>Restore test</li> <li>Follow <code>restore-test-procedure.md</code> and record results using <code>restore-test-log-template.md</code>.</li> <li>Dataset release integrity</li> <li>Confirm a dataset release exists for the expected quarter/date.</li> <li>Verify checksums: <code>sha256sum -c SHA256SUMS</code> (see <code>dataset-release-runbook.md</code>).</li> <li>Docs maintenance</li> <li>Re-read <code>incidents/severity.md</code> + <code>playbooks/incident-response.md</code> and ensure they match current reality.</li> <li>Adoption signals entry (public-safe)</li> <li>Add a dated entry under <code>/srv/healtharchive/ops/adoption/</code> (links + aggregates only).</li> <li>Mentions log refresh (public-safe)</li> <li>Update <code>mentions-log.md</code> with new public links (permission-aware; link-only).</li> <li>Automation posture check</li> <li>On the VPS run: <code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/verify_ops_automation.sh</code></li> <li>Optional (diff-friendly): <code>./scripts/verify_ops_automation.sh --json | python3 -m json.tool</code></li> <li>Optional (JSON-only artifact): <code>./scripts/verify_ops_automation.sh --json-only &gt; /srv/healtharchive/ops/automation/posture.json</code></li> <li>Spot-check logs: <code>journalctl -u &lt;service&gt; -n 200</code></li> <li>Growth constraints review</li> <li>Revisit <code>growth-constraints.md</code> (storage, source caps, performance budgets).</li> <li>Adjust only if you can still support the new limits.</li> </ul>"},{"location":"operations/ops-cadence-checklist/#annual-before-jan-01-utc","title":"Annual (before Jan 01 UTC)","text":"<ul> <li>Annual edition readiness</li> <li>Review <code>annual-campaign.md</code> for scope changes.</li> <li>Ensure enough storage headroom for a full capture cycle.</li> <li>Run the crawl preflight audit:<ul> <li><code>cd /opt/healtharchive-backend &amp;&amp; YEAR=2026 &amp;&amp; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\"</code></li> </ul> </li> <li>Dry-run the scheduler if it is enabled:<ul> <li><code>sudo systemctl start healtharchive-schedule-annual-dry-run.service</code></li> <li><code>sudo journalctl -u healtharchive-schedule-annual-dry-run.service -n 200 --no-pager</code></li> </ul> </li> </ul>"},{"location":"operations/ops-cadence-checklist/#where-to-record-outcomes","title":"Where to record outcomes","text":"<ul> <li>Changelog: public-facing changes and policy updates.</li> <li>Impact report: monthly coverage + reliability + usage snapshot.</li> <li>Incident notes: for outages/degradations/manual interventions: <code>incidents/README.md</code>.</li> <li>Internal ops log: optional private notes (date + key checks + issues).</li> </ul>"},{"location":"operations/outreach-templates/","title":"Outreach Templates (draft, public-safe)","text":"<p>Purpose: Ready-to-send templates for partner outreach. Keep these generic and non-committal until you have permission to name partners publicly.</p> <p>Do NOT add private contact details here.</p>"},{"location":"operations/outreach-templates/#template-a-distribution-partner-libraryresources-page","title":"Template A: Distribution partner (library/resources page)","text":"<p>Subject: HealthArchive.ca (archive + change tracking) for your resources page</p> <p>Hello , <p>I maintain HealthArchive.ca, an independent archive of Canadian public health web pages. It preserves time-stamped snapshots and provides descriptive change tracking so researchers and educators can cite what guidance said at a specific time.</p> <p>It is not medical advice or current guidance; it is a provenance archive.</p> <p>Would you consider adding HealthArchive.ca to your digital scholarship or public health methods resources page? The easiest option is to link to our changes feed or digest:</p> <ul> <li>https://www.healtharchive.ca/changes</li> <li>https://www.healtharchive.ca/digest</li> </ul> <p>If helpful, I can send a short one-page brief and screenshots.</p> <p>Thank you for considering it,"},{"location":"operations/outreach-templates/#template-b-researchteaching-partner","title":"Template B: Research/teaching partner","text":"<p>Subject: Resource for reproducibility of public health guidance (HealthArchive.ca)</p> <p>Hello , <p>I run HealthArchive.ca, an independent archive of Canadian public health web pages with time-stamped snapshots and descriptive change tracking.</p> <p>This can support reproducibility work and teaching about how guidance evolves over time (without interpreting or endorsing content).</p> <p>Would it be useful for your group to include HealthArchive in a methods reading list, teaching module, or research workflow? I can share a one-page brief and suggested citation format.</p> <p>Thanks for your time,"},{"location":"operations/outreach-templates/#template-c-journalism-communication-partner","title":"Template C: Journalism / communication partner","text":"<p>Subject: Archive of Canadian public health guidance (time-stamped snapshots)</p> <p>Hello , <p>HealthArchive.ca is an independent archive of Canadian public health web pages. It preserves time-stamped snapshots and provides a changes feed so journalists can audit how wording evolves over time. It is not current guidance or medical advice.</p> <p>If this could be useful to your work, the digest and changes feed are here:</p> <ul> <li>https://www.healtharchive.ca/changes</li> <li>https://www.healtharchive.ca/digest</li> </ul> <p>I can share a brief overview and citation guidance if helpful.</p> <p>Best,"},{"location":"operations/outreach-templates/#follow-up-1-week","title":"Follow-up (1 week)","text":"<p>Subject: Re: HealthArchive.ca resource link</p> <p>Hello , <p>Just following up in case my note got buried. HealthArchive.ca is an archive of Canadian public health guidance with time-stamped snapshots and descriptive change tracking. It is not current guidance or medical advice.</p> <p>If it is useful, the quickest addition is a link to:</p> <ul> <li>https://www.healtharchive.ca/digest</li> </ul> <p>Thanks again for considering it.</p> <p>"},{"location":"operations/outreach-templates/#follow-up-2-weeks-final","title":"Follow-up (2 weeks, final)","text":"<p>Subject: Closing the loop on HealthArchive.ca</p> <p>Hello , <p>I will close the loop for now, but if HealthArchive.ca becomes useful in the future, I am happy to provide a short brief or citation guidance.</p> <p>Thank you,"},{"location":"operations/outreach-templates/#optional-onboarding-blurb-for-partners-to-paste","title":"Optional: Onboarding blurb (for partners to paste)","text":"<p>HealthArchive.ca is an independent, non-governmental archive of Canadian public health web pages. It preserves time-stamped snapshots and provides descriptive change tracking for reproducibility and auditability. It is not current guidance or medical advice. Always consult official sources for up-to-date guidance.</p>"},{"location":"operations/partner-kit/","title":"Partner Kit (internal ops guide)","text":"<p>Purpose: A lightweight, partner-ready kit that makes it easy to link to HealthArchive without implying endorsement or medical guidance.</p> <p>Do not include private emails or contact lists here.</p> <p>Canonical public assets (do not duplicate copy here):</p> <ul> <li>One-page brief (source file): https://github.com/jerdaw/healtharchive-frontend/blob/main/public/partner-kit/healtharchive-brief.md</li> <li>Citation handout (source file): https://github.com/jerdaw/healtharchive-frontend/blob/main/public/partner-kit/healtharchive-citation.md</li> <li>Live pages: <code>https://www.healtharchive.ca/brief</code> and <code>https://www.healtharchive.ca/cite</code></li> </ul> <p>If you need to update public copy, edit the frontend assets above and deploy the frontend.</p>"},{"location":"operations/partner-kit/#1-distribution-blurb-pasteable","title":"1) Distribution blurb (pasteable)","text":"<p>HealthArchive.ca is an independent, non-governmental archive of Canadian public health web pages. It provides time-stamped snapshots and descriptive change tracking so researchers, journalists, and educators can audit how guidance evolves over time. It is not current guidance or medical advice.</p> <p>Suggested links:</p> <ul> <li>Digest (RSS + overview): https://www.healtharchive.ca/digest</li> <li>Changes feed: https://www.healtharchive.ca/changes</li> <li>One-page brief: https://www.healtharchive.ca/brief</li> <li>Citation guidance: https://www.healtharchive.ca/cite</li> <li>Methods and scope: https://www.healtharchive.ca/methods</li> </ul>"},{"location":"operations/partner-kit/#2-screenshot-checklist-for-partner-kit","title":"2) Screenshot checklist (for partner kit)","text":"<p>Save files with consistent names so they can be attached to outreach emails.</p> <ul> <li>01-home.png (Home page with \"What this is/is not\" block)</li> <li>02-archive.png (/archive with search + filters)</li> <li>03-snapshot.png (snapshot metadata + report link)</li> <li>04-changes.png (/changes feed)</li> <li>05-compare.png (/compare?to=, shows disclaimer) <li>06-digest.png (/digest with RSS links)</li> <li>07-status.png (/status metrics)</li> <li>08-impact.png (/impact monthly report)</li>"},{"location":"operations/partner-kit/#3-rss-links-reference","title":"3) RSS links (reference)","text":"<p>Global RSS:</p> <ul> <li>https://api.healtharchive.ca/api/changes/rss</li> </ul> <p>Per-source RSS (replace  with code, e.g., hc, phac, cihr):</p> <ul> <li>https://api.healtharchive.ca/api/changes/rss?source=</li> </ul>"},{"location":"operations/partner-kit/#4-notes-for-partners","title":"4) Notes for partners","text":"<ul> <li>HealthArchive is an archival record, not a guidance provider.</li> <li>Please avoid phrasing that implies endorsement or official status.</li> <li>Preferred language: \"archive\", \"snapshots\", \"change tracking\",   \"auditability\", \"reproducibility\".</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/","title":"Replay + Preview Automation (Plan Only)","text":"<p>Status: design draft (v1 implemented: <code>ha-backend replay-reconcile</code>; automation timers remain optional).</p> <p>This document is a thorough, safety-first plan for automating the operational steps that make HealthArchive \u201cfeel like a real site archive\u201d:</p> <ul> <li>pywb replay is kept up-to-date for each indexed crawl job (\u201cedition\u201d).</li> <li>cached homepage preview images exist for the <code>/archive</code> source cards.</li> <li>drift is detected and repaired safely (or surfaced clearly when it can\u2019t be repaired).</li> </ul> <p>It intentionally contains no implementation code. The goal is to agree on the what/where/how/why, guardrails, and edge cases before we build anything.</p> <p>Related runbooks and context:</p> <ul> <li>Replay runbook: <code>docs/deployment/replay-service-pywb.md</code></li> <li>Production VPS runbook: <code>docs/deployment/production-single-vps.md</code></li> <li>Legacy imports: <code>docs/operations/legacy-crawl-imports.md</code></li> <li>Architecture overview: <code>docs/architecture.md</code></li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#0-what-done-means-for-automation-high-level","title":"0) What \u201cdone\u201d means for automation (high level)","text":"<p>Automation is considered \u201csuccessful\u201d when:</p> <ol> <li>For every <code>ArchiveJob</code> with <code>status=indexed</code>, replay is eventually available at <code>https://replay.healtharchive.ca/job-&lt;id&gt;/...</code> (or we can point to a specific reason why it isn\u2019t).</li> <li>For every source shown on <code>/archive</code>, a cached preview image is eventually available for the latest edition (or we can point to a specific reason why it isn\u2019t).</li> <li>The system is safe-by-default:</li> <li>read-only / dry-run modes exist,</li> <li>destructive operations are excluded (cleanup),</li> <li>retries are bounded and don\u2019t spam,</li> <li>concurrency is controlled,</li> <li>we can disable automation instantly without affecting core API availability.</li> </ol>"},{"location":"operations/replay-and-preview-automation-plan/#1-scope-what-we-will-and-will-not-automate-yet","title":"1) Scope: what we will and will not automate (yet)","text":""},{"location":"operations/replay-and-preview-automation-plan/#in-scope","title":"In scope","text":"<ul> <li>Replay indexing: making <code>job-&lt;id&gt;</code> collections replayable (symlinks + CDX index).</li> <li>Preview generation: producing cached preview images for the <code>/archive</code> source cards.</li> <li>Reconciliation: a periodic \u201crepair drift\u201d process that converges the system to correctness.</li> <li>Observability: basic monitoring/alerting for replay/previews.</li> <li>Guardrails: locks, refusal rules, throttling, idempotency, backoff.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#out-of-scope-for-now","title":"Out of scope (for now)","text":"<ul> <li>Crawl scheduling / job creation automation (e.g., monthly crawls).</li> <li>Any \u201cautomatic cleanup\u201d that deletes WARCs or temp dirs without a retention policy.</li> <li>CI/CD pipeline automation (PR checks, deployment pipelines) beyond documentation.</li> <li>Anything that requires printing secrets or env files in logs or scripts.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#2-terminology-shared-language","title":"2) Terminology (shared language)","text":"<ul> <li>Job: <code>ArchiveJob</code> row in the DB. An indexed job corresponds to captured WARCs + <code>Snapshot</code> rows.</li> <li>Edition: a user-facing term for a job\u2019s backup (one job \u2192 one edition).</li> <li>Collection: pywb collection name. We use <code>job-&lt;id&gt;</code> for a job\u2019s collection.</li> <li>Replay indexed: pywb has a CDX index for <code>job-&lt;id&gt;</code> and can serve captures from that job\u2019s WARCs.</li> <li>Preview: a cached image (PNG/JPEG/WebP) used in the <code>/archive</code> source cards.</li> <li>Drift: DB says job is indexed, but replay/preview state doesn\u2019t match (missing index, missing WARCs, stale preview, etc.).</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#3-current-reality-constraints-we-must-respect","title":"3) Current reality (constraints we must respect)","text":""},{"location":"operations/replay-and-preview-automation-plan/#replay-depends-on-warcs-staying-on-disk","title":"Replay depends on WARCs staying on disk","text":"<p>Replay reads from job WARCs on disk. If WARCs are deleted, replay will break even if the DB still has snapshots.</p> <ul> <li>Guardrail already exists: <code>ha-backend cleanup-job --mode temp</code> refuses unless <code>--force</code> when replay is enabled.</li> <li>Operational posture: treat WARC retention as \u201ccritical state\u201d until we design cold storage replay.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#pywb-deployment-and-permissions-matter","title":"pywb deployment and permissions matter","text":"<p>On the VPS (per <code>docs/deployment/replay-service-pywb.md</code>):</p> <ul> <li>pywb runs in Docker as container <code>healtharchive-replay</code></li> <li>exposed locally at <code>127.0.0.1:8090</code></li> <li>WARCs are mounted read-only at <code>/warcs</code> (host <code>/srv/healtharchive/jobs</code>)</li> <li>replay state is mounted read-write at <code>/webarchive</code> (host <code>/srv/healtharchive/replay</code>)</li> <li>container runs without Linux capabilities (<code>--cap-drop=ALL</code>) \u2192 file permissions must be correct; \u201croot in container\u201d can\u2019t bypass them.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#preview-files-are-served-by-the-backend-api","title":"Preview files are served by the backend API","text":"<p>The backend supports cached preview images via:</p> <ul> <li><code>GET /api/sources/{source_code}/preview?jobId=&lt;id&gt;</code></li> </ul> <p>Files are expected in:</p> <ul> <li><code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code></li> <li>naming convention: <code>source-&lt;code&gt;-job-&lt;jobId&gt;.webp|.jpg|.jpeg|.png</code></li> </ul> <p>The frontend expects <code>entryPreviewUrl</code> to be present in <code>GET /api/sources</code> once previews exist.</p>"},{"location":"operations/replay-and-preview-automation-plan/#4-non-negotiable-design-principles","title":"4) Non-negotiable design principles","text":""},{"location":"operations/replay-and-preview-automation-plan/#safety-first-defaults","title":"Safety-first defaults","text":"<ul> <li>All automation must support dry-run mode.</li> <li>All automation must support allowlists:</li> <li>allowlist by source code (<code>hc</code>, <code>cihr</code>, \u2026)</li> <li>allowlist by job id range or \u201cnewest N\u201d</li> <li>All automation must refuse to run when required dependencies aren\u2019t healthy (docker down, pywb missing, disk low, etc.).</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#idempotency","title":"Idempotency","text":"<p>Every action must be safe to re-run:</p> <ul> <li>\u201cMake replayable\u201d can run repeatedly; it should converge to correct symlinks + index.</li> <li>\u201cGenerate preview\u201d can run repeatedly; it should be atomic and overwrite safely.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#isolation-from-user-traffic","title":"Isolation from user traffic","text":"<p>Automation must never run on:</p> <ul> <li>an API request path,</li> <li>a frontend request path,</li> <li>or any flow that could block interactive user browsing.</li> </ul> <p>It must run as a background process (manual trigger, timer, or separate worker queue).</p>"},{"location":"operations/replay-and-preview-automation-plan/#observability","title":"Observability","text":"<p>Automation must produce:</p> <ul> <li>machine-readable status (\u201cOK / needs work / blocked\u201d) per job and per source,</li> <li>actionable error messages (without secrets),</li> <li>backoff to avoid repeated failures.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#5-automation-candidate-a-replay-indexing-pywb-collections-cdx","title":"5) Automation candidate A \u2014 Replay indexing (pywb collections + CDX)","text":""},{"location":"operations/replay-and-preview-automation-plan/#51-desired-end-state","title":"5.1 Desired end state","text":"<p>For each job <code>id</code> where <code>ArchiveJob.status == indexed</code>:</p> <ul> <li>A pywb collection exists: <code>/srv/healtharchive/replay/collections/job-&lt;id&gt;/...</code></li> <li>The collection contains symlinks in <code>archive/</code> pointing to <code>/warcs/...</code> WARC paths (container-visible).</li> <li><code>indexes/index.cdxj</code> exists and corresponds to the current WARC set.</li> <li>A basic replay check succeeds for the job\u2019s entry URL:</li> <li>timegate form: <code>https://replay.healtharchive.ca/job-&lt;id&gt;/&lt;original_url&gt;</code></li> <li>(optional) CDX query returns at least one record for the entry URL.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#52-where-this-automation-should-live-options","title":"5.2 Where this automation should live (options)","text":"<p>Option A1: Reconciler timer (recommended first)</p> <ul> <li>A periodic process that looks at \u201cdesired jobs\u201d vs \u201creplay-ready jobs\u201d and repairs drift.</li> </ul> <p>Why this is the best first automation:</p> <ul> <li>decoupled from crawl/indexing,</li> <li>can be disabled instantly,</li> <li>can backfill older jobs,</li> <li>naturally repairs operator mistakes (deleted collections/indexes).</li> </ul> <p>Option A2: Worker hook (later, only if needed)</p> <ul> <li>After indexing completes, automatically trigger replay indexing.</li> </ul> <p>Risk:</p> <ul> <li>couples two heavy operations (index + replay indexing),</li> <li>increases failure surface in the worker loop,</li> <li>needs robust retry/backoff to avoid wedging jobs.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#53-required-guardrails","title":"5.3 Required guardrails","text":"<p>Concurrency</p> <ul> <li>Global lock: only one replay indexing operation at a time.</li> <li>Per-job lock: prevent two reindex attempts for the same <code>job-&lt;id&gt;</code>.</li> </ul> <p>Implementation decision (when we code):</p> <ul> <li>Prefer <code>flock</code>-based lock files under <code>/srv/healtharchive/replay/.locks/</code>.</li> <li>simple, visible, and resilient across process crashes.</li> </ul> <p>Eligibility rules (must be true to proceed)</p> <ul> <li>Job exists and is <code>status=indexed</code>.</li> <li>WARC discovery finds &gt;= 1 <code>.warc.gz</code> file.</li> <li>pywb container is running (or is startable).</li> <li>The process has:</li> <li>write access to <code>/srv/healtharchive/replay/collections/\u2026</code></li> <li>permission to run <code>docker exec</code> for <code>wb-manager</code>.</li> </ul> <p>Refusal rules (stop early, report why)</p> <ul> <li>Disk below a configured threshold (to prevent filling the VPS root disk).</li> <li>WARCs are missing / unreadable (likely cleanup ran) \u2192 mark job \u201creplay blocked: missing data\u201d.</li> <li>pywb container exists but is unhealthy (restarts / crashes repeatedly).</li> </ul> <p>Resource control</p> <ul> <li>Cap \u201cjobs per run\u201d (e.g., 1\u20132 per run initially).</li> <li>Optionally run with <code>nice</code> and/or <code>ionice</code> if indexing impacts API latency.</li> </ul> <p>Failure handling</p> <ul> <li>Classify failures into a small set:</li> <li>\u201cblocked\u201d (needs human action: missing WARCs, permissions)</li> <li>\u201cretryable\u201d (transient: docker restart, pywb busy)</li> <li>\u201cinternal\u201d (bug: unexpected exception)</li> <li>Exponential backoff for retryable failures (and a ceiling).</li> <li>Suppress repeated identical errors from spamming journald.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#54-state-tracking-how-we-know-whats-done","title":"5.4 State tracking (how we know what\u2019s done)","text":"<p>We need to know:</p> <ul> <li>which jobs are already replay indexed,</li> <li>whether their replay index matches their current WARC list,</li> <li>and when we last attempted/failed.</li> </ul> <p>Two viable designs:</p> <p>A) Filesystem marker (minimal, first iteration)</p> <ul> <li>After successful replay indexing, write a small JSON marker file into the collection:</li> <li><code>collections/job-&lt;id&gt;/replay-index.meta.json</code></li> <li>includes:<ul> <li><code>jobId</code></li> <li><code>indexedAt</code></li> <li><code>warcCount</code></li> <li><code>warcListHash</code> (hash of sorted WARC paths)</li> <li><code>pywbVersion</code> (optional)</li> </ul> </li> </ul> <p>Pros:</p> <ul> <li>doesn\u2019t require DB migrations,</li> <li>works even if DB is temporarily unavailable (but replay indexing requires DB anyway).</li> </ul> <p>Cons:</p> <ul> <li>harder to report status through APIs/admin dashboards,</li> <li>harder to query across jobs.</li> </ul> <p>B) DB state (preferred once we implement automation seriously)</p> <ul> <li>Add DB fields or a dedicated table to track replay indexing state.</li> </ul> <p>Pros:</p> <ul> <li>easy observability (admin endpoints, metrics),</li> <li>easier to reconcile at scale,</li> <li>can store retry counts and next-attempt timestamps.</li> </ul> <p>Cons:</p> <ul> <li>requires migration and careful rollout.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#55-edge-cases-to-explicitly-handle","title":"5.5 Edge cases to explicitly handle","text":"<ul> <li>WARCs deleted after indexing: DB says \u201cindexed\u201d, replay 404s.</li> <li>Detect via WARC discovery read failures.</li> <li>Mark \u201cblocked: missing WARCs\u201d; do not retry aggressively.</li> <li>Permissions drift: pywb cannot read WARCs due to chmod/chown changes.</li> <li>Detect by trying to <code>stat</code>/open a sample WARC (host) or via pywb reindex failure.</li> <li>Mark \u201cblocked: permissions\u201d; provide a runbook to fix.</li> <li>Job re-imported / WARC set changes: new WARCs added or paths differ.</li> <li>Detect via <code>warcListHash</code> change; reindex.</li> <li>Disk pressure: CDX can be large.</li> <li>Refuse below threshold; alert.</li> <li>pywb container restart during reindex:</li> <li>Reindex is rerunnable; ensure partial index doesn\u2019t block future runs.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#6-automation-candidate-b-cached-source-preview-generation","title":"6) Automation candidate B \u2014 Cached source preview generation","text":""},{"location":"operations/replay-and-preview-automation-plan/#61-desired-end-state","title":"6.1 Desired end state","text":"<p>For each source code shown on <code>/archive</code>:</p> <ul> <li>For the \u201ccurrent edition\u201d job id (latest by capture date):</li> <li>a preview file exists in <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code> named:<ul> <li><code>source-&lt;code&gt;-job-&lt;jobId&gt;.webp</code> (preferred), or</li> <li>a supported fallback format.</li> </ul> </li> <li>The backend returns <code>entryPreviewUrl</code> in <code>GET /api/sources</code>.</li> <li>The frontend displays the image without embedding live iframes.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#62-where-this-automation-should-live-options","title":"6.2 Where this automation should live (options)","text":"<p>Option B1: On-demand operator command (recommended first)</p> <ul> <li>Run it manually after a job becomes replayable, or when you want to refresh thumbnails.</li> </ul> <p>Why:</p> <ul> <li>preview generation is inherently flaky (dynamic pages, timeouts),</li> <li>it\u2019s easy to overwhelm the VPS if automated too aggressively.</li> </ul> <p>Option B2: Scheduled refresh timer (later)</p> <ul> <li>Daily/weekly, only for \u201clatest edition per source\u201d.</li> </ul> <p>Guardrail:</p> <ul> <li>cap number of previews per run,</li> <li>run during off-peak hours.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#63-guardrails-required","title":"6.3 Guardrails required","text":"<ul> <li>Always generate using a replay URL with <code>#ha_nobanner=1</code> so the screenshot matches the underlying site.</li> <li>Strict timeouts + \u201ccontinue on failure\u201d.</li> <li>Atomic writes:</li> <li>write to <code>*.tmp</code> then <code>rename()</code> to final name.</li> <li>Validate output:</li> <li>file exists and size &gt; minimum threshold.</li> <li>Rate limiting:</li> <li>cap to N previews per run.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#64-edge-cases","title":"6.4 Edge cases","text":"<ul> <li>replay entry URL 404s (job not replay indexed yet, or missing WARCs).</li> <li>replay loads but page never settles (long-running scripts).</li> <li>some pages are heavy and render inconsistently; use fixed viewport and a small settle delay.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#7-automation-candidate-c-reconciliation-loop-converge-to-correctness","title":"7) Automation candidate C \u2014 Reconciliation loop (\u201cconverge to correctness\u201d)","text":"<p>This is the safest automation pattern: a background process that continuously closes the gap between \u201cdesired\u201d and \u201cactual\u201d state.</p>"},{"location":"operations/replay-and-preview-automation-plan/#71-inputs-and-outputs","title":"7.1 Inputs and outputs","text":"<p>Inputs</p> <ul> <li>DB jobs and snapshots (<code>ArchiveJob</code>, <code>Snapshot</code>, <code>Source</code>)</li> <li>filesystem state:</li> <li>pywb collections under <code>/srv/healtharchive/replay/collections</code></li> <li>preview files under <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code></li> </ul> <p>Outputs</p> <ul> <li>replay indexing performed for some jobs (via CLI)</li> <li>preview generation performed for some sources (optional, depending on enablement)</li> <li>status reporting (logs + metrics)</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#72-reconciler-modes-must-exist-when-implemented","title":"7.2 Reconciler modes (must exist when implemented)","text":"<ul> <li><code>dry-run</code>: compute and print planned actions only.</li> <li><code>apply</code>: perform actions.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#73-recommended-initial-algorithm-when-we-implement","title":"7.3 Recommended initial algorithm (when we implement)","text":"<ol> <li>Acquire a global lock (refuse if already running).</li> <li>Query for <code>ArchiveJob.status=indexed</code> ordered newest-first.</li> <li>For each job (up to a max-per-run):</li> <li>if job is not replay indexed (marker/state missing or warc hash changed):<ul> <li>run replay indexing step</li> </ul> </li> <li>For each source (optional, up to a max-per-run):</li> <li>determine latest edition job id (from <code>/api/sources/{code}/editions</code> or DB query)</li> <li>if preview missing for that job id:<ul> <li>generate preview</li> </ul> </li> <li>Emit a summary report.</li> </ol>"},{"location":"operations/replay-and-preview-automation-plan/#74-guardrails","title":"7.4 Guardrails","text":"<ul> <li>allowlist sources for early rollouts</li> <li>max jobs per run</li> <li>max previews per run</li> <li>backoff on failures</li> <li>refuse when disk low</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#8-automation-candidate-d-monitoring-alerting-replay-aware","title":"8) Automation candidate D \u2014 Monitoring + alerting (replay-aware)","text":"<p>Replay introduces new failure modes that standard API health checks won\u2019t catch.</p>"},{"location":"operations/replay-and-preview-automation-plan/#81-recommended-monitors","title":"8.1 Recommended monitors","text":"<p>External (cheap, stable):</p> <ul> <li><code>GET https://api.healtharchive.ca/api/health</code> (already)</li> <li><code>GET https://replay.healtharchive.ca/</code> (200)</li> <li><code>HEAD https://replay.healtharchive.ca/</code> (200)</li> <li>One \u201cknown good\u201d replay entry URL per major source (200):</li> <li><code>https://replay.healtharchive.ca/job-1/.../https://www.canada.ca/en/health-canada.html</code></li> </ul> <p>Internal (optional):</p> <ul> <li>systemd timer that checks:</li> <li>pywb container is running</li> <li>disk usage below a safe threshold</li> <li>replay CDX exists for newest job</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#82-alert-playbook-what-to-do-when-it-breaks","title":"8.2 Alert playbook (what to do when it breaks)","text":"<ul> <li>Replay origin down:</li> <li>check <code>healtharchive-replay.service</code> status/logs</li> <li>check docker health</li> <li>Replay 404 for a known entry URL:</li> <li>check that the job\u2019s WARCs still exist</li> <li>check that <code>replay-index-job</code> was run and index exists</li> <li>Preview missing:</li> <li>run preview generation manually for latest job</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#9-rollout-strategy-methodical-and-safe","title":"9) Rollout strategy (methodical and safe)","text":"<ol> <li>Agree on this document.</li> <li>Implement reconciler in <code>dry-run</code> mode only.</li> <li>Run it manually and review output.</li> <li>Enable <code>apply</code> mode for a single allowlisted source.</li> <li>Add backoff and failure classification.</li> <li>Only after it\u2019s stable:</li> <li>consider enabling for all sources,</li> <li>consider adding preview generation to the reconciler,</li> <li>(optionally) consider a worker hook if needed.</li> </ol>"},{"location":"operations/replay-and-preview-automation-plan/#10-do-not-automate-cleanup-until-retention-is-designed","title":"10) Do not automate cleanup until retention is designed","text":"<p>Cleanup is the highest-risk automation.</p> <p>Before we automate any cleanup, we need a separate retention design:</p> <ul> <li>Which jobs must remain replayable and for how long?</li> <li>Where do \u201ccold\u201d WARCs live (NAS/object storage)?</li> <li>How do we replay cold WARCs without copying huge data back to the VPS?</li> <li>Can we move WARCs out of temp dirs so \u201ccleanup temp state\u201d is safe?</li> </ul> <p>Until then:</p> <ul> <li>Keep cleanup manual and conservative.</li> <li>Rely on the existing CLI guardrail (<code>cleanup-job</code> refuses unless <code>--force</code> when replay is enabled).</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#appendix-a-manual-operator-playbook-current-safe","title":"Appendix A \u2014 Manual operator playbook (current, safe)","text":"<p>When a new job is indexed:</p> <ol> <li>Make it replayable:</li> <li><code>ha-backend replay-index-job --id &lt;id&gt;</code></li> <li>(Optional) Generate preview:</li> <li>produce <code>source-&lt;code&gt;-job-&lt;id&gt;.{webp,png,jpg}</code> in <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code></li> <li>Verify:</li> <li><code>/snapshot/&lt;id&gt;</code> and <code>/browse/&lt;id&gt;</code></li> <li><code>/archive</code> source cards show preview and deep browsing works</li> </ol>"},{"location":"operations/restore-test-procedure/","title":"Restore Test Procedure (quarterly)","text":"<p>Purpose: prove backups are usable by performing a clean restore and verifying core API behavior.</p> <p>This procedure is intentionally minimal and public-safe. It does not require production secrets to be stored in the repo.</p>"},{"location":"operations/restore-test-procedure/#preconditions","title":"Preconditions","text":"<ul> <li>A recent database dump exists (from the normal backup routine).</li> <li>A temporary restore target is available (local Postgres on the VPS or a separate staging host).</li> <li>You have enough disk space to restore the dump.</li> </ul>"},{"location":"operations/restore-test-procedure/#step-1-choose-a-restore-target","title":"Step 1 \u2014 Choose a restore target","text":"<p>Options (pick one):</p> <ul> <li>Local temporary database on the VPS (preferred for speed).</li> <li>Staging database on a separate host.</li> </ul> <p>Record the target in the restore test log.</p>"},{"location":"operations/restore-test-procedure/#step-2-restore-the-database-dump","title":"Step 2 \u2014 Restore the database dump","text":"<p>Follow your standard backup tool instructions. In production, backups are typically created with:</p> <ul> <li><code>pg_dump -Fc</code> (custom-format dump)</li> </ul> <p>Examples (adjust paths):</p> <pre><code># Example: restore into a temporary database named healtharchive_restore_test_YYYYMMDD\nDBNAME=\"healtharchive_restore_test_$(date -u +%Y%m%d)\"\n\n# Pick a backup file (example naming from the production runbook)\nBACKUP=\"/srv/healtharchive/backups/healtharchive_YYYY-MM-DDTHHMMSSZ.dump\"\n\nsudo -u postgres createdb -O healtharchive \"$DBNAME\"\n\n# If backups live under a directory the `postgres` user cannot traverse, copy it first:\nTMPDUMP=\"/var/tmp/${DBNAME}.dump\"\nsudo install -m 600 -o postgres -g postgres \"$BACKUP\" \"$TMPDUMP\"\n\n# Restore the custom-format dump:\nsudo -u postgres pg_restore --no-owner --role=healtharchive -d \"$DBNAME\" \"$TMPDUMP\"\n\nsudo rm -f \"$TMPDUMP\"\n</code></pre> <p>If your backup is plain SQL (not custom-format), you can restore with <code>psql -f</code>, but production defaults are custom-format.</p>"},{"location":"operations/restore-test-procedure/#step-3-point-the-backend-to-the-restored-db","title":"Step 3 \u2014 Point the backend to the restored DB","text":"<p>Run API checks against the restored DB by temporarily overriding <code>HEALTHARCHIVE_DATABASE_URL</code>. Example:</p> <pre><code>export HEALTHARCHIVE_DATABASE_URL=\"postgresql+psycopg://.../healtharchive_restore_test\"\n/opt/healtharchive-backend/.venv/bin/alembic current\n</code></pre> <p>This confirms the restored schema is usable.</p>"},{"location":"operations/restore-test-procedure/#step-4-run-minimal-verification-checks","title":"Step 4 \u2014 Run minimal verification checks","text":"<p>Run these against the restored DB:</p> <ul> <li><code>GET /api/health</code> (DB check must be <code>ok</code>)</li> <li><code>GET /api/stats</code> (counts should be non-zero)</li> <li><code>GET /api/sources</code> (sources list should load)</li> </ul> <p>If you need a quick CLI-only check, run:</p> <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend check-db\n</code></pre>"},{"location":"operations/restore-test-procedure/#step-5-record-results","title":"Step 5 \u2014 Record results","text":"<p>Use <code>restore-test-log-template.md</code> and record:</p> <ul> <li>date + operator,</li> <li>backup source used,</li> <li>restore target,</li> <li>pass/fail,</li> <li>any anomalies or follow-up actions.</li> </ul>"},{"location":"operations/restore-test-procedure/#step-6-clean-up","title":"Step 6 \u2014 Clean up","text":"<p>Remove the temporary database when done:</p> <pre><code>dropdb healtharchive_restore_test\n</code></pre> <p>If you used a staging host, remove any temporary credentials or files.</p>"},{"location":"operations/risk-register/","title":"Risk Register (internal)","text":"<p>Track the top operational risks and mitigations.</p> <ul> <li>Misinterpretation risk (archive mistaken for current guidance)</li> <li>Mitigation: strong disclaimers; never add \u201cinterpretation\u201d features; keep high-risk pages (<code>/browse</code>, <code>/snapshot</code>) explicit.</li> <li>PHI submission risk (issue reports)</li> <li>Mitigation: clear warnings; minimize storage; admin-only access; delete/redact if PHI appears.</li> <li>Proxy/CORS misuse risk</li> <li>Mitigation: keep the frontend same-origin report proxy narrow; do not turn it into a general proxy; keep backend CORS allowlist strict.</li> <li>Single-VPS availability risk</li> <li>Mitigation: Disaster Recovery Runbook (RTO/RPO); backups + restore tests; conservative automation caps; disk monitoring; clear rollback procedures.</li> <li>Export integrity / reproducibility risk</li> <li>Mitigation: checksums + manifest; stable ordering/pagination; version fields (<code>diff_version</code>, <code>normalization_version</code>); avoid rewriting releases.</li> </ul>"},{"location":"operations/search-golden-queries/","title":"Golden queries (search relevance)","text":"<p>This file is a living checklist of \u201cgolden queries\u201d used to evaluate whether search ranking changes improve relevance.</p> <p>It complements <code>search-quality.md</code>:</p> <ul> <li><code>search-quality.md</code> explains how to run an evaluation.</li> <li>This file defines what we check (queries + expected outcomes).</li> </ul>"},{"location":"operations/search-golden-queries/#1-how-to-use-this-file","title":"1) How to use this file","text":"<p>For each query below:</p> <ol> <li>Run the API captures (prefer <code>view=pages</code> for user-facing relevance).</li> <li>Compare the top ~10 results against the expectations.</li> <li>Update expectations when the archive\u2019s coverage changes (new sources/pages).</li> </ol> <p>Guiding principle:</p> <ul> <li>Broad queries should surface hub/overview pages near the top.</li> <li>Specific queries should surface the most directly matching documents.</li> </ul>"},{"location":"operations/search-golden-queries/#2-query-set-curated-no-query-logs-required","title":"2) Query set (curated, no query logs required)","text":"<p>We do not have query logs yet, so this list is a curated approximation of:</p> <ul> <li>Broad hub intent (1\u20132 terms).</li> <li>Common refinements (testing, vaccines, isolation).</li> <li>Specific named entities (NACI, antivirals).</li> <li>A few non-COVID queries (to avoid overfitting).</li> <li>A small French \u201csmoke test\u201d set for bilingual content.</li> </ul>"},{"location":"operations/search-golden-queries/#21-broad-head-queries-hub-intent","title":"2.1 Broad \u201chead\u201d queries (hub intent)","text":"<ul> <li><code>covid</code></li> <li><code>influenza</code></li> <li><code>mpox</code></li> <li><code>measles</code></li> <li><code>rsv</code></li> <li><code>food recall</code></li> <li><code>travel advisory</code></li> <li><code>mental health</code></li> <li><code>air quality</code></li> <li><code>wildfire smoke</code></li> <li><code>immunization</code></li> <li><code>vaccines</code></li> </ul>"},{"location":"operations/search-golden-queries/#22-medium-queries-mixed-intent","title":"2.2 Medium queries (mixed intent)","text":"<ul> <li><code>covid vaccine</code></li> <li><code>covid booster</code></li> <li><code>mask</code></li> <li><code>mask guidance</code></li> <li><code>rapid testing</code></li> <li><code>testing</code></li> <li><code>wastewater</code></li> <li><code>isolation</code></li> <li><code>quarantine</code></li> <li><code>symptoms</code></li> <li><code>treatment</code></li> <li><code>prevention risks</code></li> </ul>"},{"location":"operations/search-golden-queries/#23-specific-queries-precision-intent","title":"2.3 Specific queries (precision intent)","text":"<ul> <li><code>long covid</code></li> <li><code>post covid condition</code></li> <li><code>naci</code></li> <li><code>naci booster</code></li> <li><code>myocarditis pericarditis</code></li> <li><code>omicron ventilation</code></li> <li><code>ventilation filtration</code></li> <li><code>paxlovid</code></li> <li><code>nirmatrelvir</code></li> <li><code>remdesivir</code></li> <li><code>health infobase</code></li> </ul>"},{"location":"operations/search-golden-queries/#24-non-covid-queries-overfitting-guardrail","title":"2.4 Non-COVID queries (overfitting guardrail)","text":"<ul> <li><code>opioid overdose</code></li> <li><code>naloxone</code></li> <li><code>vaping</code></li> <li><code>cannabis</code></li> <li><code>antimicrobial resistance</code></li> <li><code>water advisory</code></li> </ul>"},{"location":"operations/search-golden-queries/#25-french-smoke-tests-bilingual-content-no-stemming","title":"2.5 French smoke tests (bilingual content; no stemming)","text":"<ul> <li><code>grippe</code></li> <li><code>variole simienne</code></li> <li><code>vaccin covid</code></li> <li><code>sante mentale</code></li> </ul>"},{"location":"operations/search-golden-queries/#3-expectations-fill-these-in-over-time","title":"3) Expectations (fill these in over time)","text":"<p>Use normalized URL groups where possible (what <code>view=pages</code> groups on). For each query, keep:</p> <ul> <li>\u201cExpected\u201d pages: should appear in top 10 (ideally top 3\u20135 for broad queries).</li> <li>\u201cAnti-results\u201d: should not appear in top 20 for broad queries.</li> </ul>"},{"location":"operations/search-golden-queries/#31-covid","title":"3.1 <code>covid</code>","text":"<p>Expected (top 10):</p> <ul> <li><code>https://www.canada.ca/en/public-health/services/diseases/coronavirus-disease-covid-19.html</code></li> <li><code>https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection.html</code></li> </ul> <p>Nice-to-have (top 20):</p> <ul> <li><code>https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection/prevention-risks.html</code></li> <li><code>https://travel.gc.ca/travel-covid</code></li> </ul> <p>Anti-results (avoid in top 20 for broad <code>covid</code> unless explicitly requested):</p> <ul> <li>Titles beginning with <code>Archived</code> (e.g., <code>Archived - ...</code>, <code>Archived 50: ...</code>)</li> <li>Narrow deep pages that win solely by repeating \u201cCOVID\u201d many times in title/snippet</li> </ul>"},{"location":"operations/search-golden-queries/#32-long-covid","title":"3.2 <code>long covid</code>","text":"<p>Expected (top 10):</p> <ul> <li><code>https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection/symptoms/post-covid-19-condition.html</code></li> <li><code>https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection/health-professionals/post-covid-19-condition.html</code></li> </ul> <p>Anti-results:</p> <ul> <li>Broad COVID hubs outranking long-COVID pages (unless query is just <code>covid</code>)</li> </ul>"},{"location":"operations/search-golden-queries/#33-other-queries","title":"3.3 Other queries","text":"<p>Fill these in as coverage grows and you learn what \u201cgood\u201d looks like:</p> <ul> <li><code>influenza</code>: expected hub pages, PHAC/HC overview pages</li> <li><code>mpox</code>: expected public health hub pages</li> <li><code>food recall</code>: expected recall hub pages (e.g., CFIA)</li> <li><code>travel advisory</code>: expected Travel.gc.ca hub pages</li> <li><code>mental health</code>: expected PHAC/HC hub pages</li> </ul>"},{"location":"operations/search-golden-queries/#4-capture-commands-recommended","title":"4) Capture commands (recommended)","text":"<p>Prefer capturing API responses (fast + deterministic).</p> <p>Production examples:</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=20&amp;sort=relevance&amp;view=pages\" | python3 -m json.tool\ncurl -s \"https://api.healtharchive.ca/api/search?q=long%20covid&amp;page=1&amp;pageSize=20&amp;sort=relevance&amp;view=pages\" | python3 -m json.tool\n</code></pre> <p>To force a specific ranking version (useful for comparisons):</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=20&amp;sort=relevance&amp;view=pages&amp;ranking=v2\" | python3 -m json.tool\n</code></pre> <p>Local examples (backend on <code>127.0.0.1:8001</code>):</p> <pre><code>curl -s \"http://127.0.0.1:8001/api/search?q=covid&amp;page=1&amp;pageSize=20&amp;sort=relevance&amp;view=pages\" | python3 -m json.tool\n</code></pre> <p>For a repeatable \u201cbefore/after\u201d capture directory, use the script:</p> <pre><code>./scripts/search-eval-capture.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval\n</code></pre> <p>To capture using ranking v2 explicitly:</p> <pre><code>./scripts/search-eval-capture.sh --ranking v2 --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval\n</code></pre> <p>To additionally generate corpus-derived queries from the configured database and merge them with the curated list:</p> <pre><code>./scripts/search-eval-capture.sh --generate-from-db --out-dir /tmp/ha-search-eval\n</code></pre> <p>Notes:</p> <ul> <li>Keep captures out of git unless you explicitly want them committed.</li> <li>Prefer <code>view=pages</code> for ranking evaluation; use <code>view=snapshots</code> to debug capture-level issues.</li> </ul>"},{"location":"operations/search-quality/","title":"Search quality &amp; relevance evaluation","text":"<p>This project intentionally runs on a single VPS with Postgres, and does not introduce a separate search service (Elasticsearch/Meilisearch/etc.) unless and until we outgrow Postgres FTS + light heuristics.</p> <p>This document is a lightweight, repeatable way to evaluate whether search results \u201cfeel better\u201d after changes.</p>"},{"location":"operations/search-quality/#1-goals-what-better-means","title":"1) Goals (what \u201cbetter\u201d means)","text":"<p>For broad, common queries (e.g. <code>covid</code>):</p> <ul> <li>No obvious error/garbage captures in the top results (404/5xx \u201cNot Found\u201d pages,   missing assets, etc.).</li> <li>Hub/overview pages rise near the top (titles that clearly match the query).</li> <li>Snippets look human-readable (not \u201cSkip to main content \u2026 Search \u2026 Menu \u2026\u201d).</li> <li>API response time stays reasonable as the dataset grows.</li> </ul>"},{"location":"operations/search-quality/#2-golden-queries-starter-list","title":"2) Golden queries (starter list)","text":"<p>Start with ~10\u201330 queries and expand over time:</p> <ul> <li><code>covid</code></li> <li><code>covid vaccine</code></li> <li><code>long covid</code></li> <li><code>mask guidance</code></li> <li><code>rapid testing</code></li> <li><code>influenza</code></li> <li><code>mpox</code></li> <li><code>food recall</code></li> <li><code>travel advisory</code></li> <li><code>mental health</code></li> </ul> <p>For each query, define:</p> <ul> <li>2\u20135 \u201cexpected\u201d page titles/URLs that should appear in the top 10.</li> <li>2\u20135 \u201canti-results\u201d you never want in the top 20 (assets, obvious error pages).</li> </ul> <p>Keep the list as a simple note, or add a small markdown checklist in this repo if you want it versioned.</p>"},{"location":"operations/search-quality/#3-how-to-run-a-quick-evaluation-local-or-prod","title":"3) How to run a quick evaluation (local or prod)","text":""},{"location":"operations/search-quality/#31-use-the-api-directly-fastest","title":"3.1 Use the API directly (fastest)","text":"<p>Local dev (backend running on <code>http://127.0.0.1:8001</code>):</p> <pre><code>curl -s \"http://127.0.0.1:8001/api/search?q=covid&amp;page=1&amp;pageSize=10&amp;sort=relevance\" | python3 -m json.tool\n</code></pre> <p>Production:</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=10&amp;sort=relevance\" | python3 -m json.tool\n</code></pre> <p>To de-duplicate repeated captures of the same URL (show latest snapshot per page):</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=10&amp;sort=relevance&amp;view=pages\" | python3 -m json.tool\n</code></pre> <p>To include non\u20112xx captures for research/debugging:</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=10&amp;sort=relevance&amp;includeNon2xx=true\" | python3 -m json.tool\n</code></pre> <p>To inspect why a result ranks where it does (admin-only score breakdown):</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/admin/search-debug?q=covid&amp;view=pages&amp;sort=relevance&amp;ranking=v2&amp;pageSize=10\" \\\n  -H \"X-Admin-Token: ${HEALTHARCHIVE_ADMIN_TOKEN}\" \\\n  | python3 -m json.tool\n</code></pre>"},{"location":"operations/search-quality/#32-capture-beforeafter-snapshots-recommended","title":"3.2 Capture \u201cbefore/after\u201d snapshots (recommended)","text":"<p>For a small set of key queries (e.g. <code>covid</code>, <code>mpox</code>, <code>food recall</code>), capture page 1 JSON to files so you can compare later:</p> <pre><code>mkdir -p /tmp/ha-search-eval\ncurl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=10&amp;sort=relevance\" \\\n  &gt; /tmp/ha-search-eval/covid.after.json\n</code></pre> <p>Keep these captures out of git unless you explicitly want them committed.</p> <p>For repeatable, multi-query captures, use the helper scripts in <code>scripts/</code>:</p> <pre><code>./scripts/search-eval-capture.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval --page-size 20 --ranking v1\n./scripts/search-eval-capture.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval --page-size 20 --ranking v2\npython ./scripts/search-eval-diff.py --a /tmp/ha-search-eval/&lt;TS_A&gt; --b /tmp/ha-search-eval/&lt;TS_B&gt; --top 20\n</code></pre> <p>To capture v1 + v2 and generate a diff report in one command:</p> <pre><code>./scripts/search-eval-run.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval\n</code></pre> <p>On the production VPS, prefer a persistent output directory:</p> <pre><code>./scripts/search-eval-run.sh --base-url https://api.healtharchive.ca --out-dir /srv/healtharchive/ops/search-eval\n</code></pre>"},{"location":"operations/search-quality/#4-minimal-passfail-checklist-for-releases","title":"4) Minimal pass/fail checklist for releases","text":"<p>For each key query:</p> <ul> <li> Top 10 contains at least a few clear title matches (\u201chub/overview\u201d pages).</li> <li> No obvious 404/asset/error pages in top 20 (unless <code>includeNon2xx=true</code>).</li> <li> Snippets look readable and are not mostly boilerplate.</li> <li> Pagination behaves correctly (<code>total</code> stable; out-of-range pages return empty).</li> </ul>"},{"location":"operations/search-quality/#5-operational-tools","title":"5) Operational tools","text":"<p>These commands are intended for production maintenance on the VPS (Postgres), but can also be used in local dev environments.</p>"},{"location":"operations/search-quality/#51-backfill-postgres-fts-vectors","title":"5.1 Backfill Postgres FTS vectors","text":"<p>After deploying a schema update that adds <code>snapshots.search_vector</code>, populate it for existing rows:</p> <pre><code>ha-backend backfill-search-vector\n</code></pre>"},{"location":"operations/search-quality/#511-enable-fuzzy-search-postgres-only","title":"5.1.1 Enable fuzzy search (Postgres only)","text":"<p>Fuzzy matching for misspellings relies on the <code>pg_trgm</code> extension and trigram GIN indexes (see Alembic migration <code>0007_pg_trgm_fuzzy_search</code>).</p> <p>Notes:</p> <ul> <li><code>CREATE EXTENSION</code> may require elevated DB privileges on some hosts.</li> <li>If <code>pg_trgm</code> is unavailable, search still works (FTS + substring fallback), but   the fuzzy similarity fallback is disabled.</li> <li>The fuzzy fallback is intentionally conservative for performance: it uses   pg_trgm word similarity (not whole-field similarity) and tuned thresholds to   avoid huge candidate sets on broad queries.</li> </ul>"},{"location":"operations/search-quality/#52-refresh-snippetstitles-from-warcs-in-place","title":"5.2 Refresh snippets/titles from WARCs (in place)","text":"<p>After improving HTML extraction logic, update snapshot metadata without re-indexing (IDs remain stable):</p> <pre><code>ha-backend refresh-snapshot-metadata --job-id &lt;JOB_ID&gt;\n</code></pre>"},{"location":"operations/search-quality/#521-backfill-normalized-url-groups-page-de-duplication","title":"5.2.1 Backfill normalized URL groups (page de-duplication)","text":"<p>If older snapshots are missing <code>Snapshot.normalized_url_group</code>, <code>view=pages</code> may show duplicate \u201cpages\u201d for the same URL (especially when query parameters vary). Backfill the column:</p> <pre><code>ha-backend backfill-normalized-url-groups\n</code></pre>"},{"location":"operations/search-quality/#522-snapshot-view-hide-same-day-content-duplicates-ui","title":"5.2.2 Snapshot view: hide same-day content duplicates (UI)","text":"<p>In <code>view=snapshots</code>, the API can hide same-day duplicates of the exact same URL when the content is identical (same <code>content_hash</code>), which helps reduce noise from repeated tracker / redirect captures while keeping the underlying data intact.</p> <ul> <li>Default: duplicates are hidden.</li> <li>To include them: <code>GET /api/search?...&amp;view=snapshots&amp;includeDuplicates=true</code></li> </ul> <p>Future (storage-only, must preserve trustworthiness): if we can prove the HTML payload is identical (and preserve provenance), consider pruning same-day duplicates from storage to save disk space. Track this work in:</p> <ul> <li><code>../roadmaps/roadmap.md</code></li> </ul>"},{"location":"operations/search-quality/#53-backfill-outlinks-authority-signals","title":"5.3 Backfill outlinks + authority signals","text":"<p>If you have applied the authority schema (tables <code>snapshot_outlinks</code> and <code>page_signals</code>), you can populate link edges for a job by re-reading its WARCs:</p> <pre><code>ha-backend backfill-outlinks --job-id &lt;JOB_ID&gt; --update-signals\n</code></pre> <p>To rebuild all link signals from the full outlink graph (includes <code>inlink_count</code>, <code>outlink_count</code>, and <code>pagerank</code> when present):</p> <pre><code>ha-backend recompute-page-signals\n</code></pre>"},{"location":"operations/service-levels/","title":"Service Levels","text":"<p>This document describes the service level objectives and commitments for HealthArchive.ca. These are targets, not contractual guarantees.</p> <p>Last Updated: 2026-01-18</p>"},{"location":"operations/service-levels/#scope-and-context","title":"Scope and Context","text":"<p>HealthArchive is a public-interest research archive operated as a best-effort service. These service levels reflect commitments appropriate for the project's resources and mission:</p> <ul> <li>Infrastructure: Single VPS (Hetzner cx33: 4 vCPU / 8GB RAM / 80GB SSD)</li> <li>Staffing: Solo operator, no 24/7 coverage</li> <li>Purpose: Public good research tool, not a commercial service</li> </ul> <p>All targets are measured and reviewed on a best-effort basis. Incidents outside business hours may see delayed response.</p>"},{"location":"operations/service-levels/#availability","title":"Availability","text":""},{"location":"operations/service-levels/#target","title":"Target","text":"<p>99.5% monthly availability</p> <p>This allows for approximately 3.6 hours of downtime per month, which is realistic for: - Single-server architecture (no redundancy) - Manual maintenance operations - Solo operator response times</p>"},{"location":"operations/service-levels/#measurement","title":"Measurement","text":"<ul> <li>Primary endpoint: <code>GET /api/health</code> (https://api.healtharchive.ca/api/health)</li> <li>Monitoring method: External uptime monitoring (Healthchecks.io, UptimeRobot)</li> <li>Measurement window: Calendar month</li> <li>Exclusions: Planned maintenance with advance notice (see Maintenance Windows)</li> </ul>"},{"location":"operations/service-levels/#review","title":"Review","text":"<ul> <li>Semi-annual review of actuals vs target</li> <li>Adjust target if infrastructure or staffing changes significantly</li> </ul>"},{"location":"operations/service-levels/#response-times","title":"Response Times","text":"<p>Target response times for key API endpoints, measured server-side (excludes network latency):</p> Endpoint p50 Target p95 Target p99 Target Notes <code>GET /api/health</code> 50ms 100ms 200ms Minimal processing <code>GET /api/search</code> 500ms 2000ms 5000ms Complex queries, database-dependent <code>GET /api/sources</code> 100ms 300ms 500ms Lightweight, typically cached <code>GET /api/snapshot/{id}</code> 100ms 300ms 500ms Single record lookup <code>GET /api/changes</code> 200ms 500ms 1000ms Precomputed change feed"},{"location":"operations/service-levels/#degradation-criteria","title":"Degradation Criteria","text":"<p>The service is considered degraded when: - p95 latency exceeds target by 2\u00d7 for 5+ consecutive minutes - p99 latency exceeds target by 3\u00d7 for 5+ consecutive minutes - Any endpoint timeout rate exceeds 1%</p>"},{"location":"operations/service-levels/#exclusions","title":"Exclusions","text":"<p>These targets do not apply to: - Attack traffic or abusive request patterns - Bulk export operations (<code>/api/exports/*</code>) - Replay operations (separate service: <code>replay.healtharchive.ca</code>)</p>"},{"location":"operations/service-levels/#data-freshness","title":"Data Freshness","text":""},{"location":"operations/service-levels/#crawl-cadence","title":"Crawl Cadence","text":"<p>Primary sources (Health Canada, PHAC): Crawled at least annually, with ad-hoc updates as resources permit</p> <ul> <li>Major annual crawl campaign: typically January</li> <li>Ad-hoc crawls: triggered by significant health events or policy changes</li> <li>Schedule is best-effort and subject to operator availability</li> </ul>"},{"location":"operations/service-levels/#indexing-latency","title":"Indexing Latency","text":"<ul> <li>Crawl-to-indexed: Within 24 hours of crawl completion, subject to operator availability</li> <li>Indexed-to-searchable: Immediate (same database transaction)</li> </ul>"},{"location":"operations/service-levels/#change-tracking","title":"Change Tracking","text":"<ul> <li>Changes computed: Within 24 hours of new snapshots being indexed, subject to operator availability</li> <li>Change feed updated: On next <code>compute-changes</code> run (automated via systemd timer)</li> </ul>"},{"location":"operations/service-levels/#exceptions","title":"Exceptions","text":"<ul> <li>Manual crawls may have different timelines based on urgency</li> <li>Emergency updates (e.g., public health crises) prioritized on case-by-case basis</li> </ul>"},{"location":"operations/service-levels/#maintenance-windows","title":"Maintenance Windows","text":""},{"location":"operations/service-levels/#window-types","title":"Window Types","text":""},{"location":"operations/service-levels/#routine-maintenance","title":"Routine Maintenance","text":"<ul> <li>Examples: Security updates, dependency patches, configuration changes</li> <li>Advance Notice: 24 hours (via changelog)</li> <li>Maximum Duration: 30 minutes</li> <li>Typical Downtime: &lt; 15 minutes</li> </ul>"},{"location":"operations/service-levels/#major-maintenance","title":"Major Maintenance","text":"<ul> <li>Examples: Database migrations, infrastructure changes, new feature deployments</li> <li>Advance Notice: 72 hours (via changelog + announcement if user-facing)</li> <li>Maximum Duration: 4 hours</li> <li>Typical Downtime: 1-2 hours</li> </ul>"},{"location":"operations/service-levels/#emergency-maintenance","title":"Emergency Maintenance","text":"<ul> <li>Examples: Critical security patches, severe bug fixes</li> <li>Advance Notice: ASAP (post-hoc notification if required immediately)</li> <li>Duration: As needed</li> <li>Communication: Documented in changelog after completion</li> </ul>"},{"location":"operations/service-levels/#preferred-timing","title":"Preferred Timing","text":"<ul> <li>Weekdays, off-peak hours: Early morning (00:00-06:00 UTC) or late evening (22:00-24:00 UTC)</li> <li>Avoid: Business hours (14:00-22:00 UTC), weekends, holidays</li> </ul>"},{"location":"operations/service-levels/#post-maintenance-verification","title":"Post-Maintenance Verification","text":"<p>After all maintenance: - Health check validation (<code>/api/health</code>, <code>/archive</code>) - Smoke test (search query, snapshot retrieval) - External uptime monitor confirmation - Documented in changelog</p>"},{"location":"operations/service-levels/#communication-commitments","title":"Communication Commitments","text":""},{"location":"operations/service-levels/#channels","title":"Channels","text":"<p>Public Channels: - Changelog: https://healtharchive.ca/changelog - primary source for planned changes and incidents - Status: https://healtharchive.ca/status - service status overview - No dedicated status page (updates via changelog)</p> <p>Internal/Operator: - Incident notes (selected public-safe summaries published) - Operations logs (private)</p>"},{"location":"operations/service-levels/#incident-communication","title":"Incident Communication","text":"<p>Following the incident disclosure policy (Option B):</p> <p>Sev0/Sev1 (Service Down / Major Degradation): - Communicate within 48 hours of resolution, or as soon as practical - Public-safe summary published to changelog - Includes impact, timeline, resolution, and prevention measures</p> <p>Sev2/Sev3 (Minor Issues): - Include in regular changelog if user-facing - Internal documentation only if operator-only impact</p>"},{"location":"operations/service-levels/#changelog-cadence","title":"Changelog Cadence","text":"<ul> <li>Major changes: Immediate entry</li> <li>Minor changes: Batched weekly or monthly</li> <li>Security updates: Published as appropriate (may delay for responsible disclosure)</li> </ul>"},{"location":"operations/service-levels/#limitations","title":"Limitations","text":"<p>Communication timelines are best-effort and depend on: - Solo operator availability (no 24/7 coverage) - Incident severity and complexity - Need for coordination with external parties (e.g., infrastructure provider)</p>"},{"location":"operations/service-levels/#performance-baselines","title":"Performance Baselines","text":""},{"location":"operations/service-levels/#purpose","title":"Purpose","text":"<p>Baselines provide reference points for detecting performance degradation and validating improvements. They are not targets but rather observations of typical performance under normal conditions.</p>"},{"location":"operations/service-levels/#baseline-measurement-approach","title":"Baseline Measurement Approach","text":"<p>Baselines should be measured: - On production hardware (single VPS, current configuration) - Under typical load (not during crawls or heavy operations) - Multiple samples to account for variance - Documented with measurement date and conditions</p>"},{"location":"operations/service-levels/#current-baselines","title":"Current Baselines","text":"<p>[!NOTE] Initial baselines to be measured and documented during implementation. This section will be updated with actual measurements.</p> <p>API Response Times (server-side, measured via curl timing):</p> Endpoint Baseline p50 Baseline p95 Measured Date <code>GET /api/health</code> TBD TBD TBD <code>GET /api/search?q=covid</code> TBD TBD TBD <code>GET /api/sources</code> TBD TBD TBD <code>GET /api/snapshot/{id}</code> TBD TBD TBD <p>Operational Baselines:</p> Operation Baseline Throughput Measured Date Crawl (pages/hour) TBD TBD Indexing (records/second) TBD TBD Change computation (changes/minute) TBD TBD"},{"location":"operations/service-levels/#baseline-review","title":"Baseline Review","text":"<ul> <li>Semi-annual review: Compare current performance against baselines</li> <li>After major changes: Re-baseline if infrastructure or architecture changes</li> <li>Drift documentation: Document and investigate significant baseline drift (&gt;20%)</li> </ul>"},{"location":"operations/service-levels/#review-and-update-process","title":"Review and Update Process","text":""},{"location":"operations/service-levels/#review-cadence","title":"Review Cadence","text":"<p>Annual Review: - Assess targets vs actuals for the past year - Evaluate appropriateness of commitments - Update targets if resources or infrastructure change significantly</p> <p>Triggered Reviews: - After infrastructure changes (e.g., VPS upgrade, migration) - After staffing changes (e.g., additional operators) - After major architectural changes (e.g., HA implementation)</p>"},{"location":"operations/service-levels/#update-process","title":"Update Process","text":"<ol> <li>Propose changes: Document in roadmap or ADR</li> <li>Review against actuals: Compare proposed targets to historical data</li> <li>Update documentation: Revise this document</li> <li>Communicate changes: Announce via changelog if user-facing impact</li> <li>Update monitoring: Adjust alerts and dashboards to match new targets</li> </ol>"},{"location":"operations/service-levels/#document-maintenance","title":"Document Maintenance","text":"<ul> <li>Location: <code>docs/operations/service-levels.md</code></li> <li>Owner: Primary operator</li> <li>Format: Markdown, version-controlled in healtharchive-backend repo</li> <li>Navigation: Linked from operations index and docs site navigation</li> </ul>"},{"location":"operations/service-levels/#references","title":"References","text":"<ul> <li>Production Runbook - Infrastructure details and deployment procedures</li> <li>Incident Response Playbook - Incident classification and response procedures</li> <li>Monitoring Checklist - Monitoring setup and external checks</li> <li>Disaster Recovery - Recovery procedures and RTO/RPO targets</li> <li>Ops Cadence Checklist - Routine operational tasks</li> </ul>"},{"location":"operations/service-levels/#changelog","title":"Changelog","text":"Date Change Rationale 2026-01-18 Initial version Established baseline service level documentation"},{"location":"operations/verification-packet/","title":"Verification Packet (draft outline)","text":"<p>Purpose: A short, shareable summary for a verifier (librarian, researcher, or editor) to review and potentially confirm your role and the project's value.</p> <p>Do NOT include private emails or personal data. Keep it factual.</p>"},{"location":"operations/verification-packet/#1-project-overview-one-paragraph","title":"1) Project overview (one paragraph)","text":"<p>HealthArchive.ca is an independent, non-governmental archive of Canadian public health web pages. It preserves time-stamped snapshots and provides descriptive change tracking so researchers, journalists, and educators can audit how guidance evolves over time. It is not current guidance or medical advice.</p>"},{"location":"operations/verification-packet/#2-what-is-live-now-key-links","title":"2) What is live now (key links)","text":"<ul> <li>Home: https://www.healtharchive.ca/</li> <li>Archive search: https://www.healtharchive.ca/archive</li> <li>Snapshot viewer: https://www.healtharchive.ca/snapshot/ <li>Changes feed (edition-aware): https://www.healtharchive.ca/changes</li> <li>Compare view: https://www.healtharchive.ca/compare?to= <li>Digest + RSS: https://www.healtharchive.ca/digest</li> <li>Methods &amp; scope: https://www.healtharchive.ca/methods</li> <li>Governance: https://www.healtharchive.ca/governance</li> <li>Status/impact: https://www.healtharchive.ca/status and /impact</li>"},{"location":"operations/verification-packet/#3-what-i-built-operate-short-bullets","title":"3) What I built / operate (short bullets)","text":"<ul> <li>Archive capture pipeline (annual editions, scope rules).</li> <li>Snapshot indexing and search.</li> <li>Public changes feed and compare views (descriptive diffs).</li> <li>Governance, takedown, and issue-reporting process.</li> <li>Ongoing operations (monitoring, backups, scheduled change tracking).</li> </ul>"},{"location":"operations/verification-packet/#4-current-metrics-fill-from-status","title":"4) Current metrics (fill from /status)","text":"<ul> <li>Sources tracked: [N]</li> <li>Snapshots: [N]</li> <li>Pages: [N]</li> <li>Latest capture date (UTC): [YYYY-MM-DD]</li> </ul>"},{"location":"operations/verification-packet/#5-what-verification-would-mean-sample-wording","title":"5) What verification would mean (sample wording)","text":"<p>\"I can verify that  built and operates HealthArchive.ca, and that the project is used as a resource for reproducibility / auditability of Canadian public health guidance. I understand it is an archival tool, not medical advice.\""},{"location":"operations/verification-packet/#6-permission-request-explicit","title":"6) Permission request (explicit)","text":"<ul> <li>Are you willing to verify my role and the project's utility?</li> <li>May I list your name and title publicly as a verifier?</li> <li>Preferred wording (if any)?</li> </ul>"},{"location":"operations/verification-packet/#7-notes-for-the-verifier","title":"7) Notes for the verifier","text":"<ul> <li>The site is explicitly non-authoritative and not affiliated with any public   health agency.</li> <li>Change tracking is descriptive only; it does not interpret meaning.</li> </ul>"},{"location":"operations/incidents/","title":"Incident notes (internal)","text":"<p>This folder contains incident notes / lightweight postmortems for production and operations issues.</p> <p>Goals:</p> <ul> <li>Capture what happened (timeline + impact) while it\u2019s fresh.</li> <li>Record the root cause and recovery steps (so we can repeat them safely).</li> <li>Track \u201cstill to do\u201d work that reduces repeat incidents (docs, guardrails, automation).</li> </ul> <p>Related:</p> <ul> <li>Operator recovery steps: <code>../playbooks/incident-response.md</code></li> <li>Ops playbooks index: <code>../playbooks/README.md</code></li> <li>Severity rubric: <code>severity.md</code></li> </ul>"},{"location":"operations/incidents/#what-goes-here","title":"What goes here","text":"<p>Use an incident note when any of the following are true:</p> <ul> <li>Public site/API degraded or down.</li> <li>Crawl/indexing is stuck, repeatedly failing, or risking data integrity.</li> <li>Storage/mount issues (e.g. Errno 107) or \u201chot path\u201d problems.</li> <li>You had to do manual intervention beyond routine operations.</li> </ul> <p>Do not use incident notes for planned maintenance; record that in the changelog (and/or a runbook update) instead.</p>"},{"location":"operations/incidents/#naming","title":"Naming","text":"<p>One file per incident:</p> <ul> <li><code>YYYY-MM-DD-&lt;short-slug&gt;.md</code> (use UTC date of incident start).</li> <li>If multiple incidents share a date, add a suffix: <code>...-a</code>, <code>...-b</code>.</li> </ul> <p>Example:</p> <ul> <li><code>2026-01-09-annual-crawl-phac-output-dir-permission-denied.md</code></li> <li><code>2026-01-16-replay-smoke-503-and-warctieringfailed.md</code></li> </ul>"},{"location":"operations/incidents/#how-to-write-one","title":"How to write one","text":"<p>1) Copy the template: <code>../../_templates/incident-template.md</code> 2) Fill the top metadata and a short summary immediately. 3) Add a timeline (UTC) as you work. 4) After recovery, fill root cause + follow-ups. 5) Link any follow-up playbooks/runbooks/roadmaps you touched. 6) If this incident changes user expectations (outage/degradation, integrity risk, security posture, policy change), add a public-safe note in <code>/changelog</code> and/or <code>/status</code> (no sensitive details; changelog process: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md).</p> <p>The template includes an Action items (TODOs) section; use checkboxes so it\u2019s obvious what work remains.</p> <p>If the incident requires engineering work (automation, new scripts, behavior changes), capture it as a follow-up and create a focused implementation plan under <code>docs/roadmaps/</code> (then link it from the incident note).</p>"},{"location":"operations/incidents/#what-not-to-include","title":"What not to include","text":"<ul> <li>Secrets (tokens, passwords, Healthchecks URLs).</li> <li>Private emails or non-public IPs/hostnames.</li> <li>Full logs. Prefer:</li> <li>the exact log path(s),</li> <li>the most relevant ~20\u201350 lines, and</li> <li>one or two <code>vps-*.sh</code> snapshots.</li> </ul>"},{"location":"operations/incidents/#style","title":"Style","text":"<ul> <li>Keep it blameless: focus on systems, invariants, and guardrails (not individuals).</li> <li>Prefer concrete facts over speculation; if something is unknown, label it as such.</li> <li>Record commands that changed state (DB writes, mounts, restarts) and what they affected.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/","title":"Incident: Annual crawl \u2014 Storage hot-path sshfs mounts went stale (Errno 107) (2026-01-08)","text":"<p>Status: closed</p>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-01-08</li> <li>Severity (see <code>severity.md</code>): sev1</li> <li>Environment: production (single VPS)</li> <li>Primary area: storage</li> <li>Owner: (unassigned)</li> <li>Start (UTC): 2026-01-08T06:31:43Z (approx; first observed Errno 107 in worker logs)</li> <li>End (UTC): 2026-01-08T20:38:39Z (approx; crawler restarted and hot paths readable again)</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#summary","title":"Summary","text":"<p>Several Storage Box \u201chot path\u201d <code>sshfs</code> mountpoints under <code>/srv/healtharchive/jobs/**</code> became stale and started returning <code>OSError: [Errno 107] Transport endpoint is not connected</code>. This caused the worker to throw exceptions when reading/writing job output dirs, the crawl metrics textfile writer to fail repeatedly, and annual crawl jobs (HC/PHAC/CIHR) to fail/retry without making forward progress.</p> <p>Recovery required stopping the worker, lazily unmounting the stale hot-path mountpoints, re-applying tiering bind mounts, and marking affected jobs as <code>retryable</code> so they could safely restart. After recovery, the worker successfully restarted the HC crawl and resumed writing WARCs to the output directory.</p>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#impact","title":"Impact","text":"<ul> <li>User-facing impact: none directly observed, but annual campaign remained <code>Ready for search: NO</code> while jobs were blocked/failing.</li> <li>Internal impact (ops burden, automation failures, etc):</li> <li>Manual operator intervention required (mount recovery + job recovery).</li> <li><code>healtharchive-crawl-metrics.service</code> failed repeatedly (reduced visibility during the incident).</li> <li>Worker loop repeatedly hit <code>Errno 107</code> and could not safely proceed with affected jobs.</li> <li>Data impact:</li> <li>Data loss: unknown (no evidence of WARC deletion; risk was primarily loss of crawl continuity and partial/aborted crawl attempts).</li> <li>Data integrity risk: medium (stale mounts can interrupt writes and break assumptions about output dir readability; risk reduced after later WARC verification).</li> <li>Recovery completeness: complete for mount recovery; annual campaign completion remained in-progress.</li> <li>Duration: ~14 hours (approx; first Errno 107 observed in morning logs \u2192 successful crawl restart in the evening).</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#detection","title":"Detection","text":"<ul> <li>Operator status snapshot:</li> <li><code>./scripts/vps-crawl-status.sh --year 2026</code> showed <code>WARN job output dir not found/readable</code> and missing running-job log tails.</li> <li>Direct filesystem symptom:</li> <li><code>ls -la /srv/healtharchive/jobs/hc/</code> returned <code>Transport endpoint is not connected</code> and showed <code>d?????????</code> for the affected job dir.</li> <li>Monitoring symptom:</li> <li><code>systemctl status healtharchive-crawl-metrics.timer healtharchive-crawl-metrics.service</code> showed the metrics writer exiting non-zero.</li> <li><code>journalctl -u healtharchive-crawl-metrics.service</code> showed a traceback ending in <code>OSError: [Errno 107] Transport endpoint is not connected: '&lt;job output dir&gt;'</code>.</li> <li>Worker symptom:</li> <li><code>journalctl -u healtharchive-worker.service</code> showed <code>Unexpected error in worker iteration: [Errno 107] ...</code> while picking jobs 6/7/8.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#most-relevant-excerpts-redacted","title":"Most relevant excerpts (redacted)","text":"<p>Worker journal (error propagation into the worker loop):</p> <pre><code>Jan 08 06:31:43 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:31:43,663 [WARNING] healtharchive.worker: Crawl for job 6 failed (RC=1). Marking as retryable (retry_count=1).\nJan 08 06:31:43 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:31:43,675 [INFO] healtharchive.worker: Worker picked job 6 for source hc (Health Canada) with status retryable and retry_count 1\nJan 08 06:31:43 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:31:43,684 [ERROR] healtharchive.worker: Unexpected error in worker iteration: [Errno 107] Transport endpoint is not connected: '/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101'\nJan 08 06:32:13 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:32:13,694 [INFO] healtharchive.worker: Worker picked job 7 for source phac (Public Health Agency of Canada) with status queued and retry_count 0\nJan 08 06:32:13 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:32:13,702 [ERROR] healtharchive.worker: Unexpected error in worker iteration: [Errno 107] Transport endpoint is not connected: '/srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101'\nJan 08 06:32:43 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:32:43,711 [INFO] healtharchive.worker: Worker picked job 8 for source cihr (Canadian Institutes of Health Research) with status queued and retry_count 0\nJan 08 06:32:43 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:32:43,718 [ERROR] healtharchive.worker: Unexpected error in worker iteration: [Errno 107] Transport endpoint is not connected: '/srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101'\n</code></pre> <p>Crawl metrics writer failure (systemd service repeatedly failing due to <code>Errno 107</code> during output-dir probing):</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/healtharchive-backend/scripts/vps-crawl-metrics-textfile.py\", line 174, in main\n    log_path = _find_job_log(job)\n  File \"/opt/healtharchive-backend/scripts/vps-crawl-metrics-textfile.py\", line 33, in _find_latest_combined_log\n    if not output_dir.is_dir():\n  File \"/usr/lib/python3.12/pathlib.py\", line 842, in stat\n    return os.stat(self, follow_symlinks=follow_symlinks)\nOSError: [Errno 107] Transport endpoint is not connected: '/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101'\n</code></pre> <p>Filesystem symptom (stale FUSE mountpoint):</p> <pre><code>$ ls -la /srv/healtharchive/jobs/hc/\nls: cannot access '/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101': Transport endpoint is not connected\nd????????? ? ? ? ? ? 20260101T000502Z__hc-20260101\n</code></pre>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#decision-log-optional-but-recommended-for-sev0sev1","title":"Decision log (optional but recommended for sev0/sev1)","text":"<ul> <li>2026-01-08T20:17Z (approx) \u2014 Decision: stop <code>healtharchive-worker.service</code> before unmounting hot paths (why: avoid concurrent reads/writes against a stale FUSE mount; risks: temporarily halts all crawl work).</li> <li>2026-01-08T20:18Z (approx) \u2014 Decision: use <code>umount -l</code> (lazy) for stale mountpoints (why: avoid blocking on FUSE teardown; risks: processes holding FDs continue referencing the old mount until released).</li> <li>2026-01-08T20:22Z (approx) \u2014 Decision: mark jobs as <code>retryable</code> (and later <code>retry-job</code>) after storage recovery (why: allow the worker to restart crawls cleanly; risks: consumes retry budget if repeated).</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-01-08T06:20:00Z \u2014 Worker monitoring logged repeated HTTP/Network errors (many <code>net::ERR_HTTP2_PROTOCOL_ERROR</code>) during the HC crawl (context for the long-running crawl).</li> <li>2026-01-08T06:25:24Z \u2014 CrawlMonitor thread logged: \u201cDocker logs stream ended\u201d (crawl stage ended).</li> <li>2026-01-08T06:31:43Z \u2014 <code>archive_tool</code> and the worker encountered <code>OSError: [Errno 107] Transport endpoint is not connected</code> on the HC job output dir; worker then hit the same error when attempting PHAC and CIHR output dirs.</li> <li>2026-01-08T19:52:58Z \u2014 Operator ran <code>./scripts/vps-crawl-status.sh --year 2026</code> and observed job output dir unreadable and crawl jobs failing/retrying.</li> <li>2026-01-08T20:09:02Z \u2014 <code>healtharchive-crawl-metrics.service</code> repeatedly failed with <code>Errno 107</code> while probing output dirs/logs.</li> <li>2026-01-08T20:17Z (approx) \u2014 Operator stopped worker, unmounted stale hot-path mountpoints for job output dirs.</li> <li>2026-01-08T20:18Z (approx) \u2014 First attempt to re-apply tiering bind mounts failed due to additional stale mounts under <code>/srv/healtharchive/jobs/imports/**</code>.</li> <li>2026-01-08T20:21Z (approx) \u2014 Operator unmounted stale imports mountpoints and re-applied tiering bind mounts successfully.</li> <li>2026-01-08T20:22Z (approx) \u2014 Operator ran <code>ha-backend recover-stale-jobs --apply</code> and restarted the worker; crawl metrics writer started succeeding again.</li> <li>2026-01-08T20:34:34Z \u2014 Status snapshot showed annual jobs in <code>failed</code> (no running jobs); operator re-marked jobs <code>retryable</code> via <code>ha-backend retry-job</code>.</li> <li>2026-01-08T20:38:39Z \u2014 Worker picked job 6 and successfully launched a new <code>zimit</code> container; crawl resumed and began producing new WARCs.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger: one or more <code>sshfs</code> \u201chot path\u201d mountpoints under <code>/srv/healtharchive/jobs/**</code> became stale, causing <code>stat(2)</code> and directory reads to fail with <code>Errno 107</code> (\u201cTransport endpoint is not connected\u201d).</li> <li>Underlying cause(s): unknown.</li> <li>Hypothesis: transient network disruption between the VPS and Storage Box left multiple nested <code>sshfs</code> mounts in a stale-but-mounted state; the base Storage Box mount remained active, but hot-path submounts did not recover automatically.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#contributing-factors","title":"Contributing factors","text":"<ul> <li>The system had multiple per-job/per-path <code>sshfs</code> mountpoints (\u201chot paths\u201d), multiplying the surface area for FUSE staleness.</li> <li>Several code paths treated output-dir probes as infallible:</li> <li><code>archive_tool</code> attempted to <code>stat()</code> combined logs and raised an unhandled exception when the mount was stale.</li> <li>The crawl metrics writer crashed rather than emitting a \u201cprobe failed\u201d metric.</li> <li>No hot-path auto-recovery timer/sentinel was enabled at the time, so stale mountpoints persisted until manual intervention.</li> <li>The crawl was long-running and noisy (frequent HTTP2 protocol errors/timeouts), increasing the chance of being mid-operation when storage became unavailable.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#resolution-recovery","title":"Resolution / Recovery","text":""},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#1-confirm-the-symptom-and-scope","title":"1) Confirm the symptom and scope","text":"<ul> <li>Confirmed filesystem error:</li> <li><code>ls -la /srv/healtharchive/jobs/hc/</code> \u2192 <code>Transport endpoint is not connected</code></li> <li>Confirmed the affected paths were <code>sshfs</code> mountpoints:</li> <li><code>mount | rg '/srv/healtharchive/jobs/(hc|phac|cihr)/20260101T000502Z__'</code></li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#2-stop-the-worker-to-prevent-concurrent-io-against-stale-mounts","title":"2) Stop the worker to prevent concurrent I/O against stale mounts","text":"<pre><code>sudo systemctl stop healtharchive-worker.service\n</code></pre>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#3-lazily-unmount-stale-job-output-dir-hot-paths","title":"3) Lazily unmount stale job output-dir hot paths","text":"<pre><code>sudo umount -l /srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101\nsudo umount -l /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101\nsudo umount -l /srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101\n</code></pre> <p>What this changed:</p> <ul> <li>Removed stale FUSE mountpoints so the tiering scripts could remount cleanly.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#4-re-apply-tiering-bind-mounts-and-clear-any-additional-stale-mounts","title":"4) Re-apply tiering bind mounts (and clear any additional stale mounts)","text":"<p>First attempt surfaced additional stale mounts under legacy imports (same symptom):</p> <pre><code>sudo /opt/healtharchive-backend/scripts/vps-warc-tiering-bind-mounts.sh --apply\n</code></pre> <p>Then unmounted the stale imports mountpoints and re-ran the bind-mount script:</p> <pre><code>mount | rg '/srv/healtharchive/jobs/imports'\nsudo umount -l /srv/healtharchive/jobs/imports/legacy-hc-2025-04-21\nsudo umount -l /srv/healtharchive/jobs/imports/legacy-cihr-2025-04\nsudo /opt/healtharchive-backend/scripts/vps-warc-tiering-bind-mounts.sh --apply\n</code></pre> <p>What this changed:</p> <ul> <li>Restored canonical tiered WARC paths and removed stale \u201cimports\u201d hot paths blocking the bind-mount installer.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#5-requeue-stale-jobs-in-the-db","title":"5) Requeue stale jobs in the DB","text":"<pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 5 --apply --limit 10\n</code></pre> <p>What this changed:</p> <ul> <li>Marked jobs 6/7/8 as <code>retryable</code> so the worker could safely restart them after storage recovery.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#6-restart-the-worker-and-confirm-metrics-writer-success","title":"6) Restart the worker and confirm metrics writer success","text":"<pre><code>sudo systemctl start healtharchive-worker.service\nsystemctl status healtharchive-crawl-metrics.service --no-pager -l\n</code></pre>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#7-explicitly-retry-annual-jobs-and-restart-worker-loop","title":"7) Explicitly retry annual jobs and restart worker loop","text":"<pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend retry-job --id 6\n/opt/healtharchive-backend/.venv/bin/ha-backend retry-job --id 7\n/opt/healtharchive-backend/.venv/bin/ha-backend retry-job --id 8\nsudo systemctl restart healtharchive-worker.service\n</code></pre> <p>What this changed:</p> <ul> <li>Ensured the jobs were eligible for immediate pickup and restarted the worker to pick a retryable job promptly.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#post-incident-verification","title":"Post-incident verification","text":"<ul> <li>Public surface checks:</li> <li>Not performed as part of the storage recovery; incident scope was internal pipeline health.</li> <li>Worker/job health checks:</li> <li><code>sudo systemctl status healtharchive-worker.service --no-pager</code></li> <li><code>./scripts/vps-crawl-status.sh --year 2026</code> (confirm jobs are running/retryable and output dirs readable)</li> <li><code>docker ps | rg 'ghcr.io/openzim/zimit'</code> (confirm active crawl container)</li> <li>Storage/mount checks (if relevant):</li> <li><code>mount | rg '/srv/healtharchive/jobs/(hc|phac|cihr)/20260101T000502Z__'</code></li> <li><code>ls -la /srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101 | head</code></li> <li>Integrity checks (if relevant):</li> <li>After recovery, ran WARC verification (sampling) to reduce integrity uncertainty:<ul> <li><code>/opt/healtharchive-backend/.venv/bin/ha-backend verify-warcs --job-id 6 --level 0 --limit-warcs 20</code></li> </ul> </li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li>What was the underlying trigger for the <code>sshfs</code> hot-path staleness (network instability, server-side disconnect, local FUSE behavior, or sshfs option mismatch)?</li> <li>Why did multiple independent hot paths go stale at once (shared failure mode), while the base mount remained active?</li> <li>Should we treat <code>Errno 107</code> as a first-class \u201cinfra error\u201d everywhere (worker, archive_tool, metrics) so it never consumes retry budget and never crashes the worker loop?</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> Create a focused roadmap and implement guardrails/automation: <code>docs/roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code> (owner=eng, priority=high, due=2026-01-15)</li> <li> Add \u201chot path unreadable\u201d metrics + alerting rules (owner=eng, priority=high, due=2026-01-15)</li> <li> Add operator drill tooling for alert pipeline and stale-mount recovery (owner=eng, priority=medium, due=2026-01-20)</li> <li> Enable <code>healtharchive-storage-hotpath-auto-recover.timer</code> + sentinel on production after a maintenance window (ensure it will not interrupt active crawls unexpectedly). (owner=ops, priority=high, due=2026-01-20, done=2026-01-16)</li> <li> Add an operator runbook step to clear \u201cfailed\u201d systemd unit state after recovery (<code>systemctl reset-failed ...</code>) so warning alerts don\u2019t linger. (owner=ops, priority=medium, due=2026-01-20, done=2026-01-16)</li> <li> Investigate (and document) why hot-path mounts can become stale while the base mount remains OK; adjust sshfs options if needed. (owner=ops, priority=medium, due=unknown)</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>Safe automation implemented post-incident:</li> <li><code>scripts/vps-storage-hotpath-auto-recover.py</code> can detect <code>Errno 107</code> and perform a conservative recovery sequence (stop worker \u2192 unmount stale hot paths \u2192 re-apply tiering \u2192 requeue stale jobs \u2192 start worker), with safeguards (cooldowns, caps, \u201cconfirm runs\u201d).</li> <li>Risk/false positives to consider:</li> <li>Stopping the worker while a crawl is legitimately progressing can cause unnecessary job restarts and reduce annual coverage.</li> <li>Unmount/remount operations are destructive if targeted at the wrong mountpoint; the detector must be confident (Errno 107) and scoped.</li> <li>Automation should remain opt-in via a sentinel file and should be enabled only once its posture matches the desired operational risk tolerance.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Status snapshots:</li> <li><code>scripts/vps-crawl-status.sh</code> (see operator runs around 2026-01-08 19:52Z and 20:34Z)</li> <li>Relevant logs / error excerpts:</li> <li><code>sudo journalctl -u healtharchive-worker.service --since '2026-01-08 06:20' --until '2026-01-08 06:45' --no-pager -l</code></li> <li><code>sudo journalctl -u healtharchive-crawl-metrics.service --since '2026-01-08 20:00' --no-pager -l</code></li> <li>Job output dirs impacted:</li> <li><code>/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101</code></li> <li><code>/srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101</code></li> <li><code>/srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101</code></li> <li>Tiering / mounts:</li> <li><code>scripts/vps-warc-tiering-bind-mounts.sh</code></li> <li><code>scripts/vps-annual-output-tiering.py</code></li> <li>Playbook: <code>../playbooks/storagebox-sshfs-stale-mount-recovery.md</code></li> <li>Drill playbook: <code>../playbooks/storagebox-sshfs-stale-mount-drills.md</code></li> <li>Follow-up implementation plan:</li> <li><code>docs/roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/","title":"Incident: Annual crawl \u2014 HC job stalled (2026-01-09)","text":"<p>Status: draft (ongoing)</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-01-09</li> <li>Severity: sev1</li> <li>Environment: production</li> <li>Primary area: crawl</li> <li>Owner: (unassigned)</li> <li>Start (UTC): 2026-01-09T07:34:37Z (last observed crawl progress)</li> <li>End (UTC): ongoing</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#summary","title":"Summary","text":"<p>The annual crawl job for <code>hc</code> (job 6) entered a stalled state: <code>crawlStatus</code> stopped advancing and the crawl metrics exporter flagged it as stalled. The stall correlated with repeated <code>Navigation timeout</code> warnings on canada.ca pages.</p> <p>Manual recovery (stop worker + recover stale jobs) was intentionally deferred while <code>cihr</code> (job 8) was actively crawling, to avoid turning an in-progress crawl into a <code>failed</code> job at max retries.</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#impact","title":"Impact","text":"<ul> <li>User-facing impact: annual campaign remained <code>Ready for search: NO</code>.</li> <li>Internal impact: operator attention required; <code>hc</code> crawl not progressing.</li> <li>Data impact:</li> <li>Data loss: unknown (WARCs exist in temp dirs, but crawl completeness is unknown until completion).</li> <li>Data integrity risk: low/unknown (no specific corruption signals observed; primarily a progress/stall problem).</li> <li>Recovery completeness: not recovered at time of write-up.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#detection","title":"Detection","text":"<ul> <li><code>./scripts/vps-crawl-status.sh --year 2026 --job-id 6</code>:</li> <li><code>healtharchive_crawl_running_job_stalled{job_id=\"6\",source=\"hc\"} 1</code></li> <li><code>last_progress_age_seconds</code> climbed into multi-hour range.</li> <li><code>crawlStatus tail</code> stopped advancing.</li> <li><code>recent timeouts</code> showed repeated <code>Navigation timeout of 90000 ms exceeded</code>.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#decision-log","title":"Decision log","text":"<ul> <li>2026-01-09 \u2014 Deferred the \u201cstop worker + recover stale jobs\u201d procedure while job 8 (<code>cihr</code>) was actively crawling to reduce the risk of interrupting it at max retries.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-01-09T06:05:14Z \u2014 Job 6 started (latest observed start time in status snapshot).</li> <li>2026-01-09T07:34:37Z \u2014 Last observed <code>crawlStatus</code> progress for job 6 (<code>crawled=437</code>, <code>total=3209</code>, <code>pending=1</code>).</li> <li>2026-01-09T12:57:17Z \u2014 Status snapshot shows multi-hour no-progress and <code>stalled=1</code>.</li> <li>2026-01-09T13:33:23Z \u2014 Status snapshot still shows <code>stalled=1</code>.</li> <li>2026-01-16T02:56:12Z \u2014 Manual recovery performed (stop worker + recover stale jobs). Job 6 restarted and began a new crawl attempt.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#root-cause","title":"Root cause","text":"<p>Unknown. Strong signals point to crawl progress blocked by repeated page load failures/timeouts and/or a crawler worker getting stuck on a specific URL.</p> <p>As of 2026-01-16, the job showed many <code>net::ERR_HTTP2_PROTOCOL_ERROR</code> failures on canada.ca and <code>archive_tool</code> applied repeated backoff delays after hitting its HTTP/network error threshold.</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#contributing-factors","title":"Contributing factors","text":"<ul> <li>Many canada.ca pages timed out (90s navigation timeouts), increasing the chance of long \u201cpending page\u201d windows.</li> <li><code>hc</code> and <code>cihr</code> were both running; the safest recovery approach (stopping the worker) would interrupt both.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#decision-manual-recovery-option-c","title":"Decision: Manual Recovery (Option C)","text":"<p>We elected to stick with the manual recover-stale-jobs procedure (documented in <code>../playbooks/crawl-stalls.md</code>) rather than automating granular per-job stops. The risk of interrupting a healthy concurrent job is acceptable given the rarity of stalls, and stopping the worker is the safest way to ensure no partial state corruption.</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#resolution-recovery","title":"Resolution / Recovery","text":"<p>Performed on 2026-01-16 (VPS):</p> <ul> <li>Followed <code>docs/operations/playbooks/crawl-stalls.md</code>:</li> <li><code>sudo systemctl stop healtharchive-worker.service</code></li> <li><code>set -a; source /etc/healtharchive/backend.env; set +a</code></li> <li><code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 5 --apply --source hc --limit 1</code></li> <li><code>sudo systemctl start healtharchive-worker.service</code></li> <li>Verified the job restarted (<code>Started at</code> updated) and a new combined log was created.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#post-incident-verification","title":"Post-incident verification","text":"<p>TBD (once recovered).</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li>What exact URL/work unit is the crawler stuck on (if any), and does it repeat across retries?</li> <li>Are timeouts driven by site performance, network issues, headless browser instability, or scope rules?</li> <li>Would changing timeouts/adaptive restart thresholds reduce repeat stalls without harming completeness?</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> After <code>cihr</code> completes (or during a maintenance window), perform the planned recovery steps and update this note with outcomes. (priority=high)</li> <li> (Pending Operator Check) If the stall repeats, capture the specific repeated URL(s) and assess whether scope/timeout tuning is warranted. (owner=ops, priority=medium)</li> <li> Consider tightening/clarifying automation boundaries: per-job recovery without stopping unrelated active crawls.</li> <li>Decision: Explicitly deferred/rejected in favor of manual \"stop worker + recover\" procedure (Option C) to minimize complexity.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>Improve \u201cstalled crawl\u201d detection to include the most recent pending URL and age as part of operator output (snapshot script) and/or alert annotations.</li> <li>Investigate whether recovery can be scoped to a single crawl process/container without stopping the entire worker loop (risk: false positives and partial state).</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Operator snapshot script: <code>scripts/vps-crawl-status.sh</code></li> <li>Latest combined log (as of 2026-01-09 12:57Z snapshot): <code>/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101/archive_new_crawl_phase_-_attempt_1_20260109_060517.combined.log</code></li> <li>Latest combined log after 2026-01-16 recovery: <code>/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101/archive_new_crawl_phase_-_attempt_1_20260116_025617.combined.log</code></li> <li>Playbook: <code>../playbooks/crawl-stalls.md</code></li> <li>Playbook: <code>../playbooks/incident-response.md</code></li> <li>Related: <code>2026-01-09-annual-crawl-phac-output-dir-permission-denied.md</code></li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/","title":"Incident: Annual crawl \u2014 PHAC output dir not writable (2026-01-09)","text":"<p>Status: draft</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-01-09</li> <li>Severity: sev2</li> <li>Environment: production</li> <li>Primary area: crawl + storage</li> <li>Owner: (unassigned)</li> <li>Start (UTC): 2026-01-08T20:22:04Z</li> <li>End (UTC): 2026-01-09T13:39:52Z (mitigated; awaiting successful retry)</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#summary","title":"Summary","text":"<p>The annual crawl job for <code>phac</code> (job 7) repeatedly failed immediately because its job <code>output_dir</code> was not writable (<code>PermissionError</code> while creating a <code>.writable_test_*</code> file). The job produced no WARCs and consumed its retry budget.</p> <p>Recovery restored a writable output directory and reset the job\u2019s retry budget (<code>retry_count=0</code>) so the worker can safely reattempt it when capacity is available.</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#impact","title":"Impact","text":"<ul> <li>User-facing impact: none directly, but annual campaign remained <code>Ready for search: NO</code> while jobs were incomplete.</li> <li>Internal impact: operator intervention required; <code>phac</code> job blocked; retry budget consumed.</li> <li>Data impact:</li> <li>Data loss: no (no WARCs were produced).</li> <li>Data integrity risk: low (failure-to-start; no partial WARC writes).</li> <li>Recovery completeness: partial (job left <code>retryable</code>; not yet re-run at time of write-up).</li> <li>Duration: ~17 hours (first failure \u2192 operator repair + retry reset).</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#detection","title":"Detection","text":"<ul> <li><code>./scripts/vps-crawl-status.sh --year 2026</code> showed:</li> <li><code>phac</code> job 7: <code>status=retryable</code>, <code>crawl_rc=1</code>, <code>crawl_status=failed</code>, <code>WARC files=0</code></li> <li>Worker journal showed the root symptom during job startup:</li> <li><code>CRITICAL ... Output directory ... is invalid or not writable: [Errno 13] Permission denied: .../.writable_test_&lt;pid&gt;</code></li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#decision-log","title":"Decision log","text":"<ul> <li>2026-01-09 \u2014 Avoided interventions that stop <code>healtharchive-worker.service</code> while <code>cihr</code> was actively crawling (to reduce the risk of turning an in-progress crawl into a <code>failed</code> job at max retries).</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-01-08T20:22:04Z \u2014 Worker picked job 7 (<code>phac</code>); job failed immediately due to <code>output_dir</code> not writable (Errno 13).</li> <li>2026-01-09T05:21:15Z \u2014 Status snapshot: job 7 still <code>retryable</code>/failed with <code>0</code> WARCs.</li> <li>2026-01-09T13:10Z \u2014 Confirmed the job output dir is an <code>sshfs</code> hot path mountpoint (<code>findmnt -T &lt;output_dir&gt;</code> shows <code>fstype=fuse.sshfs</code>).</li> <li>2026-01-09T13:10Z \u2014 Attempted <code>chown</code> of the output dir failed (<code>Permission denied</code>) because the path is on <code>sshfs</code>.</li> <li>2026-01-09T13:26:21Z \u2014 <code>ha-backend validate-job-config --id 7</code> confirmed crawler command construction and output dir resolution.</li> <li>2026-01-09T13:39:52Z \u2014 Reset <code>retry_count</code> to <code>0</code> via Python + SQLAlchemy session so job can be retried safely.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger: <code>archive_tool</code> refused to start because <code>output_dir</code> was not writable.</li> <li>Underlying cause(s): job output directory mount/permissions were not compatible with the worker/crawler runtime user (details TBD).</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#contributing-factors","title":"Contributing factors","text":"<ul> <li>Direct <code>psql</code> access from the operator account failed due to missing local DB role mapping (e.g., <code>role \"haadmin\" does not exist</code>).</li> <li>The <code>output_dir</code> is on <code>sshfs</code>, so ownership fixes via <code>chown</code> are not available on the VPS; recovery requires \u201cmake the mount writable\u201d rather than \u201cchange owner\u201d.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#resolution-recovery","title":"Resolution / Recovery","text":"<ul> <li>Diagnosed job output dir mount + permissions:</li> <li>Confirmed job config and path:<ul> <li><code>ha-backend show-job --id 7</code> \u2192 <code>Output dir: /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101</code></li> </ul> </li> <li>Confirmed it is an <code>sshfs</code> hot path mountpoint:<ul> <li><code>findmnt -T /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101 -o TARGET,SOURCE,FSTYPE,OPTIONS</code></li> </ul> </li> <li>Confirmed the worker user:<ul> <li><code>systemctl show -p User -p Group healtharchive-worker.service</code> \u2192 <code>User=haadmin</code>, <code>Group=haadmin</code></li> </ul> </li> <li>Attempted to fix ownership failed (<code>Permission denied</code>) because the output dir is on <code>sshfs</code>:<ul> <li><code>sudo chown &lt;worker_user&gt;:&lt;worker_group&gt; &lt;output_dir&gt;</code></li> </ul> </li> <li>Ensured a writable output dir:</li> <li>Verified writability with a host-level probe:<ul> <li><code>touch /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101/.writable_test &amp;&amp; rm /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101/.writable_test</code></li> </ul> </li> <li>Validated annual tiering state for <code>phac</code>:<ul> <li><code>/opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py --year 2026 --sources phac --apply</code></li> </ul> </li> <li>Validated job configuration:</li> <li><code>ha-backend validate-job-config --id 7</code></li> <li>Reset the job retry budget:</li> <li>Direct <code>psql</code> access failed due to missing DB roles for the operator account (<code>role \"haadmin\" does not exist</code>, <code>role \"root\" does not exist</code>).</li> <li> <p>Used a small Python snippet with <code>ha_backend.db.get_session()</code> to set <code>retry_count=0</code> for <code>job_id=7</code>:</p> <ul> <li>```bash   /opt/healtharchive-backend/.venv/bin/python3 - &lt;&lt;'PY'   from ha_backend.db import get_session   from ha_backend.models import ArchiveJob</li> </ul> <p>job_id = 7   with get_session() as session:       job = session.get(ArchiveJob, job_id)       if job is None:           raise SystemExit(f\"job {job_id} not found\")       old = job.retry_count       job.retry_count = 0       session.commit()       print(f\"OK job_id={job_id} retry_count {old} -&gt; {job.retry_count}\")   PY   ```</p> </li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#post-incident-verification","title":"Post-incident verification","text":"<ul> <li>Confirmed output dir is writable on the host.</li> <li>Confirmed job config dry-run passes.</li> <li>Confirmed job shows <code>Status: retryable</code>, <code>Retry count: 0</code>.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li>Why did this <code>sshfs</code> hot path mount become non-writable for the worker user?</li> <li>Is there any automation that can proactively detect \u201coutput dir not writable\u201d before a crawl attempt consumes retries?</li> <li>Should the worker/crawler user be changed to a dedicated service account (instead of an operator user) to reduce permission drift?</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> Identify why this job\u2019s <code>output_dir</code> was not writable (mount type + UID/GID expectations) and document the invariant we rely on. (priority=high)</li> <li> Add an operator-safe command to reset a crawl job\u2019s retry budget (e.g., <code>ha-backend reset-retry-count --id ID</code>, or extend <code>retry-job</code> to reset <code>retry_count</code> when status is <code>retryable</code>). (priority=medium)</li> <li> Consider treating \u201coutput dir not writable\u201d as an <code>infra_error</code> class so it does not consume retry budget. (priority=medium)</li> <li> Add a short ops note: when <code>psql</code> roles are missing, use the DB session method (Python snippet) rather than forcing <code>psql</code> as root. (priority=low)</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>Add a periodic \u201cjob output dir writability probe\u201d (metrics + alert) for queued/running annual jobs.</li> <li>Expand tiering/repair automation to ensure hot-path output dirs are consistently mounted/writable before a crawl starts.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Operator snapshot script: <code>scripts/vps-crawl-status.sh</code></li> <li>Incident response playbook: <code>../playbooks/incident-response.md</code></li> <li>Crawl stalls playbook: <code>../playbooks/crawl-stalls.md</code></li> <li>Storage hot-path incidents: <code>../playbooks/storagebox-sshfs-stale-mount-recovery.md</code></li> <li>Note: job 7 produced no combined crawl logs because it failed before <code>archive_tool</code> started.</li> <li>Related: <code>2026-01-09-annual-crawl-hc-job-stalled.md</code></li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/","title":"Incident: Replay smoke tests failed (503) due to stale mounts + warc-tiering service failed (2026-01-16)","text":"<p>Status: closed</p>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-01-16</li> <li>Severity (see <code>severity.md</code>): sev1</li> <li>Environment: production</li> <li>Primary area: replay + storage</li> <li>Owner: (unassigned)</li> <li>Start (UTC): 2026-01-15T04:20:00Z (first observed failing replay-smoke metrics)</li> <li>End (UTC): 2026-01-16T02:51:56Z (replay-smoke metrics OK)</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#summary","title":"Summary","text":"<p>The daily replay smoke tests began returning <code>503</code> for the legacy imported jobs (HC + CIHR), even though <code>https://replay.healtharchive.ca/</code> itself was up (<code>200</code>). The underlying issue was that the replay container could not reliably read WARCs under <code>/srv/healtharchive/jobs/imports/**</code> due to stale mountpoints (<code>Transport endpoint is not connected</code>) and the replay container\u2019s mount namespace not reflecting repaired/updated mounts. Separately, <code>healtharchive-warc-tiering.service</code> had been left in a <code>failed</code> state since 2026-01-08, preventing tiered imports from being reliably mounted.</p> <p>Recovery: re-apply WARC tiering, clear the failed systemd state, and restart the replay service to refresh its mounts; then re-run replay smoke tests.</p>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#impact","title":"Impact","text":"<ul> <li>User-facing impact: replay for legacy jobs intermittently failed (HTTP 503 responses from pywb for snapshot requests).</li> <li>Internal impact: <code>ReplaySmokeFailed</code> monitoring noise and operator intervention required.</li> <li>Data impact:</li> <li>Data loss: no evidence</li> <li>Data integrity risk: low/unknown (symptom was read failures, not WARC corruption)</li> <li>Recovery completeness: complete (smoke tests returned <code>200</code>)</li> <li>Duration: ~22h (first failing metric to confirmed recovery)</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#detection","title":"Detection","text":"<ul> <li>node_exporter metrics:</li> <li><code>healtharchive_replay_smoke_ok{job_id=\"1\",source=\"hc\"} 0</code> + <code>status_code ... 503</code></li> <li><code>healtharchive_replay_smoke_ok{job_id=\"2\",source=\"cihr\"} 0</code> + <code>status_code ... 503</code></li> <li>systemd state:</li> <li><code>healtharchive-warc-tiering.service</code> was <code>failed</code> since 2026-01-08 with <code>Transport endpoint is not connected</code>.</li> <li>Container symptom:</li> <li><code>docker exec healtharchive-replay ... ls -la /warcs/imports/...</code> showed <code>d?????????</code> and <code>Transport endpoint is not connected</code>.</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#decision-log-recommended-for-sev1","title":"Decision log (recommended for sev1)","text":"<ul> <li>2026-01-16T02:51:00Z \u2014 Decision: restart replay after fixing tiering mounts (why: quickest way to ensure the pywb container sees a clean view of <code>/srv/healtharchive/jobs</code> and can read WARCs; risks: brief replay downtime, but no data mutation).</li> <li>2026-01-16T16:00:00Z \u2014 Decision (post-incident hardening): run pywb with <code>rshared</code> bind propagation for <code>/srv/healtharchive/jobs</code> (why: allow the container to observe repaired nested mounts without requiring an additional restart; risks: broader mount propagation surface, but still read-only inside the container).</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-01-08T06:25:23Z \u2014 <code>healtharchive-warc-tiering.service</code> failed while attempting to operate on <code>/srv/healtharchive/jobs/imports/...</code> (stale mount: <code>Transport endpoint is not connected</code>).</li> <li>2026-01-15T04:20:00Z \u2014 Replay smoke test metrics show <code>503</code> for legacy jobs (first observed failing <code>healtharchive_replay_smoke_*</code> timestamp).</li> <li>2026-01-16T02:25Z \u2014 Verified replay root is up (<code>curl -I https://replay.healtharchive.ca/</code> returns <code>200</code>), but snapshot requests return <code>503</code>.</li> <li>2026-01-16T02:30Z \u2014 Confirmed the replay container cannot read tiered import directories (<code>docker exec healtharchive-replay ...</code> shows <code>Transport endpoint is not connected</code>).</li> <li>2026-01-16T02:51Z \u2014 Recovered by re-applying tiering + restarting replay:</li> <li><code>sudo systemctl reset-failed healtharchive-warc-tiering.service</code></li> <li><code>sudo systemctl start healtharchive-warc-tiering.service</code></li> <li><code>sudo systemctl restart healtharchive-replay.service</code></li> <li><code>sudo systemctl start healtharchive-replay-smoke.service</code></li> <li>2026-01-16T02:51:56Z \u2014 Replay smoke metrics return to <code>200</code>:</li> <li><code>healtharchive_replay_smoke_ok{job_id=\"1\",source=\"hc\"} 1</code></li> <li><code>healtharchive_replay_smoke_ok{job_id=\"2\",source=\"cihr\"} 1</code></li> <li>2026-01-16T16:00Z \u2014 Post-incident hardening: updated replay systemd unit to mount <code>/srv/healtharchive/jobs</code> with <code>rshared</code> bind propagation so pywb can observe nested mount repairs without a restart (see: <code>../../deployment/replay-service-pywb.md</code>).</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger: one or more tiered paths under <code>/srv/healtharchive/jobs/imports/**</code> were stale/unreadable (<code>Errno 107: Transport endpoint is not connected</code>), causing WARC reads inside pywb to fail.</li> <li>Underlying cause(s):</li> <li><code>healtharchive-warc-tiering.service</code> remained <code>failed</code> after a prior storage incident, so tiered import mountpoints were not being applied/validated by systemd.</li> <li>The replay service is a long-running Docker container bind-mounting <code>/srv/healtharchive/jobs</code> into <code>/warcs</code>. Mount changes/repairs on the host can require a container restart for the container to observe a clean view of the mountpoints.</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#contributing-factors","title":"Contributing factors","text":"<ul> <li>Tiered import jobs are critical to replay smoke (legacy jobs are used as smoke targets).</li> <li>Stale mount symptoms were partly masked because:</li> <li>the Storage Box base mount looked healthy, and</li> <li>replay root <code>/</code> still returned <code>200</code>.</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#resolution-recovery","title":"Resolution / Recovery","text":"<p>1) Ensure WARC tiering mounts are applied and systemd is not stuck in a failed state:</p> <pre><code>sudo systemctl reset-failed healtharchive-warc-tiering.service\nsudo systemctl start healtharchive-warc-tiering.service\nsudo systemctl status healtharchive-warc-tiering.service --no-pager -l\n</code></pre> <p>2) Restart replay so the container sees a clean view of <code>/srv/healtharchive/jobs</code>:</p> <pre><code>sudo systemctl restart healtharchive-replay.service\nsudo systemctl status healtharchive-replay.service --no-pager -l\n</code></pre> <p>3) Re-run replay smoke and verify metrics:</p> <pre><code>sudo systemctl start healtharchive-replay-smoke.service\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_replay_smoke_'\n</code></pre>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#post-incident-hardening-durable-fixes","title":"Post-incident hardening (durable fixes)","text":"<ul> <li>Replay service mount propagation:</li> <li>Updated <code>/etc/systemd/system/healtharchive-replay.service</code> to mount <code>/srv/healtharchive/jobs</code> as <code>ro,rshared</code> so nested bind-mount repairs (tiering/hot-path recovery) are visible inside the container.</li> <li>Canonical doc: <code>../../deployment/replay-service-pywb.md</code></li> <li>Tiering service resilience:</li> <li>Updated the tiering systemd unit template to run <code>vps-warc-tiering-bind-mounts.sh --apply --repair-stale-mounts</code> so it can automatically unmount stale <code>Errno 107</code> mountpoints and re-apply binds on start.</li> <li>Canonical playbook: <code>../playbooks/warc-storage-tiering.md</code></li> <li>Storage hot-path auto-recovery:</li> <li>Enabled <code>healtharchive-storage-hotpath-auto-recover.timer</code> (opt-in via sentinel file) so stale mounts are detected and recovered without requiring a manual incident response for common <code>Errno 107</code> cases.</li> <li>Canonical playbook: <code>../playbooks/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#post-incident-verification","title":"Post-incident verification","text":"<ul> <li>Public surface checks:</li> <li><code>curl -I https://replay.healtharchive.ca/ | head</code> returns <code>200</code>.</li> <li>Storage/mount checks:</li> <li><code>systemctl status healtharchive-warc-tiering.service --no-pager -l</code> is successful.</li> <li>Replay job checks:</li> <li><code>healtharchive_replay_smoke_ok{job_id=\"1\",source=\"hc\"} 1</code> and <code>...{job_id=\"2\",source=\"cihr\"} 1</code></li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#public-communication-optional","title":"Public communication (optional)","text":"<ul> <li>Public status update: not posted (incident was internal and did not change public-facing expectations beyond the replay smoke targets).</li> <li>Public-safe summary: keep on file; if replay becomes a user-facing guarantee in future, revisit whether sev1 incidents should trigger a public note.</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li>Can we make replay smoke targets independent of tiered-import mounts (e.g., keep a tiny always-local \u201ccanary replay\u201d job) so storage tiering issues don\u2019t mask replay regressions?</li> <li>Decision: Deferred to backlog. Tiering alerting (now implemented) addresses the immediate need for better detection. Canary replay is a future enhancement.</li> <li>Should replay smoke include an explicit \u201cWARC file exists + readable\u201d check to disambiguate pywb failures vs storage failures?</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> Update playbooks to call out \u201crestart replay after mount/tiering repairs\u201d when smoke returns <code>503</code> but replay root is <code>200</code>. (owner=eng, priority=high, due=2026-01-16)</li> <li> Enable the storage hot-path auto-recover watchdog (<code>healtharchive-storage-hotpath-auto-recover.timer</code>) after validating thresholds. (owner=eng, priority=medium, due=2026-01-16)</li> <li> Document and apply <code>rshared</code> bind propagation for the replay service so nested mount repairs are visible without restarting pywb. (owner=eng, priority=high, due=2026-01-16)</li> <li> Enable tiering health metrics + alerting so <code>healtharchive-warc-tiering.service</code> failures are visible quickly. (owner=eng, priority=medium, due=2026-01-18)</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>Automate \u201ctiering failed\u201d detection with metrics + alerting:</li> <li>Enable <code>healtharchive-tiering-metrics.timer</code> and alert on a sustained unhealthy signal (e.g., <code>healtharchive_tiering_metrics_ok==0</code> or a \u201ctiering applied\u201d check failing).</li> <li>Keep replay smoke meaningful but safe:</li> <li>Prefer smoke probes that are read-only and low-cost.</li> <li>Treat <code>Errno 107</code> as an infra/storage failure class, and route recovery through the storage/tiering watchdogs rather than marking replay itself \u201cbroken\u201d.</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Tiering manifest (VPS): <code>/etc/healtharchive/warc-tiering.binds</code></li> <li>Tiering script (VPS): <code>scripts/vps-warc-tiering-bind-mounts.sh</code></li> <li>Replay smoke playbook: <code>../playbooks/replay-smoke-tests.md</code></li> <li>Storage recovery playbook: <code>../playbooks/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/","title":"Incident: Annual crawl \u2014 Errno 107 hot-path issue triggered infra-error thrash and worker stopped (2026-01-24)","text":"<p>Status: draft</p>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-01-24</li> <li>Severity (see <code>severity.md</code>): sev1</li> <li>Environment: production (single VPS)</li> <li>Primary area: storage</li> <li>Owner: (unassigned)</li> <li>Start (UTC): 2026-01-24T06:17:10Z (approx; storage hot-path auto-recover began running repeatedly)</li> <li>End (UTC): 2026-01-24T12:31:03Z (approx; worker restarted and annual crawl resumed)</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#summary","title":"Summary","text":"<p>During the annual 2026 campaign, Storage Box \u201chot path\u201d mountpoints under <code>/srv/healtharchive/jobs/**</code> intermittently became stale and returned <code>OSError: [Errno 107] Transport endpoint is not connected</code>. The worker then repeatedly picked the PHAC annual job (job 7), immediately failed with an infra error, and re-picked it in a tight loop. Shortly after, the worker service became inactive, leaving the campaign with no running jobs until manual intervention restarted the worker and re-launched the HC crawl (job 6).</p>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#impact","title":"Impact","text":"<ul> <li>User-facing impact: none directly observed, but annual campaign remained <code>Ready for search: NO</code> and made no progress while the worker was down.</li> <li>Internal impact (ops burden, automation failures, etc):</li> <li>Alert noise (repeated infra errors and hot-path auto-recover runs).</li> <li>Worker stopped; no jobs ran until manual restart.</li> <li>Increased risk of \u201cnew crawl phase\u201d restarts (loss of frontier continuity) after recovery.</li> <li>Data impact:</li> <li>Data loss: unknown (no evidence of WARC deletion in this incident record).</li> <li>Data integrity risk: medium (stale mounts can interrupt writes; \u201cnew crawl phase\u201d can reduce completeness by losing crawl frontier).</li> <li>Recovery completeness: partial (worker restarted and crawl resumed; underlying Errno 107 trigger not fully understood).</li> <li>Duration: ~6h 14m (approx; 06:17Z \u2192 12:31Z).</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#detection","title":"Detection","text":"<ul> <li>Operator reported overnight warning/error notifications (approx 5 hours before 12:22Z).</li> <li><code>./scripts/vps-crawl-status.sh --year 2026</code> at ~12:22Z showed:</li> <li><code>FAIL worker service is not active</code></li> <li><code>healtharchive_crawl_running_jobs 0</code></li> <li><code>phac</code> job marked <code>crawl_status=infra_error</code></li> <li>storage hot-path watchdog recorded tiering failures mentioning <code>Errno 107</code> for PHAC/CIHR hot paths.</li> <li>Worker journal around 06:28Z showed rapid repetition of the same infra error for job 7.</li> <li><code>healtharchive-crawl-auto-recover.service</code> was running every ~5 minutes throughout the window and consistently completed successfully (\u201cDeactivated successfully\u201d), which suggests it was likely not the component that stopped the worker. (See timeline and artifacts.)</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-01-24T05:45:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs (timer-driven), completes successfully.</li> <li>2026-01-24T05:50:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T05:55:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:00:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:05:01Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:10:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:15:07Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:17:10Z \u2014 <code>healtharchive-storage-hotpath-auto-recover.service</code> begins running repeatedly (timer-driven).</li> <li>2026-01-24T06:28:01Z \u2014 Worker picks job 7 (PHAC) and immediately raises <code>Errno 107</code> for the job output dir; the same job is re-picked repeatedly within the same second (infra-error thrash).</li> <li>2026-01-24T06:28:02Z \u2014 <code>healtharchive-worker.service</code> becomes inactive (stopped); no running jobs remain.</li> <li>2026-01-24T06:30:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:35:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:40:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:45:05Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:50:01Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:55:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T07:00:05Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T07:05:01Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T07:10:01Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T12:22:11Z \u2014 Operator status snapshot confirms worker inactive and no running jobs.</li> <li>2026-01-24T12:31:03Z \u2014 Manual recovery: worker started; job 6 (HC) restarts a crawl container and begins a \u201cnew crawl phase\u201d.</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger:</li> <li>One or more hot-path mountpoints returned <code>Errno 107</code> during job execution (observed for PHAC output dir; watchdog also reported PHAC/CIHR paths).</li> <li>Underlying cause(s):</li> <li>Likely intermittent <code>sshfs</code>/FUSE hot-path instability (similar failure mode to <code>2026-01-08-storage-hotpath-sshfs-stale-mount.md</code>), but root trigger remains unconfirmed.</li> <li>Worker behavior on infra errors allowed immediate re-pick of the same job with no effective cooldown, causing log spam and increased operational risk.</li> <li>The worker stop event at <code>2026-01-24T06:28:02Z</code> was a <code>systemd</code> stop; based on available logs, <code>healtharchive-crawl-auto-recover.service</code> does not appear to be the direct cause (it continued running successfully before/after).</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#contributing-factors","title":"Contributing factors","text":"<ul> <li>Infra error handling did not appear to enforce a cooldown/backoff for fast-failing jobs, enabling a tight loop.</li> <li>Hot-path auto-recovery ran frequently but did not resolve the condition before the worker stopped.</li> <li>Operator CLI confusion: running <code>ha-backend show-job --id 6</code> without exporting <code>/etc/healtharchive/backend.env</code> defaulted to SQLite and failed with <code>no such table: archive_jobs</code> (not causal, but slowed diagnosis).</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#resolution-recovery","title":"Resolution / Recovery","text":"<p>Manual recovery steps performed (state-changing):</p> <p>1) Confirm mounts were readable again (spot-check):</p> <pre><code>timeout 5 ls -la /srv/healtharchive/storagebox &gt;/dev/null\ntimeout 5 ls -la /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101 &gt;/dev/null\ntimeout 5 ls -la /srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101 &gt;/dev/null\n</code></pre> <p>2) Restart the worker:</p> <pre><code>sudo systemctl start healtharchive-worker.service\n</code></pre> <p>3) Verify the crawl restarted and a zimit container is running:</p> <pre><code>./scripts/vps-crawl-status.sh --year 2026\nsudo systemctl --no-pager --full status healtharchive-worker.service\n</code></pre> <p>Observed outcome: job 6 (HC) restarted at ~12:31Z and began writing new crawl temp dirs/WARCs under the existing job output directory.</p>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#additional-recovery-work-2026-01-25","title":"Additional recovery work (2026-01-25)","text":"<ul> <li>Reset the retry budget for jobs 7 (PHAC) and 8 (CIHR) by writing <code>retry_count=0</code> directly via the backend ORM so the re-runs behaved like fresh attempts.</li> <li>Fixed the output-directory permissions (<code>chown -R haadmin:haadmin</code>, <code>chmod 755</code>) and confirmed writability by touching <code>.writable_test_manual</code> inside each job dir as <code>haadmin</code>.</li> <li>Launched the jobs through the transient <code>systemd-run</code>-based helper (<code>scripts/vps-run-db-job-detached.py / systemd-run \u2026 run-db-job --id \u2026</code>) so the crawls kept running while our SSH sessions closed.</li> <li>Relaxed permissions on the existing <code>.tmpt*</code> directories (<code>docker run --rm -v \"\u2026:/output\" alpine sh -c 'chmod -R a+rX /output/.tmp*'</code>) so the hot-path watchdog/ops scripts could read the WARCs without manual chmods.</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#post-incident-verification","title":"Post-incident verification","text":"<ul> <li>Worker/job health checks:</li> <li><code>sudo systemctl status healtharchive-worker.service --no-pager -l</code> (worker active)</li> <li><code>./scripts/vps-crawl-status.sh --year 2026</code> (running job detected; metrics OK; crawlStatus advancing)</li> <li>Storage/mount checks:</li> <li><code>findmnt -T /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101</code></li> <li><code>findmnt -T /srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101</code></li> <li>Integrity checks:</li> <li>Not performed as part of this initial incident note; consider WARC sampling on job 6 after stabilization.</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li>What is the underlying trigger for hot-path mount staleness (network blip, Storage Box behavior, sshfs option mismatch, local FUSE behavior)?</li> <li>Why did the worker service stop shortly after the infra-error thrash (explicit stop by automation vs worker exit)?</li> <li>Does the current infra-error cooldown/backoff logic actually prevent tight re-pick loops in practice, and is it observable via metrics/logs?</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> Implement mitigations and recovery improvements in <code>docs/roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening.md</code>. (owner=eng, priority=high, due=2026-02-01)</li> <li> Investigate underlying cause of the hot-path staleness (Storage Box/sshfs/network/FUSE). (owner=eng, priority=high, due=2026-02-01)</li> <li> Add a worker-side guardrail to prevent tight \u201cpick same job instantly\u201d loops on infra errors (cooldown + metric). (owner=eng, priority=high, due=2026-02-01)</li> <li> Add a playbook section for \u201cCLI shows sqlite/no such table\u201d (env export reminder) to reduce operator confusion. (owner=ops, priority=low, due=2026-02-01)</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#follow-up-implementation-details","title":"Follow-up implementation details","text":"<ul> <li>Added <code>scripts/vps-run-db-job-detached.py</code> and updated <code>docs/deployment/systemd/README.md</code> to point operators at this helper so specific jobs can be re-run within transient <code>systemd-run</code> units without keeping a shell session attached.</li> <li>Extended <code>scripts/vps-storage-hotpath-auto-recover.py</code> to probe the output directories of queued/retryable jobs (not just the currently running ones) so stale hot paths can be detected before the worker picks them; detection still strictly unmounts only the specific stale targets.</li> <li>Extended the archive tool to discover <code>.tmp*</code> temp directories immediately after the container starts and periodically throughout the crawl. When <code>--relax-perms</code> is enabled the helper now runs during the crawl (configurable interval) so host commands can read WARCs without manual <code>chmod</code>, not just after the job finishes.</li> <li>Added a <code>last_healthy_*</code> timestamp to the storage hot-path watchdog state and Prometheus textfile metrics, while continuing to gate actual recovery attempts via the existing <code>last_apply_*</code> cooldown/cap fields. This gives dashboards a clearer signal when the watchdog has seen no stale targets.</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>Safe automation:</li> <li>Detect <code>Errno 107</code> at the job output-dir boundary and pause/park affected jobs with a cooldown rather than thrashing the worker loop.</li> <li>If <code>Errno 107</code> is detected with high confidence, automate a conservative recovery sequence (stop worker \u2192 unmount stale hot paths only \u2192 re-apply tiering/bind mounts \u2192 restart worker), but only when crawl integrity risk is acceptable.</li> <li>What should stay manual:</li> <li>Any automation that unmounts paths used by an actively running crawl container should be guarded heavily (risk of interrupting writes / frontier continuity).</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Related incident: <code>docs/operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount.md</code></li> <li>Follow-up roadmap: <code>docs/roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening.md</code></li> <li><code>healtharchive-crawl-auto-recover.service</code> journal window:</li> <li><code>sudo journalctl -u healtharchive-crawl-auto-recover.service --since '2026-01-24 05:45:00' --until '2026-01-24 07:15:00' --no-pager | tail -n 200</code></li> <li>Most relevant worker log excerpt (redacted):</li> </ul> <pre><code>Jan 24 06:28:01 &lt;vps&gt; ha-backend[...]: 2026-01-24 06:28:01,050 [INFO] healtharchive.worker: Worker picked job 7 for source phac (...) with status retryable and retry_count 0\nJan 24 06:28:01 &lt;vps&gt; ha-backend[...]: 2026-01-24 06:28:01,056 [WARNING] healtharchive.jobs: Job 7 raised during archive_tool execution: [Errno 107] Transport endpoint is not connected: '/srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101'\nJan 24 06:28:01 &lt;vps&gt; ha-backend[...]: 2026-01-24 06:28:01,063 [WARNING] healtharchive.worker: Crawl for job 7 failed due to infra error (RC=1). Not consuming retry budget (retry_count=0).\nJan 24 06:28:01 &lt;vps&gt; ha-backend[...]: 2026-01-24 06:28:01,068 [INFO] healtharchive.worker: Worker picked job 7 for source phac (...) with status retryable and retry_count 0\nJan 24 06:28:02 &lt;vps&gt; systemd[1]: Stopping healtharchive-worker.service - HealthArchive Worker...\nJan 24 06:28:02 &lt;vps&gt; systemd[1]: healtharchive-worker.service: Deactivated successfully.\n</code></pre>"},{"location":"operations/incidents/severity/","title":"Incident severity rubric (internal)","text":"<p>Severity is a shared shorthand for priority and urgency, not blame.</p> <p>Use this rubric to decide how quickly to respond, what to pause, and what level of documentation/verification is appropriate.</p> <p>Notes:</p> <ul> <li>Severity can change as you learn more; update the incident note accordingly.</li> <li>When in doubt, start higher and downgrade later.</li> <li>Data integrity risk should push severity up.</li> </ul>"},{"location":"operations/incidents/severity/#sev0-critical-outage-integrity-security","title":"sev0 \u2014 Critical outage / integrity / security","text":"<p>Criteria (any one is enough):</p> <ul> <li>Public site/API is effectively down for most users, or returning incorrect/unsafe data.</li> <li>Credible data integrity compromise (corruption, missing/cross-linked WARCs, replay integrity loss).</li> <li>Security incident (credential leak, unauthorized access, suspicious behavior).</li> </ul> <p>Response expectations:</p> <ul> <li>Immediate response.</li> <li>Prefer conservative actions that preserve integrity (pause/stop destructive jobs).</li> <li>Capture a complete timeline and include post-incident verification.</li> <li>Public note is optional but recommended when it changes user expectations (outage, integrity risk, security posture, policy change). Keep it public-safe (no sensitive details).</li> </ul>"},{"location":"operations/incidents/severity/#sev1-major-degradation-time-critical-capture-risk","title":"sev1 \u2014 Major degradation / time-critical capture risk","text":"<p>Criteria (any one is enough):</p> <ul> <li>Public site/API is usable but severely degraded (major routes broken, errors widespread).</li> <li>Annual capture campaign is blocked or likely to miss the window (or lose meaningful coverage) without intervention.</li> <li>Storage/mount instability that threatens crawl/indexing continuity even if the public surface is OK.</li> </ul> <p>Response expectations:</p> <ul> <li>Same-day response.</li> <li>Record the recovery steps precisely (commands + what they changed).</li> <li>Public note is optional but recommended when it changes user expectations (outage/degradation, integrity risk, public posture). Keep it public-safe (no sensitive details).</li> </ul>"},{"location":"operations/incidents/severity/#sev2-partial-degradation-contained-pipeline-failure","title":"sev2 \u2014 Partial degradation / contained pipeline failure","text":"<p>Criteria (typical examples):</p> <ul> <li>One source crawl repeatedly failing but others are healthy.</li> <li>Crawl/indexing slowdown or intermittent errors with a known workaround.</li> <li>Non-critical automation failing (metrics, watchdogs, timers) with manual fallback.</li> </ul> <p>Response expectations:</p> <ul> <li>Next-business-day response is usually acceptable.</li> <li>Document what happened and track follow-ups that reduce repeat issues.</li> </ul>"},{"location":"operations/incidents/severity/#sev3-minor-issue-operational-friction","title":"sev3 \u2014 Minor issue / operational friction","text":"<p>Criteria (typical examples):</p> <ul> <li>Internal-only issue with low urgency and a simple workaround.</li> <li>Documentation gaps discovered during ops.</li> <li>Cosmetic or non-impactful errors that should still be cleaned up.</li> </ul> <p>Response expectations:</p> <ul> <li>Fix opportunistically.</li> <li>Still record the incident if it required manual intervention or could recur.</li> </ul>"},{"location":"operations/playbooks/","title":"Ops playbooks (task-oriented)","text":"<p>Playbooks are short, task-oriented checklists for recurring operator work.</p> <p>If you only read one thing first:</p> <ul> <li><code>operator-responsibilities.md</code> (what you must do to keep the site healthy)</li> </ul> <p>Rules:</p> <ul> <li>Keep them brief and procedural.</li> <li>Avoid duplicating canonical docs; link to the runbook/checklist that owns the details.</li> <li>Prefer stable command entrypoints (scripts) so steps don't drift.</li> <li>Use the template for new playbooks: <code>../../_templates/playbook-template.md</code></li> </ul>"},{"location":"operations/playbooks/#core-operations","title":"Core Operations","text":"<p>Essential playbooks for daily operator work:</p> <ul> <li>Operator responsibilities: <code>operator-responsibilities.md</code> \u2014 Core operator duties</li> <li>Deploy &amp; verify: <code>deploy-and-verify.md</code> \u2014 Deployment workflow</li> <li>Incident response: <code>incident-response.md</code> \u2014 When something breaks</li> <li>Admin proxy: <code>admin-proxy.md</code> \u2014 Browser-friendly ops triage</li> <li>Automation posture: <code>automation-maintenance.md</code> \u2014 Keep automation healthy</li> </ul>"},{"location":"operations/playbooks/#observability-setup","title":"Observability Setup","text":"<p>Setting up and maintaining monitoring infrastructure:</p> <ul> <li>Bootstrap: <code>observability-bootstrap.md</code> \u2014 Dirs + secrets scaffolding</li> <li>Exporters: <code>observability-exporters.md</code> \u2014 Node + Postgres exporters (loopback-only)</li> <li>Prometheus: <code>observability-prometheus.md</code> \u2014 Scrape config + retention (loopback-only)</li> <li>Grafana: <code>observability-grafana.md</code> \u2014 Grafana install (loopback-only) + tailnet access</li> <li>Dashboards: <code>observability-dashboards.md</code> \u2014 Dashboard provisioning</li> <li>Alerting: <code>observability-alerting.md</code> \u2014 Prometheus + Alertmanager (minimal, high-signal)</li> <li>Monitoring setup: <code>monitoring-and-alerting.md</code> \u2014 Complete monitoring setup guide</li> <li>Maintenance: <code>observability-maintenance.md</code> \u2014 Keep observability healthy</li> </ul>"},{"location":"operations/playbooks/#crawl-archive-operations","title":"Crawl &amp; Archive Operations","text":"<p>Managing crawls and archive lifecycle:</p> <ul> <li>Crawl preflight: <code>crawl-preflight.md</code> \u2014 Pre-crawl audit (before annual/large crawls)</li> <li>Crawl stalls: <code>crawl-stalls.md</code> \u2014 Stalled progress + status snapshot</li> <li>Annual campaign: <code>annual-campaign.md</code> \u2014 Seasonal campaign operations</li> <li>Cleanup automation: <code>cleanup-automation.md</code> \u2014 Safe temp cleanup</li> <li>Replay service: <code>replay-service.md</code> \u2014 Replay service operations (if enabled)</li> </ul>"},{"location":"operations/playbooks/#storage-management","title":"Storage Management","text":"<p>WARC storage and integrity:</p> <ul> <li>WARC storage tiering: <code>warc-storage-tiering.md</code> \u2014 SSD + Storage Box tiering</li> <li>WARC integrity: <code>warc-integrity-verification.md</code> \u2014 Verify WARCs</li> <li>Stale mount recovery: <code>storagebox-sshfs-stale-mount-recovery.md</code> \u2014 Errno 107 recovery</li> <li>Recovery drills: <code>storagebox-sshfs-stale-mount-drills.md</code> \u2014 Safe production drills</li> </ul>"},{"location":"operations/playbooks/#validation-testing","title":"Validation &amp; Testing","text":"<p>Quality assurance and verification:</p> <ul> <li>Coverage guardrails: <code>coverage-guardrails.md</code> \u2014 Annual regression checks</li> <li>Replay smoke tests: <code>replay-smoke-tests.md</code> \u2014 Daily replay validation</li> <li>Restore test: <code>restore-test.md</code> \u2014 Quarterly restore test</li> <li>Dataset release: <code>dataset-release.md</code> \u2014 Dataset release integrity (quarterly)</li> <li>Healthchecks parity: <code>healthchecks-parity.md</code> \u2014 Env \u2194 systemd \u2194 Healthchecks sync</li> </ul>"},{"location":"operations/playbooks/#external-outreach","title":"External &amp; Outreach","text":"<p>External-facing operations:</p> <ul> <li>Outreach &amp; verification: <code>outreach-and-verification.md</code> \u2014 External outreach workflow</li> <li>Adoption signals: <code>adoption-signals.md</code> \u2014 Quarterly adoption signals entry</li> <li>Security posture: <code>security-posture.md</code> \u2014 Ongoing security checks</li> </ul>"},{"location":"operations/playbooks/#quick-reference","title":"Quick Reference","text":"Frequency Tasks Playbooks Daily Service health, crawl status ops-cadence-checklist.md, crawl-stalls.md Weekly Monitoring review, automation posture ops-cadence-checklist.md, automation-maintenance.md Monthly Reliability review, docs drift ops-cadence-checklist.md Quarterly Restore test, dataset release, adoption signals restore-test.md, dataset-release.md, adoption-signals.md Annual Campaign readiness, coverage guardrails annual-campaign.md, coverage-guardrails.md <p>For the complete operations cadence: <code>../ops-cadence-checklist.md</code></p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/","title":"2026 Annual Campaign Controlled Restart Plan","text":"<p>Date: 2026-01-27 Severity: Sev1 (major degradation - crawls running 27 days without completion) Status: Planning</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#executive-summary","title":"Executive Summary","text":"<p>The 2026 annual campaign (3 jobs: hc, phac, cihr) has been running since Jan 1 with persistent stability issues:</p> <ul> <li>Job 6 (HC): 67 temp dirs, 30 auto-recoveries, state file 2.4 days stale</li> <li>Job 7 (PHAC): stalled 36+ minutes, 11 container restarts, 12 temp dirs</li> <li>Job 8 (CIHR): regressed from 45% to 38%, permission errors, 4 temp dirs</li> </ul> <p>Goal: Controlled restart that preserves all captured WARCs while fixing infrastructure issues.</p> <p>Risk: Data loss if WARCs in <code>.tmp*</code> directories are deleted before consolidation.</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#pre-flight-current-state-assessment","title":"Pre-Flight: Current State Assessment","text":""},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#known-issues","title":"Known Issues","text":"<ol> <li>Stale sshfs mounts (Errno 107) on jobs 7 and 8 output directories</li> <li>Permission denied on Job 8's <code>.tmp_zcuywum/collections</code></li> <li>Disk usage at 68% (9GB/12.8h burn rate)</li> <li>State file staleness (Job 6: 2.4 days, suggesting state not persisting)</li> </ol>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#data-to-preserve","title":"Data to Preserve","text":"Job Temp Dirs Est. WARCs Notes 6 (hc) 67 Unknown (many) Excessive temp dirs suggest many restart phases 7 (phac) 12 Unknown 11 container restarts 8 (cihr) 4 11 discovered 38% complete on latest run"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#phase-1-snapshot-and-document-current-state-15-min","title":"Phase 1: Snapshot and Document Current State (15 min)","text":"<p>Purpose: Create recovery point before any changes.</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#11-full-status-snapshot","title":"1.1 Full Status Snapshot","text":"<pre><code>cd /opt/healtharchive-backend\n\n# Save full status to file\n./scripts/vps-crawl-status.sh --year 2026 &gt; /tmp/crawl-status-$(date -u +%Y%m%dT%H%M%SZ).txt 2&gt;&amp;1\n\n# Document disk state\ndf -h | tee /tmp/disk-state-$(date -u +%Y%m%dT%H%M%SZ).txt\n\n# Document mount state\nmount | grep healtharchive | tee /tmp/mounts-$(date -u +%Y%m%dT%H%M%SZ).txt\n\n# Document running containers\ndocker ps --format 'table {{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.Status}}' | tee /tmp/docker-ps-$(date -u +%Y%m%dT%H%M%SZ).txt\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#12-inventory-existing-warcs-critical","title":"1.2 Inventory Existing WARCs (Critical)","text":"<p>IMPORTANT: Record all WARC locations BEFORE any changes.</p> <pre><code># For each job, document WARCs found\nfor job_dir in /srv/healtharchive/jobs/*/20260101*; do\n  echo \"=== $job_dir ===\" &gt;&gt; /tmp/warc-inventory.txt\n  find \"$job_dir\" -name \"*.warc.gz\" -o -name \"*.warc\" 2&gt;/dev/null | \\\n    while read f; do\n      stat --format=\"%s %Y %n\" \"$f\" 2&gt;/dev/null || echo \"STAT_FAILED: $f\"\n    done &gt;&gt; /tmp/warc-inventory.txt\ndone\n\n# Count totals\necho \"=== WARC Counts ===\" &gt;&gt; /tmp/warc-inventory.txt\nfor job_dir in /srv/healtharchive/jobs/*/20260101*; do\n  count=$(find \"$job_dir\" -name \"*.warc.gz\" -o -name \"*.warc\" 2&gt;/dev/null | wc -l)\n  echo \"$job_dir: $count WARCs\" &gt;&gt; /tmp/warc-inventory.txt\ndone\n\ncat /tmp/warc-inventory.txt\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#13-document-state-files","title":"1.3 Document State Files","text":"<pre><code># Copy state files for reference\nmkdir -p /tmp/state-backup-$(date -u +%Y%m%d)\nfor job_dir in /srv/healtharchive/jobs/*/20260101*; do\n  job_name=$(basename \"$job_dir\")\n  cp -v \"$job_dir/.archive_state.json\" \"/tmp/state-backup-$(date -u +%Y%m%d)/${job_name}.state.json\" 2&gt;/dev/null || echo \"No state file: $job_dir\"\ndone\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#phase-2-stop-all-crawl-activity-10-min","title":"Phase 2: Stop All Crawl Activity (10 min)","text":"<p>Purpose: Prevent further changes while we assess and repair.</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#21-stop-worker-service","title":"2.1 Stop Worker Service","text":"<pre><code>sudo systemctl stop healtharchive-worker.service\nsudo systemctl status healtharchive-worker.service --no-pager -l\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#22-disable-auto-recovery-timers-temporarily","title":"2.2 Disable Auto-Recovery Timers (Temporarily)","text":"<pre><code># Disable crawl auto-recover\nsudo mv /etc/healtharchive/crawl-auto-recover-enabled /etc/healtharchive/crawl-auto-recover-enabled.disabled 2&gt;/dev/null || echo \"Already disabled\"\n\n# Disable storage hotpath auto-recover\nsudo mv /etc/healtharchive/storage-hotpath-auto-recover-enabled /etc/healtharchive/storage-hotpath-auto-recover-enabled.disabled 2&gt;/dev/null || echo \"Already disabled\"\n\n# Disable worker auto-start\nsudo mv /etc/healtharchive/worker-auto-start-enabled /etc/healtharchive/worker-auto-start-enabled.disabled 2&gt;/dev/null || echo \"Already disabled\"\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#23-stop-any-running-zimit-containers","title":"2.3 Stop Any Running Zimit Containers","text":"<pre><code># List zimit containers\ndocker ps --format '{{.ID}} {{.Image}} {{.Names}}' | grep -E 'zimit|openzim'\n\n# Stop them gracefully (adjust IDs as needed)\ndocker ps --format '{{.ID}} {{.Image}}' | grep -E 'zimit|openzim' | awk '{print $1}' | xargs -r docker stop\n\n# Verify stopped\ndocker ps | grep -E 'zimit|openzim' || echo \"No running zimit containers\"\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#phase-3-fix-infrastructure-issues-20-min","title":"Phase 3: Fix Infrastructure Issues (20 min)","text":""},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#31-fix-stale-mounts","title":"3.1 Fix Stale Mounts","text":"<p>Follow <code>docs/operations/playbooks/storagebox-sshfs-stale-mount-recovery.md</code>:</p> <pre><code># Check Storage Box base mount\nls -la /srv/healtharchive/storagebox &gt;/dev/null &amp;&amp; echo \"OK: storagebox readable\" || echo \"BAD: storagebox unreadable\"\n\n# Identify stale job mounts\nfor job_dir in /srv/healtharchive/jobs/*/20260101*; do\n  ls \"$job_dir\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; echo \"OK: $job_dir\" || echo \"STALE: $job_dir\"\ndone\n\n# Unmount stale hot paths (adjust paths based on actual findings)\nsudo umount -l /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101 2&gt;/dev/null || true\nsudo umount -l /srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101 2&gt;/dev/null || true\n\n# Re-apply tiering with repair\nsudo ./scripts/vps-warc-tiering-bind-mounts.sh --apply --repair-stale-mounts\n\n# Verify mounts are healthy\nfor job_dir in /srv/healtharchive/jobs/*/20260101*; do\n  ls \"$job_dir\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; echo \"OK: $job_dir\" || echo \"STILL_BROKEN: $job_dir\"\ndone\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#32-fix-permission-issues","title":"3.2 Fix Permission Issues","text":"<pre><code># For Job 8 (cihr) with permission denied on .tmp_zcuywum\njob_dir=\"/srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101\"\n\n# Use Docker alpine container to fix perms (avoids needing host sudo on files)\ndocker run --rm -v \"$job_dir:/output\" alpine chmod -R a+rX /output/.tmp* 2&gt;/dev/null || {\n  # Fallback: use sudo if Docker approach fails\n  sudo chmod -R a+rX \"$job_dir\"/.tmp* 2&gt;/dev/null || echo \"Permission fix may need manual intervention\"\n}\n\n# Verify\nls -la \"$job_dir\"/.tmp*/collections/ 2&gt;/dev/null | head -5\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#33-verify-disk-space","title":"3.3 Verify Disk Space","text":"<pre><code>df -h /srv/healtharchive/jobs\n\n# If above 70%, identify large temp dirs for later cleanup\ndu -sh /srv/healtharchive/jobs/*/20260101*/.tmp* 2&gt;/dev/null | sort -h | tail -20\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#phase-4-assess-and-consolidate-existing-warcs-30-min","title":"Phase 4: Assess and Consolidate Existing WARCs (30 min)","text":"<p>Purpose: Secure all captured data before any restart decisions.</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#41-verify-warcs-per-job","title":"4.1 Verify WARCs Per Job","text":"<pre><code>source /etc/healtharchive/backend.env\ncd /opt/healtharchive-backend\nsource .venv/bin/activate\n\n# Job 6 (HC)\nha-backend show-job --id 6\necho \"--- WARC Discovery ---\"\npython3 -c \"\nfrom ha_backend.indexing.warc_discovery import discover_temp_warcs_for_job\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get(6)\nwarcs = discover_temp_warcs_for_job(job)\nprint(f'Job 6 (HC): {len(warcs)} WARCs discovered')\nfor w in warcs[:5]: print(f'  {w}')\nif len(warcs) &gt; 5: print(f'  ... and {len(warcs)-5} more')\n\"\n\n# Job 7 (PHAC)\nha-backend show-job --id 7\necho \"--- WARC Discovery ---\"\npython3 -c \"\nfrom ha_backend.indexing.warc_discovery import discover_temp_warcs_for_job\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get(7)\nwarcs = discover_temp_warcs_for_job(job)\nprint(f'Job 7 (PHAC): {len(warcs)} WARCs discovered')\nfor w in warcs[:5]: print(f'  {w}')\nif len(warcs) &gt; 5: print(f'  ... and {len(warcs)-5} more')\n\"\n\n# Job 8 (CIHR)\nha-backend show-job --id 8\necho \"--- WARC Discovery ---\"\npython3 -c \"\nfrom ha_backend.indexing.warc_discovery import discover_temp_warcs_for_job\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get(8)\nwarcs = discover_temp_warcs_for_job(job)\nprint(f'Job 8 (CIHR): {len(warcs)} WARCs discovered')\nfor w in warcs[:5]: print(f'  {w}')\nif len(warcs) &gt; 5: print(f'  ... and {len(warcs)-5} more')\n\"\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#42-verify-warc-integrity-level-1","title":"4.2 Verify WARC Integrity (Level 1)","text":"<p>CRITICAL: Do this BEFORE any consolidation or cleanup.</p> <pre><code># Verify WARCs for each job (Level 1 = gzip integrity check)\nha-backend verify-warcs --job-id 6 --level 1 --json-out /tmp/verify-warcs-6.json\nha-backend verify-warcs --job-id 7 --level 1 --json-out /tmp/verify-warcs-7.json\nha-backend verify-warcs --job-id 8 --level 1 --json-out /tmp/verify-warcs-8.json\n\n# Review results\nfor f in /tmp/verify-warcs-*.json; do\n  echo \"=== $f ===\"\n  python3 -c \"import json; d=json.load(open('$f')); print(f\\\"passed={d.get('passed',0)} failed={d.get('failed',0)}\\\")\"\ndone\n</code></pre> <p>If verification shows failures: Use <code>--apply-quarantine</code> to move corrupt WARCs aside (only if job is NOT running):</p> <pre><code># ONLY if failures detected and you want to quarantine corrupt files:\n# ha-backend verify-warcs --job-id &lt;ID&gt; --level 1 --apply-quarantine\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#43-consolidate-warcs-to-stable-location","title":"4.3 Consolidate WARCs to Stable Location","text":"<p>Purpose: Move WARCs from <code>.tmp*</code> to stable <code>warcs/</code> directory with hardlinks.</p> <pre><code># Dry-run first for each job\nha-backend consolidate-warcs --job-id 6 --dry-run\nha-backend consolidate-warcs --job-id 7 --dry-run\nha-backend consolidate-warcs --job-id 8 --dry-run\n\n# If dry-run looks good, apply consolidation\nha-backend consolidate-warcs --job-id 6\nha-backend consolidate-warcs --job-id 7\nha-backend consolidate-warcs --job-id 8\n\n# Verify stable WARCs exist\nfor job_id in 6 7 8; do\n  job_dir=$(python3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get($job_id)\nprint(job.output_dir)\n\")\n  echo \"=== Job $job_id: $job_dir/warcs/ ===\"\n  ls -la \"$job_dir/warcs/\" 2&gt;/dev/null | head -5 || echo \"No stable warcs dir\"\n  ls -la \"$job_dir/warcs/manifest.json\" 2&gt;/dev/null || echo \"No manifest\"\ndone\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#phase-5-decision-point-restart-strategy","title":"Phase 5: Decision Point - Restart Strategy","text":"<p>At this point, you have: - All WARCs consolidated to stable <code>warcs/</code> directories - Infrastructure issues fixed - Worker stopped</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#option-a-resume-existing-crawls-lower-risk-slower","title":"Option A: Resume Existing Crawls (Lower Risk, Slower)","text":"<p>Use this if: - WARCs are mostly complete for some sources - You want to preserve crawl continuity - Time is not critical</p> <p>Pros: Preserves existing progress, zimit may resume from checkpoint Cons: May hit same stability issues, existing adaptation budgets may be exhausted</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#option-b-fresh-crawls-with-warc-consolidation-recommended","title":"Option B: Fresh Crawls with WARC Consolidation (Recommended)","text":"<p>Use this if: - Crawls have been unstable for weeks - Adaptation budgets are exhausted - You want a clean slate but keep existing WARCs</p> <p>Pros: Fresh adaptation budgets, cleaner state, more predictable behavior Cons: Starts crawl from scratch (but WARCs consolidate in final build)</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#option-c-mark-as-completed-and-index-partial-data","title":"Option C: Mark as Completed and Index Partial Data","text":"<p>Use this if: - Time pressure to have SOME data available - Current WARCs represent meaningful coverage - You plan to run a supplemental crawl later</p> <p>Pros: Immediate availability of partial data Cons: Incomplete coverage, may need follow-up crawl</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#phase-6a-resume-existing-crawls","title":"Phase 6A: Resume Existing Crawls","text":""},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#6a1-recover-stale-jobs","title":"6A.1 Recover Stale Jobs","text":"<pre><code># Mark jobs as retryable (preserves state for resume)\nha-backend recover-stale-jobs --older-than-minutes 5 --dry-run\nha-backend recover-stale-jobs --older-than-minutes 5 --apply\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#6a2-reset-retry-budgets-if-needed","title":"6A.2 Reset Retry Budgets (If Needed)","text":"<p>If jobs have exhausted container restart budgets:</p> <pre><code># Check current restart counts\nfor job_id in 6 7 8; do\n  job_dir=$(python3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get($job_id)\nprint(job.output_dir)\n\")\n  echo \"=== Job $job_id ===\"\n  cat \"$job_dir/.archive_state.json\" 2&gt;/dev/null | python3 -m json.tool | grep -E \"restarts|reductions|rotations\"\ndone\n\n# If restart budget exhausted, reset state (preserves temp_dirs)\nfor job_id in 6 7 8; do\n  job_dir=$(python3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get($job_id)\nprint(job.output_dir)\n\")\n  # Edit state to reset counts but keep temp_dirs\n  python3 -c \"\nimport json\nfrom pathlib import Path\nstate_file = Path('$job_dir/.archive_state.json')\nif state_file.exists():\n    state = json.loads(state_file.read_text())\n    state['container_restarts_done'] = 0\n    state['worker_reductions_done'] = 0\n    state['vpn_rotations_done'] = 0\n    state_file.write_text(json.dumps(state, indent=2))\n    print(f'Reset adaptation counts for job $job_id')\n\"\ndone\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#6a3-restart-worker","title":"6A.3 Restart Worker","text":"<pre><code># Re-enable automation\nsudo mv /etc/healtharchive/crawl-auto-recover-enabled.disabled /etc/healtharchive/crawl-auto-recover-enabled 2&gt;/dev/null || true\nsudo mv /etc/healtharchive/storage-hotpath-auto-recover-enabled.disabled /etc/healtharchive/storage-hotpath-auto-recover-enabled 2&gt;/dev/null || true\n\n# Start worker\nsudo systemctl start healtharchive-worker.service\nsudo systemctl status healtharchive-worker.service --no-pager -l\n\n# Monitor for 5 minutes\nsleep 60 &amp;&amp; ./scripts/vps-crawl-status.sh --year 2026 | head -40\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#phase-6b-fresh-crawls-with-warc-consolidation-recommended","title":"Phase 6B: Fresh Crawls with WARC Consolidation (Recommended)","text":""},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#6b1-prepare-jobs-for-fresh-start","title":"6B.1 Prepare Jobs for Fresh Start","text":"<p>CRITICAL: Do NOT use <code>--overwrite</code> flag - it deletes prior WARCs!</p> <pre><code># For each job, delete state file but KEEP temp dirs and WARCs\nfor job_id in 6 7 8; do\n  job_dir=$(python3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get($job_id)\nprint(job.output_dir)\n\")\n\n  echo \"=== Job $job_id: $job_dir ===\"\n\n  # Backup state file first\n  cp \"$job_dir/.archive_state.json\" \"/tmp/state-backup-$(date -u +%Y%m%d)/job-$job_id-pre-fresh.json\" 2&gt;/dev/null || true\n\n  # Remove state file (crawl will start fresh but discover existing WARCs)\n  rm -f \"$job_dir/.archive_state.json\"\n  rm -f \"$job_dir/.zimit_resume.yaml\"\n\n  # Verify temp dirs still exist (DO NOT DELETE)\n  echo \"Temp dirs preserved:\"\n  ls -d \"$job_dir\"/.tmp* 2&gt;/dev/null | wc -l\ndone\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#6b2-reset-job-status-in-database","title":"6B.2 Reset Job Status in Database","text":"<pre><code># Reset jobs to queued status with fresh retry count\nsource /etc/healtharchive/backend.env\ncd /opt/healtharchive-backend\nsource .venv/bin/activate\n\npython3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nfrom datetime import datetime, timezone\n\nsession = next(get_session())\n\nfor job_id in [6, 7, 8]:\n    job = session.query(ArchiveJob).get(job_id)\n    if job:\n        print(f'Resetting job {job_id} ({job.source.code}): {job.status} -&gt; queued')\n        job.status = 'queued'\n        job.retry_count = 0\n        job.crawler_exit_code = None\n        job.crawler_status = None\n        job.started_at = None\n        job.finished_at = None\n        job.queued_at = datetime.now(timezone.utc)\n\nsession.commit()\nprint('Jobs reset successfully')\n\"\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#6b3-consider-staggered-restarts","title":"6B.3 Consider Staggered Restarts","text":"<p>To reduce resource contention:</p> <pre><code># Option: Run one job at a time instead of all three\n# Start with the smallest (CIHR)\nha-backend run-db-job --id 8  # Run synchronously to monitor\n\n# Or use detached mode\npython3 ./scripts/vps-run-db-job-detached.py --job-id 8\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#6b4-restart-worker-for-automated-processing","title":"6B.4 Restart Worker for Automated Processing","text":"<pre><code># Re-enable automation\nsudo mv /etc/healtharchive/crawl-auto-recover-enabled.disabled /etc/healtharchive/crawl-auto-recover-enabled 2&gt;/dev/null || true\nsudo mv /etc/healtharchive/storage-hotpath-auto-recover-enabled.disabled /etc/healtharchive/storage-hotpath-auto-recover-enabled 2&gt;/dev/null || true\n\n# Start worker\nsudo systemctl start healtharchive-worker.service\n\n# Worker will pick up queued jobs automatically\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#phase-6c-mark-as-completed-and-index-partial-data","title":"Phase 6C: Mark as Completed and Index Partial Data","text":""},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#6c1-set-jobs-to-completed","title":"6C.1 Set Jobs to Completed","text":"<pre><code>source /etc/healtharchive/backend.env\ncd /opt/healtharchive-backend\nsource .venv/bin/activate\n\npython3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nfrom datetime import datetime, timezone\n\nsession = next(get_session())\n\nfor job_id in [6, 7, 8]:\n    job = session.query(ArchiveJob).get(job_id)\n    if job:\n        print(f'Marking job {job_id} ({job.source.code}) as completed')\n        job.status = 'completed'\n        job.crawler_exit_code = 0\n        job.crawler_status = 'completed_partial'  # Indicates manual completion\n        job.finished_at = datetime.now(timezone.utc)\n\nsession.commit()\nprint('Jobs marked as completed')\n\"\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#6c2-run-indexing-manually","title":"6C.2 Run Indexing Manually","text":"<pre><code># Index each job\nha-backend index-job --id 6\nha-backend index-job --id 7\nha-backend index-job --id 8\n\n# Verify indexing\nfor job_id in 6 7 8; do\n  echo \"=== Job $job_id ===\"\n  ha-backend show-job --id $job_id | grep -E \"status|indexed_page\"\ndone\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#phase-7-post-restart-verification","title":"Phase 7: Post-Restart Verification","text":""},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#71-monitor-crawl-progress","title":"7.1 Monitor Crawl Progress","text":"<pre><code># Check every 30 minutes for the first 2 hours\nwatch -n 1800 './scripts/vps-crawl-status.sh --year 2026 | head -60'\n\n# Or manual checks\n./scripts/vps-crawl-status.sh --year 2026\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#72-verify-disk-space-trend","title":"7.2 Verify Disk Space Trend","text":"<pre><code># Monitor disk usage\nwatch -n 300 'df -h /srv/healtharchive/jobs &amp;&amp; echo \"---\" &amp;&amp; du -sh /srv/healtharchive/jobs/*/20260101*/.tmp* 2&gt;/dev/null | sort -h | tail -10'\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#73-check-for-recurring-issues","title":"7.3 Check for Recurring Issues","text":"<pre><code># Check auto-recovery frequency\ncat /srv/healtharchive/ops/watchdog/crawl-auto-recover.json | python3 -m json.tool | tail -30\n\n# Check for new Errno 107 errors\njournalctl -u healtharchive-worker --since \"1 hour ago\" | grep -i \"errno 107\" || echo \"No Errno 107 errors\"\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#phase-8-post-indexing-cleanup","title":"Phase 8: Post-Indexing Cleanup","text":"<p>ONLY after jobs are <code>indexed</code>:</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#81-safe-temp-cleanup","title":"8.1 Safe Temp Cleanup","text":"<pre><code># Verify jobs are indexed\nfor job_id in 6 7 8; do\n  ha-backend show-job --id $job_id | grep -E \"^Status:\"\ndone\n\n# Dry-run cleanup\nha-backend cleanup-job --id 6 --mode temp-nonwarc --dry-run\nha-backend cleanup-job --id 7 --mode temp-nonwarc --dry-run\nha-backend cleanup-job --id 8 --mode temp-nonwarc --dry-run\n\n# Apply cleanup (consolidates WARCs, rewrites snapshot paths, deletes .tmp*)\nha-backend cleanup-job --id 6 --mode temp-nonwarc\nha-backend cleanup-job --id 7 --mode temp-nonwarc\nha-backend cleanup-job --id 8 --mode temp-nonwarc\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#82-verify-replay-works","title":"8.2 Verify Replay Works","text":"<pre><code># Test replay URLs for a few snapshots\n./scripts/vps-replay-smoke-textfile.py --dry-run\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#rollback-procedures","title":"Rollback Procedures","text":""},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#if-crawl-immediately-fails-after-restart","title":"If Crawl Immediately Fails After Restart","text":"<pre><code># Stop worker\nsudo systemctl stop healtharchive-worker.service\n\n# Check logs\njournalctl -u healtharchive-worker --since \"10 minutes ago\" | tail -100\n\n# Recover stale jobs\nha-backend recover-stale-jobs --older-than-minutes 5 --apply\n\n# Investigate before restarting\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#if-warcs-are-accidentally-lost","title":"If WARCs Are Accidentally Lost","text":"<pre><code># Check if stable WARCs exist\nls -la /srv/healtharchive/jobs/*/20260101*/warcs/\n\n# If no stable WARCs and .tmp* deleted, recovery options:\n# 1. Restore from Storage Box cold tier (if tiered)\n# 2. Restore from backup (if available)\n# 3. Re-crawl from scratch (last resort)\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#data-quality-verification-checklist","title":"Data Quality Verification Checklist","text":"<p>After indexing completes, verify:</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#coverage-check","title":"Coverage Check","text":"<pre><code># Compare indexed pages to previous annual campaigns\nsource /etc/healtharchive/backend.env\ncd /opt/healtharchive-backend\nsource .venv/bin/activate\n\npython3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import Snapshot, ArchiveJob\nfrom sqlalchemy import func\n\nsession = next(get_session())\n\n# 2026 campaign stats\nfor job_id in [6, 7, 8]:\n    job = session.query(ArchiveJob).get(job_id)\n    count = session.query(Snapshot).filter(Snapshot.job_id == job_id).count()\n    print(f'{job.source.code} (2026): {count} snapshots, indexed_page_count={job.indexed_page_count}')\n\"\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#search-quality-spot-check","title":"Search Quality Spot Check","text":"<pre><code># Run golden queries\n./scripts/search-eval-capture.sh --out-dir /tmp/ha-search-eval-2026 --page-size 20\n\n# Check results\nls -la /tmp/ha-search-eval-2026/\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#api-health","title":"API Health","text":"<pre><code>curl -s https://api.healtharchive.ca/api/health | python3 -m json.tool\ncurl -s https://api.healtharchive.ca/api/stats | python3 -m json.tool\ncurl -s \"https://api.healtharchive.ca/api/search?q=vaccines&amp;source=hc\" | python3 -c \"import sys,json; d=json.load(sys.stdin); print(f'Results: {d.get(\\\"totalResults\\\", 0)}')\"\n</code></pre>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#timeline-estimates","title":"Timeline Estimates","text":"Phase Duration Notes Phase 1: Snapshot 15 min Document current state Phase 2: Stop activity 10 min Stop worker and containers Phase 3: Fix infra 20 min Mounts, permissions Phase 4: Assess/Consolidate 30 min WARC verification and consolidation Phase 5: Decision 10 min Choose restart strategy Phase 6: Execute restart 15 min Apply chosen strategy Phase 7: Monitor Ongoing Until crawls complete Phase 8: Cleanup 15 min After indexing <p>Total active time: ~2 hours Crawl completion: 2-7 days depending on strategy and source sizes</p>"},{"location":"operations/playbooks/2026-01-annual-campaign-controlled-restart/#references","title":"References","text":"<ul> <li><code>docs/operations/playbooks/storagebox-sshfs-stale-mount-recovery.md</code></li> <li><code>docs/operations/playbooks/crawl-stalls.md</code></li> <li><code>docs/operations/playbooks/incident-response.md</code></li> <li><code>docs/operations/playbooks/warc-integrity-verification.md</code></li> <li><code>docs/operations/playbooks/cleanup-automation.md</code></li> <li><code>docs/tutorials/debug-crawl.md</code></li> <li><code>src/archive_tool/docs/documentation.md</code></li> </ul>"},{"location":"operations/playbooks/admin-proxy/","title":"Admin proxy (browser-friendly ops triage; VPS)","text":"<p>Goal: make it easy to browse admin/metrics endpoints in a browser without copying tokens into the browser.</p> <p>This is a lightweight alternative to building a bespoke admin UI.</p>"},{"location":"operations/playbooks/admin-proxy/#design","title":"Design","text":"<ul> <li>Runs a tiny reverse proxy on the VPS, bound to loopback only (<code>127.0.0.1</code>).</li> <li>Proxies read-only GET requests to:</li> <li><code>/api/admin/**</code></li> <li><code>/metrics</code></li> <li>Adds the backend admin token server-side from:</li> <li><code>/etc/healtharchive/observability/prometheus_backend_admin_token</code></li> <li>You access it from your laptop via SSH port-forwarding (tailnet-only SSH).</li> </ul> <p>Security notes:</p> <ul> <li>No new public ports.</li> <li>Browser never sees the admin token.</li> <li>Anyone with shell access to the VPS can reach <code>127.0.0.1</code>, so treat VPS access as privileged.</li> </ul>"},{"location":"operations/playbooks/admin-proxy/#install-apply-vps","title":"Install / apply (VPS)","text":"<p>1) Pull the latest repo:</p> <ul> <li><code>cd /opt/healtharchive-backend</code></li> <li><code>git pull</code></li> </ul> <p>2) Dry-run:</p> <ul> <li><code>./scripts/vps-install-ops-admin-proxy.sh</code></li> </ul> <p>3) Apply:</p> <ul> <li><code>sudo ./scripts/vps-install-ops-admin-proxy.sh --apply</code></li> </ul>"},{"location":"operations/playbooks/admin-proxy/#verify-vps","title":"Verify (VPS)","text":"<ul> <li><code>curl -s http://127.0.0.1:8002/-/health</code></li> <li><code>curl -s http://127.0.0.1:8002/api/admin/jobs?limit=1 | head</code></li> <li><code>curl -s http://127.0.0.1:8002/metrics | head</code></li> </ul>"},{"location":"operations/playbooks/admin-proxy/#use-from-your-laptop-ssh-port-forward","title":"Use from your laptop (SSH port-forward)","text":"<p>1) Start a tunnel:</p> <ul> <li><code>ssh -N -L 8002:127.0.0.1:8002 haadmin@&lt;vps-tailscale-ip&gt;</code></li> </ul> <p>2) Open in your browser:</p> <ul> <li><code>http://127.0.0.1:8002/</code></li> </ul> <p>Useful endpoints:</p> <ul> <li><code>http://127.0.0.1:8002/api/admin/jobs</code></li> <li><code>http://127.0.0.1:8002/api/admin/jobs/status-counts</code></li> <li><code>http://127.0.0.1:8002/api/admin/reports</code></li> <li><code>http://127.0.0.1:8002/api/admin/search-debug?q=covid</code></li> <li><code>http://127.0.0.1:8002/metrics</code></li> </ul>"},{"location":"operations/playbooks/admin-proxy/#rollback","title":"Rollback","text":"<ul> <li><code>sudo systemctl disable --now healtharchive-admin-proxy.service</code></li> <li><code>sudo rm -f /etc/systemd/system/healtharchive-admin-proxy.service</code></li> <li><code>sudo rm -f /usr/local/bin/healtharchive-admin-proxy</code></li> <li><code>sudo systemctl daemon-reload</code></li> </ul>"},{"location":"operations/playbooks/adoption-signals/","title":"Adoption signals playbook (quarterly)","text":"<p>Goal: record lightweight, public-safe \u201cis anyone using this?\u201d signals without storing private contact details.</p> <p>Canonical references:</p> <ul> <li>Template: <code>../adoption-signals-log-template.md</code></li> <li>Ops roadmap (remaining external work): <code>../healtharchive-ops-roadmap.md</code></li> </ul>"},{"location":"operations/playbooks/adoption-signals/#procedure-high-level","title":"Procedure (high level)","text":"<ol> <li>Create a new dated entry using <code>../adoption-signals-log-template.md</code>.</li> <li>Store it on the VPS under:</li> <li><code>/srv/healtharchive/ops/adoption/</code></li> </ol> <p>Rules:</p> <ul> <li>Links + aggregate counts only.</li> <li>No private emails, names, or identifying details unless permission is explicit and documented elsewhere.</li> </ul>"},{"location":"operations/playbooks/adoption-signals/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>A dated adoption signals entry exists under <code>/srv/healtharchive/ops/adoption/</code>.</li> </ul>"},{"location":"operations/playbooks/annual-campaign/","title":"Annual campaign playbook (operators)","text":"<p>Goal: keep the annual capture cycle predictable and operationally boring.</p> <p>Canonical references:</p> <ul> <li>Annual scope/seeds (source of truth): <code>../annual-campaign.md</code></li> <li>Automation implementation plan: <code>../automation-implementation-plan.md</code></li> <li>systemd units + enablement: <code>../../deployment/systemd/README.md</code></li> </ul>"},{"location":"operations/playbooks/annual-campaign/#before-jan-01-utc-readiness","title":"Before Jan 01 UTC (readiness)","text":"<ul> <li>Review and update scope/seeds in <code>../annual-campaign.md</code> (docs-only change).</li> <li>Ensure you have storage headroom and backups are healthy.</li> <li>Run the crawl preflight audit (recommended):</li> <li><code>YEAR=2026; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\"</code></li> <li>See: <code>crawl-preflight.md</code></li> <li>If the annual scheduler is enabled, dry-run it:</li> <li><code>sudo systemctl start healtharchive-schedule-annual-dry-run.service</code></li> <li><code>sudo journalctl -u healtharchive-schedule-annual-dry-run.service -n 200 --no-pager</code></li> <li>Enable the annual campaign sentinel (recommended; sends notification on failure):</li> <li><code>sudo systemctl enable --now healtharchive-annual-campaign-sentinel.timer</code></li> <li>Optional: configure Healthchecks ping URL at <code>/etc/healtharchive/healthchecks.env</code>:<ul> <li><code>HEALTHARCHIVE_HC_PING_ANNUAL_SENTINEL=https://hc-ping.com/UUID_HERE</code></li> <li>Note: this env file may also contain legacy <code>HC_*</code> variables (DB backup + disk check).</li> </ul> </li> </ul>"},{"location":"operations/playbooks/annual-campaign/#duringafter-the-campaign-high-level","title":"During/after the campaign (high level)","text":"<ul> <li>Use the automation plan (<code>../automation-implementation-plan.md</code>) to decide what is enabled and what is manual.</li> <li>Prefer safe, idempotent entrypoints (systemd services/timers or the provided scripts).</li> <li>Annual jobs are scheduled with crawler monitoring enabled so stalls / error storms can trigger adaptive worker reduction.</li> </ul>"},{"location":"operations/playbooks/annual-campaign/#if-a-crawl-stalls-or-is-interrupted","title":"If a crawl stalls or is interrupted","text":"<ul> <li>If a crawl is stalled, check the job logs under its <code>output_dir</code> (look for <code>archive_*_attempt_*_*.combined.log</code>) and the worker journal:</li> <li><code>sudo journalctl -u healtharchive-worker -n 200 --no-pager</code></li> <li>If the VPS reboots (or the worker/service is killed) mid-crawl, a job can be left in <code>status=running</code>. Recover safely:</li> <li>Load env: <code>set -a; source /etc/healtharchive/backend.env; set +a</code></li> <li>Dry-run: <code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 180</code></li> <li>Apply: <code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 180 --apply</code></li> </ul>"},{"location":"operations/playbooks/annual-campaign/#manual-trigger-day-of","title":"Manual trigger (day-of)","text":"<p>If you want to run the annual sentinel immediately (safe; read-only except for metrics output):</p> <pre><code>sudo systemctl start healtharchive-annual-campaign-sentinel.service\nsudo journalctl -u healtharchive-annual-campaign-sentinel.service -n 200 --no-pager\n</code></pre>"},{"location":"operations/playbooks/annual-campaign/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>Annual scope is current in <code>../annual-campaign.md</code>.</li> <li>If automation is enabled, the scheduler and follow-up tasks run as intended and are verifiable in logs/artifacts.</li> </ul>"},{"location":"operations/playbooks/automation-maintenance/","title":"Automation maintenance playbook (systemd timers)","text":"<p>Goal: keep automation boring, observable, and explicitly controlled.</p> <p>Canonical references:</p> <ul> <li>systemd unit templates + enable/rollback: <code>../../deployment/systemd/README.md</code></li> <li>Verification ritual: <code>../automation-verification-rituals.md</code></li> </ul>"},{"location":"operations/playbooks/automation-maintenance/#installupdate-templates-after-repo-updates","title":"Install/update templates (after repo updates)","text":"<p>On the VPS:</p> <ul> <li><code>cd /opt/healtharchive-backend</code></li> <li><code>sudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker</code></li> </ul>"},{"location":"operations/playbooks/automation-maintenance/#bootstrap-ops-directories-one-time","title":"Bootstrap ops directories (one-time)","text":"<p>If <code>/srv/healtharchive/ops/</code> is not prepared:</p> <ul> <li><code>sudo ./scripts/vps-bootstrap-ops-dirs.sh</code></li> </ul>"},{"location":"operations/playbooks/automation-maintenance/#enablement-controls-sentinel-files","title":"Enablement controls (sentinel files)","text":"<p>Automation is intentionally gated by sentinel files under <code>/etc/healtharchive/</code>.</p> <p>Follow the enable/rollback steps in <code>../../deployment/systemd/README.md</code>.</p>"},{"location":"operations/playbooks/automation-maintenance/#verify-posture","title":"Verify posture","text":"<ul> <li><code>./scripts/verify_ops_automation.sh</code></li> <li>Spot-check logs:</li> <li><code>journalctl -u &lt;service&gt; -n 200</code></li> </ul>"},{"location":"operations/playbooks/cleanup-automation/","title":"Cleanup automation (safe temp cleanup)","text":"<p>Goal: remove <code>.tmp*</code> crawl directories from older indexed jobs without breaking replay.</p> <p>Canonical refs:</p> <ul> <li>cleanup command: <code>ha-backend cleanup-job --mode temp-nonwarc</code></li> <li>systemd unit templates: <code>../../deployment/systemd/README.md</code></li> <li>replay retention note: <code>../../operations/growth-constraints.md</code></li> </ul>"},{"location":"operations/playbooks/cleanup-automation/#what-this-does","title":"What this does","text":"<ul> <li>Picks indexed jobs older than a minimum age.</li> <li>Keeps the latest N per source.</li> <li>Runs safe cleanup (<code>temp-nonwarc</code>) to preserve WARCs.</li> <li>Emits node_exporter metrics:</li> <li><code>healtharchive_cleanup_applied_total</code></li> </ul>"},{"location":"operations/playbooks/cleanup-automation/#enablement-vps","title":"Enablement (VPS)","text":"<pre><code>sudo touch /etc/healtharchive/cleanup-automation-enabled\nsudo systemctl enable --now healtharchive-cleanup-automation.timer\n</code></pre>"},{"location":"operations/playbooks/cleanup-automation/#manual-dry-run","title":"Manual dry-run","text":"<p>Warning: starting <code>healtharchive-cleanup-automation.service</code> will apply cleanup (it is the automation entrypoint). Use the script directly for a dry-run preview.</p> <pre><code>sudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-cleanup-automation.py --config /opt/healtharchive-backend/ops/automation/cleanup-automation.toml --out-dir /tmp --out-file healtharchive_cleanup_dryrun.prom'\ncat /tmp/healtharchive_cleanup_dryrun.prom\n</code></pre>"},{"location":"operations/playbooks/cleanup-automation/#if-cleanup-fails","title":"If cleanup fails","text":"<ol> <li>Check the job output directory exists and is readable:    <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend show-job --id &lt;JOB_ID&gt;\n</code></pre></li> <li>Run the cleanup command manually:    <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend cleanup-job --id &lt;JOB_ID&gt; --mode temp-nonwarc --dry-run\n</code></pre></li> </ol>"},{"location":"operations/playbooks/cleanup-automation/#config","title":"Config","text":"<p>Edit <code>ops/automation/cleanup-automation.toml</code> to adjust age, caps, and retain count.</p>"},{"location":"operations/playbooks/coverage-guardrails/","title":"Coverage guardrails (annual regression checks)","text":"<p>Goal: detect large year-over-year coverage drops after annual jobs are indexed.</p> <p>Canonical refs:</p> <ul> <li>systemd unit templates: <code>../../deployment/systemd/README.md</code></li> <li>monitoring checklist: <code>../monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"operations/playbooks/coverage-guardrails/#what-this-does","title":"What this does","text":"<ul> <li>Compares the latest indexed annual job for each source to the prior year.</li> <li>Emits node_exporter textfile metrics:</li> <li><code>healtharchive_coverage_ratio{source=\"hc\",year=\"2026\"}</code></li> <li><code>healtharchive_coverage_regression{source=\"hc\",year=\"2026\"}</code></li> <li><code>healtharchive_coverage_warning{source=\"hc\",year=\"2026\"}</code></li> </ul>"},{"location":"operations/playbooks/coverage-guardrails/#enablement-vps","title":"Enablement (VPS)","text":"<pre><code>sudo touch /etc/healtharchive/coverage-guardrails-enabled\nsudo systemctl enable --now healtharchive-coverage-guardrails.timer\n</code></pre>"},{"location":"operations/playbooks/coverage-guardrails/#manual-check","title":"Manual check","text":"<pre><code>sudo systemctl start healtharchive-coverage-guardrails.service\nsudo journalctl -u healtharchive-coverage-guardrails.service -n 200 --no-pager\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_coverage_'\n</code></pre>"},{"location":"operations/playbooks/coverage-guardrails/#if-an-alert-fires","title":"If an alert fires","text":"<ol> <li>Identify the affected source and year from the metric labels.</li> <li>Confirm current and prior annual jobs:    <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend list-jobs --source hc --status indexed --limit 10\n/opt/healtharchive-backend/.venv/bin/ha-backend show-job --id &lt;JOB_ID&gt;\n</code></pre></li> <li>If the drop is real, inspect crawl logs for stalls/timeouts and consider:</li> <li>re-running the crawl (retryable),</li> <li>adjusting scope rules for that source,</li> <li>or filing a follow-up for annual tuning.</li> </ol>"},{"location":"operations/playbooks/coverage-guardrails/#config","title":"Config","text":"<p>Edit <code>ops/automation/coverage-guardrails.toml</code> to change thresholds.</p>"},{"location":"operations/playbooks/crawl-preflight/","title":"Crawl preflight audit playbook (production VPS)","text":"<p>Goal: catch obvious blockers before a large crawl (especially the Jan 01 UTC annual campaign).</p> <p>Canonical references:</p> <ul> <li>Production runbook: <code>../../deployment/production-single-vps.md</code></li> <li>Annual scope/seeds (source of truth): <code>../annual-campaign.md</code></li> <li>Growth/storage policy: <code>../growth-constraints.md</code></li> <li>Baseline drift (policy vs reality): <code>../baseline-drift.md</code></li> <li>Automation posture: <code>../automation-implementation-plan.md</code>, <code>../../deployment/systemd/README.md</code></li> </ul>"},{"location":"operations/playbooks/crawl-preflight/#preconditions","title":"Preconditions","text":"<ul> <li>You are on the VPS.</li> <li>Backend repo is present (default): <code>/opt/healtharchive-backend</code></li> <li>Venv exists at: <code>/opt/healtharchive-backend/.venv</code></li> <li>Backend env file exists: <code>/etc/healtharchive/backend.env</code></li> </ul>"},{"location":"operations/playbooks/crawl-preflight/#if-you-need-to-temporarily-defer-the-annual-crawl","title":"If you need to temporarily defer the annual crawl","text":"<p>If you aren\u2019t ready to run the annual campaign on Jan 01 UTC, disable the systemd timer and remove the automation sentinel:</p> <pre><code>sudo systemctl disable --now healtharchive-schedule-annual.timer\nsudo rm -f /etc/healtharchive/automation-enabled\n</code></pre> <p>Verify:</p> <pre><code>systemctl is-enabled healtharchive-schedule-annual.timer\nsystemctl status healtharchive-schedule-annual.timer\nls -la /etc/healtharchive/automation-enabled\n</code></pre> <p>Notes:</p> <ul> <li>This prevents automatic annual job enqueueing; it does not cancel any   already-queued jobs.</li> <li>The safe validation unit <code>healtharchive-schedule-annual-dry-run.service</code> can   still be run manually.</li> </ul>"},{"location":"operations/playbooks/crawl-preflight/#procedure-recommended","title":"Procedure (recommended)","text":"<ol> <li>Choose the annual campaign year:</li> <li>If it\u2019s before Jan 01 (UTC), use the upcoming year (e.g., Dec 2025 \u2192 <code>2026</code>).</li> <li>(Recommended) Run a rehearsal with caps (generates active-load evidence):</li> <li><code>cd /opt/healtharchive-backend</code></li> <li><code>./scripts/vps-smoke-crawl-rehearsal.sh --apply --source cihr --page-limit 25 --depth 1</code></li> <li>Run the preflight audit:</li> <li><code>cd /opt/healtharchive-backend</code></li> <li><code>YEAR=2026; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\"</code></li> </ol> <p>This writes a timestamped report directory under <code>/srv/healtharchive/ops/preflight/</code>.</p>"},{"location":"operations/playbooks/crawl-preflight/#if-it-fails-common-fixes","title":"If it fails (common fixes)","text":"<ul> <li>Campaign storage forecast fails (even if you\u2019re below 80% today): the annual campaign is projected to exceed disk headroom or the 80% review threshold. Follow the report output to free space or expand disk before Jan 01 UTC.</li> <li>Campaign storage forecast fails but you are using tiered storage (Storage Box): run preflight with the campaign tier root so the forecast uses the correct filesystem, e.g. <code>YEAR=2026; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\" --campaign-archive-root /srv/healtharchive/storagebox/jobs</code>.</li> <li>Rehearsal evidence (active crawl headroom) fails: you don\u2019t have a recent <code>--apply</code> rehearsal (or it recorded low MemAvailable / high swap). Run <code>./scripts/vps-smoke-crawl-rehearsal.sh --apply ...</code> to generate evidence, or upgrade the VPS / reduce crawl concurrency.</li> <li>CPU/RAM headroom fails: the VPS is already under sustained load / memory pressure (or swap usage). Stop other heavy work (indexing, other crawls), then re-run preflight; if it persists, reduce crawl concurrency or upgrade the VPS.</li> <li>Time sync (NTP) fails: fix time sync before crawling (TLS, scheduling, and log correlation all assume correct UTC).</li> <li>Docker daemon access fails: Docker is installed but not usable by the current user (or the daemon is down). Fix <code>systemctl status docker</code>, user group membership, and re-run.</li> <li>DB connectivity / Alembic-at-head fails: DB is down or the schema is behind the code version; apply migrations (<code>alembic upgrade head</code>) and re-run.</li> <li>Seed reachability fails: the annual seed URLs aren\u2019t reachable from the VPS right now; fix DNS/network/firewall issues (or investigate upstream downtime) before Jan 01 UTC.</li> <li>Disk usage is high (&gt;= 80%): pause, free space or expand disk before crawling.</li> <li>Backups are missing/stale: fix backups before crawling (don\u2019t run long jobs without recoverability).</li> <li><code>/api/health</code> fails on loopback: fix API/DB/service health first (check <code>systemctl status</code> + <code>journalctl</code>).</li> <li>Annual scheduler dry-run errors:</li> <li>Missing <code>Source</code> rows \u2192 run <code>ha-backend seed-sources</code>.</li> <li>Duplicated annual jobs for the year \u2192 resolve duplicates before scheduling.</li> <li>Active jobs blocking scheduling \u2192 finish/index them (or decide not to run annual yet).</li> <li>Temp cleanup candidates: the report lists indexed jobs that still have <code>.tmp*</code> dirs; use <code>ha-backend cleanup-job --mode temp-nonwarc</code> (safe) to reclaim space.</li> <li>Baseline drift failures: reconcile production with <code>docs/operations/production-baseline-policy.toml</code>, then re-run drift checks.</li> <li>Ops automation posture fails: enable required timers and sentinels (at minimum baseline drift), e.g. <code>sudo systemctl enable --now healtharchive-baseline-drift-check.timer &amp;&amp; sudo touch /etc/healtharchive/baseline-drift-enabled</code>.</li> <li>Admin/metrics auth check fails: ensure a real <code>HEALTHARCHIVE_ADMIN_TOKEN</code> is set in production and routing is correct.</li> </ul>"},{"location":"operations/playbooks/crawl-preflight/#optional-deep-checks","title":"Optional deep checks","text":"<ul> <li>Run a small crawl rehearsal (capped crawl + indexing, isolated sandbox DB). This is the best way to validate headroom under active crawl load, not just idle host metrics:</li> <li><code>cd /opt/healtharchive-backend</code></li> <li>Dry-run: <code>./scripts/vps-smoke-crawl-rehearsal.sh --source cihr</code></li> <li>Apply: <code>./scripts/vps-smoke-crawl-rehearsal.sh --apply --source cihr --page-limit 25 --depth 1</code></li> <li> <p>Evidence artifacts: <code>.../98-resource-monitor.jsonl</code> and <code>.../98-resource-summary.json</code></p> </li> <li> <p>Validate the systemd wrapper (safe dry-run):</p> </li> <li><code>sudo systemctl start healtharchive-schedule-annual-dry-run.service</code></li> <li><code>sudo journalctl -u healtharchive-schedule-annual-dry-run.service -n 200 --no-pager</code></li> <li>Capture a redacted \u201cbaseline inventory\u201d snapshot:</li> <li><code>OUT_DIR=\"/srv/healtharchive/ops/preflight/$(date -u +%Y%m%dT%H%M%SZ)\"; ./scripts/capture-baseline-inventory.sh --env-file /etc/healtharchive/backend.env --out \"$OUT_DIR/baseline-inventory.txt\"</code></li> </ul>"},{"location":"operations/playbooks/crawl-preflight/#optional-cleanup-disk-hygiene","title":"Optional cleanup (disk hygiene)","text":"<p>If you ran multiple rehearsals or preflight runs, keep only the most recent few directories as evidence and reclaim space.</p> <p>Keep the latest 3 rehearsal runs (removes older ones):</p> <pre><code>ls -1dt /srv/healtharchive/ops/rehearsal/* | tail -n +4 | sudo xargs -r rm -rf --\n</code></pre> <p>Keep the latest 10 preflight reports (removes older ones):</p> <pre><code>ls -1dt /srv/healtharchive/ops/preflight/* | tail -n +11 | sudo xargs -r rm -rf --\n</code></pre>"},{"location":"operations/playbooks/crawl-preflight/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li><code>YEAR=2026; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\"</code> exits <code>0</code>.</li> <li>The report directory exists and is retained as operator evidence for that crawl run.</li> </ul>"},{"location":"operations/playbooks/crawl-preflight/#during-the-crawl-ongoing-monitoring","title":"During the crawl (ongoing monitoring)","text":"<p>Once a large crawl is running, use the read-only status snapshot script for a quick \u201call the basics\u201d check:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-crawl-status.sh --year 2026\n</code></pre>"},{"location":"operations/playbooks/crawl-stalls/","title":"Crawl stalls (monitoring + recovery)","text":"<p>Use this playbook when a crawl job is running but appears stalled (no progress for an extended period), or when you receive the <code>HealthArchiveCrawlStalled</code> alert.</p> <p>Quick triage (recommended first):</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-crawl-status.sh --year 2026\n</code></pre> <p>Notes:</p> <ul> <li>The status script is read-only (no restarts, no DB writes); it\u2019s safe mid-crawl.</li> <li>If the combined log is very large and you only want recent timeout signals, use:</li> <li><code>./scripts/vps-crawl-status.sh --year 2026 --recent-lines 20000</code></li> </ul>"},{"location":"operations/playbooks/crawl-stalls/#1-identify-the-stalled-job","title":"1) Identify the stalled job","text":"<p>On the VPS:</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend list-jobs --status running --limit 10\n</code></pre> <p>Then inspect the specific job:</p> <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend show-job --id JOB_ID\n</code></pre>"},{"location":"operations/playbooks/crawl-stalls/#2-confirm-no-progress","title":"2) Confirm \u201cno progress\u201d","text":"<p>Find the newest combined log for the job\u2019s output directory:</p> <pre><code>JOBDIR=\"/srv/healtharchive/jobs/SOURCE/YYYYMMDDTHHMMSSZ__name\"\nls -lt \"${JOBDIR}\"/archive_*.combined.log | head -n 5\nLOG=\"$(ls -t \"${JOBDIR}\"/archive_*.combined.log | head -n 1)\"\n</code></pre> <p>Check the most recent crawlStatus line(s):</p> <pre><code>rg -n '\"context\":\"crawlStatus\"' \"${LOG}\" | tail -n 5\n</code></pre> <p>If <code>crawled</code> is not increasing for a long time (often with repeated <code>Navigation timeout</code> warnings), treat it as stalled.</p>"},{"location":"operations/playbooks/crawl-stalls/#3-recovery-safe-by-default","title":"3) Recovery (safe-by-default)","text":"<p>If you confirm the crawl is stalled and you want to restart it, do:</p> <pre><code># Stop the worker (interrupts the current crawl process).\nsudo systemctl stop healtharchive-worker.service\n\n# Mark the running job retryable so the worker can pick it up again.\nset -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs \\\n  --older-than-minutes 5 \\\n  --require-no-progress-seconds 3600 \\\n  --apply \\\n  --source SOURCE \\\n  --limit 5\n\n# Start the worker again.\nsudo systemctl start healtharchive-worker.service\n</code></pre> <p>Then confirm the worker picked the job up again and crawlStatus is moving:</p> <pre><code>sudo systemctl status healtharchive-worker.service --no-pager\nsudo journalctl -u healtharchive-worker.service -n 50 --no-pager\n</code></pre>"},{"location":"operations/playbooks/crawl-stalls/#notes","title":"Notes","text":"<ul> <li><code>archive_tool</code> has built-in monitoring/adaptation; most stalls should self-heal, but this recovery is the \u201cbreak glass\u201d operator workflow.</li> <li>Optional: you can enable the <code>healtharchive-crawl-auto-recover.timer</code> watchdog (sentinel: <code>/etc/healtharchive/crawl-auto-recover-enabled</code>) once you\u2019re confident in the thresholds/caps.</li> <li>The watchdog is designed to avoid interrupting a healthy crawl; when another job is actively making progress, it may \u201csoft recover\u201d zombie <code>status=running</code> jobs by marking them <code>retryable</code> without restarting the worker.</li> <li>If the watchdog is enabled but prints <code>SKIP ... max recoveries reached</code>, you can still do the manual recovery above, or (carefully) run the watchdog script once with a higher cap:   <pre><code>sudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-crawl-auto-recover.py --apply --max-recoveries-per-job-per-day 4'\n</code></pre></li> <li>If stalls repeat for the same URL(s), consider narrowing scope rules or adjusting crawler timeouts in the source\u2019s job configuration.</li> </ul>"},{"location":"operations/playbooks/dataset-release/","title":"Dataset release integrity playbook (quarterly)","text":"<p>Goal: confirm a dataset release exists and its checksums verify cleanly.</p> <p>Canonical reference:</p> <ul> <li><code>../dataset-release-runbook.md</code></li> </ul>"},{"location":"operations/playbooks/dataset-release/#procedure-high-level","title":"Procedure (high level)","text":"<ol> <li>Identify the latest dataset release (GitHub Releases, datasets repo).</li> <li>Download the release assets for the quarter/date you expect.</li> <li>Verify integrity:</li> <li><code>sha256sum -c SHA256SUMS</code></li> </ol>"},{"location":"operations/playbooks/dataset-release/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li><code>sha256sum -c SHA256SUMS</code> completes without errors for the latest release.</li> </ul>"},{"location":"operations/playbooks/deploy-and-verify/","title":"Deploy + verify playbook (production VPS)","text":"<p>Goal: deploy a known-good <code>main</code> and verify production matches policy.</p> <p>Canonical references:</p> <ul> <li>Production runbook: <code>../../deployment/production-single-vps.md</code></li> <li>Monitoring/CI gate: <code>../monitoring-and-ci-checklist.md</code></li> <li>Baseline drift: <code>../baseline-drift.md</code></li> </ul>"},{"location":"operations/playbooks/deploy-and-verify/#preconditions","title":"Preconditions","text":"<ul> <li>CI is green on the commit you intend to deploy.</li> <li>You are on the production VPS and can <code>sudo</code>.</li> </ul>"},{"location":"operations/playbooks/deploy-and-verify/#procedure","title":"Procedure","text":"<ol> <li> <p>Update the repo on the VPS:</p> </li> <li> <p><code>cd /opt/healtharchive-backend &amp;&amp; git pull</code></p> </li> <li> <p>Run the deploy gate (recommended one command):</p> </li> <li> <p><code>./scripts/vps-deploy.sh --apply --baseline-mode live</code></p> </li> </ol> <p>This includes:</p> <ul> <li>DB migrations</li> <li>service restarts</li> <li>baseline drift verification</li> <li>public surface verification</li> </ul> <p>If your change updates systemd unit templates or Prometheus alert rules, you can    apply those as part of the deploy:</p> <ul> <li><code>./scripts/vps-deploy.sh --apply --baseline-mode live --install-systemd-units --apply-alerting</code></li> </ul> <p>Notes:</p> <ul> <li><code>--apply-alerting</code> requires alerting to be configured on the VPS (webhook secret present at      <code>/etc/healtharchive/observability/alertmanager_webhook_url</code>).</li> </ul> <p>If you are updating the replay banner/template or replay service config on a    single-VPS deployment, include replay restart + banner install:</p> <ul> <li> <p><code>./scripts/vps-deploy.sh --apply --baseline-mode live --restart-replay</code></p> </li> <li> <p>If the deploy gate fails:</p> </li> <li> <p>Do not retry blindly.</p> </li> <li>Read the failure output:<ul> <li>drift report artifacts under <code>/srv/healtharchive/ops/baseline/</code></li> <li>verifier output from <code>verify_public_surface.py</code></li> </ul> </li> <li>Fix the underlying mismatch (production state vs policy) or intentionally update policy.</li> </ul>"},{"location":"operations/playbooks/deploy-and-verify/#quick-follow-ups-optional","title":"Quick follow-ups (optional)","text":"<ul> <li>Confirm timers/sentinels posture (if you operate automation):</li> <li><code>./scripts/verify_ops_automation.sh</code></li> </ul>"},{"location":"operations/playbooks/healthchecks-parity/","title":"Healthchecks.io parity (env \u2194 systemd \u2194 Healthchecks)","text":"<p>Do not enable or change production automations until the annual crawl/scrape is finished and the campaign jobs are indexed.</p> <p>Goal: ensure the Healthchecks.io dashboard is a faithful reflection of what the VPS actually runs (and only that).</p> <p>This playbook focuses on three sources of truth:</p> <p>1) systemd timers on the VPS (what actually runs) 2) <code>/etc/healtharchive/healthchecks.env</code> (which pings are wired on the VPS) 3) Healthchecks.io checks (what the dashboard expects to hear from)</p> <p>Key rule:</p> <ul> <li>A Healthchecks.io check should exist iff there is a corresponding ping URL in <code>/etc/healtharchive/healthchecks.env</code> (or a legacy <code>HC_*</code> URL used by the disk/backup scripts).</li> </ul> <p>If you follow that rule, the dashboard cannot drift into \u201cchecks we don\u2019t use\u201d or \u201cmissing checks for enabled jobs\u201d.</p>"},{"location":"operations/playbooks/healthchecks-parity/#current-state-as-of-2026-01-03","title":"Current state (as of 2026-01-03)","text":"<p>These pings are configured in <code>/etc/healtharchive/healthchecks.env</code>:</p> <ul> <li><code>HEALTHARCHIVE_HC_PING_REPLAY_RECONCILE</code> \u2192 <code>healtharchive-replay-reconcile.timer</code> (daily)</li> <li><code>HEALTHARCHIVE_HC_PING_SCHEDULE_ANNUAL</code> \u2192 <code>healtharchive-schedule-annual.timer</code> (yearly)</li> <li><code>HEALTHARCHIVE_HC_PING_ANNUAL_SENTINEL</code> \u2192 <code>healtharchive-annual-campaign-sentinel.timer</code> (yearly)</li> <li><code>HEALTHARCHIVE_HC_PING_BASELINE_DRIFT</code> \u2192 <code>healtharchive-baseline-drift-check.timer</code> (weekly)</li> <li><code>HEALTHARCHIVE_HC_PING_PUBLIC_VERIFY</code> \u2192 <code>healtharchive-public-surface-verify.timer</code> (daily)</li> <li><code>HEALTHARCHIVE_HC_PING_ANNUAL_SEARCH_VERIFY</code> \u2192 <code>healtharchive-annual-search-verify.timer</code> (daily)</li> <li><code>HEALTHARCHIVE_HC_PING_CHANGE_TRACKING</code> \u2192 <code>healtharchive-change-tracking.timer</code> (daily)</li> <li><code>HEALTHARCHIVE_HC_PING_COVERAGE_GUARDRAILS</code> \u2192 <code>healtharchive-coverage-guardrails.timer</code> (daily)</li> <li><code>HEALTHARCHIVE_HC_PING_REPLAY_SMOKE</code> \u2192 <code>healtharchive-replay-smoke.timer</code> (daily)</li> </ul> <p>Legacy script checks (separate from the systemd wrapper):</p> <ul> <li><code>HC_DB_BACKUP_URL</code> \u2192 <code>healtharchive-db-backup.timer</code> (daily)</li> <li><code>HC_DISK_URL</code> + <code>HC_DISK_THRESHOLD</code> \u2192 <code>healtharchive-disk-check.timer</code> (hourly)</li> </ul> <p>Known \u201cnot wired (by design) right now\u201d:</p> <ul> <li><code>HEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION</code> exists as a ping var in the installed unit, but:</li> <li><code>healtharchive-cleanup-automation.timer</code> is disabled, and</li> <li><code>/etc/healtharchive/cleanup-automation-enabled</code> sentinel is missing.</li> <li>Result: do not create a Healthchecks.io check or env var until cleanup automation is intentionally enabled.</li> </ul>"},{"location":"operations/playbooks/healthchecks-parity/#audit-checklist-safe-no-restarts","title":"Audit checklist (safe; no restarts)","text":""},{"location":"operations/playbooks/healthchecks-parity/#1-list-the-ping-vars-currently-configured-vps-env-file","title":"1) List the ping vars currently configured (VPS env file)","text":"<p>This prints only variable names (not URLs):</p> <pre><code>sudo awk -F= '$1 ~ /^(HEALTHARCHIVE_HC_PING_|HC_)/ {print $1}' /etc/healtharchive/healthchecks.env | sort -u\n</code></pre>"},{"location":"operations/playbooks/healthchecks-parity/#2-list-what-timers-are-actually-enabled-what-will-run","title":"2) List what timers are actually enabled (what will run)","text":"<pre><code>systemctl list-timers --all | grep healtharchive-\n</code></pre>"},{"location":"operations/playbooks/healthchecks-parity/#3-confirm-healthchecksio-check-schedules-match-reality","title":"3) Confirm Healthchecks.io check schedules match reality","text":"<p>Use the <code>NEXT</code> column from <code>systemctl list-timers</code> to configure Healthchecks schedules:</p> <ul> <li>Hourly timers (disk): Healthchecks \u201c1 hour\u201d period + ~2 hours grace.</li> <li>Daily timers: Healthchecks \u201c1 day\u201d period + ~6 hours grace.</li> <li>Yearly timers (schedule annual + annual sentinel): Healthchecks cron in UTC + large grace (7\u201314 days).</li> </ul> <p>If a yearly check is configured with a small grace (hours), it will look \u201cdown\u201d most of the year.</p>"},{"location":"operations/playbooks/healthchecks-parity/#reconcile-achieve-11-parity-what-exists-vs-what-should-exist","title":"Reconcile: achieve 1:1 parity (what exists vs what should exist)","text":""},{"location":"operations/playbooks/healthchecks-parity/#rule-a-if-a-timer-is-enabled-and-important-it-should-have-a-healthchecks-ping","title":"Rule A \u2014 If a timer is enabled and important, it should have a Healthchecks ping","text":"<p>For each enabled \u201cimportant outcome\u201d timer, ensure:</p> <p>1) A Healthchecks.io check exists 2) Its ping URL is stored in <code>/etc/healtharchive/healthchecks.env</code></p> <p>Important outcome timers (recommended to monitor):</p> <ul> <li><code>healtharchive-replay-reconcile.timer</code></li> <li><code>healtharchive-public-surface-verify.timer</code></li> <li><code>healtharchive-change-tracking.timer</code></li> <li><code>healtharchive-coverage-guardrails.timer</code></li> <li><code>healtharchive-replay-smoke.timer</code></li> <li><code>healtharchive-annual-search-verify.timer</code></li> <li><code>healtharchive-baseline-drift-check.timer</code></li> <li><code>healtharchive-schedule-annual.timer</code> (yearly)</li> <li><code>healtharchive-annual-campaign-sentinel.timer</code> (yearly)</li> <li>legacy: <code>healtharchive-db-backup.timer</code>, <code>healtharchive-disk-check.timer</code></li> </ul> <p>High-frequency timers (recommended NOT to monitor in Healthchecks; too noisy):</p> <ul> <li><code>healtharchive-crawl-metrics.timer</code></li> <li><code>healtharchive-tiering-metrics.timer</code></li> <li><code>healtharchive-crawl-auto-recover.timer</code></li> <li><code>healtharchive-storage-hotpath-auto-recover.timer</code></li> </ul>"},{"location":"operations/playbooks/healthchecks-parity/#rule-b-if-a-healthchecksio-check-exists-it-must-correspond-to-a-real-job-you-run","title":"Rule B \u2014 If a Healthchecks.io check exists, it must correspond to a real job you run","text":"<p>If a Healthchecks.io check exists but:</p> <ul> <li>there is no enabled timer for it, and</li> <li>it is not one of the legacy script checks,</li> </ul> <p>then delete it in Healthchecks.io and remove its env var from <code>/etc/healtharchive/healthchecks.env</code>.</p>"},{"location":"operations/playbooks/healthchecks-parity/#cleanup-automation-what-remains-to-do-deferred-until-after-crawl","title":"Cleanup automation: what remains to do (deferred until after crawl)","text":"<p>Cleanup automation is currently installed but intentionally disabled:</p> <ul> <li>Timer: <code>healtharchive-cleanup-automation.timer</code> (disabled)</li> <li>Sentinel: <code>/etc/healtharchive/cleanup-automation-enabled</code> (missing)</li> <li>Ping var supported by the unit: <code>HEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION</code></li> </ul>"},{"location":"operations/playbooks/healthchecks-parity/#why-we-are-waiting","title":"Why we are waiting","text":"<p>Enabling cleanup changes production behavior (even if intended to be safe). Defer until after crawl so:</p> <ul> <li>we avoid adding churn during the annual campaign,</li> <li>we can review retention expectations and confirm cleanup boundaries.</li> </ul>"},{"location":"operations/playbooks/healthchecks-parity/#post-crawl-enablement-checklist-when-you-decide-yes-enable-cleanup","title":"Post-crawl enablement checklist (when you decide \u201cyes, enable cleanup\u201d)","text":"<p>1) Review the cleanup behavior and config:    - Playbook: <code>cleanup-automation.md</code>    - Config: <code>/opt/healtharchive-backend/ops/automation/cleanup-automation.toml</code></p> <p>2) Decide Healthchecks schedule (from the timer):</p> <pre><code>systemctl cat healtharchive-cleanup-automation.timer\n</code></pre> <p>Current schedule (template): weekly Sunday 04:45 UTC.</p> <p>Recommended Healthchecks schedule for that timer:</p> <ul> <li>Cron (UTC): <code>45 4 * * 0</code></li> <li>Grace: 2 days</li> </ul> <p>3) Create the Healthchecks.io check: - Name: <code>healtharchive-cleanup-automation</code> - Schedule: cron above (UTC) - Grace: 2 days</p> <p>4) Add the ping URL to <code>/etc/healtharchive/healthchecks.env</code>:</p> <pre><code>sudoedit /etc/healtharchive/healthchecks.env\n</code></pre> <p>Add:</p> <pre><code>HEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION=https://hc-ping.com/&lt;uuid&gt;\n</code></pre> <p>5) Enable cleanup automation (two gates):</p> <pre><code>sudo install -d -m 0755 /etc/healtharchive\nsudo touch /etc/healtharchive/cleanup-automation-enabled\nsudo systemctl enable --now healtharchive-cleanup-automation.timer\n</code></pre> <p>6) Verify the ping wiring (safe; does not run cleanup):</p> <pre><code>sudo bash -lc 'set -a; source /etc/healtharchive/healthchecks.env; set +a; /opt/healtharchive-backend/scripts/systemd-healthchecks-wrapper.sh --ping-var HEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION -- echo ok'\n</code></pre> <p>7) Verify real runs on the next scheduled window:</p> <pre><code>sudo journalctl -u healtharchive-cleanup-automation.service -n 200 --no-pager\n</code></pre> <p>If you decide \u201cno, don\u2019t enable cleanup\u201d, keep it disabled and do not create the Healthchecks check or env var.</p>"},{"location":"operations/playbooks/incident-response/","title":"Incident response playbook (operators)","text":"<p>Goal: restore service safely and capture enough context to prevent repeat incidents.</p> <p>Canonical references:</p> <ul> <li>Production runbook: <code>../../deployment/production-single-vps.md</code></li> <li>Monitoring checklist: <code>../monitoring-and-ci-checklist.md</code></li> <li>Service levels: <code>../service-levels.md</code> \u2014 for communication commitments and SLOs</li> <li>Escalation procedures: <code>../escalation-procedures.md</code></li> <li>Disaster recovery runbook: <code>../../deployment/disaster-recovery.md</code></li> <li>Baseline drift: <code>../baseline-drift.md</code></li> <li>Incident notes (template + where to file): <code>../incidents/README.md</code></li> </ul>"},{"location":"operations/playbooks/incident-response/#first-start-an-incident-note","title":"First: start an incident note","text":"<p>As soon as you suspect this is \u201can incident\u201d (not routine maintenance), start a note so you can record a timeline and the exact recovery steps.</p> <ul> <li>Create a new file: <code>docs/operations/incidents/YYYY-MM-DD-short-slug.md</code></li> <li>Copy the template: <code>docs/operations/incidents/incident-template.md</code></li> <li>Pick an initial severity using: <code>docs/operations/incidents/severity.md</code></li> </ul> <p>If you can\u2019t easily edit the repo on the VPS, capture the note in a local scratchpad and copy it into the repo later.</p>"},{"location":"operations/playbooks/incident-response/#when-the-siteapi-looks-broken","title":"When the site/API looks broken","text":"<ol> <li>Confirm what\u2019s failing (public surface):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/verify_public_surface.py</code></li> <li>Check services:</li> <li><code>sudo systemctl status healtharchive-api healtharchive-worker --no-pager -l</code></li> <li>Check recent logs:</li> <li><code>sudo journalctl -u healtharchive-api -n 200 --no-pager</code></li> <li><code>sudo journalctl -u healtharchive-worker -n 200 --no-pager</code></li> <li>Check baseline drift (production correctness):</li> <li><code>./scripts/check_baseline_drift.py --mode live</code></li> </ol>"},{"location":"operations/playbooks/incident-response/#when-jobs-are-stuck-crawlindexing-pipeline","title":"When jobs are stuck (crawl/indexing pipeline)","text":"<p>If the worker is running but jobs never advance, check for a job stuck in <code>status=running</code> after a reboot or unexpected termination.</p> <ol> <li>Load production environment (so the CLI targets Postgres):</li> <li><code>set -a; source /etc/healtharchive/backend.env; set +a</code></li> <li>Inspect recent jobs:</li> <li><code>/opt/healtharchive-backend/.venv/bin/ha-backend list-jobs --limit 50</code></li> <li>Recover stale running jobs (safe dry-run first):</li> <li><code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 180</code></li> <li>Apply (sets <code>status=retryable</code>): <code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 180 --apply</code></li> <li>Verify the worker picks them up:</li> <li><code>sudo journalctl -u healtharchive-worker -n 200 --no-pager</code></li> </ol>"},{"location":"operations/playbooks/incident-response/#if-you-need-to-deploy-a-fix","title":"If you need to deploy a fix","text":"<ul> <li>Follow <code>deploy-and-verify.md</code> (don\u2019t skip the deploy gate).</li> </ul>"},{"location":"operations/playbooks/incident-response/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>The public surface verification passes again.</li> <li>The underlying cause is identified (config drift, failed migration, disk, external dependency, etc.).</li> </ul>"},{"location":"operations/playbooks/monitoring-and-alerting/","title":"Monitoring + alerting playbook (operators)","text":"<p>Goal: detect user-visible outages and silent automation failures with low noise.</p> <p>Canonical reference:</p> <ul> <li><code>../monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"operations/playbooks/monitoring-and-alerting/#external-uptime-monitors-required","title":"External uptime monitors (required)","text":"<p>Ensure monitors exist for:</p> <ul> <li><code>https://api.healtharchive.ca/api/health</code></li> <li><code>https://www.healtharchive.ca/archive</code></li> <li><code>https://replay.healtharchive.ca/</code> (only if you rely on replay)</li> </ul> <p>After changes, you can smoke-test from any machine with internet:</p> <ul> <li><code>healtharchive-backend/scripts/smoke-external-monitors.sh</code></li> </ul>"},{"location":"operations/playbooks/monitoring-and-alerting/#timer-ran-monitoring-optional-recommended","title":"\u201cTimer ran\u201d monitoring (optional, recommended)","text":"<p>If you want alerts when systemd timers stop running:</p> <ol> <li>Create checks in your Healthchecks provider.</li> <li>Store ping URLs only on the VPS:</li> <li><code>/etc/healtharchive/healthchecks.env</code> (root-owned)</li> <li>This file may be shared across multiple automations; it is OK to keep both:<ul> <li>legacy <code>HC_*</code> variables (DB backup + disk check)</li> <li>newer <code>HEALTHARCHIVE_HC_PING_*</code> variables (systemd unit templates)</li> </ul> </li> <li>Keep the unit templates installed/updated on the VPS:</li> <li><code>sudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker</code></li> </ol>"},{"location":"operations/playbooks/monitoring-and-alerting/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>External monitors are green and alert routing is confirmed.</li> <li>If enabled, Healthchecks pings are configured without committing URLs to git.</li> <li>If you use internal Prometheus-based alerts, Alertmanager is configured and test alerts deliver:</li> <li><code>observability-alerting.md</code></li> <li>If you use WARC tiering to a Storage Box, tiering metrics are enabled so you get high-signal alerts:</li> <li><code>sudo systemctl enable --now healtharchive-tiering-metrics.timer</code></li> </ul>"},{"location":"operations/playbooks/observability-alerting/","title":"Observability alerting (Prometheus + Alertmanager; VPS)","text":"<p>Goal: get notified about real outages without creating pager fatigue.</p> <p>Canonical boundary doc (read first):</p> <ul> <li><code>../observability-and-private-stats.md</code></li> </ul>"},{"location":"operations/playbooks/observability-alerting/#design-low-maintenance","title":"Design (low maintenance)","text":"<ul> <li>Prometheus evaluates a small set of high-signal rules.</li> <li>Alertmanager routes alerts to one operator channel (webhook).</li> <li>No new public ports: Alertmanager binds to <code>127.0.0.1:9093</code>.</li> </ul> <p>Why this approach:</p> <ul> <li>Fully reproducible \u201cconfig-as-code\u201d (scripted install + deterministic config files).</li> <li>Avoids Grafana alerting provisioning complexity (datasource UIDs, version drift).</li> </ul>"},{"location":"operations/playbooks/observability-alerting/#preconditions","title":"Preconditions","text":"<ul> <li>You are on the VPS and can <code>sudo</code>.</li> <li>Prometheus is installed and scraping targets:</li> <li><code>curl -s http://127.0.0.1:9090/-/ready</code></li> <li>Node exporter is installed (disk metrics):</li> <li><code>curl -s http://127.0.0.1:9100/metrics | head</code></li> <li>If you use WARC tiering to a Storage Box, enable the tiering metrics writer timer so   Prometheus can alert on mount/tiering failures:</li> <li><code>sudo systemctl enable --now healtharchive-tiering-metrics.timer</code></li> </ul>"},{"location":"operations/playbooks/observability-alerting/#choose-your-operator-channel-webhook","title":"Choose your operator channel (webhook)","text":"<p>Create a webhook URL for your operator channel (examples):</p> <ul> <li>Discord channel webhook</li> <li>Slack incoming webhook</li> <li>Any HTTPS endpoint that accepts Alertmanager webhook JSON</li> </ul> <p>Store it on the VPS:</p> <ul> <li><code>sudoedit /etc/healtharchive/observability/alertmanager_webhook_url</code></li> </ul> <p>Keep this URL private (it is effectively a secret).</p>"},{"location":"operations/playbooks/observability-alerting/#pushover-cleanersafer-pattern","title":"Pushover (cleaner/safer pattern)","text":"<p>Pushover does not accept Alertmanager\u2019s webhook JSON directly. Use the included loopback-only relay so secrets stay under <code>/etc/healtharchive/observability/</code>.</p> <p>1) Put your Pushover secrets on the VPS:</p> <ul> <li><code>sudoedit /etc/healtharchive/observability/pushover_app_token</code></li> <li><code>sudoedit /etc/healtharchive/observability/pushover_user_key</code></li> </ul> <p>2) Install the relay:</p> <ul> <li><code>sudo ./scripts/vps-install-observability-pushover-relay.sh --apply</code></li> </ul> <p>3) Point Alertmanager at the relay:</p> <ul> <li><code>sudoedit /etc/healtharchive/observability/alertmanager_webhook_url</code></li> <li>Set the single line to:</li> <li><code>http://127.0.0.1:9911/alertmanager</code></li> </ul>"},{"location":"operations/playbooks/observability-alerting/#install-apply","title":"Install / apply","text":"<p>1) Pull latest repo on the VPS:</p> <ul> <li><code>cd /opt/healtharchive-backend</code></li> <li><code>git pull</code></li> </ul> <p>2) Dry-run:</p> <ul> <li><code>./scripts/vps-install-observability-alerting.sh</code></li> </ul> <p>3) Apply:</p> <ul> <li><code>sudo ./scripts/vps-install-observability-alerting.sh --apply</code></li> </ul> <p>Optional: if your storage is not on <code>/</code>, set the mountpoint explicitly:</p> <ul> <li><code>sudo ./scripts/vps-install-observability-alerting.sh --apply --mountpoint /</code></li> </ul>"},{"location":"operations/playbooks/observability-alerting/#verify","title":"Verify","text":"<p>1) Alertmanager is up:</p> <ul> <li><code>curl -s http://127.0.0.1:9093/-/ready</code></li> </ul> <p>2) Prometheus loaded rules:</p> <ul> <li><code>curl -s http://127.0.0.1:9090/api/v1/rules | head</code></li> </ul> <p>3) Confirm loopback-only:</p> <ul> <li><code>ss -lntp | grep -E ':9093\\\\b|:9090\\\\b'</code></li> </ul>"},{"location":"operations/playbooks/observability-alerting/#test-delivery-recommended","title":"Test delivery (recommended)","text":"<p>If <code>amtool</code> is installed:</p> <ul> <li><code>amtool alert add HealthArchiveTestAlert severity=warning service=healtharchive</code></li> </ul> <p>Then confirm the test notification arrives in your operator channel.</p> <p>If <code>amtool</code> is not installed, you can still confirm Alertmanager is receiving alerts in its UI by SSH port-forwarding:</p> <ul> <li><code>ssh -N -L 9093:127.0.0.1:9093 haadmin@&lt;vps-tailscale-ip&gt;</code></li> <li>Open <code>http://127.0.0.1:9093/</code></li> </ul>"},{"location":"operations/playbooks/observability-alerting/#alert-set-what-you-get","title":"Alert set (what you get)","text":"<p>Installed rules (minimal, high-signal):</p> <ul> <li>Backend scrape down (&gt;5m)</li> <li>Disk usage &gt;80% (warning) and &gt;90% (critical) on the selected filesystem</li> <li>Sustained <code>/api/search</code> errors (traffic-gated)</li> <li>Job failures increased (failed/index_failed count delta &gt; 0 over 30m)</li> <li>Storage Box mount down (if tiering is enabled and metrics are present)</li> <li>WARC tiering bind-mount service failed (if tiering is enabled and metrics are present)</li> <li>Tiering metrics stale (if metrics timer fails for &gt;2 hours)</li> <li>Tiering hot path unreadable (stale sshfs mount)</li> <li>Annual campaign sentinel failed or did not run (Jan 01 UTC; if enabled)</li> </ul>"},{"location":"operations/playbooks/observability-alerting/#rollback","title":"Rollback","text":"<ul> <li>Disable Alertmanager:</li> <li><code>sudo systemctl disable --now prometheus-alertmanager.service || sudo systemctl disable --now alertmanager.service</code></li> <li>Remove the rules file:</li> <li><code>sudo rm -f /etc/prometheus/rules/healtharchive-alerts.yml</code></li> <li>Restart Prometheus:</li> <li><code>sudo systemctl restart prometheus.service</code></li> </ul>"},{"location":"operations/playbooks/observability-bootstrap/","title":"Observability scaffolding playbook (private; VPS)","text":"<p>Goal: prepare the filesystem + secrets layout for private observability without installing any new services yet.</p> <p>Canonical reference for boundaries (read first):</p> <ul> <li><code>../observability-and-private-stats.md</code></li> </ul>"},{"location":"operations/playbooks/observability-bootstrap/#preconditions","title":"Preconditions","text":"<ul> <li>You are on the production VPS and can <code>sudo</code>.</li> <li><code>/srv/healtharchive/</code> exists.</li> <li>The ops group exists (usually <code>healtharchive</code>).</li> </ul>"},{"location":"operations/playbooks/observability-bootstrap/#procedure","title":"Procedure","text":"<ol> <li> <p>From the repo on the VPS:</p> </li> <li> <p><code>cd /opt/healtharchive-backend</code></p> </li> <li> <p>Run the bootstrap script:</p> </li> <li> <p><code>sudo ./scripts/vps-bootstrap-observability-scaffold.sh</code></p> </li> <li> <p>Populate secret files (do not store secrets under <code>/srv/healtharchive/ops/</code>):</p> </li> <li> <p><code>sudoedit /etc/healtharchive/observability/prometheus_backend_admin_token</code></p> </li> <li><code>sudoedit /etc/healtharchive/observability/grafana_admin_password</code></li> <li><code>sudoedit /etc/healtharchive/observability/postgres_grafana_password</code></li> </ol> <p>Notes:</p> <ul> <li>These files are created root-only (<code>0600</code>) by default.</li> <li>Later installation steps may adjust permissions so services can read them.</li> <li>You may also see <code>postgres_exporter.env</code> and <code>postgres_exporter_password</code> in this directory;      those are created/used by the exporters installer.</li> </ul>"},{"location":"operations/playbooks/observability-bootstrap/#verify","title":"Verify","text":"<ul> <li>Confirm directories exist and have expected ownership/modes:</li> <li><code>stat -c '%U:%G %a %n' /srv/healtharchive/ops/observability /srv/healtharchive/ops/observability/*</code></li> <li>Confirm secret files exist and are root-only:</li> <li><code>stat -c '%U:%G %a %n' /etc/healtharchive/observability/*</code></li> </ul>"},{"location":"operations/playbooks/observability-bootstrap/#rollback","title":"Rollback","text":"<ul> <li>Remove the scaffolding directories:</li> <li><code>sudo rm -rf /srv/healtharchive/ops/observability</code></li> <li>Remove the secret files:</li> <li><code>sudo rm -rf /etc/healtharchive/observability</code></li> </ul>"},{"location":"operations/playbooks/observability-dashboards/","title":"Observability dashboards (Grafana provisioning; VPS)","text":"<p>Goal: install the ops health and private aggregate usage dashboards into Grafana in a reproducible way.</p> <p>Canonical boundary doc (read first):</p> <ul> <li><code>../observability-and-private-stats.md</code></li> </ul>"},{"location":"operations/playbooks/observability-dashboards/#preconditions","title":"Preconditions","text":"<ul> <li>You are on the VPS and can <code>sudo</code>.</li> <li>Prometheus is installed and scraping targets are <code>UP</code>:</li> <li><code>curl -s http://127.0.0.1:9090/-/ready</code></li> <li>Grafana is installed and reachable on loopback:</li> <li><code>curl -s http://127.0.0.1:3000/api/health</code></li> <li>Grafana data sources exist (configured once in the UI):</li> <li>Prometheus data source named <code>prometheus</code> pointing to <code>http://127.0.0.1:9090</code></li> <li>Postgres data source named <code>grafana-postgresql-datasource</code> pointing to <code>127.0.0.1:5432</code> (DB <code>healtharchive</code>, user <code>grafana_readonly</code>)</li> </ul>"},{"location":"operations/playbooks/observability-dashboards/#procedure","title":"Procedure","text":"<p>1) From the repo on the VPS:</p> <ul> <li><code>cd /opt/healtharchive-backend</code></li> <li><code>git pull</code></li> </ul> <p>2) Dry-run the installer script:</p> <ul> <li><code>./scripts/vps-install-observability-dashboards.sh</code></li> </ul> <p>3) Apply:</p> <ul> <li><code>sudo ./scripts/vps-install-observability-dashboards.sh --apply</code></li> </ul> <p>This will:</p> <ul> <li>copy dashboard JSON into <code>/srv/healtharchive/ops/observability/dashboards/healtharchive/</code></li> <li>write Grafana provisioning config under <code>/etc/grafana/provisioning/dashboards/</code></li> <li>restart Grafana</li> </ul>"},{"location":"operations/playbooks/observability-dashboards/#verify","title":"Verify","text":"<p>1) In Grafana:</p> <ul> <li>Go to <code>Dashboards</code></li> <li>Find the <code>HealthArchive</code> folder</li> <li>Open:</li> <li><code>HealthArchive - Ops Overview</code></li> <li><code>HealthArchive - Ops Console (Read-only)</code></li> <li><code>HealthArchive - Pipeline Health</code></li> <li><code>HealthArchive - Search Performance</code></li> <li><code>HealthArchive - Usage (Private, Aggregate)</code></li> <li><code>HealthArchive - Impact Summary (Private, Aggregate)</code></li> </ul> <p>2) Spot-check queries:</p> <ul> <li>Prometheus (Explore): <code>up</code></li> <li>Expect 4 series with value <code>1</code>.</li> <li>Usage (Postgres) dashboards:</li> <li>Expect charts to be empty at first if usage_metrics is sparse; they should not error.</li> </ul>"},{"location":"operations/playbooks/observability-dashboards/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If Grafana fails to start after provisioning dashboards and you see permission errors for <code>/srv/healtharchive/ops/observability</code> in <code>journalctl</code>:</li> <li>Ensure the Grafana service user can traverse the ops tree by joining the shared ops group:<ul> <li><code>sudo usermod -aG healtharchive grafana</code></li> <li><code>sudo systemctl restart grafana-server.service</code></li> </ul> </li> <li> <p>Re-run the installer script to confirm permissions and restart:</p> <ul> <li><code>sudo ./scripts/vps-install-observability-dashboards.sh --apply</code></li> </ul> </li> <li> <p>If dashboards appear but panels show \u201cdata source not found\u201d:</p> </li> <li>Rename your Grafana data sources to match the expected names:<ul> <li>Prometheus: <code>prometheus</code></li> <li>Postgres: <code>grafana-postgresql-datasource</code></li> </ul> </li> <li>Or edit the dashboard JSON under <code>/srv/healtharchive/ops/observability/dashboards/healtharchive/</code> and re-run the installer script.</li> </ul>"},{"location":"operations/playbooks/observability-dashboards/#rollback","title":"Rollback","text":"<ul> <li>Remove provisioning config:</li> <li><code>sudo rm -f /etc/grafana/provisioning/dashboards/healtharchive.yaml</code></li> <li>Remove dashboard JSON:</li> <li><code>sudo rm -rf /srv/healtharchive/ops/observability/dashboards/healtharchive</code></li> <li>Restart Grafana:</li> <li><code>sudo systemctl restart grafana-server.service</code></li> </ul>"},{"location":"operations/playbooks/observability-exporters/","title":"Exporters install playbook (node + Postgres; private; VPS)","text":"<p>Goal: install the minimal exporter set for private observability:</p> <ul> <li>node exporter (host CPU/mem/disk)</li> <li>postgres exporter (DB health)</li> </ul> <p>This playbook keeps exporters loopback-only so nothing is exposed publicly.</p> <p>Canonical boundary doc (read first):</p> <ul> <li><code>../observability-and-private-stats.md</code></li> </ul>"},{"location":"operations/playbooks/observability-exporters/#preconditions","title":"Preconditions","text":"<ul> <li>You are on the VPS and can <code>sudo</code>.</li> <li>Observability directories exist:</li> <li><code>/srv/healtharchive/ops/observability/</code> exists</li> <li><code>/etc/healtharchive/observability/</code> exists</li> <li>Postgres is installed and running locally (production default).</li> </ul>"},{"location":"operations/playbooks/observability-exporters/#procedure-recommended-one-command","title":"Procedure (recommended: one command)","text":"<ol> <li> <p>From the repo on the VPS:</p> </li> <li> <p><code>cd /opt/healtharchive-backend</code></p> </li> <li> <p>Dry-run first (prints actions):</p> </li> <li> <p><code>./scripts/vps-install-observability-exporters.sh</code></p> </li> <li> <p>Apply:</p> </li> <li> <p><code>sudo ./scripts/vps-install-observability-exporters.sh --apply</code></p> </li> </ol> <p>What this does:</p> <ul> <li>Installs packages (<code>prometheus-node-exporter</code>, <code>prometheus-postgres-exporter</code>).</li> <li>Enables the node_exporter textfile collector at:</li> <li><code>/var/lib/node_exporter/textfile_collector</code>   (used by HealthArchive ops scripts to emit small, high-signal health metrics).</li> <li>Creates a DB role <code>postgres_exporter</code> with <code>pg_monitor</code>.</li> <li>Writes exporter credentials (root-owned) to:</li> <li><code>/etc/healtharchive/observability/postgres_exporter.env</code></li> <li><code>/etc/healtharchive/observability/postgres_exporter_password</code></li> <li>Forces exporters to bind only to loopback:</li> <li><code>127.0.0.1:9100</code> and <code>127.0.0.1:9187</code>.</li> </ul>"},{"location":"operations/playbooks/observability-exporters/#verify","title":"Verify","text":"<ol> <li> <p>Confirm the metrics endpoints respond locally:</p> </li> <li> <p><code>curl -s http://127.0.0.1:9100/metrics | head</code></p> </li> <li> <p><code>curl -s http://127.0.0.1:9187/metrics | head</code></p> </li> <li> <p>Confirm the exporters are loopback-only:</p> </li> <li> <p><code>ss -lntp | rg ':9100|:9187' || ss -lntp | grep -E ':9100|:9187'</code></p> </li> </ol> <p>Expect the <code>Local Address:Port</code> to be <code>127.0.0.1:9100</code> and <code>127.0.0.1:9187</code>.</p> <ol> <li> <p>Confirm systemd services are active:</p> </li> <li> <p><code>systemctl --no-pager status prometheus-node-exporter.service prometheus-postgres-exporter.service || true</code></p> </li> </ol> <p>(Service names may vary slightly by distro; the install script handles common names.)</p>"},{"location":"operations/playbooks/observability-exporters/#rollback","title":"Rollback","text":"<ul> <li>Stop + disable exporters:</li> <li><code>sudo systemctl disable --now prometheus-node-exporter.service prometheus-postgres-exporter.service || true</code></li> <li>Remove unit overrides:</li> <li><code>sudo rm -rf /etc/systemd/system/prometheus-node-exporter.service.d /etc/systemd/system/prometheus-postgres-exporter.service.d</code></li> <li><code>sudo systemctl daemon-reload</code></li> <li>Uninstall packages:</li> <li><code>sudo apt-get remove -y prometheus-node-exporter prometheus-postgres-exporter</code></li> <li>Remove exporter secrets:</li> <li><code>sudo rm -f /etc/healtharchive/observability/postgres_exporter.env /etc/healtharchive/observability/postgres_exporter_password</code></li> </ul>"},{"location":"operations/playbooks/observability-grafana/","title":"Grafana install + tailnet access playbook (private; VPS)","text":"<p>Goal: install Grafana as the operator-only \u201cprivate stats page\u201d, reachable only on the tailnet.</p> <p>Canonical boundary doc (read first):</p> <ul> <li><code>../observability-and-private-stats.md</code></li> </ul>"},{"location":"operations/playbooks/observability-grafana/#preconditions","title":"Preconditions","text":"<ul> <li>You are on the VPS and can <code>sudo</code>.</li> <li>Prometheus is installed and loopback-only on <code>127.0.0.1:9090</code>.</li> <li>Tailscale is installed and the VPS is connected to your tailnet.</li> <li>You have set these secret files:</li> <li><code>/etc/healtharchive/observability/grafana_admin_password</code></li> <li><code>/etc/healtharchive/observability/postgres_grafana_password</code></li> </ul>"},{"location":"operations/playbooks/observability-grafana/#procedure","title":"Procedure","text":""},{"location":"operations/playbooks/observability-grafana/#1-install-and-harden-grafana-loopback-only","title":"1) Install and harden Grafana (loopback-only)","text":"<p>From the repo on the VPS:</p> <ul> <li><code>cd /opt/healtharchive-backend</code></li> </ul> <p>Dry-run:</p> <ul> <li><code>./scripts/vps-install-observability-grafana.sh</code></li> </ul> <p>Apply:</p> <ul> <li><code>sudo ./scripts/vps-install-observability-grafana.sh --apply</code></li> </ul> <p>Note: on Ubuntu, the <code>grafana</code> package is not always available in the default apt sources; the installer script will add the Grafana Labs apt repo automatically when needed.</p> <p>This will:</p> <ul> <li>bind Grafana to <code>127.0.0.1:3000</code> (not public)</li> <li>disable anonymous access + signups</li> <li>reset the Grafana admin password</li> <li>create/update Postgres role <code>grafana_readonly</code> with minimal read access for dashboards</li> </ul>"},{"location":"operations/playbooks/observability-grafana/#2-expose-grafana-via-tailnet-only-https-tailscale-serve","title":"2) Expose Grafana via tailnet-only HTTPS (Tailscale Serve)","text":"<p>Preferred option (more private than Serve): SSH port-forward over Tailscale</p> <p>This avoids enabling Tailscale HTTPS certificates (which would place the node hostname in public certificate logs).</p> <p>From an operator machine on the tailnet:</p> <ul> <li><code>ssh -L 3000:127.0.0.1:3000 haadmin@&lt;vps-tailscale-ip-or-name&gt;</code></li> </ul> <p>Then open:</p> <ul> <li><code>http://127.0.0.1:3000</code></li> </ul> <p>Optional: tailnet-only HTTPS via Tailscale Serve</p> <p>This requires enabling HTTPS certificates in the Tailscale admin console for your tailnet.</p> <p>Dry-run:</p> <ul> <li><code>./scripts/vps-enable-tailscale-serve-grafana.sh</code></li> </ul> <p>Apply:</p> <ul> <li><code>sudo ./scripts/vps-enable-tailscale-serve-grafana.sh --apply</code></li> </ul> <p>Note: Tailscale has changed the <code>tailscale serve</code> CLI over time; the script detects the installed version and uses the compatible command form.</p> <p>Note: this script also checks that Grafana is reachable on loopback first (via <code>http://127.0.0.1:3000/api/health</code>) to avoid hanging on misconfigured systems.</p> <p>Then run:</p> <ul> <li><code>sudo tailscale serve status</code></li> </ul> <p>Copy the HTTPS URL and open it from an operator machine that is on the tailnet.</p> <p>Troubleshooting:</p> <ul> <li>If you see: <code>Serve is not enabled on your tailnet</code>, enable it in the Tailscale admin console (the error message includes a link), then re-run the script.</li> <li>If you don\u2019t want to enable Serve yet, use an SSH port-forward over Tailscale instead:</li> <li><code>ssh -L 3000:127.0.0.1:3000 haadmin@&lt;tailscale-host&gt;</code></li> <li>Then open <code>http://127.0.0.1:3000</code></li> </ul>"},{"location":"operations/playbooks/observability-grafana/#configure-data-sources-grafana-ui","title":"Configure data sources (Grafana UI)","text":"<p>This plan keeps secrets out of git and does not provision data sources yet. Configure them once in the UI:</p> <p>1) Prometheus:</p> <ul> <li>URL: <code>http://127.0.0.1:9090</code></li> </ul> <p>2) Postgres (read-only):</p> <ul> <li>Host: <code>127.0.0.1:5432</code></li> <li>Database: <code>healtharchive</code></li> <li>User: <code>grafana_readonly</code></li> <li>Password: value from <code>/etc/healtharchive/observability/postgres_grafana_password</code></li> <li>TLS/SSL mode: disable (local loopback)</li> </ul>"},{"location":"operations/playbooks/observability-grafana/#verify","title":"Verify","text":"<ul> <li>Grafana is not publicly reachable (it binds to loopback):</li> <li><code>ss -lntp | grep -E ':3000\\b'</code></li> <li>Expect <code>127.0.0.1:3000</code></li> <li>Grafana is reachable via tailnet HTTPS:</li> <li>open the <code>tailscale serve status</code> URL from a tailnet-connected machine</li> <li>Prometheus data source test succeeds.</li> <li>Postgres data source test succeeds.</li> </ul>"},{"location":"operations/playbooks/observability-grafana/#rollback","title":"Rollback","text":"<ul> <li>Remove tailnet exposure:</li> <li><code>sudo tailscale serve reset</code></li> <li>Stop + disable Grafana:</li> <li><code>sudo systemctl disable --now grafana-server.service</code></li> <li>Remove overrides:</li> <li><code>sudo rm -rf /etc/systemd/system/grafana-server.service.d</code></li> <li><code>sudo systemctl daemon-reload</code></li> </ul>"},{"location":"operations/playbooks/observability-maintenance/","title":"Observability maintenance (Prometheus + Grafana; VPS)","text":"<p>Goal: keep the private observability stack healthy and reproducible with low operator effort.</p> <p>Canonical boundary doc (read first):</p> <ul> <li><code>../observability-and-private-stats.md</code></li> </ul>"},{"location":"operations/playbooks/observability-maintenance/#what-this-stack-is-eli5","title":"What this stack is (ELI5)","text":"<ul> <li>Prometheus is the \u201ccollector\u201d: it scrapes numbers from services (metrics) and stores them.</li> <li>Grafana is the \u201cdashboard\u201d: it shows graphs/tables from Prometheus (and Postgres).</li> <li>Alertmanager is the \u201cnotifier\u201d: it sends a message to you when something looks wrong.</li> </ul> <p>In dashboards, a \u201chit\u201d usually means \u201ca request happened\u201d (e.g., someone used search or viewed a snapshot).</p> <p>Nothing here should be public. Everything is loopback-only on the VPS and you access it via Tailscale + SSH port-forwarding.</p>"},{"location":"operations/playbooks/observability-maintenance/#quick-verify-recommended","title":"Quick verify (recommended)","text":"<p>On the VPS:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-verify-observability.sh\n</code></pre> <p>From your laptop (tailnet-only SSH tunnel; keep the terminal open):</p> <pre><code>ssh -N \\\n  -L 3000:127.0.0.1:3000 \\\n  -L 9090:127.0.0.1:9090 \\\n  -L 8002:127.0.0.1:8002 \\\n  haadmin@&lt;vps-tailscale-ip&gt;\n</code></pre> <p>Then open:</p> <ul> <li>Grafana: <code>http://127.0.0.1:3000/</code></li> <li>Prometheus (optional): <code>http://127.0.0.1:9090/</code></li> <li>Admin proxy: <code>http://127.0.0.1:8002/</code></li> </ul> <p>In Grafana, check these dashboards (they should show numbers/graphs, not errors):</p> <ul> <li><code>HealthArchive - Ops Overview</code></li> <li><code>HealthArchive - Pipeline Health</code></li> <li><code>HealthArchive - Usage (Private, Aggregate)</code></li> </ul>"},{"location":"operations/playbooks/observability-maintenance/#quarterly-upgrade-cadence-recommended","title":"Quarterly upgrade cadence (recommended)","text":"<p>1) Update packages:</p> <pre><code>sudo apt-get update\nsudo apt-get -y upgrade\n</code></pre> <p>2) Restart observability services:</p> <pre><code>sudo systemctl restart \\\n  prometheus \\\n  prometheus-alertmanager \\\n  prometheus-node-exporter \\\n  prometheus-postgres-exporter \\\n  grafana-server \\\n  healtharchive-pushover-relay \\\n  healtharchive-admin-proxy\n</code></pre> <p>3) Verify:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-verify-observability.sh\n</code></pre>"},{"location":"operations/playbooks/observability-maintenance/#dashboards-updates","title":"Dashboards updates","text":"<p>Dashboards are provisioned from JSON in this repo.</p> <p>On the VPS:</p> <pre><code>cd /opt/healtharchive-backend\ngit pull\nsudo ./scripts/vps-install-observability-dashboards.sh --apply\n</code></pre>"},{"location":"operations/playbooks/observability-maintenance/#credential-rotation-when-needed","title":"Credential rotation (when needed)","text":"<p>All secrets live under <code>/etc/healtharchive/observability/</code> (never commit them).</p>"},{"location":"operations/playbooks/observability-maintenance/#backend-admin-token-affects-prometheus-scrape-admin-proxy","title":"Backend admin token (affects: Prometheus scrape + admin proxy)","text":"<p>If you rotate <code>HEALTHARCHIVE_ADMIN_TOKEN</code> in <code>/etc/healtharchive/backend.env</code>, also update:</p> <ul> <li><code>/etc/healtharchive/observability/prometheus_backend_admin_token</code></li> </ul> <p>Then restart:</p> <pre><code>sudo systemctl restart healtharchive-api\nsudo systemctl restart prometheus\nsudo systemctl restart healtharchive-admin-proxy\n</code></pre> <p>Verify:</p> <ul> <li>Prometheus targets are healthy: <code>curl -s http://127.0.0.1:9090/api/v1/targets | head</code></li> <li>Admin proxy works: <code>curl -s http://127.0.0.1:8002/api/admin/jobs?limit=1 | head</code></li> </ul>"},{"location":"operations/playbooks/observability-maintenance/#grafana-admin-password","title":"Grafana admin password","text":"<p>1) Update:</p> <ul> <li><code>/etc/healtharchive/observability/grafana_admin_password</code></li> </ul> <p>2) Re-apply Grafana config (resets the Grafana admin password from the file):</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-install-observability-grafana.sh --apply --skip-apt --skip-db-role\n</code></pre>"},{"location":"operations/playbooks/observability-maintenance/#grafana-postgres-password-grafana_readonly","title":"Grafana Postgres password (grafana_readonly)","text":"<p>1) Update:</p> <ul> <li><code>/etc/healtharchive/observability/postgres_grafana_password</code></li> </ul> <p>2) Re-apply Grafana config (updates the DB role password):</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-install-observability-grafana.sh --apply --skip-apt\n</code></pre> <p>Then update the Postgres data source in Grafana UI if needed.</p>"},{"location":"operations/playbooks/observability-maintenance/#alert-destination-pushover-relay","title":"Alert destination (Pushover relay)","text":"<p>This setup routes Alertmanager to a local relay:</p> <ul> <li><code>/etc/healtharchive/observability/alertmanager_webhook_url</code> should be:</li> <li><code>http://127.0.0.1:9911/alertmanager</code></li> </ul> <p>If you change it, re-apply alerting to regenerate <code>/etc/prometheus/alertmanager.yml</code>:</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-install-observability-alerting.sh --apply\n</code></pre>"},{"location":"operations/playbooks/observability-maintenance/#prometheus-retention-tuning-disk-safety","title":"Prometheus retention tuning (disk safety)","text":"<p>To change retention (example: 15 days, 1GB cap):</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-install-observability-prometheus.sh --apply --skip-apt --retention-time 15d --retention-size 1GB\n</code></pre> <p>Then verify:</p> <ul> <li><code>curl -s http://127.0.0.1:9090/-/ready</code></li> </ul>"},{"location":"operations/playbooks/observability-maintenance/#troubleshooting-fast-path","title":"Troubleshooting (fast path)","text":"<ul> <li>Check service status:</li> <li><code>systemctl status grafana-server prometheus prometheus-alertmanager --no-pager -l</code></li> <li>Check ports are loopback-only:</li> <li><code>ss -lntp | grep -E ':3000|:8002|:9090|:9093|:9100|:9187|:9911'</code></li> <li>Check Prometheus targets:</li> <li><code>curl -s http://127.0.0.1:9090/api/v1/targets | head</code></li> </ul>"},{"location":"operations/playbooks/observability-prometheus/","title":"Prometheus install playbook (private; VPS)","text":"<p>Goal: install Prometheus and configure it to scrape HealthArchive metrics privately via loopback-only targets.</p> <p>Canonical boundary doc (read first):</p> <ul> <li><code>../observability-and-private-stats.md</code></li> </ul>"},{"location":"operations/playbooks/observability-prometheus/#preconditions","title":"Preconditions","text":"<ul> <li>You are on the VPS and can <code>sudo</code>.</li> <li>Observability secrets exist:</li> <li><code>/etc/healtharchive/observability/prometheus_backend_admin_token</code> is set to the backend <code>HEALTHARCHIVE_ADMIN_TOKEN</code>.</li> <li>Exporters are installed and loopback-only:</li> <li>node exporter: <code>127.0.0.1:9100</code></li> <li>postgres exporter: <code>127.0.0.1:9187</code> and <code>pg_up 1</code></li> <li>Backend API is reachable locally:</li> <li><code>curl -s http://127.0.0.1:8001/api/health | head</code></li> </ul>"},{"location":"operations/playbooks/observability-prometheus/#procedure","title":"Procedure","text":"<ol> <li> <p>From the repo on the VPS:</p> </li> <li> <p><code>cd /opt/healtharchive-backend</code></p> </li> <li> <p>Dry-run first:</p> </li> <li> <p><code>./scripts/vps-install-observability-prometheus.sh</code></p> </li> <li> <p>Apply:</p> </li> <li> <p><code>sudo ./scripts/vps-install-observability-prometheus.sh --apply</code></p> </li> </ol> <p>This will:</p> <ul> <li>install the <code>prometheus</code> package</li> <li>write <code>/etc/prometheus/prometheus.yml</code></li> <li>create <code>/etc/prometheus/rules/healtharchive-alerts.yml</code> (empty placeholder; the alerting installer overwrites with real rules)</li> <li>force Prometheus to listen on <code>127.0.0.1:9090</code> via a systemd override</li> <li>cap retention (time; and size if supported)</li> </ul> <p>Note:</p> <ul> <li>The Prometheus config includes an Alertmanager target at <code>127.0.0.1:9093</code>; the alerting installer installs and configures Alertmanager.</li> </ul>"},{"location":"operations/playbooks/observability-prometheus/#verify","title":"Verify","text":"<ol> <li> <p>Confirm Prometheus is up:</p> </li> <li> <p><code>curl -s http://127.0.0.1:9090/-/ready</code></p> </li> <li> <p>Confirm it is loopback-only:</p> </li> <li> <p><code>ss -lntp | grep -E ':9090\\b'</code></p> </li> </ol> <p>Expect <code>127.0.0.1:9090</code>.</p> <ol> <li> <p>Confirm scrape targets are <code>UP</code>:</p> </li> <li> <p><code>curl -s http://127.0.0.1:9090/api/v1/targets | head</code></p> </li> </ol> <p>Optional (if you have <code>jq</code> installed):</p> <ul> <li> <p><code>curl -s http://127.0.0.1:9090/api/v1/targets | jq '.data.activeTargets[] | {job: .labels.job, scrapeUrl: .scrapeUrl, health: .health, lastError: .lastError}'</code></p> </li> <li> <p>Confirm the backend scrape is working:</p> </li> <li> <p><code>curl -s \"http://127.0.0.1:9090/api/v1/query?query=up%7Bjob%3D%22healtharchive_backend%22%7D\" | head</code></p> </li> </ul>"},{"location":"operations/playbooks/observability-prometheus/#rollback","title":"Rollback","text":"<ul> <li>Disable Prometheus:</li> <li><code>sudo systemctl disable --now prometheus.service</code></li> <li>Remove the override:</li> <li><code>sudo rm -rf /etc/systemd/system/prometheus.service.d</code></li> <li><code>sudo systemctl daemon-reload</code></li> <li>Remove package (optional):</li> <li><code>sudo apt-get remove -y prometheus</code></li> </ul>"},{"location":"operations/playbooks/operator-responsibilities/","title":"Operator responsibilities (must-do list)","text":"<p>Goal: keep HealthArchive operating safely and predictably over time.</p> <p>This file is intentionally brief; it points to canonical docs when you need details.</p>"},{"location":"operations/playbooks/operator-responsibilities/#always-every-deploy","title":"Always (every deploy)","text":"<ul> <li>Treat green <code>main</code> as the deploy gate (run checks, push, wait for CI).</li> <li>Canonical: <code>../monitoring-and-ci-checklist.md</code></li> <li>Run the deploy helper on the VPS (safe deploy + verify):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-deploy.sh --apply --baseline-mode live</code></li> <li>Playbook: <code>deploy-and-verify.md</code></li> <li>If the deploy script fails: don\u2019t retry blindly.</li> <li>Read the drift report and verifier output and fix the underlying mismatch.</li> <li>Canonical: <code>../baseline-drift.md</code></li> <li>If you had to do manual steps or discovered drift, update the canonical runbook/playbook so the next operator can follow reality.</li> <li>Canonical: <code>../ops-cadence-checklist.md</code></li> </ul>"},{"location":"operations/playbooks/operator-responsibilities/#ongoing-automation-maintenance","title":"Ongoing automation maintenance","text":"<ul> <li>Keep systemd unit templates installed/updated on the VPS after repo updates:</li> <li><code>sudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker</code></li> <li>Playbook: <code>automation-maintenance.md</code></li> <li>Maintain sentinel files under <code>/etc/healtharchive/</code> (explicit automation on/off controls).</li> <li>Canonical: <code>../../deployment/systemd/README.md</code></li> <li>If you enable Healthchecks pings:</li> <li>keep ping URLs only in the root-owned VPS env file (never in git):<ul> <li><code>/etc/healtharchive/healthchecks.env</code></li> </ul> </li> <li>Note: this file may contain both legacy <code>HC_*</code> variables (DB backup + disk)     and newer <code>HEALTHARCHIVE_HC_PING_*</code> variables (systemd unit templates).</li> <li>Canonical: <code>../monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"operations/playbooks/operator-responsibilities/#quarterly-ops-cadence-sustainability-loop","title":"Quarterly ops cadence (sustainability loop)","text":"<ul> <li>Run a restore test and write a public-safe log entry.</li> <li>Playbook: <code>restore-test.md</code></li> <li>Verify dataset release checksum integrity (<code>SHA256SUMS</code>).</li> <li>Playbook: <code>dataset-release.md</code></li> <li>Add an adoption signals entry (links + aggregate counts only).</li> <li>Playbook: <code>adoption-signals.md</code></li> <li>Confirm timers are still enabled and not silently failing.</li> <li><code>./scripts/verify_ops_automation.sh</code> and spot-check <code>journalctl</code></li> <li>Playbook: <code>automation-maintenance.md</code></li> <li>Do a quick docs drift skim (production runbook + incident response) and fix anything stale.</li> <li>Canonical: <code>../ops-cadence-checklist.md</code></li> </ul>"},{"location":"operations/playbooks/operator-responsibilities/#security-posture-always","title":"Security posture (always)","text":"<ul> <li>Keep secrets (admin token, DB URL, ping URLs) out of git forever.</li> <li>Canonical: <code>../../deployment/production-single-vps.md</code></li> <li>Maintain HSTS at Caddy for <code>api.healtharchive.ca</code>.</li> <li>Canonical: <code>../../deployment/hosting-and-live-server-to-dos.md</code></li> <li>Maintain a strict CORS allowlist; treat widening it as a deliberate security decision.</li> <li>Canonical: <code>../../deployment/environments-and-configuration.md</code></li> </ul>"},{"location":"operations/playbooks/outreach-and-verification/","title":"Outreach + verification playbook (ongoing)","text":"<p>Goal: run external outreach and verification work without storing private contact details in git.</p> <p>Canonical references:</p> <ul> <li>Outreach email templates: <code>../outreach-templates.md</code></li> <li>Partner kit (links + screenshot checklist): <code>../partner-kit.md</code></li> <li>Verification packet outline: <code>../verification-packet.md</code></li> <li>Mentions log (public-safe, link-only): <code>../mentions-log.md</code></li> <li>Data handling &amp; retention rules: <code>../data-handling-retention.md</code></li> <li>Adoption signals (quarterly, VPS-only): <code>adoption-signals.md</code></li> </ul>"},{"location":"operations/playbooks/outreach-and-verification/#rules-hard","title":"Rules (hard)","text":"<ul> <li>Never store emails, phone numbers, names, or private notes in git.</li> <li>Public logs must be link-only and permission-aware:</li> <li>If permission to name is unclear, use \u201cPending\u201d and keep the name out of public copy.</li> </ul>"},{"location":"operations/playbooks/outreach-and-verification/#procedure","title":"Procedure","text":""},{"location":"operations/playbooks/outreach-and-verification/#1-create-a-private-tracker-operator-only-not-in-git","title":"1) Create a private tracker (operator-only; not in git)","text":"<p>Pick one:</p> <ul> <li>A password manager note (preferred).</li> <li>A local spreadsheet in a folder outside the repo (e.g. <code>~/HealthArchive-private/outreach.xlsx</code>).</li> <li>A private doc in your personal notes system.</li> </ul> <p>Suggested fields:</p> <ul> <li><code>date_first_contacted_utc</code></li> <li><code>name</code> / <code>role</code> / <code>org</code> (private)</li> <li><code>contact_channel</code> (private)</li> <li><code>why_them</code></li> <li><code>template_used</code> (A/B/C)</li> <li><code>status</code> (no response / declined / interested / accepted)</li> <li><code>followup_1_sent_utc</code>, <code>followup_2_sent_utc</code></li> <li><code>public_link</code> (only if it exists)</li> <li><code>permission_to_name</code> (yes/no/pending) + date confirmed (private)</li> </ul>"},{"location":"operations/playbooks/outreach-and-verification/#2-prepare-partner-ready-assets-public","title":"2) Prepare partner-ready assets (public)","text":"<ul> <li>Confirm these pages are accurate and up-to-date:</li> <li><code>https://www.healtharchive.ca/brief</code></li> <li><code>https://www.healtharchive.ca/cite</code></li> <li>(Optional) Capture screenshots using <code>../partner-kit.md</code> so you can attach them.</li> </ul>"},{"location":"operations/playbooks/outreach-and-verification/#3-build-a-target-list-operator-only","title":"3) Build a target list (operator-only)","text":"<p>Start with a small batch (e.g., 10\u201320), split between:</p> <ul> <li>Distribution partners (libraries / digital scholarship resource pages).</li> <li>Research / teaching partners.</li> <li>Journalism / communication partners.</li> <li>Verifier candidates (librarian / researcher / editor).</li> </ul>"},{"location":"operations/playbooks/outreach-and-verification/#4-send-outreach-operator-only","title":"4) Send outreach (operator-only)","text":"<ul> <li>Use <code>../outreach-templates.md</code> and customize only what\u2019s needed:</li> <li>recipient name</li> <li>why this is relevant to them</li> <li>the single best link to include (usually <code>/digest</code> or <code>/changes</code>)</li> <li>Follow-up cadence:</li> <li>follow-up #1 at ~1 week</li> <li>follow-up #2 at ~2 weeks (final)</li> </ul>"},{"location":"operations/playbooks/outreach-and-verification/#5-update-the-public-safe-mentions-log-git-when-appropriate","title":"5) Update the public-safe mentions log (git) when appropriate","text":"<p>Only when there is a public link (and/or explicit permission to name), add an entry to:</p> <ul> <li><code>../mentions-log.md</code></li> </ul>"},{"location":"operations/playbooks/outreach-and-verification/#6-run-the-verifier-workflow-operator-only","title":"6) Run the verifier workflow (operator-only)","text":"<ul> <li>Send <code>../verification-packet.md</code> to the verifier.</li> <li>Ask explicitly:</li> <li>permission to name them publicly (yes/no)</li> <li>preferred wording (if any)</li> <li>If they grant permission and there is a public link (or they agree to be listed), record it in:</li> <li><code>../mentions-log.md</code></li> </ul>"},{"location":"operations/playbooks/outreach-and-verification/#7-quarterly-adoption-signals-vps-only-public-safe","title":"7) Quarterly adoption signals (VPS-only; public-safe)","text":"<p>Run the adoption signals playbook and store the entry on the VPS:</p> <ul> <li><code>adoption-signals.md</code></li> </ul>"},{"location":"operations/playbooks/outreach-and-verification/#what-done-means-phase-4","title":"What \u201cdone\u201d means (Phase 4)","text":"<ul> <li>Private tracker exists outside git.</li> <li>At least one outreach batch is sent (with follow-ups scheduled).</li> <li>Mentions log exists and is updated only with public links and permission-aware entries.</li> </ul>"},{"location":"operations/playbooks/replay-service/","title":"Replay service playbook (operators)","text":"<p>Goal: keep replay (<code>replay.healtharchive.ca</code>) available when the project relies on it.</p> <p>Canonical references:</p> <ul> <li>Replay runbook: <code>../../deployment/replay-service-pywb.md</code></li> <li>Production runbook: <code>../../deployment/production-single-vps.md</code></li> <li>Replay automation design: <code>../replay-and-preview-automation-plan.md</code></li> </ul>"},{"location":"operations/playbooks/replay-service/#setup-recovery-if-replay-is-missing","title":"Setup / recovery (if replay is missing)","text":"<p>Follow <code>../../deployment/replay-service-pywb.md</code>.</p>"},{"location":"operations/playbooks/replay-service/#verify-replay-is-working","title":"Verify replay is working","text":"<ol> <li>Check the base URL is up:</li> <li><code>curl -I https://replay.healtharchive.ca/ | head</code></li> <li>Verify the public surface script can resolve a replay <code>browseUrl</code> for a known snapshot:</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/verify_public_surface.py</code></li> <li>Verify the replay banner works on a direct replay page:</li> <li>Open a known <code>browseUrl</code> on <code>https://replay.healtharchive.ca/</code> and confirm the banner loads quickly, shows the page title + meta line (capture date + original URL) + disclaimer, and that the action links (View diff, Details, All snapshots, Raw HTML, Metadata JSON, Cite, Report issue, Hide) behave as expected.</li> <li>From HealthArchive search results, click <code>View</code> and confirm \u201c\u2190 HealthArchive.ca\u201d returns to the same search results page.</li> </ol>"},{"location":"operations/playbooks/replay-service/#retention-warning","title":"Retention warning","text":"<p>Replay depends on WARCs staying on disk. Do not delete WARCs for jobs you expect to replay.</p>"},{"location":"operations/playbooks/replay-service/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li><code>https://replay.healtharchive.ca/</code> responds successfully.</li> <li><code>./scripts/verify_public_surface.py</code> reports a working replay <code>browseUrl</code> where expected.</li> </ul>"},{"location":"operations/playbooks/replay-smoke-tests/","title":"Replay smoke tests (daily replay validation)","text":"<p>Goal: confirm replay is serving real content for the latest indexed jobs.</p> <p>Canonical refs:</p> <ul> <li>replay runbook: <code>../../deployment/replay-service-pywb.md</code></li> <li>systemd unit templates: <code>../../deployment/systemd/README.md</code></li> </ul>"},{"location":"operations/playbooks/replay-smoke-tests/#what-this-does","title":"What this does","text":"<ul> <li>Picks the latest indexed job per source.</li> <li>Uses the first seed URL as a replay target (or falls back to the source registry defaults for legacy jobs that lack seeds in <code>ArchiveJob.config</code>):</li> <li><code>https://replay.healtharchive.ca/job-&lt;id&gt;/&lt;seed&gt;</code></li> <li>Emits node_exporter textfile metrics:</li> <li><code>healtharchive_replay_smoke_target_present{source=\"hc\"}</code></li> <li><code>healtharchive_replay_smoke_ok{source=\"hc\",job_id=\"123\"}</code></li> </ul>"},{"location":"operations/playbooks/replay-smoke-tests/#enablement-vps","title":"Enablement (VPS)","text":"<pre><code>sudo touch /etc/healtharchive/replay-smoke-enabled\nsudo systemctl enable --now healtharchive-replay-smoke.timer\n</code></pre>"},{"location":"operations/playbooks/replay-smoke-tests/#manual-check","title":"Manual check","text":"<pre><code>sudo systemctl start healtharchive-replay-smoke.service\nsudo journalctl -u healtharchive-replay-smoke.service -n 200 --no-pager\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_replay_smoke_'\n</code></pre>"},{"location":"operations/playbooks/replay-smoke-tests/#if-an-alert-fires","title":"If an alert fires","text":"<ol> <li>Confirm replay is enabled:    <pre><code>rg -n 'HEALTHARCHIVE_REPLAY_BASE_URL' /etc/healtharchive/backend.env\n</code></pre></li> <li>Confirm replay service health:    <pre><code>sudo systemctl status healtharchive-replay.service --no-pager -l\ncurl -I https://replay.healtharchive.ca/ | head\n</code></pre></li> <li>If replay is up (<code>/</code> is <code>200</code>) but smoke requests return <code>503</code>, suspect WARC/mount access (often after <code>sshfs</code>/tiering incidents).</li> </ol> <p>1) Ensure the WARC tiering unit is not stuck in a failed state:    <pre><code>systemctl is-failed healtharchive-warc-tiering.service &amp;&amp; sudo systemctl reset-failed healtharchive-warc-tiering.service || true\nsudo systemctl start healtharchive-warc-tiering.service\nsudo systemctl status healtharchive-warc-tiering.service --no-pager -l | sed -n '1,120p'\n</code></pre></p> <p>2) Restart replay to refresh its view of <code>/srv/healtharchive/jobs</code>:    <pre><code>sudo systemctl restart healtharchive-replay.service\n</code></pre></p> <p>3) Re-run smoke:    <pre><code>sudo systemctl start healtharchive-replay-smoke.service\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_replay_smoke_'\n</code></pre></p> <ol> <li>If replay is up but a source still fails, re-run replay reconcile:    <pre><code>sudo systemctl start healtharchive-replay-reconcile.service\n</code></pre></li> </ol>"},{"location":"operations/playbooks/replay-smoke-tests/#config","title":"Config","text":"<p>Edit <code>ops/automation/replay-smoke.toml</code> to adjust timeouts or sources.</p>"},{"location":"operations/playbooks/restore-test/","title":"Restore test playbook (quarterly)","text":"<p>Goal: prove backups are usable by performing a restore and minimal API checks.</p> <p>Canonical reference:</p> <ul> <li><code>../restore-test-procedure.md</code></li> </ul>"},{"location":"operations/playbooks/restore-test/#procedure-high-level","title":"Procedure (high level)","text":"<ol> <li>Follow <code>../restore-test-procedure.md</code>.</li> <li>Record results using the template:</li> <li><code>../restore-test-log-template.md</code></li> <li>Store the public-safe log on the VPS:</li> <li><code>/srv/healtharchive/ops/restore-tests/</code></li> </ol>"},{"location":"operations/playbooks/restore-test/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>A dated restore-test log exists under <code>/srv/healtharchive/ops/restore-tests/</code>.</li> <li>Core API checks against the restored DB succeed (health, stats, sources).</li> </ul>"},{"location":"operations/playbooks/security-posture/","title":"Security posture playbook (operators)","text":"<p>Goal: keep the public surface safe-by-default and avoid accidental exposure.</p> <p>Canonical references:</p> <ul> <li>Production runbook: <code>../../deployment/production-single-vps.md</code></li> <li>Hosting checklist (TLS/HSTS): <code>../../deployment/hosting-and-live-server-to-dos.md</code></li> <li>Env wiring + CORS: <code>../../deployment/environments-and-configuration.md</code></li> <li>Admin verification: <code>./scripts/verify-security-and-admin.sh</code></li> </ul>"},{"location":"operations/playbooks/security-posture/#secrets-discipline-always","title":"Secrets discipline (always)","text":"<ul> <li>Store secrets only in VPS/Vercel env (or a secret manager), never in git.</li> <li><code>HEALTHARCHIVE_ADMIN_TOKEN</code></li> <li>DB URL/password</li> <li>Healthchecks ping URLs</li> </ul>"},{"location":"operations/playbooks/security-posture/#https-hsts-api","title":"HTTPS + HSTS (API)","text":"<ul> <li>Maintain HSTS at the reverse proxy (Caddy) for <code>api.healtharchive.ca</code>.</li> <li>After changes, verify HSTS is present:</li> <li><code>./scripts/verify-security-and-admin.sh --api-base https://api.healtharchive.ca --require-hsts</code></li> </ul>"},{"location":"operations/playbooks/security-posture/#strict-cors-allowlist-api","title":"Strict CORS allowlist (API)","text":"<ul> <li>Keep <code>HEALTHARCHIVE_CORS_ORIGINS</code> narrow.</li> <li>Treat widening CORS as a deliberate decision (and re-verify headers).</li> <li>Verify real headers from production (example):</li> <li><code>curl -sS -D- -o /dev/null -H 'Origin: https://healtharchive.ca' https://api.healtharchive.ca/api/health | rg -i '^access-control-allow-origin:'</code></li> </ul>"},{"location":"operations/playbooks/security-posture/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>Admin endpoints are not publicly accessible.</li> <li>HSTS is present on <code>https://api.healtharchive.ca/api/health</code>.</li> <li>CORS behavior matches the allowlist policy.</li> </ul>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-drills/","title":"Storage Box / <code>sshfs</code> recovery drills (safe on production)","text":"<p>Goal: periodically prove that:</p> <ul> <li>the watchdog logic would take the right recovery actions, without actually touching mounts, and</li> <li>the alert pipeline (Prometheus \u2192 Alertmanager) is wired correctly, without paging operators.</li> </ul> <p>These drills are designed to be safe on production even mid-crawl.</p> <p>Canonical background:</p> <ul> <li>Roadmap context: <code>../../roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> <li>Real incident recovery procedure: <code>storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-drills/#0-safety-rules","title":"0) Safety rules","text":"<ul> <li>Never run recovery automation with <code>--apply</code> as part of a drill.</li> <li>For drills, always use:</li> <li>a temporary <code>--state-file</code> and <code>--lock-file</code> under <code>/tmp</code></li> <li>a temporary <code>--textfile-out-dir</code> under <code>/tmp</code>   so you don\u2019t affect production watchdog state or Prometheus metrics.</li> </ul>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-drills/#1-drill-watchdog-planned-actions-dry-run-simulation","title":"1) Drill: watchdog planned actions (dry-run simulation)","text":"<p>This validates the Phase 2 watchdog logic without breaking mounts.</p> <p>1) Pick a real \u201chot path\u201d to simulate as stale.</p> <p>Good candidates:</p> <ul> <li>an annual job output dir: <code>/srv/healtharchive/jobs/&lt;source&gt;/&lt;job_dir&gt;</code></li> <li>an imports hot path from tiering: <code>/srv/healtharchive/jobs/imports/...</code></li> </ul> <p>2) Run the watchdog in dry-run simulation mode (do not use <code>--apply</code>):</p> <pre><code>cd /opt/healtharchive-backend\nsudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; \\\n  /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-storage-hotpath-auto-recover.py \\\n    --confirm-runs 1 \\\n    --min-failure-age-seconds 0 \\\n    --state-file /tmp/healtharchive-storage-hotpath-drill.state.json \\\n    --lock-file /tmp/healtharchive-storage-hotpath-drill.lock \\\n    --textfile-out-dir /tmp \\\n    --textfile-out-file healtharchive_storage_hotpath_auto_recover.drill.prom \\\n    --simulate-broken-path /srv/healtharchive/jobs/hc/&lt;JOB_DIR&gt;'\n</code></pre> <p>3) Confirm output includes:</p> <ul> <li><code>DRILL: simulate-broken-path active</code></li> <li><code>Planned actions (dry-run):</code></li> <li>a sensible sequence (stop worker \u2192 unmount stale mountpoints \u2192 re-apply tiering \u2192 recover stale jobs \u2192 start worker)</li> </ul> <p>If this looks wrong, fix the watchdog logic before enabling the production timer.</p>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-drills/#2-drill-alert-pipeline-no-paging","title":"2) Drill: alert pipeline (no paging)","text":"<p>This validates Prometheus rule loading + Alertmanager ingestion without sending notifications.</p> <p>Precondition:</p> <ul> <li>Alertmanager routes <code>severity=\"drill\"</code> to a null receiver. This is handled by the repo installer:</li> <li><code>scripts/vps-install-observability-alerting.sh</code></li> </ul>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-drills/#21-trigger-the-drill-alert-metric-auto-cleanup","title":"2.1 Trigger the drill alert metric (auto-cleanup)","text":"<pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-alert-pipeline-drill.sh --apply --duration-seconds 600\n</code></pre>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-drills/#22-confirm-prometheus-sees-the-alert","title":"2.2 Confirm Prometheus sees the alert","text":"<pre><code>curl -s http://127.0.0.1:9090/api/v1/alerts | rg 'HealthArchiveAlertPipelineDrill' || true\n</code></pre>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-drills/#23-confirm-alertmanager-received-the-alert-but-does-not-notify","title":"2.3 Confirm Alertmanager received the alert (but does not notify)","text":"<pre><code>curl -s http://127.0.0.1:9093/api/v2/alerts | rg 'HealthArchiveAlertPipelineDrill' || true\n</code></pre> <p>After ~10 minutes, the script removes the metric file and the alert should resolve.</p> <p>If you ever need to clean up manually:</p> <pre><code>sudo rm -f /var/lib/node_exporter/textfile_collector/healtharchive_alert_pipeline_drill.prom\n</code></pre>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-drills/#3-full-recovery-drill-staging-or-scheduled-maintenance-only","title":"3) Full recovery drill (staging or scheduled maintenance only)","text":"<p>Only do this on:</p> <ul> <li>a staging VPS (preferred), or</li> <li>a production maintenance window where crawl interruption is acceptable.</li> </ul> <p>High-level steps:</p> <p>1) Ensure you can tolerate crawl interruption (stop the worker first). 2) Intentionally create a stale mount condition (Errno 107) on a dedicated test hot path. 3) Confirm:    - alerts fire (<code>HealthArchiveTieringHotPathUnreadable</code> / <code>HealthArchiveStorageBoxMountDown</code>)    - watchdog recovers (tiering re-apply, stale job recovery)    - worker resumes and jobs make progress 4) Run post-incident integrity checks:    - <code>ha-backend verify-warcs --job-id &lt;ID&gt; --level 1 --since-minutes &lt;window&gt;</code>    - follow <code>warc-integrity-verification.md</code> if anything fails</p>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/","title":"Storage Box / <code>sshfs</code> stale mount recovery (Errno 107)","text":"<p>Use this playbook when HealthArchive crawls/indexing/metrics start failing with:</p> <ul> <li><code>OSError: [Errno 107] Transport endpoint is not connected</code></li> </ul> <p>This typically indicates a stale FUSE mount (often <code>sshfs</code>) where the mountpoint still exists, but basic filesystem operations (<code>stat</code>, <code>ls</code>, <code>is_dir</code>) fail.</p> <p>Operational note:</p> <ul> <li>The worker skips jobs that recently failed with <code>crawler_status=infra_error</code> for a short cooldown window to prevent retry storms. This reduces alert noise but does not fix the underlying mount issue; use this playbook (or the hot-path auto-recover automation) to repair the stale mountpoint.</li> </ul> <p>For background and the full implementation plan (prevention + automation + integrity), see:</p> <ul> <li><code>../../roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> <li>Drills (safe on production): <code>storagebox-sshfs-stale-mount-drills.md</code></li> </ul>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#quick-triage-60-seconds","title":"Quick triage (60 seconds)","text":"<p>On the VPS (<code>/opt/healtharchive-backend</code>):</p> <p>1) Snapshot current crawl state:</p> <pre><code>./scripts/vps-crawl-status.sh --year \"$(date -u +%Y)\"\n</code></pre> <p>Optional: if Phase 2 automation has been enabled, check whether it is already attempting recovery (it is disabled-by-default unless the sentinel exists):</p> <pre><code>systemctl status healtharchive-storage-hotpath-auto-recover.timer --no-pager -l || true\nls -la /etc/healtharchive/storage-hotpath-auto-recover-enabled 2&gt;/dev/null || true\ncat /srv/healtharchive/ops/watchdog/storage-hotpath-auto-recover.json 2&gt;/dev/null || true\n</code></pre> <p>If the worker auto-start watchdog is enabled (optional), check it too:</p> <pre><code>systemctl status healtharchive-worker-auto-start.timer --no-pager -l || true\nls -la /etc/healtharchive/worker-auto-start-enabled 2&gt;/dev/null || true\ncat /srv/healtharchive/ops/watchdog/worker-auto-start.json 2&gt;/dev/null || true\n</code></pre> <p>2) Confirm Storage Box base mount health:</p> <pre><code>mount | rg '/srv/healtharchive/storagebox'\nls -la /srv/healtharchive/storagebox &gt;/dev/null &amp;&amp; echo \"OK: storagebox readable\" || echo \"BAD: storagebox unreadable\"\n</code></pre> <p>3) Identify broken \u201chot paths\u201d (job output dirs):</p> <pre><code>./scripts/vps-crawl-status.sh --year \"$(date -u +%Y)\" | rg '^Output dir:'\nls -la /srv/healtharchive/jobs/hc/  # replace with source path(s) as needed\n</code></pre> <p>If you see <code>Transport endpoint is not connected</code> or <code>d?????????</code> for job output dirs, continue.</p>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#recovery-procedure-safe-ordering","title":"Recovery procedure (safe ordering)","text":""},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#1-stop-the-worker","title":"1) Stop the worker","text":"<p>Stop the worker first to prevent repeated filesystem touches while mounts are broken:</p> <pre><code>sudo systemctl stop healtharchive-worker.service\n</code></pre> <p>Note: if <code>healtharchive-worker-auto-start.timer</code> is enabled, it may restart the worker while you are mid-repair. Either:</p> <ul> <li>temporarily disable the timer, or</li> <li>temporarily remove <code>/etc/healtharchive/worker-auto-start-enabled</code>,</li> </ul> <p>then re-enable after recovery.</p> <p>Optional: if you suspect a crawler container is still running and stuck on IO, inspect it:</p> <pre><code>docker ps --format 'table {{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.Status}}' | rg 'zimit|openzim' || true\n</code></pre> <p>Only stop a container if you\u2019re sure it is part of the broken job and not making progress.</p>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#2-identify-stale-mountpoints-targeted","title":"2) Identify stale mountpoints (targeted)","text":"<p>This incident class often affects specific job output directories (not necessarily the whole Storage Box mount).</p> <p>For each affected job output dir (examples shown):</p> <pre><code>mount | rg '/srv/healtharchive/jobs/(hc|phac|cihr)/'\nsudo findmnt -T /srv/healtharchive/jobs/hc/&lt;JOB_DIR&gt; || true\n</code></pre> <p>If <code>ls</code> against a path returns <code>Transport endpoint is not connected</code>, treat it as stale.</p>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#3-unmount-stale-hot-paths-use-umount-first-then-l-only-if-needed","title":"3) Unmount stale hot paths (use <code>umount</code> first, then <code>-l</code> only if needed)","text":"<p>For each stale mountpoint, try:</p> <pre><code>sudo umount /srv/healtharchive/jobs/&lt;source&gt;/&lt;JOB_DIR&gt;\n</code></pre> <p>If it fails and the path is still broken/unstat\u2019able, use lazy unmount:</p> <pre><code>sudo umount -l /srv/healtharchive/jobs/&lt;source&gt;/&lt;JOB_DIR&gt;\n</code></pre> <p>Notes:</p> <ul> <li>Use targeted unmounts only (specific job dirs), not broad parent directories.</li> <li><code>umount -l</code> is an emergency tool; use it only for confirmed-stale mountpoints.</li> </ul>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#4-re-apply-tiering-mounts","title":"4) Re-apply tiering mounts","text":"<p>1) Re-apply WARC tiering bind mounts (manifest-driven):</p> <pre><code>sudo ./scripts/vps-warc-tiering-bind-mounts.sh --apply\n</code></pre> <p>If you have confirmed-stale mountpoints and want the script to attempt targeted repair automatically (still requires the worker to be stopped first):</p> <pre><code>sudo ./scripts/vps-warc-tiering-bind-mounts.sh --apply --repair-stale-mounts\n</code></pre> <p>If this fails with Errno 107 under <code>/srv/healtharchive/jobs/imports/...</code>, unmount those stale import mountpoints too and re-run.</p> <p>If the systemd unit is in a <code>failed</code> state, clear it and re-run (prevents repeated <code>WarcTieringFailed</code> alerts):</p> <pre><code>systemctl is-failed healtharchive-warc-tiering.service &amp;&amp; sudo systemctl reset-failed healtharchive-warc-tiering.service || true\nsudo systemctl start healtharchive-warc-tiering.service\n</code></pre> <p>2) Re-apply annual output tiering (campaign job output dirs \u2192 Storage Box):</p> <p>Preferred (avoids the systemd unit\u2019s internal worker stop/start):</p> <pre><code>sudo /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py --apply --year \"$(date -u +%Y)\"\n</code></pre> <p>If you want the script to attempt targeted repair for stale mountpoints (Errno 107), pass:</p> <pre><code>sudo /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py --apply --repair-stale-mounts --year \"$(date -u +%Y)\"\n</code></pre> <p>Alternative (uses the systemd unit, which stops/starts the worker internally):</p> <pre><code>sudo systemctl start healtharchive-annual-output-tiering.service\n</code></pre>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#5-recover-job-state-stuck-running-retryable","title":"5) Recover job state (stuck <code>running</code> \u2192 <code>retryable</code>)","text":"<p>Load the backend env (production DB connection):</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n</code></pre> <p>Recover stale jobs:</p> <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 5 --apply --limit 25\n</code></pre> <p>If a job ended up <code>failed</code> due to the mount issue and you want it to run again:</p> <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend retry-job --id &lt;JOB_ID&gt;\n</code></pre>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#6-restart-the-worker","title":"6) Restart the worker","text":"<pre><code>sudo systemctl start healtharchive-worker.service\n</code></pre>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#replay-note-after-mount-repairs","title":"Replay note (after mount repairs)","text":"<p>If replay smoke tests start returning <code>503</code> for previously indexed jobs after a mount/tiering incident, restart replay to refresh its view of <code>/srv/healtharchive/jobs</code>:</p> <pre><code>sudo systemctl restart healtharchive-replay.service\nsudo systemctl start healtharchive-replay-smoke.service\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_replay_smoke_'\n</code></pre>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#validation-confirm-were-actually-healthy","title":"Validation (confirm we\u2019re actually healthy)","text":""},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#1-worker-is-running-and-picking-jobs","title":"1) Worker is running and picking jobs","text":"<pre><code>sudo systemctl status healtharchive-worker.service --no-pager -l\nsudo journalctl -u healtharchive-worker.service -n 80 --no-pager -l\n</code></pre>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#2-crawls-are-making-progress-not-just-running","title":"2) Crawls are making progress (not just \u201crunning\u201d)","text":"<p>Pick the active job ID and check progress:</p> <pre><code>./scripts/vps-crawl-status.sh --year \"$(date -u +%Y)\" --job-id &lt;JOB_ID&gt;\n</code></pre> <p>Look for:</p> <ul> <li><code>crawlStatus</code> counters increasing over time (<code>crawled</code> ticks up).</li> <li><code>healtharchive_crawl_running_job_stalled == 0</code></li> <li><code>last_progress_age_seconds</code> small (tens of seconds to a few minutes).</li> <li><code>healtharchive_crawl_running_job_state_parse_ok == 1</code> (state file readable; no sshfs weirdness)</li> <li><code>healtharchive_crawl_running_job_container_restarts_done</code> not climbing rapidly (avoid restart thrash)</li> <li>new <code>.warc.gz</code> files appearing under the job\u2019s active temp dir.</li> </ul>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#3-metrics-writers-are-healthy","title":"3) Metrics writers are healthy","text":"<pre><code>sudo systemctl start healtharchive-crawl-metrics.service\nsudo systemctl start healtharchive-tiering-metrics.service\nsudo systemctl status healtharchive-crawl-metrics.service healtharchive-tiering-metrics.service --no-pager -l\n</code></pre>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#if-recovery-fails","title":"If recovery fails","text":"<p>If hot paths are still unreadable after unmount + tiering reapply:</p> <p>1) Verify Storage Box base mount is readable:</p> <pre><code>ls -la /srv/healtharchive/storagebox &gt;/dev/null &amp;&amp; echo OK || echo BAD\nsudo systemctl status healtharchive-storagebox-sshfs.service --no-pager -l\n</code></pre> <p>2) Consider restarting the base mount:</p> <pre><code>sudo systemctl restart healtharchive-storagebox-sshfs.service\n</code></pre> <p>3) Re-run tiering reapply steps (WARC tiering + annual output tiering).</p> <p>If this becomes a recurring pattern, treat it as an infrastructure incident and follow:</p> <ul> <li><code>incident-response.md</code></li> </ul>"},{"location":"operations/playbooks/storagebox-sshfs-stale-mount-recovery/#sshfs-tuning-options","title":"sshfs tuning options","text":"<p>The <code>healtharchive-storagebox-sshfs.service</code> uses these sshfs options:</p> <pre><code>-o reconnect,ServerAliveInterval=15,ServerAliveCountMax=3,kernel_cache\n</code></pre> <p>These defaults are tuned for reliability:</p> <ul> <li><code>reconnect</code> - automatically reconnect when the SSH connection drops</li> <li><code>ServerAliveInterval=15</code> - send SSH keepalives every 15 seconds</li> <li><code>ServerAliveCountMax=3</code> - disconnect after 3 missed keepalives (~45s)</li> <li><code>kernel_cache</code> - use kernel caching for better performance</li> </ul> <p>If you experience frequent Errno 107 issues, consider these additional options in <code>/etc/healtharchive/storagebox.env</code> (requires service restart):</p> Option Description When to use <code>ServerAliveCountMax=5</code> Increase from 3 to tolerate more keepalive misses Unreliable network with brief dropouts <code>ConnectTimeout=30</code> Limit initial connection wait Slow network, avoids long hangs <code>max_write=65536</code> Smaller write chunks Large file writes cause timeouts <code>workaround=rename</code> Better rename handling If file moves fail intermittently <code>auto_cache</code> Smarter caching based on mtime If you see stale data <p>Note: Changing sshfs options can have unintended effects on performance and behavior. Test changes in a non-production environment first.</p>"},{"location":"operations/playbooks/warc-integrity-verification/","title":"WARC integrity verification (post-incident + pre-index)","text":"<p>Use this playbook when you suspect WARC corruption or replay integrity risk, especially after:</p> <ul> <li>sshfs/FUSE mount instability (<code>Errno 107: Transport endpoint is not connected</code>)</li> <li>unexpected crawler/container termination during WARC writes</li> <li>manual intervention on job output directories</li> </ul> <p>This playbook is intentionally procedural; for background see:</p> <ul> <li>Roadmap/incident context: <code>docs/roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> <li>Storage infra recovery: <code>storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"operations/playbooks/warc-integrity-verification/#0-safety-rules-do-not-skip","title":"0) Safety rules (do not skip)","text":"<ul> <li>Never quarantine while a job is <code>running</code>.</li> <li>Never quarantine after a job has been indexed (i.e., when <code>Snapshot</code> rows exist): moving WARCs breaks replay because <code>Snapshot.warc_path</code> must remain valid.</li> <li>If verification failures are <code>infra_error</code>, treat it as a storage incident first (recover mounts), not corruption.</li> </ul> <p>The CLI enforces the most important guards and will refuse unsafe operations.</p>"},{"location":"operations/playbooks/warc-integrity-verification/#1-pick-a-verification-level-cost-vs-confidence","title":"1) Pick a verification level (cost vs confidence)","text":"<p>The <code>ha-backend verify-warcs</code> command supports three levels:</p> <ul> <li>Level 0 (cheap): file exists, is readable, size &gt; 0</li> <li>Level 1 (moderate, default): gzip stream integrity (detect truncation/CRC issues)</li> <li>Level 2 (heavier): WARC parseability (iterate records; streams bodies)</li> </ul> <p>Recommended posture on a single VPS:</p> <ul> <li>Post-incident window: Level 1 for WARCs touched during the incident window.</li> <li>\u201cAlways on\u201d before indexing: Level 0 (built into the indexing pipeline; optional deeper checks via env).</li> </ul>"},{"location":"operations/playbooks/warc-integrity-verification/#2-verify-warcs-for-a-job-report-only","title":"2) Verify WARCs for a job (report-only)","text":"<p>Run a report-only verification:</p> <pre><code>cd /opt/healtharchive-backend\nsudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; ha-backend verify-warcs --job-id &lt;JOB_ID&gt; --level 1'\n</code></pre> <p>Bound the work if you\u2019re validating an incident window:</p> <pre><code>ha-backend verify-warcs --job-id &lt;JOB_ID&gt; --level 1 --since-minutes 180 --limit-warcs 50\n</code></pre> <p>Optional: write a Prometheus node_exporter textfile metric:</p> <pre><code>ha-backend verify-warcs --job-id &lt;JOB_ID&gt; --level 1 --metrics-file /var/lib/node_exporter/textfile_collector/healtharchive_warc_verify.prom\n</code></pre>"},{"location":"operations/playbooks/warc-integrity-verification/#3-if-verification-fails-with-infra_error","title":"3) If verification fails with <code>infra_error</code>","text":"<p>This is usually mount instability, not corruption.</p> <ul> <li>Follow <code>storagebox-sshfs-stale-mount-recovery.md</code>.</li> <li>After recovery, re-run <code>verify-warcs</code>.</li> </ul>"},{"location":"operations/playbooks/warc-integrity-verification/#4-if-verification-fails-with-corrupt_or_unreadable-pre-index-only","title":"4) If verification fails with <code>corrupt_or_unreadable</code> (pre-index only)","text":"<p>If the job has no Snapshot rows (not indexed), quarantine the corrupt WARCs:</p> <pre><code>ha-backend verify-warcs --job-id &lt;JOB_ID&gt; --level 1 --apply-quarantine\n</code></pre> <p>This will:</p> <ul> <li>move corrupt WARCs under <code>&lt;output_dir&gt;/warcs_quarantine/&lt;timestamp&gt;/...</code></li> <li>write <code>&lt;output_dir&gt;/WARCS_QUARANTINED.txt</code> with provenance + sha256</li> <li>set the job back to <code>retryable</code> and reset <code>retry_count</code> so the worker can re-run it</li> </ul> <p>Then let the worker pick it up (or restart the worker if it\u2019s not running).</p>"},{"location":"operations/playbooks/warc-integrity-verification/#5-if-verification-fails-after-indexing-snapshots-exist","title":"5) If verification fails after indexing (snapshots exist)","text":"<p>Do not quarantine: this breaks replay.</p> <p>Treat it as a critical integrity incident:</p> <ul> <li>stop automated cleanup for the affected job</li> <li>preserve the job output directory as-is</li> <li>capture a verification report (<code>--json-out</code> recommended)</li> <li>decide whether to rebuild the dataset / replay from backups, or to re-crawl the affected source</li> </ul> <p>Escalate via <code>incident-response.md</code> and record the outcome in <code>docs/operations/mentions-log.md</code>.</p>"},{"location":"operations/playbooks/warc-storage-tiering/","title":"WARC storage tiering (SSD + Storage Box)","text":"<p>Goal: keep HealthArchive running on a small VPS SSD by tiering large WARC job directories onto a Hetzner Storage Box (\u201ccold\u201d storage), while still being able to replay pages from cold storage.</p> <p>This playbook assumes:</p> <ul> <li>Production-like host paths (<code>/srv/healtharchive/**</code>)</li> <li>Existing snapshots may already reference absolute paths under   <code>/srv/healtharchive/jobs/**</code></li> <li>You want to keep paths stable (so replay keeps working) while relocating   bytes to cheaper storage.</li> </ul>"},{"location":"operations/playbooks/warc-storage-tiering/#architecture-what-runs-where","title":"Architecture (what runs where)","text":"<ul> <li>VPS (hot / canonical paths)</li> <li>Canonical archive root: <code>/srv/healtharchive/jobs</code></li> <li>Backend services read WARCs from paths recorded in the DB (often absolute     paths under <code>/srv/healtharchive/jobs/**</code>).</li> <li> <p>For tiering, we keep these canonical paths intact and mount/bind cold data     into them.</p> </li> <li> <p>Storage Box (cold bytes)</p> </li> <li>Mounted on the VPS at: <code>/srv/healtharchive/storagebox</code></li> <li>Cold mirror root (suggested): <code>/srv/healtharchive/storagebox/jobs</code></li> <li>You store large job directories here and then bind-mount them into the     canonical paths under <code>/srv/healtharchive/jobs/**</code>.</li> </ul>"},{"location":"operations/playbooks/warc-storage-tiering/#create-the-storage-box-hetzner-console","title":"Create the Storage Box (Hetzner console)","text":"<p>Recommended choices for HealthArchive:</p> <ul> <li>Plan: <code>BX11</code> (1 TB) is a good starting tier for cold WARCs.</li> <li>Location: same region as the VPS.</li> <li>Access: SSH key auth (recommended).</li> <li>Additional settings</li> <li>Enable: <code>SSH Support</code></li> <li>Disable (not needed): <code>SMB Support</code>, <code>WebDAV Support</code></li> <li>External reachability: prefer disabled (you can access via the VPS; no     need to expose to the public internet).</li> <li>Labels: optional; if you use them, keep them simple:</li> <li><code>project=healtharchive</code></li> <li><code>role=warc-cold-storage</code></li> <li><code>env=prod</code></li> </ul> <p>Notes:</p> <ul> <li>\u201cSet as default key\u201d in Hetzner means \u201cpreselect this key for future Storage   Boxes by default\u201d (it doesn\u2019t change the key material).</li> <li>On the VPS, ensure private keys are locked down:</li> <li><code>chmod 700 ~/.ssh</code></li> <li><code>chmod 600 ~/.ssh/hetzner_storagebox</code></li> <li><code>chmod 644 ~/.ssh/hetzner_storagebox.pub</code></li> </ul>"},{"location":"operations/playbooks/warc-storage-tiering/#mount-the-storage-box-on-the-vps-sshfs","title":"Mount the Storage Box on the VPS (sshfs)","text":"<p>Run these on the VPS:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y sshfs\nsudo sed -i 's/^#user_allow_other/user_allow_other/' /etc/fuse.conf\nsudo mkdir -p /srv/healtharchive/storagebox\n</code></pre> <p>Mount (SSH runs on port <code>23</code> for Storage Boxes):</p> <pre><code>GID=\"$(getent group healtharchive | cut -d: -f3)\"\nsudo sshfs -p 23 \\\n  -o IdentityFile=/home/haadmin/.ssh/hetzner_storagebox \\\n  -o allow_other,default_permissions \\\n  -o uid=\"$(id -u haadmin)\",gid=\"${GID}\",umask=0027 \\\n  -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=3,kernel_cache \\\n  uNNNNNN@uNNNNNN.your-storagebox.de:/ \\\n  /srv/healtharchive/storagebox\n</code></pre> <p>Sanity check:</p> <pre><code>touch /srv/healtharchive/storagebox/_probe &amp;&amp; rm /srv/healtharchive/storagebox/_probe\ndf -h /srv/healtharchive/storagebox\n</code></pre> <p>Create the cold mirror root:</p> <pre><code>mkdir -p /srv/healtharchive/storagebox/jobs/imports\n</code></pre>"},{"location":"operations/playbooks/warc-storage-tiering/#make-the-mount-persistent-recommended","title":"Make the mount persistent (recommended)","text":"<p>If the Storage Box isn\u2019t mounted (e.g., after reboot), tiered paths may fall back to empty local directories and replay will break. Add a small systemd unit to mount it on boot.</p> <p>Use the repo templates under <code>docs/deployment/systemd/</code> (installed via <code>scripts/vps-install-systemd-units.sh</code>).</p> <p>1) Create <code>/etc/healtharchive/storagebox.env</code> (VPS):</p> <pre><code>sudo install -d -m 0755 /etc/healtharchive\nsudo tee /etc/healtharchive/storagebox.env &gt;/dev/null &lt;&lt;'EOF'\nSTORAGEBOX_HOST=uNNNNNN.your-storagebox.de\nSTORAGEBOX_USER=uNNNNNN\nSTORAGEBOX_IDENTITY=/home/haadmin/.ssh/hetzner_storagebox\nSTORAGEBOX_UID=1000\nSTORAGEBOX_GID=999\n# Optional (defaults are fine for most setups):\n# STORAGEBOX_REMOTE_PATH=\n# STORAGEBOX_PORT=23\n# STORAGEBOX_MOUNT=/srv/healtharchive/storagebox\nEOF\n</code></pre> <p>Replace:</p> <ul> <li><code>uNNNNNN</code> values with your Storage Box username/host.</li> <li><code>STORAGEBOX_UID</code> with <code>id -u haadmin</code>.</li> <li><code>STORAGEBOX_GID</code> with <code>getent group healtharchive | cut -d: -f3</code>.</li> </ul> <p>2) Install templates + enable the mount (VPS):</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-install-systemd-units.sh --apply\nsudo systemctl enable --now healtharchive-storagebox-sshfs.service\nsystemctl status healtharchive-storagebox-sshfs.service --no-pager\nmount | rg /srv/healtharchive/storagebox || true\n</code></pre> <p>If it fails due to host key prompts, prime root\u2019s known_hosts once:</p> <pre><code>sudo ssh -p 23 -i /home/haadmin/.ssh/hetzner_storagebox uNNNNNN@uNNNNNN.your-storagebox.de exit\n</code></pre>"},{"location":"operations/playbooks/warc-storage-tiering/#move-a-job-directory-from-ssd-storage-box-safe-swap","title":"Move a job directory from SSD \u2192 Storage Box (safe swap)","text":"<p>This procedure keeps the canonical path stable.</p> <p>1) Define paths (VPS):</p> <pre><code>HOT=/srv/healtharchive/jobs/imports/&lt;job_dir_name&gt;\nCOLD=/srv/healtharchive/storagebox/jobs/imports/&lt;job_dir_name&gt;\n</code></pre> <p>2) Copy to cold tier (VPS):</p> <pre><code>mkdir -p \"$(dirname \"$COLD\")\"\nrsync -rltH --info=progress2 --no-owner --no-group --no-perms \"$HOT/\" \"$COLD/\"\n</code></pre> <p>3) Stop services (VPS):</p> <pre><code>sudo systemctl stop healtharchive-worker.service\nsudo systemctl stop healtharchive-replay.service\nsudo systemctl stop healtharchive-api.service\n</code></pre> <p>4) Swap the canonical path to point at the cold copy (VPS):</p> <pre><code>sudo mv \"$HOT\" \"${HOT}.hot-backup\"\nsudo mkdir -p \"$HOT\"\nsudo mount --bind \"$COLD\" \"$HOT\"\n</code></pre> <p>5) Start services + verify (VPS):</p> <pre><code>sudo systemctl start healtharchive-api.service\nsudo systemctl start healtharchive-worker.service\nsudo systemctl start healtharchive-replay.service\n\ncurl -fsS http://127.0.0.1:8001/api/health &gt;/dev/null &amp;&amp; echo OK\n</code></pre> <p>6) If everything is OK, delete the backup (VPS):</p> <pre><code>sudo rm -rf \"${HOT}.hot-backup\"\n</code></pre> <p>Rollback (if needed):</p> <pre><code>sudo umount \"$HOT\"\nsudo rm -rf \"$HOT\"\nsudo mv \"${HOT}.hot-backup\" \"$HOT\"\n</code></pre>"},{"location":"operations/playbooks/warc-storage-tiering/#make-tiered-job-mounts-persistent-recommended","title":"Make tiered job mounts persistent (recommended)","text":"<p>Bind mounts created manually will not survive a reboot. Use the repo\u2019s unit template + manifest so tiered jobs come back automatically.</p> <p>1) Create the manifest <code>/etc/healtharchive/warc-tiering.binds</code> (VPS):</p> <pre><code>sudo tee /etc/healtharchive/warc-tiering.binds &gt;/dev/null &lt;&lt;'EOF'\n# cold_path hot_path\n/srv/healtharchive/storagebox/jobs/imports/legacy-hc-2025-04-21 /srv/healtharchive/jobs/imports/legacy-hc-2025-04-21\n/srv/healtharchive/storagebox/jobs/imports/legacy-cihr-2025-04 /srv/healtharchive/jobs/imports/legacy-cihr-2025-04\nEOF\n</code></pre> <p>2) Enable the service (VPS):</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-install-systemd-units.sh --apply\nsudo systemctl enable --now healtharchive-warc-tiering.service\nsystemctl status healtharchive-warc-tiering.service --no-pager\nmount | rg /srv/healtharchive/jobs/imports/legacy- || true\n</code></pre> <p>Note: the template service runs <code>vps-warc-tiering-bind-mounts.sh --apply --repair-stale-mounts</code> so it can automatically unmount stale Errno 107 mountpoints and re-apply bind mounts.</p> <p>If the unit is in a <code>failed</code> state from a prior incident, clear it before retrying:</p> <pre><code>systemctl is-failed healtharchive-warc-tiering.service &amp;&amp; sudo systemctl reset-failed healtharchive-warc-tiering.service || true\nsudo systemctl start healtharchive-warc-tiering.service\n</code></pre> <p>Manual validation (safe):</p> <pre><code>sudo /opt/healtharchive-backend/scripts/vps-warc-tiering-bind-mounts.sh\n</code></pre> <p>If <code>healtharchive-warc-tiering.service</code> repeatedly ends up in <code>failed</code> (e.g., after an sshfs disconnect), consider enabling the tiering health metrics timer so failures are visible quickly:</p> <pre><code>sudo systemctl enable --now healtharchive-tiering-metrics.timer\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_tiering_' || true\n</code></pre>"},{"location":"operations/playbooks/warc-storage-tiering/#replay-note-restart-after-tiering-changes","title":"Replay note (restart after tiering changes)","text":"<p>Replay runs in a long-lived Docker container and bind-mounts <code>/srv/healtharchive/jobs</code> into <code>/warcs</code>. After fixing stale mounts or changing tiering binds, restart replay so it sees a clean view of the mountpoints:</p> <pre><code>sudo systemctl restart healtharchive-replay.service\nsudo systemctl start healtharchive-replay-smoke.service\n</code></pre>"},{"location":"operations/playbooks/warc-storage-tiering/#annual-outputs-automatically-tier-to-storage-box","title":"Annual outputs: automatically tier to Storage Box","text":"<p>If you use the annual scheduler timer (<code>healtharchive-schedule-annual.timer</code>), the systemd template now triggers <code>healtharchive-annual-output-tiering.service</code> on success. This bind-mounts the newly enqueued annual job output directories onto the Storage Box tier and briefly stops the worker to reduce race conditions.</p> <p>To apply the updated template on the VPS:</p> <pre><code>cd /opt/healtharchive-backend\ngit pull\nsudo ./scripts/vps-install-systemd-units.sh --apply\nsudo systemctl daemon-reload\n</code></pre>"},{"location":"operations/playbooks/warc-storage-tiering/#rehearsal-before-jan-01-optional","title":"Rehearsal before Jan 01 (optional)","text":"<p>The tiering script selects annual jobs using the Jan 01\u2013Jan 03 UTC window by default. If you want to rehearse the end-to-end scheduling + tiering workflow before Jan 01, you can override the selection window:</p> <p>1) Stop the worker (prevents any queued jobs from running):</p> <pre><code>sudo systemctl stop healtharchive-worker.service\n</code></pre> <p>2) Enqueue annual jobs (this affects the production DB; delete them afterwards if you do not want them queued):</p> <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend schedule-annual --apply --year 2026 --sources hc phac cihr\n</code></pre> <p>3) Apply tiering for the jobs you just created (use a short window around \u201cnow\u201d):</p> <pre><code>sudo /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py \\\n  --apply \\\n  --year 2026 \\\n  --created-after \"$(date -u -d '2 hours ago' +%Y-%m-%dT%H:%M:%SZ)\"\n</code></pre> <p>4) Validate the expected output dirs are mounted (and that storagebox is still mounted):</p> <pre><code>mount | rg '/srv/healtharchive/storagebox|/srv/healtharchive/jobs/(hc|phac|cihr)/' || true\n</code></pre> <p>5) Decide what to do with the queued annual jobs:</p> <ul> <li>If you want to keep them queued for Jan 01, leave the worker stopped until you are ready.</li> <li>If you do not want them queued yet, delete them via the admin UI or CLI before restarting the worker.</li> </ul>"},{"location":"operations/playbooks/warc-storage-tiering/#alerting-for-tiering-recommended","title":"Alerting for tiering (recommended)","text":"<p>If you use Prometheus alerting, enable the tiering metrics writer:</p> <pre><code>sudo systemctl enable --now healtharchive-tiering-metrics.timer\n</code></pre> <p>This requires node_exporter to have the textfile collector enabled (the repo installer does this):</p> <pre><code>cd /opt/healtharchive-backend\ngit pull\nsudo ./scripts/vps-install-observability-exporters.sh --apply\n</code></pre>"},{"location":"operations/playbooks/warc-storage-tiering/#promote-cold-hot-later-optional","title":"Promote (cold \u2192 hot) later (optional)","text":"<p>If you decide a job should be \u201chot\u201d again:</p> <p>1) Stop services. 2) <code>umount</code> the canonical path. 3) <code>rsync</code> cold \u2192 hot (SSD). 4) Start services.</p> <p>This is the inverse of the \u201csafe swap\u201d above.</p>"},{"location":"operations/playbooks/warc-storage-tiering/#preflight-implications","title":"Preflight implications","text":"<ul> <li>If you intend the upcoming annual campaign outputs to land on the Storage Box,   run preflight with:</li> <li><code>YEAR=2026; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\" --campaign-archive-root /srv/healtharchive/storagebox/jobs</code></li> <li>Ensure the Storage Box mount is active before preflight and before the annual   campaign runs.</li> </ul>"},{"location":"operations/playbooks/warc-storage-tiering/#annual-campaign-with-a-tiny-ssd-operational-pattern","title":"Annual campaign with a tiny SSD (operational pattern)","text":"<p>If the campaign won\u2019t fit on SSD:</p> <p>1) Keep the Storage Box mounted. 2) Schedule the annual jobs (dry-run first), then create a cold output directory    and bind-mount it into the canonical job output directory before the    worker runs the job.</p> <p>Sketch:</p> <pre><code># After jobs exist (queued), get the job output dir:\nJOB_ID=123\n/opt/healtharchive-backend/.venv/bin/ha-backend show-job --id \"$JOB_ID\" | rg output_dir\n\n# Suppose output_dir is:\nHOT=/srv/healtharchive/jobs/hc/20260101T000000Z__hc-2026\n\n# Create a matching cold location and mount it into place:\nCOLD=/srv/healtharchive/storagebox/jobs/hc/20260101T000000Z__hc-2026\nmkdir -p \"$COLD\"\nsudo mount --bind \"$COLD\" \"$HOT\"\n</code></pre> <p>This keeps DB WARC paths under <code>/srv/healtharchive/jobs/**</code> (stable), while the bytes live on Storage Box.</p>"},{"location":"operations/playbooks/warc-storage-tiering/#common-pitfalls","title":"Common pitfalls","text":"<ul> <li><code>mkdir: Permission denied</code> under <code>/srv/healtharchive/storagebox/**</code>:</li> <li>Your sshfs mount is mapped to the wrong UID/GID; remount with     <code>uid=$(id -u haadmin)</code>, <code>gid=$(getent group healtharchive | cut -d: -f3)</code>,     and a restrictive <code>umask</code>.</li> <li><code>rsync ... chgrp failed: Permission denied</code>:</li> <li>Use <code>--no-owner --no-group --no-perms</code> for cross-filesystem copies to     Storage Box.</li> <li>Storage Box not mounted (but directories still exist):</li> <li>The filesystem checks will silently target the SSD unless you validate the     mount; keep the mount persistent and verify with <code>mount | grep storagebox</code>.</li> </ul>"},{"location":"operations/reports/2026-01-19-deployment-log/","title":"Deployment Log: Annual Crawl Hardening 2026","text":"<p>Date: 2026-01-19 Operator: Auto-Deployment Agent Scope: VPS Production Environment (Job 6, 7, 8)</p>"},{"location":"operations/reports/2026-01-19-deployment-log/#objectives","title":"Objectives","text":"<ul> <li>Deploy strict timeout handling (180s) to prevent stalls.</li> <li>Enable auto-recovery for SSHFS mounts and worker processes.</li> <li>Implement deep operational monitoring (metrics &amp; alerts).</li> </ul>"},{"location":"operations/reports/2026-01-19-deployment-log/#execution-log","title":"Execution Log","text":"Time (EST) Phase Action Result 10:45 Phase 1 Pre-deployment state capture Baseline recorded. Job 6 running. 10:50 Phase 2 Codebase Update Pulled <code>main</code> (commit <code>18a8818</code>). 10:55 Phase 3 Service Restart Worker restarted. <code>daemon-reload</code> applied. 11:00 Phase 4 Verification Job detected. Metrics confirmed flowing. 11:15 Phase 5 Investigation Confirmed <code>indexed_pages=0</code> is expected behavior. 11:30 Phase 6 Alerting 9 Alert rules verified with <code>promtool</code>."},{"location":"operations/reports/2026-01-19-deployment-log/#final-status-verified","title":"Final Status Verified","text":"<ul> <li>Job 6 Status: Running (Active).</li> <li>Progress: 359/2908 pages scanned. 56 WARCs generated.</li> <li>Monitoring: Active. <code>node_exporter</code> scraping <code>healtharchive_crawl.prom</code>.</li> <li>Alerts: 9 Rules active. \"Zero Rules Firing\" (Green state).</li> </ul>"},{"location":"operations/reports/2026-01-19-deployment-log/#handoff-notes","title":"Handoff Notes","text":"<ul> <li>New Ops Docs: See <code>docs/operations/monitoring-and-alerting.md</code> and <code>docs/operations/runbooks/</code>.</li> <li>Next scheduled action: None. System is in auto-pilot.</li> </ul>"},{"location":"operations/reports/2026-01-19-indexing-investigation/","title":"Investigation Report: Indexing Delay / Zero Indexed Pages","text":"<p>Date: 2026-01-19 Subject: Job 6 \"indexed_pages\" count remaining at 0 despite WARC generation. Status: RESOLVED (Expected Behavior)</p>"},{"location":"operations/reports/2026-01-19-indexing-investigation/#issue-description","title":"Issue Description","text":"<p>During the deployment of the 2026 Annual Crawl Hardening, it was observed that Job 6 (Health Canada) had generated 56 WARC files but the <code>indexed_pages</code> metric in the database remained at <code>0</code>. This raised concerns that the indexing pipeline was broken or stalled.</p>"},{"location":"operations/reports/2026-01-19-indexing-investigation/#investigation-steps-phase-5","title":"Investigation Steps (Phase 5)","text":"<ol> <li>Static Analysis: Searched for <code>index_job</code> calls in the worker source code.</li> <li>Runtime Analysis: Verified <code>healtharchive-worker</code> logs.</li> <li>State Verification: Checked filesystem for WARCs vs DB status.</li> </ol>"},{"location":"operations/reports/2026-01-19-indexing-investigation/#findings","title":"Findings","text":"<ol> <li> <p>Indexing is Terminal:    Code analysis of <code>src/ha_backend/worker/main.py</code> confirmed that <code>index_job(job_id)</code> is only called after the crawl loop exits successfully.    Unlike some crawlers that index incrementally, HealthArchive currently indexes in batches after the crawl completes.</p> </li> <li> <p>Crawl is Active:    Job 6 is still in <code>running</code> state.</p> </li> <li>56 WARC files exist on disk.</li> <li> <p><code>last_progress</code> timestamps are updating.</p> </li> <li> <p>Conclusion:    The <code>indexed_pages=0</code> metric is correct for a running job. It will update to the full count once the job finishes and the indexing phase begins.</p> </li> </ol>"},{"location":"operations/reports/2026-01-19-indexing-investigation/#hardening-actions-taken","title":"Hardening Actions Taken","text":"<p>To prevent future confusion and catch actual indexing failures:</p> <ol> <li>New Alert: <code>IndexingNotStartedAfterCrawl</code> (in <code>prometheus-alerts-crawl.yml</code>).</li> <li>Fires if <code>status='completed'</code> AND <code>indexed_pages=0</code> for &gt; 1 hour.</li> <li>Runbook: <code>docs/operations/runbooks/indexing-not-started.md</code>.</li> </ol>"},{"location":"operations/reports/2026-01-19-indexing-investigation/#resolution","title":"Resolution","text":"<p>No fix required. The system is functioning as designed. Monitoring will alert if the post-crawl indexing fails.</p>"},{"location":"operations/runbooks/crawl-restart-budget-low/","title":"Runbook: CrawlRestartBudgetLow","text":"<p>Alert Name: <code>CrawlRestartBudgetLow</code> Severity: Warning Trigger: <code>healtharchive_crawl_running_job_container_restarts_done &gt; 15</code> (limit is 20).</p>"},{"location":"operations/runbooks/crawl-restart-budget-low/#description","title":"Description","text":"<p>The annual crawl job is restarting its zimit container frequently. Annual jobs are configured with <code>max_container_restarts=20</code> to survive occasional timeouts or memory leaks. Reaching 15 restarts means the job is consuming its budget faster than expected and risks failing completely.</p>"},{"location":"operations/runbooks/crawl-restart-budget-low/#impact","title":"Impact","text":"<ul> <li>If restarts hit 20, the job enters <code>failed</code> state and crawling stops.</li> <li>This protects the infrastructure from infinite loops but might leave the crawl incomplete.</li> </ul>"},{"location":"operations/runbooks/crawl-restart-budget-low/#diagnosis","title":"Diagnosis","text":"<ol> <li>Check Restart Rate:    Is the job restarting every few minutes (thrashing) or once every few hours?</li> </ol> <pre><code># Check restart timestamps\n/opt/healtharchive-backend/scripts/vps-crawl-status.sh\n</code></pre> <ol> <li>Review Crash Reasons:    Check the combined log for the reason before the restart.</li> </ol> <pre><code>tail -n 500 /srv/healtharchive/jobs/&lt;source&gt;/archive_*.combined.log\n</code></pre> <ul> <li>TimeoutErrors: Site is too slow.</li> <li>HTTP 5xx: Site is overloaded.</li> <li>OOM / Killed: Zimit is running out of RAM.</li> </ul>"},{"location":"operations/runbooks/crawl-restart-budget-low/#mitigation","title":"Mitigation","text":"<ol> <li>Increase Budget (If progress is good):    If the job is making good progress (thousands of pages) and just hitting occasional glitches, you can manually increase the budget in the database to keep it going.</li> </ol> <pre><code>ha-backend db-shell\n# UPDATE archive_jobs SET tool_options = jsonb_set(tool_options, '{max_container_restarts}', '30') WHERE id=6;\n</code></pre> <p>Then restart the worker to pick up the new config:</p> <pre><code>sudo systemctl restart healtharchive-worker.service\n</code></pre> <ol> <li>Pause Job (If thrashing):    If restarts are happening rapidly with no progress, pause the job to save resources.</li> </ol> <pre><code>ha-backend db-shell\n# UPDATE archive_jobs SET status='paused' WHERE id=6;\n</code></pre>"},{"location":"operations/runbooks/crawl-state-file-probe-failure/","title":"Runbook: CrawlStateFileProbeFailure","text":"<p>Alert Name: <code>CrawlStateFileProbeFailure</code> Severity: Warning Trigger: <code>healtharchive_crawl_running_job_state_file_ok == 0</code> for 5m.</p>"},{"location":"operations/runbooks/crawl-state-file-probe-failure/#description","title":"Description","text":"<p>The monitoring script on the VPS cannot read the <code>.archive_state.json</code> file for a running job. This almost always indicates that the SSHFS mount to the Hetzner StorageBox has dropped or disconnected.</p>"},{"location":"operations/runbooks/crawl-state-file-probe-failure/#impact","title":"Impact","text":"<ul> <li>Metrics will flatline.</li> <li>Adaptive Worker scaling will pause (cannot read state).</li> <li>The crawl itself might still be running (container has its own mount namespace), but new output writes might eventually fail if the host mount is totally dead.</li> </ul>"},{"location":"operations/runbooks/crawl-state-file-probe-failure/#diagnosis","title":"Diagnosis","text":"<ol> <li>Check Mounts:    ssh to VPS and run:</li> </ol> <pre><code>findmnt -T /srv/healtharchive/jobs\n</code></pre> <p>If it returns nothing or shows \"unreachable\", the mount is gone.</p> <ol> <li>Check StorageBox Connectivity:</li> </ol> <pre><code>ping -c 3 u524803.your-storagebox.de\n</code></pre> <ol> <li>Check Permissions:    If mount is up, check if the file exists and is readable:</li> </ol> <pre><code>ls -la /srv/healtharchive/jobs/&lt;source&gt;/&lt;job_timestamp&gt;/.archive_state.json\n</code></pre>"},{"location":"operations/runbooks/crawl-state-file-probe-failure/#mitigation","title":"Mitigation","text":"<ol> <li>Remount SSHFS:</li> </ol> <pre><code>sudo systemctl restart healtharchive-storagebox-sshfs.service\n</code></pre> <p>Verify mount is back:</p> <pre><code>df -h | grep storagebox\n</code></pre> <ol> <li>Restart Workers (If simple remount fails):    If the mount was stale, the worker process might be hung on I/O.</li> </ol> <pre><code>sudo systemctl restart healtharchive-worker.service\n</code></pre>"},{"location":"operations/runbooks/indexing-not-started/","title":"Runbook: IndexingNotStartedAfterCrawl","text":"<p>Alert Name: <code>IndexingNotStartedAfterCrawl</code> Severity: Critical Trigger: <code>status=\"completed\"</code> AND <code>indexed_pages == 0</code> for &gt; 1 hour.</p>"},{"location":"operations/runbooks/indexing-not-started/#description","title":"Description","text":"<p>A crawl job has successfully completed (reached the \"completed\" state with RC 0), but the <code>indexed_pages</code> count remains at 0 for more than an hour. This suggests the indexing pipeline\u2014which should run immediately after crawl completion\u2014failed to start or crash-looped silently.</p>"},{"location":"operations/runbooks/indexing-not-started/#background","title":"Background","text":"<p>The <code>healtharchive-worker</code> process runs jobs in two phases:</p> <ol> <li>Crawl Phase: Runs <code>archive-tool</code> container.</li> <li>Index Phase: Runs <code>index_job()</code> Python function.</li> </ol> <p>If phase 1 finishes but phase 2 crashes or fails to commit, this alert fires.</p>"},{"location":"operations/runbooks/indexing-not-started/#diagnosis","title":"Diagnosis","text":"<ol> <li>Check Indexing Logs:    Search for the transition from crawl to index in the worker logs.</li> </ol> <pre><code>sudo journalctl -u healtharchive-worker.service --since \"4 hours ago\" | grep -i \"indexing\"\n</code></pre> <p>Look for:    - <code>Starting indexing for job &lt;ID&gt;</code> (Good)    - <code>Indexing for job &lt;ID&gt; failed: ...</code> (Bad)</p> <ol> <li>Verify WARC Existence:    Confirm the WARCs physically exist.</li> </ol> <pre><code>/opt/healtharchive-backend/scripts/vps-crawl-status.sh --job-id &lt;ID&gt;\n</code></pre> <ol> <li>Check Job Status:    Is the job status actually <code>completed</code> or <code>index_failed</code>?</li> </ol> <pre><code>ha-backend show-job &lt;ID&gt;\n</code></pre>"},{"location":"operations/runbooks/indexing-not-started/#mitigation","title":"Mitigation","text":"<ol> <li>Manual Re-indexing:    If the pipeline failed transiently (e.g. DB lock), you can reset the job status to trigger re-indexing.    Warning: This restarts the logic loop. Ensure the crawl is truly done.</li> </ol> <pre><code>ha-backend reindex-job &lt;ID&gt;\n</code></pre> <p>(Note: Check if <code>reindex-job</code> CLI command exists, otherwise use python shell):</p> <pre><code>from ha_backend.indexing import index_job\nindex_job(&lt;ID&gt;)\n</code></pre>"},{"location":"reference/archive-tool/","title":"Archive Tool Reference","text":"<p>The archive_tool is HealthArchive's internal crawler and orchestrator subpackage.</p>"},{"location":"reference/archive-tool/#quick-overview","title":"Quick Overview","text":"<p>archive_tool is a Docker-based web crawler that: - Wraps the <code>zimit</code> crawler (from OpenZIM) - Manages crawl state and resumption - Monitors crawl health (stall detection, error thresholds) - Supports adaptive worker scaling - Optionally rotates VPN connections</p> <p>Location: <code>src/archive_tool/</code></p> <p>Technology: - Python 3.11+ - Docker (runs <code>ghcr.io/openzim/zimit</code> container) - State persistence (<code>.archive_state.json</code>)</p> <p>Environment Variables: - <code>HEALTHARCHIVE_ZIMIT_DOCKER_IMAGE</code>: Override Docker image (default: <code>ghcr.io/openzim/zimit</code>) - <code>HEALTHARCHIVE_DOCKER_MEMORY_LIMIT</code>: Container memory limit (default: <code>4g</code>) - <code>HEALTHARCHIVE_DOCKER_CPU_LIMIT</code>: Container CPU limit (default: <code>1.5</code>)</p> <p>Note: the HealthArchive backend indexes WARCs into <code>Snapshot</code> rows; it does not read <code>.zim</code> files. ZIM output is an optional artifact and can be skipped with <code>--skip-final-build</code>.</p>"},{"location":"reference/archive-tool/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 HealthArchive Backend                           \u2502\n\u2502                                                 \u2502\n\u2502  ha_backend.jobs.run_persistent_job()           \u2502\n\u2502         \u2502                                       \u2502\n\u2502         \u251c\u2500\u2500&gt; Builds CLI args from job config   \u2502\n\u2502         \u2502                                       \u2502\n\u2502         \u2514\u2500\u2500&gt; subprocess.run()                   \u2502\n\u2502                     \u2502                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n                      \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502   archive-tool CLI         \u2502\n         \u2502   (archive_tool/cli.py)    \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n                      \u251c\u2500\u2500&gt; Validates Docker\n                      \u251c\u2500\u2500&gt; Determines run mode\n                      \u251c\u2500\u2500&gt; Spawns zimit in Docker\n                      \u251c\u2500\u2500&gt; Monitors progress\n                      \u251c\u2500\u2500&gt; Writes WARCs to .tmp_N/\n                      \u2514\u2500\u2500&gt; Builds ZIM (optional)\n</code></pre>"},{"location":"reference/archive-tool/#canonical-documentation","title":"Canonical Documentation","text":"<p>Full technical reference: <code>src/archive_tool/docs/documentation.md</code></p> <p>1,508 lines covering: - CLI interface and all flags - Run modes (Fresh, Resume, New-with-Consolidation, Overwrite) - State management (<code>.archive_state.json</code>) - Docker orchestration details - Monitoring and adaptive workers - VPN rotation mechanism - WARC discovery and consolidation - Error handling and recovery - Testing and development</p> <p>Read the full docs for: - Detailed CLI flag reference - State machine diagrams - Docker volume mapping - Log parsing internals - Adding new features to archive_tool</p>"},{"location":"reference/archive-tool/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/archive-tool/#cli-usage","title":"CLI Usage","text":"<pre><code>archive-tool \\\n  --name CRAWL_NAME \\\n  --output-dir /path/to/output \\\n  --initial-workers N \\\n  [--enable-monitoring] \\\n  [--enable-adaptive-workers] \\\n  [--enable-vpn-rotation --vpn-connect-command \"...\"] \\\n  SEED_URL [SEED_URL...]\n</code></pre>"},{"location":"reference/archive-tool/#common-flags","title":"Common Flags","text":"Flag Purpose <code>--name</code> Crawl name (used in output naming; ZIM is optional) <code>--output-dir</code> Output directory path <code>--initial-workers</code> Number of parallel workers (default: 1) <code>--enable-monitoring</code> Enable stall/error detection <code>--stall-timeout-minutes</code> Abort if no progress (requires monitoring) <code>--enable-adaptive-workers</code> Reduce workers on errors (requires monitoring) <code>--enable-vpn-rotation</code> Rotate VPN on stalls (requires monitoring) <code>--docker-shm-size</code> Increase container <code>/dev/shm</code> (can improve stability) <code>--skip-final-build</code> Skip the final <code>.zim</code> build stage (WARCs still produced) <code>--cleanup</code> Delete temp dirs after successful crawl <code>--overwrite</code> Delete existing output before starting"},{"location":"reference/archive-tool/#run-modes","title":"Run Modes","text":"<p>archive-tool automatically determines the run mode based on state:</p> <ol> <li>Fresh - No prior state, start new crawl</li> <li>Resume - State exists and incomplete, resume from checkpoint</li> <li>New-with-Consolidation - State complete, start new crawl but consolidate WARCs</li> <li>Overwrite - <code>--overwrite</code> flag set, delete everything and start fresh</li> </ol> <p>See: <code>src/archive_tool/docs/documentation.md</code> (Run Modes) for decision tree</p>"},{"location":"reference/archive-tool/#output-structure","title":"Output Structure","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 .archive_state.json              # Persistent state\n\u251c\u2500\u2500 .tmp_1/                          # First crawl attempt\n\u2502   \u2514\u2500\u2500 collections/\n\u2502       \u2514\u2500\u2500 crawl-YYYYMMDD.../\n\u2502           \u251c\u2500\u2500 archive/             # WARCs here\n\u2502           \u2502   \u251c\u2500\u2500 rec-00000-....warc.gz\n\u2502           \u2502   \u2514\u2500\u2500 rec-00001-....warc.gz\n\u2502           \u2514\u2500\u2500 logs/\n\u251c\u2500\u2500 .tmp_2/                          # Second attempt (if restarted)\n\u251c\u2500\u2500 archive_STAGE_TIMESTAMP.log      # Individual stage logs\n\u251c\u2500\u2500 archive_STAGE_TIMESTAMP.combined.log  # Aggregated logs\n\u2514\u2500\u2500 zim/\n    \u2514\u2500\u2500 NAME_DATE.zim                # Optional ZIM file\n</code></pre>"},{"location":"reference/archive-tool/#state-file-format","title":"State File Format","text":"<p><code>.archive_state.json</code>: <pre><code>{\n  \"current_workers\": 4,\n  \"initial_workers\": 4,\n  \"temp_dirs_host_paths\": [\"/some/output/.tmp123\", \"...\"],\n  \"vpn_rotations_done\": 1,\n  \"worker_reductions_done\": 1,\n  \"container_restarts_done\": 1\n}\n</code></pre></p>"},{"location":"reference/archive-tool/#backend-integration","title":"Backend Integration","text":"<p>The backend calls archive-tool via subprocess. Key files:</p>"},{"location":"reference/archive-tool/#job-execution","title":"Job Execution","text":"<p><code>ha_backend/jobs.py:run_persistent_job()</code> (lines 439-560): - Loads <code>ArchiveJob.config</code> from database - Translates <code>tool_options</code> to CLI flags - Builds command: <code>archive-tool --flag1 val1 --flag2 val2 ... SEEDS</code> - Executes with <code>subprocess.run()</code> - Updates job status based on exit code</p> <p>Config \u2192 CLI Mapping: <pre><code>config[\"tool_options\"][\"enable_monitoring\"] \u2192 --enable-monitoring\nconfig[\"tool_options\"][\"initial_workers\"] \u2192 --initial-workers N\nconfig[\"tool_options\"][\"stall_timeout_minutes\"] \u2192 --stall-timeout-minutes N\n</code></pre></p>"},{"location":"reference/archive-tool/#warc-discovery","title":"WARC Discovery","text":"<p><code>ha_backend/indexing/warc_discovery.py</code>: - Uses <code>archive_tool.state.CrawlState</code> to load <code>.archive_state.json</code> - Uses <code>archive_tool.utils.find_all_warc_files()</code> to locate WARCs - Ensures backend and archive-tool use identical logic</p>"},{"location":"reference/archive-tool/#cleanup","title":"Cleanup","text":"<p><code>ha_backend/cli/cmd_cleanup_job.py</code>: - Uses <code>archive_tool.utils.cleanup_temp_dirs()</code> to remove <code>.tmp*</code> directories - Deletes <code>.archive_state.json</code> - Updates <code>ArchiveJob.cleanup_status</code></p>"},{"location":"reference/archive-tool/#monitoring-features","title":"Monitoring Features","text":""},{"location":"reference/archive-tool/#stall-detection","title":"Stall Detection","text":"<p>When <code>--enable-monitoring</code> is set: - Monitors log output every <code>--monitor-interval-seconds</code> (default: 30) - Parses \"Crawl statistics\" JSON from logs - Detects stalls: no new pages for <code>--stall-timeout-minutes</code> - Action: Abort crawl with non-zero exit code</p>"},{"location":"reference/archive-tool/#error-thresholds","title":"Error Thresholds","text":"<ul> <li><code>--error-threshold-timeout N</code>: Abort if N timeout errors</li> <li><code>--error-threshold-http N</code>: Abort if N HTTP errors</li> <li>Prevents runaway crawls that repeatedly fail</li> </ul>"},{"location":"reference/archive-tool/#adaptive-workers","title":"Adaptive Workers","text":"<p>When <code>--enable-adaptive-workers</code> is set: - Reduces worker count on sustained errors - Min workers: <code>--min-workers</code> (default: 1) - Max reductions: <code>--max-worker-reductions</code> (default: 2) - Strategy: Reduce by 1 each time threshold exceeded</p>"},{"location":"reference/archive-tool/#vpn-rotation","title":"VPN Rotation","text":"<p>When <code>--enable-vpn-rotation</code> is set: - Rotates VPN connection on stalls or errors - Command: <code>--vpn-connect-command \"vpn connect server\"</code> - Frequency: Every <code>--vpn-rotation-frequency-minutes</code> - Max rotations: <code>--max-vpn-rotations</code></p> <p>Use case: Avoid IP bans during large crawls</p>"},{"location":"reference/archive-tool/#development","title":"Development","text":""},{"location":"reference/archive-tool/#running-locally","title":"Running Locally","text":"<pre><code># Direct execution\ncd src/archive_tool\npython -m archive_tool.cli \\\n  --name test \\\n  --output-dir /tmp/test-crawl \\\n  https://example.com\n\n# Via installed command\narchive-tool --name test --output-dir /tmp/test https://example.com\n</code></pre>"},{"location":"reference/archive-tool/#testing","title":"Testing","text":"<pre><code># Run archive_tool tests\npytest tests/test_archive_tool*.py\n\n# Test state management\npytest tests/test_archive_state.py\n\n# Test WARC discovery\npytest tests/test_warc_discovery.py\n</code></pre>"},{"location":"reference/archive-tool/#adding-new-features","title":"Adding New Features","text":"<ol> <li>Modify CLI (<code>archive_tool/cli.py</code>):</li> <li>Add new argument to <code>argparse</code></li> <li> <p>Update <code>run_with_parsed_args()</code></p> </li> <li> <p>Update contract (<code>ha_backend/archive_contract.py</code>):</p> </li> <li>Add field to <code>ArchiveToolOptions</code> TypedDict</li> <li> <p>Update <code>validate_tool_options()</code></p> </li> <li> <p>Update backend (<code>ha_backend/jobs.py</code>):</p> </li> <li> <p>Add CLI flag construction in <code>run_persistent_job()</code></p> </li> <li> <p>Update job registry (<code>ha_backend/job_registry.py</code>):</p> </li> <li> <p>Add to <code>default_tool_options</code> if needed</p> </li> <li> <p>Add tests:</p> </li> <li><code>tests/test_archive_contract.py</code> - Config validation</li> <li><code>tests/test_jobs_persistent.py</code> - CLI construction</li> <li><code>tests/test_archive_tool_*.py</code> - archive_tool behavior</li> </ol> <p>See: <code>src/archive_tool/docs/documentation.md</code> (Development) for details</p>"},{"location":"reference/archive-tool/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/archive-tool/#docker-issues","title":"Docker Issues","text":"<p>Problem: \"Cannot connect to Docker daemon\"</p> <p>Solution: <pre><code>sudo systemctl start docker\ndocker ps  # Verify\n</code></pre></p> <p>Problem: Permission denied accessing Docker socket</p> <p>Solution: <pre><code>sudo usermod -aG docker $USER\n# Log out and back in\n</code></pre></p>"},{"location":"reference/archive-tool/#state-issues","title":"State Issues","text":"<p>Problem: Crawl won't resume</p> <p>Solution: <pre><code># Check state file\ncat output_dir/.archive_state.json\n\n# Force fresh start\narchive-tool --overwrite ...\n</code></pre></p> <p>Problem: WARCs not found</p> <p>Solution: <pre><code># Manually check\nfind output_dir -name \"*.warc.gz\"\n\n# Verify state points to correct dirs\ncat output_dir/.archive_state.json | jq '.temp_dirs'\n</code></pre></p>"},{"location":"reference/archive-tool/#monitoring-issues","title":"Monitoring Issues","text":"<p>Problem: Adaptive workers not triggering</p> <p>Check: 1. <code>--enable-monitoring</code> is set 2. <code>--enable-adaptive-workers</code> is set 3. Errors exceed threshold 4. Not already at <code>--min-workers</code></p>"},{"location":"reference/archive-tool/#performance-tuning","title":"Performance Tuning","text":""},{"location":"reference/archive-tool/#worker-count","title":"Worker Count","text":"<ul> <li>Default: 1 worker (conservative)</li> <li>Small sites: 1-2 workers</li> <li>Medium sites: 2-4 workers</li> <li>Large sites: 4-8 workers (watch resource usage)</li> </ul> <p>Factors: - Server CPU/memory - Network bandwidth - Site's rate limiting - Politeness requirements</p>"},{"location":"reference/archive-tool/#memory-usage","title":"Memory Usage","text":"<p>Docker container memory (per worker): - ~500MB base - +200-500MB per worker - +500MB-1GB for large sites</p> <p>Example: 4 workers \u2248 2-4GB RAM</p>"},{"location":"reference/archive-tool/#disk-io","title":"Disk I/O","text":"<p>WARCs write continuously: - 10-50MB/min for typical sites - 100-500MB/min for large sites</p> <p>Ensure: - Fast disk (SSD recommended) - Sufficient space (check <code>df -h</code> before starting) - No I/O bottlenecks (<code>iostat -x 1</code>)</p>"},{"location":"reference/archive-tool/#related-documentation","title":"Related Documentation","text":"<ul> <li>Full archive_tool docs: <code>src/archive_tool/docs/documentation.md</code> (Start here for details)</li> <li>Backend integration: ../architecture.md#5-archive_tool-integration</li> <li>Job execution: ../architecture.md#52-run_persistent_job</li> <li>CLI commands: cli-commands.md</li> <li>Debugging crawls: ../tutorials/debug-crawl.md</li> </ul>"},{"location":"reference/cli-commands/","title":"CLI Commands Reference","text":"<p>Complete reference for <code>ha-backend</code> command-line interface.</p>"},{"location":"reference/cli-commands/#installation","title":"Installation","text":"<p>The <code>ha-backend</code> command is installed when you install the package:</p> <pre><code>pip install -e .\n# or\nmake venv\n</code></pre> <p>Verify installation: <pre><code>ha-backend --help\n</code></pre></p>"},{"location":"reference/cli-commands/#command-categories","title":"Command Categories","text":"Category Commands Environment <code>check-env</code>, <code>check-archive-tool</code>, <code>check-db</code> Job Management <code>create-job</code>, <code>run-db-job</code>, <code>index-job</code>, <code>register-job-dir</code> Direct Execution <code>run-job</code> Inspection <code>list-jobs</code>, <code>show-job</code> Maintenance <code>retry-job</code>, <code>cleanup-job</code>, <code>replay-index-job</code> Seeding <code>seed-sources</code> Worker <code>start-worker</code> Change Tracking <code>compute-changes</code>"},{"location":"reference/cli-commands/#environment-commands","title":"Environment Commands","text":""},{"location":"reference/cli-commands/#check-env","title":"check-env","text":"<p>Check environment configuration and ensure archive root exists.</p> <p>Usage: <pre><code>ha-backend check-env\n</code></pre></p> <p>Output: <pre><code>Archive root: /mnt/nasd/nobak/healtharchive/jobs\nArchive root exists: True\nArchive tool command: archive-tool\n</code></pre></p> <p>Exit codes: - <code>0</code> - Success - <code>1</code> - Archive root missing</p>"},{"location":"reference/cli-commands/#check-archive-tool","title":"check-archive-tool","text":"<p>Verify archive-tool is available and functional.</p> <p>Usage: <pre><code>ha-backend check-archive-tool\n</code></pre></p> <p>What it does: - Runs <code>archive-tool --help</code> - Validates command is available</p> <p>Exit codes: - <code>0</code> - archive-tool available - <code>1</code> - archive-tool not found or failed</p>"},{"location":"reference/cli-commands/#check-db","title":"check-db","text":"<p>Test database connectivity.</p> <p>Usage: <pre><code>ha-backend check-db\n</code></pre></p> <p>Output: <pre><code>Database connection successful\n</code></pre></p> <p>Exit codes: - <code>0</code> - Database reachable - <code>1</code> - Connection failed</p>"},{"location":"reference/cli-commands/#job-management-commands","title":"Job Management Commands","text":""},{"location":"reference/cli-commands/#create-job","title":"create-job","text":"<p>Create a new archive job using source defaults.</p> <p>Usage: <pre><code>ha-backend create-job --source SOURCE_CODE [--override JSON]\n</code></pre></p> <p>Arguments: - <code>--source</code>, <code>-s</code> (required) - Source code (<code>hc</code>, <code>phac</code>) - <code>--override</code> (optional) - JSON string with config overrides</p> <p>Examples:</p> <pre><code># Create Health Canada job with defaults\nha-backend create-job --source hc\n\n# Create with custom worker count\nha-backend create-job --source hc --override '{\"tool_options\": {\"initial_workers\": 2}}'\n\n# Create a \"search-first\" crawl (skip optional .zim build) with a larger Docker /dev/shm\nha-backend create-job --source hc --override '{\"tool_options\": {\"initial_workers\": 2, \"skip_final_build\": true, \"docker_shm_size\": \"1g\"}}'\n\n# Enable monitoring and stall detection\nha-backend create-job --source phac --override '{\n  \"tool_options\": {\n    \"enable_monitoring\": true,\n    \"stall_timeout_minutes\": 60\n  }\n}'\n</code></pre> <p>Output: <pre><code>Created job ID: 42\nName: hc-20260118\nOutput directory: /mnt/nasd/nobak/healtharchive/jobs/hc/20260118T210911Z__hc-20260118\nStatus: queued\n</code></pre></p> <p>Exit codes: - <code>0</code> - Job created successfully - <code>1</code> - Failed (invalid source, config validation error)</p>"},{"location":"reference/cli-commands/#run-db-job","title":"run-db-job","text":"<p>Execute a queued job by ID.</p> <p>Usage: <pre><code>ha-backend run-db-job --id JOB_ID\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID to run</p> <p>Example: <pre><code>ha-backend run-db-job --id 42\n</code></pre></p> <p>What it does: 1. Validates job status is <code>queued</code> or <code>retryable</code> 2. Sets status to <code>running</code> 3. Executes archive-tool subprocess 4. Updates status to <code>completed</code> or <code>failed</code></p> <p>Exit codes: - <code>0</code> - Crawl succeeded - <code>1</code> - Crawl failed or job invalid</p>"},{"location":"reference/cli-commands/#index-job","title":"index-job","text":"<p>Index WARCs from a completed job into the database.</p> <p>Usage: <pre><code>ha-backend index-job --id JOB_ID\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID to index</p> <p>Example: <pre><code>ha-backend index-job --id 42\n</code></pre></p> <p>What it does: 1. Discovers WARC files in job output directory 2. Parses WARC records 3. Extracts text, title, snippet 4. Creates Snapshot rows 5. Sets job status to <code>indexed</code></p> <p>Output: <pre><code>Indexing job 42...\nFound 245 WARC files\nIndexed 12,347 snapshots\nJob status: indexed\n</code></pre></p> <p>Exit codes: - <code>0</code> - Indexing succeeded - <code>1</code> - Failed (no WARCs, parsing error)</p>"},{"location":"reference/cli-commands/#register-job-dir","title":"register-job-dir","text":"<p>Attach an existing archive_tool output directory to a new database job.</p> <p>Usage: <pre><code>ha-backend register-job-dir --source SOURCE --output-dir PATH [--name NAME]\n</code></pre></p> <p>Arguments: - <code>--source</code> (required) - Source code - <code>--output-dir</code> (required) - Existing directory path - <code>--name</code> (optional) - Job name (default: derived from directory)</p> <p>Example: <pre><code>ha-backend register-job-dir \\\n  --source hc \\\n  --output-dir /mnt/nasd/nobak/healtharchive/jobs/hc/20260101T120000Z__hc-20260101\n</code></pre></p> <p>Use case: Import externally-run crawls into database</p> <p>Exit codes: - <code>0</code> - Job registered - <code>1</code> - Directory doesn't exist or validation failed</p>"},{"location":"reference/cli-commands/#direct-execution","title":"Direct Execution","text":""},{"location":"reference/cli-commands/#run-job","title":"run-job","text":"<p>Run archive-tool directly without database persistence.</p> <p>Usage: <pre><code>ha-backend run-job \\\n  --name NAME \\\n  --seeds URL [URL...] \\\n  [--initial-workers N] \\\n  [--output-dir DIR]\n</code></pre></p> <p>Arguments: - <code>--name</code> (required) - Job name - <code>--seeds</code> (required) - One or more seed URLs - <code>--initial-workers</code> (optional) - Worker count (default: 1) - <code>--output-dir</code> (optional) - Output directory (default: auto-generated)</p> <p>Example: <pre><code>ha-backend run-job \\\n  --name test-crawl \\\n  --seeds https://www.canada.ca/en/health-canada.html \\\n  --initial-workers 2\n</code></pre></p> <p>Use case: Quick testing without database overhead</p> <p>Exit codes: - <code>0</code> - Crawl succeeded - Non-zero - archive-tool exit code</p>"},{"location":"reference/cli-commands/#inspection-commands","title":"Inspection Commands","text":""},{"location":"reference/cli-commands/#list-jobs","title":"list-jobs","text":"<p>List recent jobs with summary information.</p> <p>Usage: <pre><code>ha-backend list-jobs [--limit N] [--status STATUS] [--source SOURCE]\n</code></pre></p> <p>Arguments: - <code>--limit</code> (optional) - Number of jobs to show (default: 20) - <code>--status</code> (optional) - Filter by status - <code>--source</code> (optional) - Filter by source code</p> <p>Examples: <pre><code># List 20 most recent jobs\nha-backend list-jobs\n\n# Show only failed jobs\nha-backend list-jobs --status failed\n\n# Show Health Canada jobs\nha-backend list-jobs --source hc\n\n# Show last 50 jobs\nha-backend list-jobs --limit 50\n</code></pre></p> <p>Output: <pre><code>ID  Name            Source  Status    Queued              Started             Finished            Pages\n42  hc-20260118     hc      indexed   2026-01-18 20:00    2026-01-18 20:05    2026-01-18 21:30    12,347\n41  phac-20260117   phac    completed 2026-01-17 19:00    2026-01-17 19:10    2026-01-17 20:45    8,234\n</code></pre></p>"},{"location":"reference/cli-commands/#show-job","title":"show-job","text":"<p>Display detailed information about a specific job.</p> <p>Usage: <pre><code>ha-backend show-job --id JOB_ID [--format {text|json}]\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID - <code>--format</code> (optional) - Output format (default: <code>text</code>)</p> <p>Examples: <pre><code># Human-readable output\nha-backend show-job --id 42\n\n# JSON output (for scripting)\nha-backend show-job --id 42 --format json\n</code></pre></p> <p>Output (text format): <pre><code>Job ID: 42\nName: hc-20260118\nSource: Health Canada (hc)\nStatus: indexed\nOutput Directory: /mnt/nasd/nobak/healtharchive/jobs/hc/20260118T210911Z__hc-20260118\n\nTimeline:\n  Queued:  2026-01-18 20:00:00\n  Started: 2026-01-18 20:05:00\n  Finished: 2026-01-18 21:30:00\n  Duration: 1h 25m\n\nCrawl Metrics:\n  Exit Code: 0\n  Status: success\n  Pages Crawled: 12,347\n  Pages Total: 12,500\n  Pages Failed: 153\n\nIndexing:\n  WARC Files: 245\n  Snapshots: 12,347\n\nCleanup:\n  Status: none\n</code></pre></p>"},{"location":"reference/cli-commands/#maintenance-commands","title":"Maintenance Commands","text":""},{"location":"reference/cli-commands/#retry-job","title":"retry-job","text":"<p>Retry a failed or index-failed job.</p> <p>Usage: <pre><code>ha-backend retry-job --id JOB_ID\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID to retry</p> <p>Example: <pre><code>ha-backend retry-job --id 42\n</code></pre></p> <p>What it does: - If job status is <code>failed</code>: Sets to <code>retryable</code> (for re-crawl) - If job status is <code>index_failed</code>: Sets to <code>completed</code> (for re-index)</p> <p>Exit codes: - <code>0</code> - Job marked for retry - <code>1</code> - Job not in retryable state</p>"},{"location":"reference/cli-commands/#cleanup-job","title":"cleanup-job","text":"<p>Clean up temporary crawl artifacts.</p> <p>Usage: <pre><code>ha-backend cleanup-job --id JOB_ID [--mode MODE] [--force]\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID - <code>--mode</code> (optional) - Cleanup mode (default: <code>temp</code>, only supported value) - <code>--force</code> (optional) - Force cleanup even if replay is enabled</p> <p>Example: <pre><code># Clean up temp directories and state file\nha-backend cleanup-job --id 42 --mode temp\n\n# Force cleanup (use with caution)\nha-backend cleanup-job --id 42 --mode temp --force\n</code></pre></p> <p>What it does: - Removes <code>.tmp*</code> directories - Removes <code>.archive_state.json</code> - Updates job: <code>cleanup_status = \"temp_cleaned\"</code>, <code>cleaned_at = now</code></p> <p>\u26a0\ufe0f Warning: This deletes WARCs if they're in <code>.tmp*</code> directories. Only run on indexed jobs where you don't need replay.</p> <p>Exit codes: - <code>0</code> - Cleanup succeeded - <code>1</code> - Failed (job not indexed, replay enabled without --force)</p>"},{"location":"reference/cli-commands/#replay-index-job","title":"replay-index-job","text":"<p>Create/refresh pywb collection index for a job.</p> <p>Usage: <pre><code>ha-backend replay-index-job --id JOB_ID\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID</p> <p>Example: <pre><code>ha-backend replay-index-job --id 42\n</code></pre></p> <p>What it does: - Creates pywb collection for job WARCs - Generates CDX index for fast replay - Enables browsing via pywb</p> <p>Prerequisites: - <code>HEALTHARCHIVE_REPLAY_BASE_URL</code> set - pywb installed and configured</p> <p>Exit codes: - <code>0</code> - Index created - <code>1</code> - Failed or replay not configured</p>"},{"location":"reference/cli-commands/#seeding","title":"Seeding","text":""},{"location":"reference/cli-commands/#seed-sources","title":"seed-sources","text":"<p>Initialize source records in the database.</p> <p>Usage: <pre><code>ha-backend seed-sources\n</code></pre></p> <p>What it does: - Inserts <code>Source</code> rows for <code>hc</code> and <code>phac</code> - Idempotent (safe to run multiple times)</p> <p>Example: <pre><code>ha-backend seed-sources\n</code></pre></p> <p>Output: <pre><code>Seeded source: hc (Health Canada)\nSeeded source: phac (Public Health Agency of Canada)\n</code></pre></p> <p>Exit codes: - <code>0</code> - Sources seeded or already exist</p>"},{"location":"reference/cli-commands/#worker","title":"Worker","text":""},{"location":"reference/cli-commands/#start-worker","title":"start-worker","text":"<p>Start the job processing worker loop.</p> <p>Usage: <pre><code>ha-backend start-worker [--poll-interval SECONDS] [--once]\n</code></pre></p> <p>Arguments: - <code>--poll-interval</code> (optional) - Seconds between polls (default: 30) - <code>--once</code> (optional) - Process one job then exit</p> <p>Examples: <pre><code># Run continuously with 30s polling\nha-backend start-worker\n\n# Poll every 60 seconds\nha-backend start-worker --poll-interval 60\n\n# Process one job and exit (for testing)\nha-backend start-worker --once\n</code></pre></p> <p>What it does: 1. Polls for jobs with status <code>queued</code> or <code>retryable</code> 2. Runs oldest job first 3. Crawls \u2192 Indexes \u2192 Repeats 4. Sleeps if no jobs found</p> <p>Exit: Press Ctrl+C to stop gracefully</p>"},{"location":"reference/cli-commands/#change-tracking","title":"Change Tracking","text":""},{"location":"reference/cli-commands/#compute-changes","title":"compute-changes","text":"<p>Compute change events between adjacent snapshots.</p> <p>Usage: <pre><code>ha-backend compute-changes [--limit N] [--source SOURCE]\n</code></pre></p> <p>Arguments: - <code>--limit</code> (optional) - Max snapshot groups to process - <code>--source</code> (optional) - Limit to specific source</p> <p>Example: <pre><code># Compute changes for all snapshots\nha-backend compute-changes\n\n# Process 100 page groups\nha-backend compute-changes --limit 100\n\n# Only Health Canada changes\nha-backend compute-changes --source hc\n</code></pre></p> <p>What it does: - Groups snapshots by <code>normalized_url_group</code> - Compares adjacent captures (by timestamp) - Generates <code>SnapshotChange</code> rows with diff metadata</p> <p>Exit codes: - <code>0</code> - Changes computed - <code>1</code> - Error</p>"},{"location":"reference/cli-commands/#global-options","title":"Global Options","text":"<p>All commands support:</p> <pre><code>ha-backend COMMAND --help  # Show command help\n</code></pre>"},{"location":"reference/cli-commands/#environment-variables","title":"Environment Variables","text":"<p>Commands respect these environment variables:</p> Variable Purpose Default <code>HEALTHARCHIVE_DATABASE_URL</code> Database connection <code>sqlite:///healtharchive.db</code> <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> Base directory for jobs <code>/mnt/nasd/nobak/healtharchive/jobs</code> <code>HEALTHARCHIVE_TOOL_CMD</code> archive-tool command <code>archive-tool</code> <code>HEALTHARCHIVE_LOG_LEVEL</code> Logging level <code>INFO</code> <p>Set in <code>.env</code> file: <pre><code>HEALTHARCHIVE_DATABASE_URL=postgresql://user:pass@localhost/healtharchive\nHEALTHARCHIVE_ARCHIVE_ROOT=/data/healtharchive/jobs\nHEALTHARCHIVE_LOG_LEVEL=DEBUG\n</code></pre></p>"},{"location":"reference/cli-commands/#exit-codes","title":"Exit Codes","text":"<p>Standard exit codes: - <code>0</code> - Success - <code>1</code> - General error - <code>2</code> - Command-line usage error</p>"},{"location":"reference/cli-commands/#scripting-examples","title":"Scripting Examples","text":""},{"location":"reference/cli-commands/#process-a-job-end-to-end","title":"Process a job end-to-end","text":"<pre><code>#!/bin/bash\nset -e\n\n# Create job\nJOB_ID=$(ha-backend create-job --source hc | grep \"Created job ID:\" | awk '{print $4}')\necho \"Created job $JOB_ID\"\n\n# Run crawl\nha-backend run-db-job --id $JOB_ID\n\n# Index WARCs\nha-backend index-job --id $JOB_ID\n\n# Clean up\nha-backend cleanup-job --id $JOB_ID --mode temp\n\necho \"Job $JOB_ID complete\"\n</code></pre>"},{"location":"reference/cli-commands/#monitor-worker","title":"Monitor worker","text":"<pre><code>#!/bin/bash\n\nwhile true; do\n  clear\n  echo \"=== Job Status ===\"\n  ha-backend list-jobs --limit 10\n  sleep 10\ndone\n</code></pre>"},{"location":"reference/cli-commands/#retry-all-failed-jobs","title":"Retry all failed jobs","text":"<pre><code>#!/bin/bash\n\nha-backend list-jobs --status failed --limit 100 --format json | \\\n  jq -r '.[].id' | \\\n  while read job_id; do\n    echo \"Retrying job $job_id\"\n    ha-backend retry-job --id $job_id\n  done\n</code></pre>"},{"location":"reference/cli-commands/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture Guide: ../architecture.md</li> <li>Job Registry: ../architecture.md#4-job-registry--creation</li> <li>Worker Loop: ../architecture.md#9-worker-loop</li> <li>Data Model: data-model.md</li> <li>Live Testing: ../development/live-testing.md</li> </ul>"},{"location":"reference/data-model/","title":"Data Model Reference","text":"<p>Quick reference for HealthArchive database models.</p> <p>Full details: See Architecture Guide</p>"},{"location":"reference/data-model/#entity-relationship","title":"Entity Relationship","text":"<pre><code>erDiagram\n    Source ||--o{ ArchiveJob : has\n    Source ||--o{ Snapshot : has\n    ArchiveJob ||--o{ Snapshot : produces\n\n    Source {\n        int id PK\n        string code UK\n        string name\n        string base_url\n        bool enabled\n    }\n\n    ArchiveJob {\n        int id PK\n        int source_id FK\n        string name\n        string status\n        string output_dir\n        json config\n        int warc_file_count\n        int indexed_page_count\n    }\n\n    Snapshot {\n        int id PK\n        int job_id FK\n        int source_id FK\n        string url\n        string normalized_url_group\n        datetime capture_timestamp\n        string title\n        string snippet\n        string language\n    }</code></pre>"},{"location":"reference/data-model/#source","title":"Source","text":"<p>Represents a content origin (e.g., Health Canada, PHAC).</p> <p>Table: <code>sources</code></p> Field Type Nullable Description <code>id</code> Integer No Primary key <code>code</code> String(50) No Unique short code (<code>\"hc\"</code>, <code>\"phac\"</code>) <code>name</code> String(200) No Human-readable name <code>base_url</code> String(500) Yes Base URL of source <code>description</code> Text Yes Optional description <code>enabled</code> Boolean No Whether source is active (default: <code>true</code>) <code>created_at</code> DateTime No Creation timestamp <code>updated_at</code> DateTime No Last update timestamp <p>Indexes: - Unique on <code>code</code></p> <p>Relationships: - <code>jobs</code>: One-to-many \u2192 <code>ArchiveJob</code> - <code>snapshots</code>: One-to-many \u2192 <code>Snapshot</code></p>"},{"location":"reference/data-model/#archivejob","title":"ArchiveJob","text":"<p>Represents a single crawl job execution.</p> <p>Table: <code>archive_jobs</code></p> Field Type Nullable Description Identity <code>id</code> Integer No Primary key <code>source_id</code> Integer Yes Foreign key \u2192 <code>sources.id</code> <code>name</code> String(200) No Job name (used in ZIM naming) <code>output_dir</code> String(500) No Absolute path to job directory Lifecycle <code>status</code> String(50) No <code>queued</code>, <code>running</code>, <code>completed</code>, <code>failed</code>, <code>indexing</code>, <code>indexed</code>, etc. <code>queued_at</code> DateTime Yes When job was queued <code>started_at</code> DateTime Yes When crawl started <code>finished_at</code> DateTime Yes When crawl finished <code>retry_count</code> Integer No Number of retry attempts (default: 0) Configuration <code>config</code> JSON Yes Job configuration (seeds, tool_options, zimit args) Crawl Metrics <code>crawler_exit_code</code> Integer Yes Exit code from archive_tool process <code>crawler_status</code> String(50) Yes Summarized status (<code>\"success\"</code>, <code>\"failed\"</code>) <code>crawler_stage</code> String(50) Yes Last known stage <code>last_stats_json</code> JSON Yes Parsed crawl stats from logs <code>pages_crawled</code> Integer Yes Pages successfully crawled <code>pages_total</code> Integer Yes Total pages discovered <code>pages_failed</code> Integer Yes Pages that failed to crawl Indexing <code>warc_file_count</code> Integer No Number of WARC files discovered (default: 0) <code>indexed_page_count</code> Integer No Number of snapshots created (default: 0) File Paths <code>final_zim_path</code> String(500) Yes Path to ZIM file (if built) <code>combined_log_path</code> String(500) Yes Path to combined crawl log <code>state_file_path</code> String(500) Yes Path to <code>.archive_state.json</code> Cleanup <code>cleanup_status</code> String(50) No <code>\"none\"</code>, <code>\"temp_cleaned\"</code> (default: <code>\"none\"</code>) <code>cleaned_at</code> DateTime Yes When cleanup was performed Timestamps <code>created_at</code> DateTime No Record creation <code>updated_at</code> DateTime No Last update <p>Indexes: - Index on <code>source_id</code> - Index on <code>status</code> - Index on <code>queued_at</code></p> <p>Relationships: - <code>source</code>: Many-to-one \u2192 <code>Source</code> - <code>snapshots</code>: One-to-many \u2192 <code>Snapshot</code></p>"},{"location":"reference/data-model/#snapshot","title":"Snapshot","text":"<p>Represents a single captured web page.</p> <p>Table: <code>snapshots</code></p> Field Type Nullable Description Identity <code>id</code> Integer No Primary key <code>job_id</code> Integer Yes Foreign key \u2192 <code>archive_jobs.id</code> <code>source_id</code> Integer Yes Foreign key \u2192 <code>sources.id</code> URL &amp; Grouping <code>url</code> String(2000) No Full URL of captured page <code>normalized_url_group</code> String(2000) Yes Canonical URL for grouping Timing <code>capture_timestamp</code> DateTime No When page was captured (from WARC) HTTP &amp; Content <code>mime_type</code> String(100) Yes MIME type (usually <code>\"text/html\"</code>) <code>status_code</code> Integer Yes HTTP status code <code>title</code> String(500) Yes Extracted page title <code>snippet</code> Text Yes Short text preview <code>language</code> String(10) Yes ISO language code (<code>\"en\"</code>, <code>\"fr\"</code>) Storage/Replay <code>warc_path</code> String(500) No Path to WARC file <code>warc_record_id</code> String(200) Yes WARC record identifier <code>raw_snapshot_path</code> String(500) Yes Optional static HTML export path <code>content_hash</code> String(64) Yes Hash of HTML body (for deduplication) Timestamps <code>created_at</code> DateTime No Record creation <code>updated_at</code> DateTime No Last update <p>Indexes: - Index on <code>job_id</code> - Index on <code>source_id</code> - Index on <code>url</code> - Index on <code>normalized_url_group</code> - Index on <code>capture_timestamp</code> - Index on <code>status_code</code></p> <p>Relationships: - <code>job</code>: Many-to-one \u2192 <code>ArchiveJob</code> - <code>source</code>: Many-to-one \u2192 <code>Source</code></p>"},{"location":"reference/data-model/#job-status-lifecycle","title":"Job Status Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; queued: create-job\n    queued --&gt; running: worker starts\n    running --&gt; completed: crawl succeeds\n    running --&gt; failed: crawl fails\n    failed --&gt; retryable: retry if count &lt; MAX\n    retryable --&gt; running: worker retries\n    completed --&gt; indexing: index-job starts\n    indexing --&gt; indexed: indexing succeeds\n    indexing --&gt; index_failed: indexing fails\n    indexed --&gt; [*]\n    failed --&gt; [*]\n    index_failed --&gt; completed: retry-job (reindex)</code></pre> <p>Common status values: - <code>queued</code> - Job created, waiting for worker - <code>running</code> - Crawl in progress - <code>completed</code> - Crawl succeeded - <code>failed</code> - Crawl failed (terminal if retries exhausted) - <code>retryable</code> - Failed but can retry - <code>indexing</code> - WARC indexing in progress - <code>indexed</code> - Fully indexed and ready to serve - <code>index_failed</code> - Indexing failed</p>"},{"location":"reference/data-model/#config-json-schema","title":"Config JSON Schema","text":"<p>ArchiveJob.config structure:</p> <pre><code>{\n  \"seeds\": [\n    \"https://www.canada.ca/en/health-canada.html\"\n  ],\n  \"zimit_passthrough_args\": [\n    \"--profile\", \"social-media\"\n  ],\n  \"tool_options\": {\n    \"cleanup\": false,\n    \"overwrite\": false,\n    \"skip_final_build\": false,\n    \"enable_monitoring\": false,\n    \"enable_adaptive_workers\": false,\n    \"enable_adaptive_restart\": false,\n    \"enable_vpn_rotation\": false,\n    \"initial_workers\": 2,\n    \"log_level\": \"INFO\",\n    \"relax_perms\": true,\n    \"docker_shm_size\": \"1g\",\n    \"monitor_interval_seconds\": 30,\n    \"stall_timeout_minutes\": 30,\n    \"error_threshold_timeout\": 10,\n    \"error_threshold_http\": 10,\n    \"min_workers\": 1,\n    \"max_worker_reductions\": 2,\n    \"vpn_connect_command\": \"vpn connect ca\",\n    \"max_vpn_rotations\": 3,\n    \"vpn_rotation_frequency_minutes\": 60,\n    \"backoff_delay_minutes\": 15\n  }\n}\n</code></pre> <p>See: Job Registry for defaults per source</p>"},{"location":"reference/data-model/#database-configuration","title":"Database Configuration","text":"<p>Location: <code>src/ha_backend/models.py</code></p> <p>ORM: SQLAlchemy 2.0</p> <p>Migrations: Alembic (in <code>alembic/</code> directory)</p>"},{"location":"reference/data-model/#running-migrations","title":"Running Migrations","text":"<pre><code># Upgrade to latest\nalembic upgrade head\n\n# Downgrade one revision\nalembic downgrade -1\n\n# Show current revision\nalembic current\n\n# Show migration history\nalembic history\n</code></pre>"},{"location":"reference/data-model/#supported-databases","title":"Supported Databases","text":"<ul> <li>SQLite (default for dev): <code>sqlite:///healtharchive.db</code></li> <li>PostgreSQL (recommended for production): <code>postgresql://user:pass@host/dbname</code></li> </ul> <p>Environment variable: <code>HEALTHARCHIVE_DATABASE_URL</code></p>"},{"location":"reference/data-model/#common-queries","title":"Common Queries","text":""},{"location":"reference/data-model/#find-jobs-by-status","title":"Find jobs by status","text":"<pre><code>from ha_backend.models import ArchiveJob\nfrom ha_backend.db import get_session\n\nsession = get_session()\njobs = session.query(ArchiveJob).filter_by(status=\"queued\").all()\n</code></pre>"},{"location":"reference/data-model/#get-source-with-all-jobs","title":"Get source with all jobs","text":"<pre><code>from ha_backend.models import Source\n\nsession = get_session()\nsource = session.query(Source).filter_by(code=\"hc\").one()\nprint(f\"{source.name}: {len(source.jobs)} jobs\")\n</code></pre>"},{"location":"reference/data-model/#find-snapshots-by-url","title":"Find snapshots by URL","text":"<pre><code>from ha_backend.models import Snapshot\n\nsession = get_session()\nsnapshots = session.query(Snapshot).filter(\n    Snapshot.url.like(\"%health-canada%\")\n).limit(10).all()\n</code></pre>"},{"location":"reference/data-model/#related-documentation","title":"Related Documentation","text":"<ul> <li>Full Architecture Guide: architecture.md</li> <li>Job Creation: architecture.md#4-job-registry--creation</li> <li>Indexing Pipeline: architecture.md#6-indexing-pipeline</li> <li>CLI Commands: cli-commands.md</li> </ul>"},{"location":"roadmaps/","title":"Roadmaps","text":""},{"location":"roadmaps/#current-backlog","title":"Current backlog","text":"<ul> <li>Future roadmap (what is not implemented yet): <code>roadmap.md</code></li> </ul>"},{"location":"roadmaps/#implementation-plans-active","title":"Implementation plans (active)","text":"<p>Implementation plans live directly under <code>docs/roadmaps/</code> while they are active. When complete, move them to <code>docs/roadmaps/implemented/</code> and date them.</p> <p>Active plans:</p> <ul> <li>(none currently)</li> </ul>"},{"location":"roadmaps/#implemented-plans-history","title":"Implemented plans (history)","text":"<ul> <li>Implemented plans archive: <code>implemented/README.md</code></li> <li>Annual crawl throughput and WARC-first artifacts: <code>implemented/2026-01-23-annual-crawl-throughput-and-artifacts.md</code></li> <li>Infra-error retry storms + Storage Box hot-path resilience: <code>implemented/2026-01-24-infra-error-and-storage-hotpath-hardening.md</code></li> <li>SLA and service commitments (v1): <code>implemented/2026-01-17-sla-and-service-commitments.md</code></li> <li>Test coverage: critical business logic: <code>implemented/2026-01-17-test-coverage-critical-business-logic.md</code></li> <li>Disaster recovery and escalation procedures: <code>implemented/2026-01-17-disaster-recovery-and-escalation-procedures.md</code></li> <li>Operational hardening: tiering alerting + incident follow-ups: <code>implemented/2026-01-17-ops-tiering-alerting-and-incident-followups.md</code></li> <li>Search ranking + snippet quality iteration (v3): <code>implemented/2026-01-03-search-ranking-and-snippets-v3.md</code></li> <li>Storage Box / sshfs stale mount recovery + integrity: <code>implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> </ul>"},{"location":"roadmaps/#historical-context","title":"Historical context","text":"<ul> <li>HealthArchive 6-Phase Upgrade Roadmap (2025; archived): <code>implemented/2025-12-24-6-phase-upgrade-roadmap-2025.md</code></li> </ul>"},{"location":"roadmaps/roadmap/","title":"Future roadmap (backlog)","text":"<p>This file tracks not-yet-implemented work and planned upgrades.</p> <p>It is intentionally not an implementation plan.</p>"},{"location":"roadmaps/roadmap/#how-to-use-this-file-workflow","title":"How to use this file (workflow)","text":"<ol> <li>Pick a small set of items from this backlog.</li> <li>Create a focused implementation plan in <code>docs/roadmaps/</code> (example name: <code>YYYY-MM-&lt;topic&gt;.md</code>).</li> <li>Implement the work.</li> <li>Update canonical documentation so operators/users can run and maintain the result.</li> <li>Move the completed implementation plan to <code>docs/roadmaps/implemented/</code> and date it.</li> </ol>"},{"location":"roadmaps/roadmap/#external-irl-work-not-implementable-in-git","title":"External / IRL work (not implementable in git)","text":"<p>These items are intentionally \u201cexternal\u201d and require ongoing human follow-through.</p> <ul> <li>External outreach + verification execution (operator-only):</li> <li>Playbook: <code>../operations/playbooks/outreach-and-verification.md</code></li> <li>Secure at least 1 distribution partner (permission to name them publicly).</li> <li>Secure at least 1 verifier (permission to name them publicly).</li> <li>Maintain a public-safe mentions/citations log with real entries:</li> <li><code>../operations/mentions-log.md</code> (links only; no private contact data)</li> <li>Healthchecks.io alignment: keep systemd timers, <code>/etc/healtharchive/healthchecks.env</code>, and the Healthchecks UI in sync.</li> <li>See: <code>../operations/playbooks/healthchecks-parity.md</code> and <code>../deployment/production-single-vps.md</code></li> </ul> <p>Track the current status and next actions in:</p> <ul> <li><code>../operations/healtharchive-ops-roadmap.md</code></li> </ul> <p>Supporting materials:</p> <ul> <li><code>../operations/outreach-templates.md</code></li> <li><code>../operations/partner-kit.md</code></li> <li><code>../operations/verification-packet.md</code></li> </ul>"},{"location":"roadmaps/roadmap/#transparency-public-reporting-policy-posture","title":"Transparency &amp; public reporting (policy posture)","text":"<ul> <li>Incident disclosure posture (current default: Option B):</li> <li>Publish public-safe notes only when an incident changes user expectations (outage/degradation, integrity risk, security posture, policy change).</li> <li>Decision record: <code>../decisions/2026-01-09-public-incident-disclosure-posture.md</code></li> <li>Revisit later: consider moving to \u201cOption A\u201d (always publish public-safe notes for sev0/sev1) once operations are demonstrably stable over multiple full campaign cycles.</li> </ul>"},{"location":"roadmaps/roadmap/#technical-backlog-candidates","title":"Technical backlog (candidates)","text":"<p>Keep this list short; prefer linking to the canonical doc that explains the item.</p>"},{"location":"roadmaps/roadmap/#search-relevance-backend","title":"Search &amp; relevance (backend)","text":"<ul> <li>Authority signals for relevance (optional): outlinks / page signals feeding into ranking and/or tie-breakers.</li> <li>See: <code>../operations/search-quality.md</code> (\u201cBackfill outlinks + authority signals\u201d)</li> </ul>"},{"location":"roadmaps/roadmap/#storage-retention-backend","title":"Storage &amp; retention (backend)","text":"<ul> <li>Same-day dedupe path (storage-only optimization; provenance-preserving).</li> <li>Requirements: dry-run mode, reversible/auditable log, and strict invariants (e.g., \u201csame URL, same day, identical <code>Snapshot.content_hash</code>\u201d).</li> <li>See: <code>../operations/search-quality.md</code>, <code>../operations/growth-constraints.md</code></li> <li>Storage/retention upgrades (only with a designed replay retention policy).</li> <li>See: <code>../operations/growth-constraints.md</code>, <code>../deployment/replay-service-pywb.md</code></li> </ul>"},{"location":"roadmaps/roadmap/#crawling-indexing-reliability-backend","title":"Crawling &amp; indexing reliability (backend)","text":"<ul> <li>Ensure WARC discovery and \"WARC files\" reporting are consistent across:</li> <li>operator status output (<code>ha-backend show-job</code>, <code>scripts/vps-crawl-status.sh</code>)</li> <li>indexing pipeline discovery (<code>ha_backend/indexing/warc_discovery.py</code>)</li> <li>cleanup semantics (<code>ha-backend cleanup-job</code>)</li> <li> <p>See: <code>../architecture.md</code> (indexing + archive-tool integration)</p> </li> <li> <p>Consider whether a separate staging backend is worth it (increases ops surface; only do if it buys real safety).</p> </li> <li>See: <code>../deployment/environments-and-configuration.md</code></li> <li>Canary replay job (replay smoke independence):</li> <li>Create a small, local-only replay job (no tiering) to use as a baseline smoke target.</li> <li>Distinguishes \"pywb is broken\" from \"storage tiering is broken\".</li> </ul>"},{"location":"roadmaps/roadmap/#repo-governance-future","title":"Repo governance (future)","text":"<ul> <li>Tighten GitHub merge discipline when there are multiple committers (PR-only + required checks).</li> <li>See: <code>../operations/monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"roadmaps/roadmap/#adjacent-optional-in-this-monorepo-not-core-ha","title":"Adjacent / optional (in this monorepo, not core HA)","text":"<ul> <li><code>rcdc/CDC_zim_mirror</code>: add startup DB sanity checks and clearer failure modes (empty/invalid LevelDB, missing prefixes, etc.).</li> </ul>"},{"location":"roadmaps/implemented/","title":"Implemented plans (archive)","text":"<p>This folder contains historical implementation plans that have already been executed.</p> <p>Implemented plans:</p> <ul> <li><code>2025-12-24-6-phase-upgrade-roadmap-2025.md</code></li> <li><code>2025-12-24-sequential-implementation-plan.md</code></li> <li><code>2025-12-24-ops-observability-and-private-usage.md</code></li> <li><code>2025-12-25-compare-live.md</code></li> <li><code>2026-01-03-ci-e2e-smoke.md</code></li> <li><code>2026-01-03-ci-e2e-smoke-hardening.md</code></li> <li><code>2026-01-03-crawl-safe-roadmap-batch.md</code></li> <li><code>2026-01-03-ops-automation-verification-json.md</code></li> <li><code>2026-01-03-ops-automation-verifier-improvements.md</code></li> <li><code>2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> <li><code>2026-01-17-documentation-architecture-improvements.md</code></li> <li><code>2026-01-17-ops-tiering-alerting-and-incident-followups.md</code></li> <li><code>2026-01-17-disaster-recovery-and-escalation-procedures.md</code></li> <li><code>2026-01-03-search-ranking-and-snippets-v3.md</code></li> <li><code>2026-01-19-annual-crawl-resiliency-hardening.md</code></li> <li><code>2026-01-23-annual-crawl-throughput-and-artifacts.md</code></li> <li><code>2026-01-24-infra-error-and-storage-hotpath-hardening.md</code></li> <li><code>2026-01-27-archive-tool-hardening-and-ops-improvements.md</code></li> </ul> <p>Rules:</p> <ul> <li>Treat these as a record; avoid ongoing edits except for small errata.</li> <li>Use dated filenames: <code>YYYY-MM-DD-&lt;short-slug&gt;.md</code>.</li> <li>If a plan\u2019s outcomes should be discoverable by future operators, ensure the relevant   details are captured in canonical docs under:</li> <li><code>docs/deployment/</code></li> <li><code>docs/operations/</code></li> <li><code>docs/development/</code>   and keep the plan here as supporting history.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/","title":"HealthArchive 6-Phase Upgrade Roadmap (2025; archived 2025-12-24)","text":"<p>This is the historical roadmap for the 6-phase upgrade program (Phases 0\u20136).</p> <p>It is not the current ops checklist. For live state and remaining ops tasks, see:</p> <ul> <li><code>docs/operations/healtharchive-ops-roadmap.md</code></li> <li><code>docs/operations/README.md</code></li> </ul> <p>Status: historical reference. If anything in this document conflicts with ops docs, the ops docs win.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#how-to-use-this-document-for-a-future-agent","title":"How To Use This Document (for a future agent)","text":"<p>1) Confirm current state first (don\u2019t guess). This repo is split across:    - <code>healtharchive-frontend/</code> (Next.js 16 UI on Vercel)    - <code>healtharchive-backend/</code> (FastAPI + worker + indexing + in-tree crawler)    - <code>healtharchive-backend/docs/deployment/environments-and-configuration.md</code> (cross-repo env wiring; canonical)</p> <p>2) Read the canonical docs (high signal).    - Cross-repo wiring: <code>healtharchive-backend/docs/deployment/environments-and-configuration.md</code>    - Backend architecture: <code>healtharchive-backend/docs/architecture.md</code>    - Backend production runbook: <code>healtharchive-backend/docs/deployment/production-single-vps.md</code>    - Annual campaign scope/policy: <code>healtharchive-backend/docs/operations/annual-campaign.md</code>    - Replay runbook: <code>healtharchive-backend/docs/deployment/replay-service-pywb.md</code>    - Replay/preview automation design: <code>healtharchive-backend/docs/operations/replay-and-preview-automation-plan.md</code>    - Frontend implementation guide: <code>healtharchive-frontend/docs/implementation-guide.md</code>    - Frontend deployment verification: <code>healtharchive-frontend/docs/deployment/verification.md</code></p> <p>3) Work in small increments. Each deliverable should be implementable as a small PR:    - Don\u2019t mix governance copywork with major backend refactors in one change.    - Keep checks passing (backend <code>make check</code>, frontend <code>npm run check</code>).</p> <p>4) Non-negotiable safety posture (do not weaken).    - HealthArchive is an archive + provenance tool, not a guidance provider.    - Never imply affiliation with government sources.    - Avoid features that produce medical interpretation (especially AI summaries).    - Preserve the existing browser hardening: CSP/security headers, iframe sandboxing, strict CORS, admin token gating.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#current-architecture-as-implemented","title":"Current Architecture (as implemented)","text":""},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#components","title":"Components","text":"<ul> <li>Frontend: <code>healtharchive-frontend/</code> (Next.js 16 App Router + TypeScript, Tailwind + <code>.ha-*</code> design system)</li> <li>Routes: <code>/</code>, <code>/archive</code>, <code>/archive/browse-by-source</code>, <code>/snapshot/[id]</code>, <code>/browse/[id]</code>, <code>/methods</code>, <code>/researchers</code>, <code>/about</code>, <code>/contact</code></li> <li>Offline fallback: demo dataset + static HTML stubs under <code>healtharchive-frontend/public/demo-archive/**</code> (used when API unreachable / CORS blocked)</li> <li>Backend: <code>healtharchive-backend/</code> (FastAPI + SQLAlchemy + Alembic)</li> <li>Public API endpoints used by the frontend:<ul> <li><code>GET /api/health</code>, <code>GET /api/stats</code>, <code>GET /api/sources</code>, <code>GET /api/search</code>, <code>GET /api/snapshot/{id}</code>, <code>GET /api/snapshots/raw/{id}</code></li> <li>Replay support: <code>GET /api/sources/{source}/editions</code>, <code>GET /api/replay/resolve</code></li> <li>Preview images (optional): <code>GET /api/sources/{source}/preview?jobId=...</code></li> </ul> </li> <li>Admin/ops endpoints (must remain off-limits for public UI): <code>/api/admin/**</code>, <code>/metrics</code></li> <li>Crawler/orchestrator: <code>healtharchive-backend/src/archive_tool/</code></li> <li>Drives Docker + Zimit to generate WARCs and job artifacts in a resumable pipeline.</li> <li>Worker: <code>healtharchive-backend/src/ha_backend/worker/</code></li> <li>Runs queued jobs end-to-end (crawl \u2192 index), with retries.</li> <li>Optional replay service: pywb behind Caddy at <code>https://replay.healtharchive.ca</code></li> <li>Backend generates <code>browseUrl</code> values when <code>HEALTHARCHIVE_REPLAY_BASE_URL</code> is configured.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#deployment-reality-current-posture","title":"Deployment reality (current posture)","text":"<ul> <li>Frontend: Vercel (<code>https://healtharchive.ca</code>, <code>https://www.healtharchive.ca</code>, plus <code>https://healtharchive.vercel.app</code>)</li> <li>Backend API: Single production API at <code>https://api.healtharchive.ca</code> (used for both Preview and Production), with a strict CORS allowlist.</li> <li>Single VPS model: Postgres + API + worker + storage + Caddy on one server (see <code>healtharchive-backend/docs/deployment/production-single-vps.md</code>).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#key-constraints-that-affect-implementation-choices","title":"Key constraints that affect implementation choices","text":"<ul> <li>CSP/security headers: Frontend sets a restrictive CSP in report-only mode (<code>healtharchive-frontend/next.config.ts</code>).</li> <li>This will constrain \u201cdrop-in\u201d third-party scripts for analytics/forms unless explicitly allowed.</li> <li>Strict CORS: Backend allows only specific frontend origins (intentionally). Branch preview URLs may fall back to demo mode by design.</li> <li>Replay depends on WARCs: Deleting WARCs breaks replay. Cleanup is intentionally cautious.</li> <li>Single VPS capacity: Heavy background work (diff generation, replay indexing, preview rendering) must be controlled so it doesn\u2019t harm API responsiveness.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#terminology-shared-language","title":"Terminology (shared language)","text":"<ul> <li>Source: A logical origin like \u201cHealth Canada\u201d (<code>hc</code>) or \u201cPHAC\u201d (<code>phac</code>).</li> <li>ArchiveJob (job): A single crawl run with a config and output directory; after indexing, it becomes an \u201cedition\u201d.</li> <li>Edition: A user-facing \u201cbackup\u201d of a source, typically <code>job-&lt;id&gt;</code> (used for replay browsing + edition switching).</li> <li>Snapshot: A single captured page instance (URL + timestamp + metadata) extracted from WARCs and stored in the DB.</li> <li>Page / page group: A canonical grouping of multiple snapshots for the \u201csame page\u201d across time, typically keyed by <code>normalized_url_group</code> (backend supports <code>view=pages</code>).</li> <li>Raw HTML replay: <code>GET /api/snapshots/raw/{id}</code> reconstructs HTML from the WARC for embedding in the frontend.</li> <li>Full-fidelity replay: pywb replay of the same content (CSS/JS/assets when captured), embedded via <code>browseUrl</code>.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#roadmap-summary-high-level","title":"Roadmap Summary (High-Level)","text":"<p>Ground Truth From The Current Repos (what\u2019s already \u201cupgraded\u201d)</p> <ul> <li>Strong, repeated non-government / non-advice framing already exists in UI copy, especially in the footer and snapshot viewer (<code>healtharchive-frontend/src/components/layout/Footer.tsx:8</code>, <code>healtharchive-frontend/src/app/snapshot/[id]/page.tsx:192</code>) and a \u201cWhat this site is/isn\u2019t\u201d block is already on the homepage (<code>healtharchive-frontend/src/app/page.tsx:191</code>).</li> <li>The homepage already surfaces live archive metrics via the backend <code>/api/stats</code> (with an offline fallback), which is a big \u201cservice maturity\u201d signal (<code>healtharchive-frontend/src/app/page.tsx:13</code>).</li> <li>Researcher-oriented copy already includes citation guidance and explicitly calls out \u201ccompare/timeline/exports\u201d as planned capabilities (<code>healtharchive-frontend/src/app/researchers/page.tsx:65</code>).</li> <li>Backend already has: robust search semantics, \u201cpages vs snapshots\u201d view, optional <code>pages</code> table fast path, replay integration (pywb), per-source \u201ceditions\u201d for switching backups, and ops runbooks + systemd templates for annual scheduling and verification (<code>healtharchive-backend/docs/architecture.md:22</code>, <code>healtharchive-backend/docs/deployment/pages-table-rollout.md:1</code>, <code>healtharchive-backend/docs/deployment/systemd/README.md:1</code>, <code>healtharchive-backend/docs/operations/annual-campaign.md:1</code>).</li> <li>Frontend security posture is already deliberate (security headers + CSP in report-only) and will constrain how you add third-party analytics/forms unless you plan it (<code>healtharchive-frontend/next.config.ts:3</code>).</li> </ul> <p>Below is a revised, sequential upgrade plan that assumes those realities and avoids duplicating what you already have.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-0-tighten-narrative-reduce-copy-drift-high-roi-mostly-editing","title":"Phase 0 \u2014 Tighten Narrative + Reduce Copy Drift (high ROI, mostly editing)","text":"<p>Goal: You already have good messaging; this phase makes it consistent, \u201cresearch-grade,\u201d and accurate to what\u2019s actually deployed.</p> <p>0.1 Standardize the mission statement (everywhere it matters)</p> <ul> <li>Unify language across: homepage hero, About, Methods, Researchers, metadata description, and any future Governance page.</li> <li>Keep the mission short and \u201cverifiable\u201d: time-stamped snapshots + auditable changes + citation, not broad claims.</li> </ul> <p>Suggested tone (aligned with current copy but sharper):</p> <ul> <li>\u201cHealthArchive.ca preserves time-stamped snapshots of selected Canadian public health web pages so changes remain auditable and citable.\u201d</li> <li>\u201cThis is not medical advice and not a substitute for current official guidance.\u201d</li> </ul> <p>0.2 Make the \u201carchived, not current\u201d message unavoidable on workflow pages You already do this well on the homepage and snapshot detail; bring the same clarity to:</p> <ul> <li><code>/archive</code> (search is where casual users land)</li> <li><code>/browse/[id]</code> (high-risk page: looks like you\u2019re \u201cshowing a site\u201d)</li> </ul> <p>Deliverable concept (no implementation detail): one short, consistent block that appears on Home + Archive + Snapshot + Browse, tuned per context:</p> <ul> <li>Archive: \u201cSearch historical snapshots. Always verify against the official site for current guidance.\u201d</li> <li>Browse: \u201cYou\u2019re browsing an archived capture from . Links may not reflect current guidance.\u201d <p>0.3 Update Methods text to reflect what is already real (reduce \u201cfuture tense\u201d) Right now Methods reads partially like a conceptual design document (<code>healtharchive-frontend/src/app/methods/page.tsx:34</code>). Your backend + production runbook indicates this is already materially implemented.</p> <ul> <li>Shift language from \u201cwould / intended\u201d \u2192 \u201cdoes / currently,\u201d while keeping the \u201cin development\u201d caveats.</li> <li>Bring in (high-level) the annual edition concept that already exists in ops docs, without exposing infrastructure details.</li> </ul> <p>0.4 Choose (and explicitly state) primary audiences Homepage currently lists clinicians + researchers/journalists + public (<code>healtharchive-frontend/src/app/page.tsx:104</code>). That\u2019s fine, but for risk posture you may want:</p> <ul> <li>Primary: researchers/journalists/educators</li> <li>Secondary: clinicians/public (with stronger guardrails)     Deliverable: a subtle re-weighting of copy (not removing audiences, just clarifying intended use).</li> </ul> <p>Definition of done (Phase 0)</p> <ul> <li>A new visitor landing on <code>/archive</code> or <code>/browse/[id]</code> cannot miss \u201carchived, not current guidance; not medical advice; independent project.\u201d</li> <li>Methods/About/Researchers copy matches the actual deployed architecture and policies (annual editions, limited scope, optional replay).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-1-public-governance-layer-abs-critical-mostly-writing-lightweight-process","title":"Phase 1 \u2014 Public Governance Layer (ABS-critical; mostly writing + lightweight process)","text":"<p>Goal: Convert your already-solid engineering into \u201cpublic-interest infrastructure\u201d with clear rules.</p> <p>1.1 Publish a Governance section (public-facing) You already have deep internal ops docs; this phase turns the right parts into a stable, public policy surface. Include (plain language, not legalese):</p> <ul> <li>Mission + scope boundaries (what you archive, what you intentionally don\u2019t)</li> <li>Source inclusion criteria (why a source is in/out)</li> <li>Provenance commitments (what metadata you guarantee; how you label captures)</li> <li>Corrections policy (what counts as a \u201ccorrection,\u201d expected response time)</li> <li>Takedown/opt-out policy (including how you handle edge cases and third-party content)</li> <li>Non-affiliation / non-authoritativeness statement (already strong in footer; reference it consistently)</li> </ul> <p>1.2 Terms + Privacy pages You currently have no explicit terms/privacy pages in the frontend route structure. Keep them short and specific:</p> <ul> <li>\u201cNo accounts; no patient info collected\u201d</li> <li>What telemetry exists (today it appears \u201cnone\u201d; if you add analytics later, this page becomes the contract)</li> <li>Content use posture (research/education; not medical advice; link to official sources)</li> </ul> <p>1.3 Public Changelog Your repos will naturally evolve; a changelog makes \u201cmaintenance over time\u201d legible.</p> <ul> <li>Monthly cadence is enough.</li> <li>Content examples: \u201cAdded source CIHR to annual scope,\u201d \u201cImproved replay edition switching,\u201d \u201cSearch improvements,\u201d \u201cIncident resolved.\u201d</li> </ul> <p>1.4 \u201cReport an issue\u201d intake that\u2019s more structured than \u201cemail us\u201d Contact already invites issue reports (<code>healtharchive-frontend/src/app/contact/page.tsx:32</code>), but the process isn\u2019t explicit. Deliverable concept:</p> <ul> <li>A clearly labeled \u201cReport an issue\u201d entry point (footer and/or viewer UI).</li> <li>A simple set of categories + what happens next + response expectations.</li> </ul> <p>1.5 Advisory circle (process, not code) Your internal documentation quality is already high; an advisory circle makes it externally defensible.</p> <ul> <li>Aim for 2\u20134 people; publish a charter and cadence.</li> <li>Keep meeting notes minimal and policy-focused (scope, risk posture, corrections).</li> </ul> <p>Definition of done (Phase 1)</p> <ul> <li>The site has publicly visible: Governance, Terms, Privacy, Changelog, Report-an-issue flow.</li> <li>A neutral reviewer can understand your rules and safety posture in under 2 minutes.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-1-implementation-plan-detailed-sub-phases","title":"Phase 1 Implementation Plan (Detailed; sub-phases)","text":"<p>This phase is deliberately mostly writing + lightweight UI + lightweight backend plumbing. The goal is not bureaucracy \u2014 it\u2019s to make HealthArchive legible, defensible, and easy to verify as \u201cpublic-interest infrastructure\u201d.</p> <p>Design principles (Phase 1)</p> <ul> <li>Plain language over legalese. You are defining procedures and expectations, not trying to replace a lawyer.</li> <li>Minimal collection. No accounts, no tracking IDs tied to people, no sensitive submissions. Default to \u201cdo not submit personal/health information.\u201d</li> <li>Consistency. The same core \u201cthis is an archive / not guidance / not medical advice\u201d language should appear across the new pages, reusing frontend\u2019s canonical copy source (<code>healtharchive-frontend/src/lib/siteCopy.ts</code>).</li> <li>Operational reality. Policies should match how you actually operate today (annual editions, constrained scope, optional replay).</li> <li>Fail safe. When the API is unreachable, reporting should still work (e.g., mailto fallback).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-1a-inventory-decisions-12-day","title":"Sub-phase 1A \u2014 Inventory + decisions (\u00bd day)","text":"<p>Goal: Avoid writing policies that contradict the real system or overpromise.</p> <p>Checklist:</p> <ul> <li>Confirm what is already stated on <code>/methods</code>, <code>/about</code>, <code>/researchers</code>, footer disclaimers, and Phase 0 callouts.</li> <li>Confirm what user data is currently collected (likely: server access logs via Caddy/uvicorn; no explicit frontend analytics).</li> <li>Decide the minimum policy commitments you are comfortable operationalizing:</li> <li>Corrections response time (example: \u201cwithin 7 days\u201d; urgent labeling issues: \u201cwithin 48 hours\u201d).</li> <li>What you will and won\u2019t take down (government sources vs third-party; link-outs vs full removal; \u201crestrict access\u201d option).</li> <li>Which contact channel is authoritative (email; optionally GitHub issues for technical items).</li> <li>Decide the initial \u201cadvisory circle\u201d stance:</li> <li>If you don\u2019t yet have advisors, plan to publish \u201cseeking advisors\u201d language rather than faking a board.</li> </ul> <p>Deliverables:</p> <ul> <li>A short \u201cPhase 1 policy decisions\u201d note (can live as a section in the changelog or as internal notes).</li> <li>A list of new public routes to create (see Sub-phase 1C).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-1b-draft-public-governance-content-12-days","title":"Sub-phase 1B \u2014 Draft public Governance content (1\u20132 days)","text":"<p>Goal: Ship a governance page that answers the big questions quickly.</p> <p>Governance page structure (recommended single page with anchored sections):</p> <p>1) Mission + audience    - One sentence mission (align with Phase 0).    - Primary audiences (researchers/journalists/educators) + secondary audiences (public/clinicians) with guardrails.</p> <p>2) Scope + inclusion criteria    - What sources qualify (Canadian public health agencies; criteria examples):      - Publicly accessible pages      - High-impact guidance/data/communications      - Stable provenance labeling possible    - What\u2019s out of scope (examples to explicitly state):      - Private/internal content; anything behind login      - User-submitted or personal data sources      - Any attempt to \u201cmirror the entire internet\u201d    - \u201cReliability over breadth\u201d statement.</p> <p>3) Provenance commitments    - What you guarantee to show on snapshot pages (examples):      - Capture timestamp (timezone)      - Source name/code      - Original URL      - Snapshot permalink    - Explicit limitation examples:      - Some JS dashboards may not replay perfectly      - Missing assets may occur      - Captures represent \u201cwhat the crawler saw\u201d, not a perfect reconstruction</p> <p>4) Corrections policy    - What counts as a correction:      - Wrong metadata, broken replay/raw HTML, mislabeled source, missing warnings    - What does not count as a correction:      - Disagreements with what an agency published    - Response expectations (SLA language you can meet)    - How corrections are documented (ties to changelog; optional per-snapshot note later)</p> <p>5) Takedown / opt-out policy    - Most content is from government sources; still define:      - How to request review      - What you do when a request is credible (e.g., restrict access while reviewing)      - How you handle third-party embedded content captured inside a government page    - Make it explicit you don\u2019t promise removal of public-interest government material unless there\u2019s a compelling reason.</p> <p>6) Non-affiliation + \u201cnot medical advice\u201d    - Reference footer disclaimers.    - Make the \u201cwhat this is / isn\u2019t\u201d block visible.</p> <p>7) Advisory circle    - Charter summary and cadence.    - If not yet formed: \u201cseeking advisors\u201d + what backgrounds you\u2019re looking for.</p> <p>Examples to include (short, non-interpretive):</p> <ul> <li>\u201cResearchers: cite what was visible on Jan 01, 2025.\u201d</li> <li>\u201cJournalists: track when wording on a guidance page changed.\u201d</li> </ul> <p>Deliverables:</p> <ul> <li>Draft Governance copy ready for implementation as a new public route.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>A first-time visitor can read only the headings and understand: purpose, scope, provenance, corrections, takedown, and non-advice posture.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-1c-add-public-pages-and-navigation-frontend-12-days","title":"Sub-phase 1C \u2014 Add public pages and navigation (frontend) (1\u20132 days)","text":"<p>Goal: Make governance + policies discoverable without cluttering the primary nav.</p> <p>Recommended new frontend routes:</p> <ul> <li><code>/governance</code> (main page; anchored sections)</li> <li><code>/terms</code></li> <li><code>/privacy</code></li> <li><code>/changelog</code></li> <li><code>/report</code> (or <code>/report-issue</code>)</li> </ul> <p>Navigation/linking strategy:</p> <ul> <li>Add links in the footer (preferred) under a small \u201cProject\u201d or \u201cPolicies\u201d column.</li> <li>Keep the header nav unchanged for now (avoid overwhelming top-level IA).</li> <li>Ensure each of these pages repeats the core disclaimers (reuse canonical copy + the existing footer).</li> </ul> <p>Changelog page content model (choose one):</p> <ul> <li>Option A (simplest): A Markdown file committed in the frontend repo and rendered by the page.</li> <li>Option B: A lightweight JSON/YAML content file with date/title/body entries.</li> </ul> <p>Deliverables:</p> <ul> <li>Public pages implemented with consistent typography and accessible structure.</li> <li>Footer links added.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>All pages build on Vercel, pass lint/tests, and do not introduce third-party scripts or weaken CSP.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-1d-report-an-issue-intake-ux-frontend-1-day","title":"Sub-phase 1D \u2014 \u201cReport an issue\u201d intake UX (frontend) (1 day)","text":"<p>Goal: Structured intake without collecting sensitive info.</p> <p>Form fields (recommended):</p> <ul> <li>Category (dropdown):</li> <li>Broken snapshot/replay</li> <li>Incorrect metadata (date/source/URL)</li> <li>Missing snapshot / request a capture</li> <li>Takedown / content concern</li> <li>General feedback</li> <li>Optional context:</li> <li>Snapshot ID (if applicable)</li> <li>Original URL (if known)</li> <li>Description (required; include \u201cDo not include personal/health info\u201d warning)</li> <li>Optional contact email (explicitly optional; users can also just email <code>contact@healtharchive.ca</code>)</li> </ul> <p>Failover behavior:</p> <ul> <li>If backend API is reachable: submit the form and return a short \u201creceived\u201d confirmation with a reference ID.</li> <li>If backend API is unreachable: provide a \u201cmailto\u201d fallback that pre-fills subject/body with the selected category + details.</li> </ul> <p>Deliverables:</p> <ul> <li>A clear \u201cReport an issue\u201d page that explains what happens next and expected response times.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>A user can report an issue even if the live API is down (offline fallback still works).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-1e-minimal-backend-support-for-issue-intake-12-days","title":"Sub-phase 1E \u2014 Minimal backend support for issue intake (1\u20132 days)","text":"<p>Goal: Make reports actionable and auditable, without creating a large moderation system.</p> <p>Recommended backend capabilities:</p> <ul> <li>A small DB-backed \u201cissue report\u201d record with:</li> <li>category, description, optional snapshot_id, optional original_url, created_at</li> <li>optional reporter_email (nullable)</li> <li>status (new / triaged / resolved)</li> <li>internal notes (admin-only)</li> <li>A public POST endpoint for submissions (with input validation and spam protection).</li> <li>An admin-only endpoint to list and view reports, protected by existing admin token rules.</li> </ul> <p>Spam/risk controls (pick a minimal set you can sustain):</p> <ul> <li>Rate limit by IP (coarse) and/or a \u201choney pot\u201d hidden field.</li> <li>Hard cap payload sizes.</li> <li>Explicitly reject submissions that look like they include personal health information (at minimum, a warning plus optional keyword heuristics).</li> </ul> <p>Deliverables:</p> <ul> <li>Issue intake pipeline exists end-to-end (frontend \u2192 backend \u2192 admin view).</li> </ul> <p>Acceptance criteria:</p> <ul> <li>Public submissions work from production origin(s) without weakening CORS.</li> <li>Admin token remains required for browsing report details.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-1f-advisory-circle-non-code-ongoing-start-in-phase-1","title":"Sub-phase 1F \u2014 Advisory circle (non-code) (ongoing; start in Phase 1)","text":"<p>Goal: Create real external credibility.</p> <p>Steps:</p> <ul> <li>Draft a 1-page advisory charter:</li> <li>Purpose (scope/risk/governance review, not operations)</li> <li>Cadence (quarterly)</li> <li>What advisors do and don\u2019t do</li> <li>Identify and contact candidates:</li> <li>librarian/archivist, public health researcher, science communication/journalism</li> <li>Publish either:</li> <li>Names/titles (with permission), or</li> <li>A \u201cseeking advisors\u201d section until you have consent to publish names</li> </ul> <p>Deliverables:</p> <ul> <li>Charter text included on <code>/governance</code> (or linked from it).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-1g-update-docs-and-prove-maintenance-12-day","title":"Sub-phase 1G \u2014 Update docs and prove maintenance (\u00bd day)","text":"<p>Goal: Make Phase 1 changes easy for a future maintainer to understand.</p> <ul> <li>Update:</li> <li>Frontend docs index or implementation guide (where the new routes live)</li> <li>Backend docs index (new issue intake endpoints, if added)</li> <li>Add a \u201cPhase 1 complete\u201d entry to <code>/changelog</code> with date + bullet list.</li> </ul> <p>Definition of done (Phase 1, detailed)</p> <ul> <li>New public pages exist and are linked from the site: <code>/governance</code>, <code>/terms</code>, <code>/privacy</code>, <code>/changelog</code>, <code>/report</code> (or equivalent).</li> <li>Policies describe how HealthArchive actually operates today, including annual editions, constrained scope, and replay limitations.</li> <li>Issue reporting is structured and works even when the API is down (via fallback).</li> <li>No new sensitive data is collected; privacy page matches reality.</li> <li>All tests pass (frontend + backend) and CSP/CORS/admin protections are not weakened.</li> </ul> <p>Status (Phase 1 implementation)</p> <ul> <li>Implemented on 2025-12-21.</li> <li>Public pages added: <code>/governance</code>, <code>/terms</code>, <code>/privacy</code>, <code>/changelog</code>, <code>/report</code>.</li> <li>Issue intake pipeline implemented with a backend <code>/api/reports</code> endpoint and admin views.</li> <li>Frontend uses a same-origin proxy route (<code>/api/report</code>) so reporting works even when the backend CORS policy remains strict.</li> <li>Footer and snapshot/browse views link to the report flow.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-2-make-impact-measurable-visible-build-on-what-you-already-have","title":"Phase 2 \u2014 Make Impact Measurable + Visible (build on what you already have)","text":"<p>Goal: You already show \u201csnapshots/pages\u201d on the homepage; now add the metrics that prove reliability and adoption.</p> <p>2.1 Define the official metric set (small, stable, defensible) You already have \u201cArchived snapshots\u201d and \u201cUnique pages\u201d via <code>/api/stats</code>. Add (conceptually):</p> <ul> <li>Coverage: sources tracked + per-source coverage window</li> <li>Freshness: time from capture completion \u2192 searchable</li> <li>Reliability: crawl success rate, indexing success rate, API uptime</li> <li>Usage: searches/day, snapshot views/day, browse views/day (requires analytics or backend logging strategy)</li> <li>Engagement: digest subscribers (once Phase 3 exists)</li> <li>External validation: partner links/embeds, citations/mentions</li> </ul> <p>2.2 Add a public Status/Metrics page This shouldn\u2019t be \u201cvanity metrics\u201d; it should make the service look professionally operated. Suggested sections:</p> <ul> <li>\u201cCurrent status\u201d: API health, last successful capture per source, replay availability (if enabled)</li> <li>\u201cCoverage\u201d: sources tracked; first/last capture dates per source (already available via <code>/api/sources</code>)</li> <li>\u201cReliability\u201d: 30/90-day uptime + recent incidents (even manual initially)</li> <li>\u201cData notes\u201d: known limitations; what \u201cmissing\u201d means</li> </ul> <p>2.3 Add analytics deliberately (or explicitly don\u2019t) Right now there\u2019s no analytics code in the frontend. Decide:</p> <ul> <li>If you add analytics, pick a privacy-preserving approach and align CSP/security headers accordingly (<code>healtharchive-frontend/next.config.ts:3</code>).</li> <li>If you do not, say so explicitly in Privacy, and rely on server-side aggregate counts + partner evidence.</li> </ul> <p>2.4 Monthly impact report artifact Make this a repeatable, boring discipline:</p> <ul> <li>\u201cWhat changed in the archive\u201d</li> <li>\u201cReliability improvements\u201d</li> <li>\u201cCoverage changes\u201d</li> <li>\u201cUsage snapshot\u201d</li> <li>\u201cPartner highlights\u201d     This becomes your ongoing \u201cproof file.\u201d</li> </ul> <p>Definition of done (Phase 2)</p> <ul> <li>You can point to a stable public page that answers: \u201cIs it up? what\u2019s covered? how current? is it being used?\u201d</li> <li>You can produce (even manually) a monthly impact report with consistent fields.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-3-change-tracking-compare-digest-major-new-capability-biggest-adoption-unlock","title":"Phase 3 \u2014 Change Tracking + Compare + Digest (major new capability; biggest adoption unlock)","text":"<p>Goal: You already have search + snapshots + replay editions. This phase adds the missing \u201cwhat changed?\u201d layer while staying non-interpretive.</p> <p>3.1 Define \u201cchange tracking\u201d in a strictly descriptive way Guardrail language (recommend adopting everywhere this appears):</p> <ul> <li>\u201cWe report textual changes between archived captures. We do not interpret or recommend actions.\u201d</li> </ul> <p>Decisions to make up front (policy-level, not code):</p> <ul> <li>What counts as a \u201cmeaningful change\u201d vs boilerplate</li> <li>How to handle noisy pages (dashboards, frequently regenerated pages)</li> <li>Whether you support per-page timelines for all pages or only \u201chigh-signal\u201d pages first</li> </ul> <p>3.2 Compare view User-facing outcomes:</p> <ul> <li>\u201cCompare two versions\u201d from snapshot detail (and/or from a page timeline)</li> <li>Highlight changed sections; show \u201cadded/removed/changed\u201d counts</li> <li>A clear \u201ccontext\u201d strip: source, original URL, capture timestamps, edition/job</li> </ul> <p>3.3 \u201cWhat changed\u201d feed A new surface that drives repeat visits:</p> <ul> <li>\u201cChanged this week\u201d feed</li> <li>Filterable by source and date</li> <li>Optional curated topic groupings later (be careful: topic tagging can imply editorial authority; keep it mechanical if possible)</li> </ul> <p>3.4 Digest (start web + RSS; email later) Given your CSP/security posture, start with:</p> <ul> <li>Web digest archive page</li> <li>RSS feed     Add email only after you\u2019re confident in content quality and cadence.</li> </ul> <p>Digest categories (aligned with your current project goals):</p> <ul> <li>Top changes (by magnitude/importance heuristic, explained plainly)</li> <li>New pages discovered</li> <li>Pages removed/redirected (as observed via crawl)</li> <li>\u201cCoverage notes\u201d (e.g., a source had capture issues)</li> </ul> <p>Definition of done (Phase 3)</p> <ul> <li>A user can answer: \u201cWhat changed on PHAC pages last week?\u201d without manual searching.</li> <li>Compare output is descriptive, provenance-rich, and does not read like guidance.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-4-distribution-external-validation-mostly-non-code-but-critical","title":"Phase 4 \u2014 Distribution + External Validation (mostly non-code, but critical)","text":"<p>Goal: Turn \u201cuseful site\u201d into \u201cverified public service.\u201d</p> <p>4.1 Partner target list + pitch assets You already have strong narrative copy; formalize it into partner-ready material:</p> <ul> <li>1-page brief (mission, safety posture, screenshots, metrics) \u2014 now published at <code>/brief</code></li> <li>Citation guidance (snapshots + compare views) \u2014 now published at <code>/cite</code></li> <li>Example compare + digest pages \u2014 now published at <code>/compare</code> and <code>/digest</code></li> </ul> <p>4.2 Secure one distribution partner + one verifier Treat them as different roles:</p> <ul> <li>Distribution partner: links/embeds/shares digest</li> <li>Verifier: can credibly confirm you built/operate it and it\u2019s used</li> </ul> <p>4.3 Partner-facing \u201cembed\u201d surface (keep it simple) Start with the lowest-friction distribution format:</p> <ul> <li>\u201cRecent changes for \u201d widget and/or RSS feeds per source     Keep this lightweight so you don\u2019t create a new support burden.</li> </ul> <p>Definition of done (Phase 4)</p> <ul> <li>One external org publicly links/embeds; one named verifier is willing to attest to use and impact; you can show basic usage metrics.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-5-research-grade-outputs-youre-already-halfway-there","title":"Phase 5 \u2014 Research-Grade Outputs (you\u2019re already halfway there)","text":"<p>Goal: Your backend is already closer to \u201cresearch API\u201d than most projects; now package it safely.</p> <p>5.1 Formal citation guidance Citation guidance is now published at <code>/cite</code>, but Phase 5 should make it harder to misuse and easier to reuse:</p> <ul> <li>Ensure the recommended archived URL format corresponds to what production serves.</li> <li>Include capture timestamp (with timezone) and original URL.</li> <li>Consider adding \u201chow to cite when replay is enabled vs raw HTML only\u201d (conceptually).</li> </ul> <p>5.2 Research access pathway Two tracks:</p> <ul> <li>Human: clear documentation, limitations, and contact path for bulk needs.</li> <li>Machine: stable exports for snapshot metadata and (later) change events.     Keep it \u201csmall and safe\u201d first (metadata only), and make sustainability explicit (rate limits, fair use).</li> </ul> <p>5.3 Scholarly output Once Phase 3 exists, you have publishable material:</p> <ul> <li>A methods note/poster on provenance + change tracking without interpretation</li> <li>A small descriptive analysis: \u201cguidance drift\u201d patterns over time (careful framing)</li> </ul> <p>Definition of done (Phase 5)</p> <ul> <li>A researcher can cite snapshots correctly and request or retrieve structured metadata and change-event data without bespoke coordination.     See the expanded Phase 5 implementation plan later in this document for sub-phases and acceptance criteria.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-6-reliability-sustainability-a-lot-already-exists-now-operationalize-it","title":"Phase 6 \u2014 Reliability + Sustainability (a lot already exists; now operationalize it)","text":"<p>Goal: You already have serious ops docs and automation templates; this phase turns them into a lived routine and a public-facing posture.</p> <p>6.1 Publish capture cadence policy (public) Internally you already have the annual campaign definition and scope rules (<code>healtharchive-backend/docs/operations/annual-campaign.md:1</code>). Publicly, you want:</p> <ul> <li>Annual edition concept (Jan 01 UTC)</li> <li>What triggers ad-hoc captures (rare, explicit)</li> <li>Why scope is limited (reliability &gt; breadth)</li> </ul> <p>6.2 Formalize ops cadence (internal) You already have checklists and systemd templates (<code>healtharchive-backend/docs/deployment/systemd/README.md:1</code>). Make the \u201cboring routine\u201d explicit:</p> <ul> <li>Weekly health review</li> <li>Monthly reliability review (ties into the impact report)</li> <li>Quarterly restore test</li> <li>Dependency patch cadence</li> </ul> <p>6.3 Growth constraints You already have a single-VPS production runbook (<code>healtharchive-backend/docs/deployment/production-single-vps.md:1</code>) and strict CORS design. Define explicit constraints to avoid scope creep:</p> <ul> <li>Source cap for the year</li> <li>Storage budget and retention posture (especially with replay depending on WARCs)</li> <li>Performance budgets (API latency, indexing time)</li> </ul> <p>Definition of done (Phase 6)</p> <ul> <li>You have a documented and practiced operational routine, plus a public statement of cadence/scope constraints.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#the-updated-if-you-only-do-6-things-highest-roi-given-whats-already-built","title":"The Updated \u201cIf You Only Do 6 Things\u201d (highest ROI given what\u2019s already built)","text":"<ol> <li>Public Governance + Terms/Privacy + Corrections/Takedown process</li> <li>Public Changelog + monthly impact report discipline</li> <li>Public Status/Metrics page (build on existing <code>/api/stats</code> homepage metrics)</li> <li>Change tracking + Compare + \u201cWhat changed\u201d feed (strictly descriptive)</li> <li>Digest (web + RSS first) + subscriber metric once email is added</li> <li>One distribution partner + one authoritative verifier (non-code, but decisive)</li> </ol>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#expanded-guidance-copious-context-examples","title":"Expanded Guidance (Copious Context + Examples)","text":"<p>This section \u201cadds meat to the bones\u201d so an implementation agent can translate the roadmap into concrete, incremental tasks without reinventing the rationale.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#non-negotiable-project-posture-repeat-in-every-implementation","title":"Non-negotiable project posture (repeat in every implementation)","text":"<p>HealthArchive must always read as:</p> <ul> <li>Independent (not government, not endorsed, not affiliated)</li> <li>Archival (historical record, time-stamped)</li> <li>Non-authoritative (not current guidance; not medical advice)</li> <li>Reproducibility-first (citations point to what was visible on date X)</li> <li>Safety-first (avoid features that look like interpretation or advice)</li> </ul> <p>If a new feature increases the risk of misinterpretation (e.g., \u201csummaries,\u201d \u201ckey takeaways,\u201d \u201cwhat it means\u201d), it should be treated as out-of-scope unless it is purely descriptive and has strong guardrails.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#a-note-on-abs-style-impact-framing-why-these-upgrades-matter","title":"A note on \u201cABS-style\u201d impact framing (why these upgrades matter)","text":"<p>The upgrade plan is intentionally not \u201cmore code for the sake of code.\u201d It\u2019s about converting a technically solid archive into:</p> <ul> <li>A governed public-interest service (clear rules, corrections, takedown posture)</li> <li>A measurable service (metrics, reliability signals, visible operational maturity)</li> <li>A repeat-usage product (change tracking + digest)</li> <li>A verifiable project (external partners + credible verifier)</li> </ul> <p>In other words: institutionalization + proof of use, not just implementation.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-0-tighten-narrative-reduce-copy-drift-expanded","title":"Phase 0 \u2014 Tighten Narrative + Reduce Copy Drift (expanded)","text":""},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#why-this-matters","title":"Why this matters","text":"<ul> <li>HealthArchive already has strong disclaimers, but they\u2019re not uniformly present on the highest-traffic workflow pages (search/browse).</li> <li>A consistent \u201cwhat this is/isn\u2019t\u201d block reduces:</li> <li>confusion (\u201cis this current guidance?\u201d),</li> <li>reputational risk (\u201care you speaking for PHAC?\u201d),</li> <li>and legal/compliance ambiguity (\u201care you collecting data?\u201d).</li> <li>Consistent copy also reduces future maintenance cost: you update one canonical block, not five divergent variations.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#what-already-exists-repo-evidence","title":"What already exists (repo evidence)","text":"<ul> <li>Footer disclaimers: independence + non-advice (<code>healtharchive-frontend/src/components/layout/Footer.tsx</code>).</li> <li>Homepage \u201cWhat this site is/isn\u2019t\u201d block (<code>healtharchive-frontend/src/app/page.tsx</code>).</li> <li>Snapshot viewer \u201cImportant note\u201d block (<code>healtharchive-frontend/src/app/snapshot/[id]/page.tsx</code>).</li> <li>About and Methods pages already emphasize independence and non-partisanship (<code>healtharchive-frontend/src/app/about/page.tsx</code>, <code>healtharchive-frontend/src/app/methods/page.tsx</code>).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#main-gaps-to-close","title":"Main gaps to close","text":"<ul> <li><code>/archive</code> currently emphasizes search UX and offline fallback; it doesn\u2019t prominently restate the archive/non-advice posture where users make decisions about content.</li> <li><code>/browse/[id]</code> does a good job technically, but it\u2019s still the highest-risk page for misinterpretation because it looks like \u201cthe website\u201d (not a record).</li> <li>Methods page is written partly in \u201cfuture tense,\u201d even though the backend and runbook show much of this is real.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#deliverables-what-to-produce","title":"Deliverables (what to produce)","text":"<p>1) A single canonical mission block (2\u20133 sentences) used consistently across:    - <code>/</code> (home)    - <code>/about</code>    - <code>/methods</code>    - <code>/researchers</code>    - metadata description in <code>healtharchive-frontend/src/app/layout.tsx</code></p> <p>2) A single canonical \u201cWhat this is/isn\u2019t\u201d block surfaced on:    - <code>/</code> (already exists; may tighten language)    - <code>/archive</code> (add)    - <code>/snapshot/[id]</code> (already has similar; align wording)    - <code>/browse/[id]</code> (add a more prominent version)</p> <p>3) Methods copy updated from \u201cconceptual design\u201d \u2192 \u201ccurrent reality + limitations.\u201d</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#copy-examples-ready-to-adapt","title":"Copy examples (ready-to-adapt)","text":"<p>Mission (2\u20133 sentences):</p> <p>HealthArchive.ca preserves time-stamped snapshots of selected Canadian public health web pages so changes remain auditable and citable. It is an independent, non-governmental archival project \u2014 not medical advice and not a substitute for current official guidance.</p> <p>What this is / isn\u2019t (workflow-safe):</p> <p>This is: an archival record of what public health websites displayed at a specific time, with capture dates and citations. This is not: current guidance, medical advice, or an official government website. For current recommendations: always consult the official source website.</p> <p>Browse-mode warning (short, unavoidable):</p> <p>You are viewing an archived capture from . Links and content may be outdated or superseded. For current guidance, use the official website."},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#acceptance-criteria-practical","title":"Acceptance criteria (practical)","text":"<ul> <li>A user can enter via <code>/archive</code> or a shared <code>/browse/[id]</code> link and still see an explicit, plain-language disclaimer without scrolling.</li> <li>Copy on Methods/About/Researchers does not imply \u201cplanned someday\u201d for already-live infrastructure (but still clearly marks the project as \u201cin development\u201d and acknowledges limitations).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-0-implementation-plan-highly-detailed-sub-phases","title":"Phase 0 implementation plan (highly detailed; sub-phases)","text":"<p>This plan is written so another agent can implement Phase 0 as a sequence of small, low-risk PRs. The intent is to improve copy consistency, risk posture, and accuracy without adding new product features.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-0a-inventory-and-decide-the-canonical-copy","title":"Phase 0A \u2014 Inventory and decide the \u201ccanonical copy\u201d","text":"<p>Purpose</p> <ul> <li>Reduce copy drift across pages.</li> <li>Avoid accidental \u201cfuture tense\u201d descriptions that contradict current backend reality.</li> <li>Ensure the \u201carchived/not current\u201d message appears on high-risk entrypoints (<code>/archive</code>, <code>/browse/[id]</code>) as well as home/snapshot.</li> </ul> <p>Tasks</p> <p>1) Inventory all existing disclaimers and mission-like sentences in the frontend:    - Footer independence and \u201cnot medical advice\u201d language.    - Homepage hero description.    - Homepage \u201cWhat this site is/isn\u2019t\u201d block.    - Snapshot viewer \u201cImportant note.\u201d    - About/Methods/Researchers intro copy.    - Document metadata description (<code>healtharchive-frontend/src/app/layout.tsx</code>). 2) Identify any copy conflicts or drift:    - Places where \u201cintended to\u201d / \u201cwould\u201d is used despite current implementation.    - Places where the disclaimer is missing entirely on workflow pages.    - Inconsistent phrasing that could be quoted out of context (e.g., \u201cpublic health information\u201d vs \u201cpublic health guidance\u201d vs \u201cwebpages\u201d). 3) Decide the canonical text for:    - Mission block (2\u20133 sentences).    - What this is / isn\u2019t (3 short bullets or 3 lines).    - Browse-mode warning (1 short line suitable for \u201cabove the fold\u201d).</p> <p>Deliverables</p> <ul> <li>A final, agreed \u201ccanonical copy\u201d snippet that will be reused across the site.</li> <li>A short note (1\u20132 paragraphs) explaining the rationale and any intentional wording choices (e.g., why \u201cweb pages\u201d vs \u201cguidance\u201d).</li> </ul> <p>Recommended canonical text (starting point; edit as needed)</p> <ul> <li>Mission (short):</li> <li>\u201cHealthArchive.ca preserves time-stamped snapshots of selected Canadian public health web pages so changes remain auditable and citable.\u201d</li> <li>\u201cIt is an independent, non-governmental archival project \u2014 not medical advice and not a substitute for current official guidance.\u201d</li> <li>What this is / isn\u2019t:</li> <li>\u201cThis is: an archival record of what public websites displayed at a specific time, with capture dates and citations.\u201d</li> <li>\u201cThis is not: current guidance, medical advice, or an official government website.\u201d</li> <li>\u201cFor current recommendations: consult the official source website.\u201d</li> <li>Browse warning:</li> <li>\u201cYou are viewing an archived capture from  \u2014 not current guidance.\u201d <p>Acceptance criteria</p> <ul> <li>There is exactly one \u201ccanonical\u201d version of each copy block, and any variations are intentional and documented.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-0b-create-a-reuse-mechanism-to-prevent-drift","title":"Phase 0B \u2014 Create a reuse mechanism (to prevent drift)","text":"<p>Purpose</p> <ul> <li>Make it hard for future edits to accidentally diverge.</li> <li>Ensure the same message appears consistently across multiple routes without manual copy-paste.</li> </ul> <p>Recommended approach (choose one)</p> <ul> <li>Option 1 (preferred): create a small shared UI component (or two) that renders:</li> <li>Mission block (short)</li> <li>What this is/isn\u2019t block (compact)</li> <li>Browse warning (ultra-compact)</li> <li>Option 2: create a single exported \u201ccopy constants\u201d object used by page components.</li> </ul> <p>Decision criteria</p> <ul> <li>If the block has layout/structure (headings, list) \u2192 component is usually better.</li> <li>If it\u2019s a single sentence reused in metadata and in-page copy \u2192 constants can reduce duplication.</li> </ul> <p>Deliverables</p> <ul> <li>A single source of truth for the canonical copy, referenced by:</li> <li><code>/</code> (homepage)</li> <li><code>/archive</code></li> <li><code>/snapshot/[id]</code></li> <li><code>/browse/[id]</code></li> <li>and optionally metadata description (or a close paraphrase, if metadata needs to be shorter).</li> </ul> <p>Acceptance criteria</p> <ul> <li>Future contributors can update the canonical disclaimer in one place.</li> <li>No page introduces new disclaimer phrasing unless it\u2019s a deliberate exception.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-0c-update-workflow-entrypoints-highest-risk-pages","title":"Phase 0C \u2014 Update workflow entrypoints (highest risk pages)","text":"<p>Purpose</p> <ul> <li>Users often enter the site via:</li> <li><code>/archive</code> (search/browse),</li> <li><code>/snapshot/[id]</code> (shared links),</li> <li><code>/browse/[id]</code> (embedded archived page).</li> <li>These are also where misinterpretation risk is highest.</li> </ul> <p>Tasks</p> <p>1) <code>/archive</code>:    - Add a compact \u201carchived/not current guidance\u201d block near the top of the page (above search/results), tuned to browsing/search context.    - Ensure it remains visible in both \u201clive API\u201d and \u201coffline fallback\u201d modes. 2) <code>/browse/[id]</code>:    - Add a short warning above the iframe and keep it \u201cabove fold\u201d on typical laptop/mobile widths.    - Include capture date/time in the message when available (this is already present in the browse header UI; the warning should explicitly use it). 3) <code>/snapshot/[id]</code>:    - Align existing \u201cImportant note\u201d wording with the canonical text (don\u2019t remove detail; just remove drift).    - Ensure provenance fields displayed (source, capture date, original URL) stay prominent and readable.</p> <p>Acceptance criteria</p> <ul> <li>A user landing directly on <code>/archive</code> or <code>/browse/[id]</code> sees the \u201carchived/not current\u201d message without scrolling.</li> <li>The message does not imply medical interpretation or government endorsement.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-0d-normalize-aboutmethodsresearchers-to-current-reality","title":"Phase 0D \u2014 Normalize \u201cabout/methods/researchers\u201d to current reality","text":"<p>Purpose</p> <ul> <li>These pages frame the project; they must be accurate to the implemented stack and operational posture.</li> <li>The goal is to remove unnecessary \u201cfuture tense\u201d while preserving honest \u201cin development\u201d status and limitations.</li> </ul> <p>Tasks</p> <p>1) <code>/methods</code>:    - Replace \u201cintended to\u201d / \u201cwould rely on\u201d language where the backend already does the thing today (WARC storage, indexing, replay options).    - Add a short, high-level statement about the annual edition concept (Jan 01 UTC) as the default cadence, and explicitly state scope is constrained by reliability. 2) <code>/about</code>:    - Keep the motivation and non-partisanship stance.    - Add one sentence linking \u201cwhy the archive exists\u201d to research/journalism reproducibility (\u201ccitable snapshots,\u201d \u201cauditability\u201d). 3) <code>/researchers</code>:    - Keep current \u201cplanned capabilities\u201d callout but make sure it matches the roadmap (timeline/compare/exports/diffs).    - Ensure citation guidance matches the site\u2019s actual URL patterns and the snapshot viewer\u2019s semantics (replay vs raw HTML).</p> <p>Acceptance criteria</p> <ul> <li>These pages do not contradict the backend runbooks (annual campaign, single VPS reality, optional replay).</li> <li>These pages do not overpromise completeness or fidelity.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-0e-metadata-consistency-checks-finish-work","title":"Phase 0E \u2014 Metadata + consistency checks (finish work)","text":"<p>Purpose</p> <ul> <li>Reduce the chance that the site is indexed or quoted with misleading metadata.</li> <li>Ensure site-wide consistency and accessibility of the disclaimers.</li> </ul> <p>Tasks</p> <p>1) Metadata description (<code>healtharchive-frontend/src/app/layout.tsx</code>):    - Ensure it includes independence + archive + \u201cwhat it was at the time\u201d framing.    - Keep it short enough for search engine snippets. 2) Consistency pass:    - Confirm the same canonical disclaimer appears on:      - Home, Archive, Snapshot, Browse.    - Confirm Terms/Privacy/Governance are not referenced yet (Phase 1 will add them), unless you explicitly want \u201ccoming soon\u201d links. 3) Accessibility pass:    - Ensure the disclaimer blocks use readable text sizes and don\u2019t rely on color alone.    - Ensure any new callouts have appropriate semantics (e.g., headings, lists). 4) \u201cOut of scope\u201d confirmation:    - Confirm Phase 0 did not introduce any new backend endpoints, analytics scripts, or data collection.</p> <p>Acceptance criteria</p> <ul> <li>Copy is consistent, accurate to current deployment, and visible where it matters.</li> <li>No security posture regressions (CSP, iframe sandboxing, strict \u201cpublic API only\u201d usage).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#suggested-pr-breakdown-sequencing","title":"Suggested PR breakdown (sequencing)","text":"<p>To keep changes small and reviewable:</p> <p>1) PR-0: Add canonical copy mechanism (component/constants) + update homepage to use it. 2) PR-1: Add disclaimer block to <code>/archive</code>. 3) PR-2: Add browse-mode warning to <code>/browse/[id]</code> (and align snapshot page wording). 4) PR-3: Update <code>/methods</code>, <code>/about</code>, <code>/researchers</code> to match current reality and the canonical copy. 5) PR-4: Metadata + final consistency/accessibility pass.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-0-definition-of-done-checklist","title":"Phase 0 \u201cDefinition of done\u201d (checklist)","text":"<ul> <li>Canonical mission + \u201cis/isn\u2019t\u201d + browse warning copy exists in one place and is reused.</li> <li><code>/archive</code> and <code>/browse/[id]</code> show \u201carchived/not current guidance\u201d without scrolling.</li> <li>Methods/About/Researchers copy no longer reads like a hypothetical system when it is already deployed.</li> <li>No new tracking/analytics added (Phase 2 will decide this deliberately).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-0-implementation-notes-completed","title":"Phase 0 implementation notes (completed)","text":"<p>The following Phase 0 items are implemented in the current repo state:</p> <ul> <li>Canonical copy source of truth: <code>healtharchive-frontend/src/lib/siteCopy.ts</code></li> <li>Used for metadata description: <code>healtharchive-frontend/src/app/layout.tsx</code></li> <li>Used on the homepage \u201cis/isn\u2019t\u201d block: <code>healtharchive-frontend/src/app/page.tsx</code></li> <li>Workflow disclaimers added/normalized:</li> <li><code>/archive</code> callout: <code>healtharchive-frontend/src/app/archive/page.tsx</code></li> <li><code>/archive/browse-by-source</code> callout: <code>healtharchive-frontend/src/app/archive/browse-by-source/page.tsx</code></li> <li><code>/browse/[id]</code> warning with capture date and \u201cnot current guidance or medical advice\u201d: <code>healtharchive-frontend/src/components/replay/BrowseReplayClient.tsx</code></li> <li><code>/snapshot/[id]</code> \u201cImportant note\u201d aligned to canonical language: <code>healtharchive-frontend/src/app/snapshot/[id]/page.tsx</code></li> <li>\u201cFuture tense\u201d reduction / accuracy updates:</li> <li>About: <code>healtharchive-frontend/src/app/about/page.tsx</code></li> <li>Methods (capture pipeline + annual edition posture described as policy): <code>healtharchive-frontend/src/app/methods/page.tsx</code></li> <li>Researchers citation guidance updated to match live numeric snapshot URLs: <code>healtharchive-frontend/src/app/researchers/page.tsx</code></li> <li>Confirmed non-goals for Phase 0: no new analytics scripts, no new backend endpoints, no changes to CSP/CORS/iframe sandboxing.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-1-public-governance-layer-expanded","title":"Phase 1 \u2014 Public Governance Layer (expanded)","text":""},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#why-this-matters_1","title":"Why this matters","text":"<p>Governance is the difference between:</p> <ul> <li>\u201ca cool archive site\u201d and</li> <li>\u201ca public-interest archival service that others can safely rely on.\u201d</li> </ul> <p>It also reduces risk by making your policies explicit before anyone asks (and before you scale).</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#what-already-exists","title":"What already exists","text":"<ul> <li>Internal operational and architecture documentation is already unusually strong in <code>healtharchive-backend/docs/**</code>.</li> <li>Public-facing pages cover motivation and limitations but do not yet provide:</li> <li>correction procedure,</li> <li>takedown/opt-out posture,</li> <li>a formal scope/inclusion policy,</li> <li>or Terms/Privacy as explicit pages.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#deliverables-public-non-code-first","title":"Deliverables (public, non-code-first)","text":"<p>These are best implemented as simple public pages in the frontend (static content first; automation later).</p> <p>1) Governance page (new public route; content-first)    - What the project is / isn\u2019t (canonical block)    - Scope boundaries (what sources count; what\u2019s out of scope; why)    - Source inclusion criteria (mechanical, not vibes)    - Provenance commitments (what metadata you guarantee on snapshots)    - Corrections policy (what can be corrected and typical response times)    - Takedown/opt-out policy (how to request, how decisions are made, how you handle third-party content)    - Contact / escalation path</p> <p>2) Terms page    - Research/reference use, no medical reliance    - No endorsement / no affiliation    - Copyright/takedown posture (plain language; do not overclaim legal certainty)</p> <p>3) Privacy page    - Explicit \u201cno patient data / no accounts\u201d    - What logs/analytics exist today    - If analytics are added later, this page becomes the contract; update it in lockstep.</p> <p>4) Changelog page    - Monthly cadence is fine.    - Include \u201cwhat changed\u201d across: scope, features, reliability, policy.</p> <p>5) Report an issue page (intake)    - A structured way to report: broken snapshot, wrong metadata, replay failure, missing page, correction request, takedown request, source suggestion.    - Explain what information helps you triage (snapshot ID, original URL, screenshot, time, browser).</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#examples-policy-language-plain-defensible","title":"Examples: policy language (plain, defensible)","text":"<p>Corrections:</p> <p>Corrections: If metadata is wrong (capture date, source labeling, broken link) or a snapshot fails to load, report it. We aim to acknowledge reports within 7 days, and urgent safety labeling issues within 48 hours. Not all issues are fixable (some depend on what the crawl captured), but we document limitations and outcomes.</p> <p>Takedown / opt-out:</p> <p>Takedown requests: If you are a site owner or rights holder and believe content should not be displayed, contact us with the URL(s) and your rationale. We review requests in good faith and may remove access, limit distribution, or add context. We publish aggregate counts of takedown requests in our transparency reporting.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#acceptance-criteria","title":"Acceptance criteria","text":"<ul> <li>A neutral user can find Governance/Terms/Privacy in under two clicks from any page (header or footer).</li> <li>The governance pages do not introduce new risk (no promises you can\u2019t keep; no legal overreach; no secrets).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-2-make-impact-measurable-visible-expanded","title":"Phase 2 \u2014 Make Impact Measurable + Visible (expanded)","text":""},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#why-this-matters_2","title":"Why this matters","text":"<ul> <li>You already expose \u201csnapshots/pages\u201d via <code>/api/stats</code> and show them on the homepage. That\u2019s a strong start.</li> <li>The next step is making reliability and coverage legible in a \u201cservice-like\u201d way:</li> <li>last capture per source,</li> <li>uptime posture,</li> <li>known issues,</li> <li>and (if you choose) usage metrics.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#what-already-exists_1","title":"What already exists","text":"<ul> <li>Backend has public endpoints that can power a status/metrics page without admin access:</li> <li><code>/api/health</code> (basic status + DB + counts)</li> <li><code>/api/stats</code> (snapshots/pages/sources totals)</li> <li><code>/api/sources</code> (per-source record counts + first/last capture dates + optional entry points and preview URLs)</li> <li>Frontend already uses <code>/api/stats</code> live on the homepage with fallback (<code>healtharchive-frontend/src/app/page.tsx</code>).</li> <li>Ops docs already recommend external uptime checks (<code>healtharchive-backend/docs/operations/monitoring-and-ci-checklist.md</code>).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#deliverables","title":"Deliverables","text":"<p>1) Public status/metrics page (new route)    - Keep it simple and honest; don\u2019t claim full observability if you don\u2019t have it.    - Prefer metrics you can compute from existing public API responses.</p> <p>2) Metric definitions (public or semi-public)    - A short, explicit definition of each metric you report (so it\u2019s not \u201cnumbers with vibes\u201d).</p> <p>3) Monthly impact report template    - A one-page, repeatable artifact you can publish (web post or PDF).</p> <p>4) Analytics decision    - Decide: no analytics (privacy-first) vs privacy-preserving analytics vs server-side aggregates.    - Important: frontend CSP is restrictive; adding third-party scripts is a deliberate security decision, not a \u201cdrop-in.\u201d</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#suggested-metrics-with-precise-definitions","title":"Suggested metrics (with precise definitions)","text":"<ul> <li>Coverage</li> <li><code>sourcesTotal</code>: number of sources visible in <code>/api/sources</code>.</li> <li><code>pagesTotal</code>: unique page groups (from <code>/api/stats</code>).</li> <li><code>snapshotsTotal</code>: total snapshots (from <code>/api/stats</code>).</li> <li>Freshness</li> <li>per source: \u201clast capture date\u201d from <code>/api/sources</code> (this is a proxy for freshness; \u201ctime-to-index\u201d requires additional tracking).</li> <li>Reliability</li> <li>API uptime: measured via external monitor of <code>/api/health</code> + frontend <code>/archive</code>.</li> <li>Crawl/index success rate: requires tracking job outcomes; can be public later as aggregates.</li> <li>Usage</li> <li>Only if you choose to measure: \u201csearches/day,\u201d \u201csnapshot views/day,\u201d \u201cbrowse views/day.\u201d</li> <li>If you don\u2019t measure, say so explicitly and rely on partner/verifier evidence.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#status-page-layout-example-content-not-ui-code","title":"Status page layout example (content, not UI code)","text":"<p>Top: - \u201cCurrent status: Operational / degraded / outage\u201d (based on <code>/api/health</code> + static incident notes) - \u201cLast updated: \u201d <p>Coverage: - Sources tracked: N - Snapshots: N - Pages: N</p> <p>Per-source table: - Source name - Records captured - First capture date - Last capture date - \u201cBrowse archived site\u201d link (if entry point exists)</p> <p>Reliability: - \u201cUptime last 30 days\u201d (if you have an external monitor) - \u201cRecent incidents\u201d (manual log is fine initially)</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#acceptance-criteria_1","title":"Acceptance criteria","text":"<ul> <li>The status/metrics page works even when replay is not configured (and does not leak admin endpoints).</li> <li>The status/metrics page is explicit about what is measured and what is not.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-2-implementation-plan-detailed-sub-phases","title":"Phase 2 Implementation Plan (Detailed; sub-phases)","text":"<p>Phase 2 is about credibility through measurability. The goal is not \u201cmore numbers\u201d; it\u2019s to make HealthArchive look and behave like a maintained public service with transparent coverage and clear limitations.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#design-principles-phase-2","title":"Design principles (Phase 2)","text":"<ul> <li>Honesty over completeness. Only publish metrics you can define and reproduce.</li> <li>Privacy-first. Prefer aggregated counts over user-level tracking. Default to collecting less.</li> <li>No new risk surface by accident. Avoid adding third-party scripts until you have explicitly decided to do so and updated CSP accordingly.</li> <li>Keep public vs admin boundaries strict. The status page must not depend on admin-only endpoints.</li> <li>Single-VPS realism. Any new aggregation pipeline must be lightweight and must not compete with indexing/crawling for CPU/IO.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-2a-decide-the-official-metric-contract-121-day","title":"Sub-phase 2A \u2014 Decide the \u201cofficial\u201d metric contract (\u00bd\u20131 day)","text":"<p>Goal: Create a stable set of metrics with explicit definitions, and decide how you\u2019ll measure usage (or explicitly not).</p> <p>Deliverables:</p> <ul> <li>A \u201cMetrics definitions\u201d section (can live on the new <code>/status</code> page, or as a short doc linked from it).</li> <li>A decision on usage measurement (see Sub-phase 2D).</li> </ul> <p>Recommended official metrics (initial set):</p> <p>1) Coverage    - <code>sourcesTotal</code>, <code>pagesTotal</code>, <code>snapshotsTotal</code> (already available via public API).    - Per-source:      - <code>recordCount</code>, <code>firstCapture</code>, <code>lastCapture</code> (already available via <code>/api/sources</code>).</p> <p>2) Freshness    - Per-source \u201clast capture date\u201d (proxy for freshness).    - Optional later: \u201ctime to index\u201d (requires tracking job completion vs indexing completion).</p> <p>3) Reliability    - Public: \u201cAPI reachable\u201d (based on <code>/api/health</code>).    - Public: \u201cReplay enabled\u201d and \u201cpreviews enabled\u201d (based on whether URLs are returned).    - External monitor uptime (%): start as \u201cnot yet measured\u201d or \u201cmeasured externally\u201d until you have a real monitor.</p> <p>4) Usage (optional, and only if you choose)    - <code>searchRequestsPerDay</code>, <code>snapshotViewsPerDay</code>, <code>browseViewsPerDay</code>, <code>reportSubmissionsPerDay</code> (aggregated).    - If you do not measure usage, state that explicitly in <code>/privacy</code> and in the metrics definitions.</p> <p>Acceptance criteria:</p> <ul> <li>Each metric has: definition, data source, and update cadence (e.g., \u201ccomputed nightly\u201d vs \u201clive\u201d).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-2b-public-statusmetrics-page-mvp-frontend-existing-endpoints-12-days","title":"Sub-phase 2B \u2014 Public Status/Metrics page MVP (frontend + existing endpoints) (1\u20132 days)","text":"<p>Goal: Ship a status page that works immediately without new backend changes.</p> <p>Recommended route:</p> <ul> <li><code>/status</code> (or <code>/status</code> + <code>/metrics</code> as a separate section; avoid naming collisions with backend <code>/metrics</code>).</li> </ul> <p>Data sources (public endpoints only):</p> <ul> <li><code>/api/health</code> \u2192 \u201cAPI status\u201d</li> <li><code>/api/stats</code> \u2192 totals</li> <li><code>/api/sources</code> \u2192 per-source coverage table</li> </ul> <p>Recommended sections (copywriting guidance):</p> <ul> <li>Current status</li> <li>\u201cOperational / degraded / down\u201d (simple, based on whether <code>/api/health</code> succeeds).</li> <li>Timestamp: \u201cLast checked: \u2026\u201d</li> <li> <p>Small note: \u201cThis is a public archive; not medical advice.\u201d</p> </li> <li> <p>Coverage snapshot</p> </li> <li>Sources tracked, snapshots, pages</li> <li> <p>Latest capture date (if available via stats)</p> </li> <li> <p>Per-source coverage</p> </li> <li>Source name + counts + first/last capture</li> <li> <p>Link to browse entry point (if available)</p> </li> <li> <p>Data notes</p> </li> <li>What \u201cmissing\u201d can mean (not captured yet, capture failure, out of scope)</li> <li>Replay limitations and third-party asset caveats</li> </ul> <p>Acceptance criteria:</p> <ul> <li>Page loads even if the backend is down (show an honest fallback message; do not fabricate metrics).</li> <li>Page stays within existing security posture (no new scripts; no CSP weakening).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-2c-metrics-definitions-and-transparency-language-12-day","title":"Sub-phase 2C \u2014 \u201cMetrics definitions\u201d and transparency language (\u00bd day)","text":"<p>Goal: Prevent \u201cnumbers with vibes\u201d by publishing the meaning and limits of each metric.</p> <p>Include:</p> <ul> <li>What counts as a snapshot vs a page group</li> <li>Timezones (capture dates are UTC)</li> <li>What \u201clast capture date\u201d means (it is not necessarily \u201clast updated by the source\u201d)</li> <li>Whether usage metrics are collected (and if so, how coarse they are)</li> </ul> <p>Acceptance criteria:</p> <ul> <li>A researcher can quote your definitions in a methods section without guesswork.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-2d-decide-and-implement-a-usage-measurement-approach-plan-level-options-14-days","title":"Sub-phase 2D \u2014 Decide and implement a usage measurement approach (plan-level options) (1\u20134 days)","text":"<p>This is the key decision point. Choose one:</p> <p>Option D1: No usage analytics (privacy-first) - Publish only coverage + freshness + reliability proxies. - Evidence of impact comes from: partner adoption, citations, verifier statements, issue report volume. - Lowest risk and operational overhead.</p> <p>Option D2: Server-side aggregate counters (recommended \u201cbest practice\u201d for your posture) - Track only aggregated daily counts for key public actions:   - search requests, snapshot detail views, raw snapshot views, browse page loads, report submissions - Store aggregates (e.g., per day) in the backend DB, not user-level logs. - No cookies and no third-party scripts required. - This is usually the best balance: measurable impact without surveillance optics.</p> <p>Option D3: Privacy-preserving third-party analytics - Only if you explicitly decide to accept the CSP and supply-chain tradeoffs. - Must update <code>/privacy</code> to reflect the provider and data collected.</p> <p>Acceptance criteria:</p> <ul> <li>Whatever option you pick is reflected in <code>/privacy</code> and on the status page.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-2e-reliability-measurement-incident-notes-122-days-incremental","title":"Sub-phase 2E \u2014 Reliability measurement + incident notes (\u00bd\u20132 days, incremental)","text":"<p>Goal: Make \u201cservice operations\u201d legible without pretending you have enterprise SRE.</p> <p>Approach:</p> <ul> <li>Start with a manual \u201cRecent incidents\u201d section on <code>/status</code> (or <code>/changelog</code> entries tagged \u201cincident\u201d).</li> <li>Add external uptime monitoring later (recommended by existing ops docs) and then display 30-day uptime once you have a data source you trust.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>If something breaks, you have a public place to acknowledge it (even briefly).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-2f-monthly-impact-report-template-storage-cadence-121-day","title":"Sub-phase 2F \u2014 Monthly Impact Report (template + storage + cadence) (\u00bd\u20131 day)","text":"<p>Goal: Create a repeatable \u201cproof artifact\u201d that builds ABS verifiability over time.</p> <p>Deliverables:</p> <ul> <li>A template (web page or markdown) with these sections:</li> <li>\u201cWhat\u2019s new\u201d</li> <li>\u201cCoverage changes\u201d</li> <li>\u201cReliability notes\u201d</li> <li>\u201cUsage snapshot\u201d (only if you measure)</li> <li>\u201cPartner/mention highlights\u201d</li> <li>\u201cKnown limitations / next month focus\u201d</li> </ul> <p>Storage/location:</p> <ul> <li>Keep impact reports in a stable folder (e.g., <code>docs/impact/YYYY-MM.md</code> in the frontend repo, or a dedicated section on the site).</li> <li>Link each report from <code>/changelog</code>.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>You can produce the report in under 30 minutes each month.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-2g-documentation-runbook-updates-12-day","title":"Sub-phase 2G \u2014 Documentation + runbook updates (\u00bd day)","text":"<ul> <li>Update:</li> <li>Frontend docs to mention <code>/status</code> and where its data comes from.</li> <li>Backend docs if new public endpoints or aggregation logic are introduced.</li> <li>Add a changelog entry: \u201cStatus/Metrics page launched; metrics definitions published.\u201d</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-2h-tests-as-behavior-changes-122-days","title":"Sub-phase 2H \u2014 Tests (as behavior changes) (\u00bd\u20132 days)","text":"<ul> <li>Frontend: add tests that <code>/status</code> renders expected sections (and handles \u201cbackend unreachable\u201d gracefully).</li> <li>Backend (only if Option D2 is implemented): add tests that aggregates update correctly and do not store PII.</li> </ul> <p>Definition of done (Phase 2, detailed)</p> <ul> <li>A public <code>/status</code> page exists and answers: \u201cIs it up? what\u2019s covered? how current? what are the limitations?\u201d</li> <li>Metric definitions are published and consistent with <code>/privacy</code>.</li> <li>If usage is measured, it is aggregated and privacy-preserving; if not, that is explicitly stated.</li> <li>A monthly impact report template exists and is linked from <code>/changelog</code>.</li> </ul> <p>Status (Phase 2 implementation)</p> <ul> <li>Implemented on 2025-12-21.</li> <li>Added <code>/status</code> and <code>/impact</code> pages to the frontend.</li> <li>Added <code>/api/usage</code> to the backend with daily aggregate counts (search, snapshot detail, raw snapshot, reports).</li> <li>Enabled/controlled via backend env: <code>HEALTHARCHIVE_USAGE_METRICS_ENABLED</code> and <code>HEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS</code>.</li> <li>Updated <code>/privacy</code> to disclose aggregate usage counts.</li> <li>Added a baseline impact report and changelog entry for Phase 2.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-3-change-tracking-compare-digest-expanded","title":"Phase 3 \u2014 Change Tracking + Compare + Digest (expanded)","text":""},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#why-this-matters_3","title":"Why this matters","text":"<p>Without change tracking, HealthArchive answers \u201cwhat did it say?\u201d but not \u201cwhat changed?\u201d. Change tracking is the primary upgrade that drives:</p> <ul> <li>repeat visits,</li> <li>subscriber growth,</li> <li>and research/journalism use (audit trails).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#what-already-exists-that-should-be-leveraged","title":"What already exists that should be leveraged","text":"<ul> <li>Canonical page grouping: <code>normalized_url_group</code> and the optional <code>pages</code> table (<code>healtharchive-backend/src/ha_backend/models.py</code>).</li> <li>Content hashing: snapshots already store <code>content_hash</code> (SHA-256 of body bytes) (<code>healtharchive-backend/src/ha_backend/indexing/mapping.py</code>).</li> <li>This is a powerful \u201ccheap change detector\u201d before doing expensive diffs.</li> <li>Search modes: <code>view=pages</code> vs <code>view=snapshots</code> and strong filtering semantics (<code>healtharchive-backend/docs/architecture.md</code>).</li> <li>UI foundations: snapshot and browse views already handle provenance, replay/raw fallback, and edition switching.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#what-change-tracking-must-not-be","title":"What \u201cchange tracking\u201d must not be","text":"<ul> <li>It must not read like medical interpretation.</li> <li>It must not summarize in a way that implies \u201cthis means you should\u2026\u201d.</li> <li>It must not depend on heavy compute during normal user requests.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#deliverables-conceptual","title":"Deliverables (conceptual)","text":"<p>1) Page timeline    - For a given \u201cpage group,\u201d show the list of captures over time (dates, editions/jobs).    - User can select two captures to compare.</p> <p>2) Compare view    - Show two versions (A and B) with clear provenance: timestamps, source, URL, edition/job.    - Provide a descriptive diff:      - \u201cAdded/removed/changed\u201d section counts      - Highlight textual changes      - Explicitly label \u201chigh-noise\u201d pages when appropriate</p> <p>3) Changes feed    - A feed of recent change events, filterable by:      - source,      - date range,      - (optional later) mechanical tags (avoid editorial topics early)</p> <p>4) Digest    - Weekly digest published as:      - web page archive,      - RSS feed,      - email later (if desired).</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#practical-meaningful-change-policy-examples","title":"Practical \u201cmeaningful change\u201d policy (examples)","text":"<p>Meaningful changes might include: - Headings/sections added/removed - Guidance text changed in paragraphs or lists - Tables updated (when text extraction can detect it)</p> <p>Not meaningful (or \u201clow signal\u201d) changes include: - cookie banners, - global nav, - timestamps \u201clast updated\u201d in page chrome, - minor layout/whitespace shifts.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#suggested-approach-still-no-code","title":"Suggested approach (still no code)","text":"<p>Use a staged pipeline so you don\u2019t compute diffs unnecessarily:</p> <p>1) Detect change candidates:    - If <code>content_hash</code> differs between successive captures for the same page group, it\u2019s a candidate. 2) Generate a \u201creadable text representation\u201d:    - Normalize HTML to text with noise reduction rules. 3) Produce diff artifacts:    - Store enough to render compare views and feeds quickly. 4) Surface the output in UI:    - Compare page, timeline page, changes feed, digest feed.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#user-facing-copy-examples-guardrail-language","title":"User-facing copy examples (guardrail language)","text":"<p>Compare page disclaimer:</p> <p>This comparison highlights text changes between two archived captures. It does not interpret the change or provide guidance. For current recommendations, consult the official source website.</p> <p>Digest disclaimer:</p> <p>This digest lists pages whose archived text changed during the period. It is not clinical guidance and may include formatting or boilerplate changes.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#acceptance-criteria_2","title":"Acceptance criteria","text":"<ul> <li>A user can answer \u201cwhat changed since the last edition (or between editions)?\u201d without manual searching.</li> <li>Compare output is descriptive, provenance-rich, and clearly non-authoritative.</li> <li>Heavy processing happens off the request path (no slow compare pages that compute diffs live).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-3-implementation-plan-detailed-sub-phases","title":"Phase 3 Implementation Plan (Detailed; sub-phases)","text":"<p>Phase 3 turns HealthArchive from \u201ca searchable archive\u201d into \u201ca living audit tool\u201d. The goal is to let a user answer: what changed, when, and between which captures \u2014 without implying medical interpretation.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#design-principles-phase-3","title":"Design principles (Phase 3)","text":"<ul> <li>Descriptive only: show text diffs, not \u201cmeaning\u201d or \u201crecommendations\u201d.</li> <li>Provenance-first: every change event must be anchored to two snapshot IDs (A \u2192 B) with timestamps and the source URL/group.</li> <li>No heavy work on requests: diff computation must happen in background/ops workflows, not inside normal page loads.</li> <li>Noise-aware: explicitly label high-noise pages and avoid overconfident summaries.</li> <li>Versioned methodology: store a <code>diff_version</code>/<code>normalization_version</code> so later improvements don\u2019t silently rewrite history.</li> <li>Scope fits the annual edition model: a user should still get value even when captures are annual (timeline + compare remain useful even if \u201cweekly changes\u201d are sparse).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-3a-define-change-events-and-user-stories-121-day","title":"Sub-phase 3A \u2014 Define \u201cchange events\u201d and user stories (\u00bd\u20131 day)","text":"<p>Define the minimum set of user-facing questions and map each to an artifact:</p> <ul> <li>Timeline: \u201cShow me all captures for this page over time.\u201d \u2192 page timeline dataset</li> <li>Compare: \u201cShow changes between capture A and B.\u201d \u2192 diff artifact for (A,B)</li> <li>Recent changes: \u201cWhat changed recently?\u201d \u2192 changes feed over computed events</li> <li>Digest: \u201cSummarize changes for a period.\u201d \u2192 web + RSS output derived from the feed</li> </ul> <p>Also define change types and guardrails:</p> <ul> <li><code>updated</code> (content changed between two captures)</li> <li><code>unchanged</code> (hash identical; no diff needed)</li> <li><code>new_page</code> (first-ever capture for a page group)</li> <li><code>removed_page</code> (optional; requires edition-to-edition set comparison)</li> <li><code>error</code> (diff could not be computed; still track the event)</li> </ul> <p>Deliverable: a short \u201cPhase 3 semantics\u201d section (internal) that is used consistently in API/UI copy.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-3b-data-model-migration-plan-12-days","title":"Sub-phase 3B \u2014 Data model + migration plan (1\u20132 days)","text":"<p>Create a minimal, future-proof storage model for change tracking:</p> <ul> <li>Snapshot pair anchor: store <code>from_snapshot_id</code> and <code>to_snapshot_id</code> (or <code>a_snapshot_id</code>/<code>b_snapshot_id</code>).</li> <li>Page anchor: store <code>source_id</code> + <code>normalized_url_group</code> (and/or <code>page_id</code> if you choose to make pages mandatory).</li> <li>Summary fields (fast feed rendering):</li> <li>timestamps, diff size signals (e.g., \u201cchanged characters\u201d, \u201cchanged sections count\u201d),</li> <li>coarse \u201cnoise score\u201d / \u201chigh-noise\u201d boolean,</li> <li>short, descriptive \u201cwhat changed\u201d sentence that never implies interpretation (e.g., \u201c3 sections changed; 2 added; 1 removed\u201d).</li> <li>Diff artifact fields (for compare rendering):</li> <li>a rendered HTML diff or structured diff blocks,</li> <li>optional \u201csection list\u201d for navigation.</li> <li>Version fields: <code>diff_version</code>, <code>normalization_version</code>, <code>computed_at</code>, <code>computed_by</code>.</li> </ul> <p>Deliverable: one Alembic migration introducing change-event storage (plus indexes needed for feeds and timelines).</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-3c-change-detection-pipeline-background-compute-25-days","title":"Sub-phase 3C \u2014 Change detection pipeline (background compute) (2\u20135 days)","text":"<p>Build a staged pipeline that minimizes work:</p> <p>1) Identify candidates cheaply    - Use existing <code>normalized_url_group</code> + <code>capture_timestamp</code> ordering to find \u201cadjacent captures\u201d.    - Use <code>content_hash</code> to classify <code>unchanged</code> vs <code>candidate</code> (skip expensive diff when hashes match). 2) Normalize HTML to \u201cdiffable text\u201d    - Reuse the project\u2019s text extraction approach where possible.    - Add noise-reduction rules (e.g., drop headers/footers/nav/cookie banners/\u201clast updated\u201d chrome where detectable). 3) Compute diff    - Produce a human-readable diff with stable formatting.    - Emit \u201csummary stats\u201d (counts only, no interpretations). 4) Persist artifacts    - Store event row + (optional) rendered diff artifact for fast compare pages.</p> <p>Operational requirement: the pipeline must be idempotent, resumable, and rate-limited so it doesn\u2019t overwhelm the VPS after big crawls.</p> <p>Deliverables:</p> <ul> <li>A command or background task that computes diffs for:</li> <li>\u201cnewly indexed snapshots\u201d, and</li> <li>a backfill range (for existing data).</li> <li>A consistent way to measure backlog and error rate (even if only as counts).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-3d-public-api-contract-13-days","title":"Sub-phase 3D \u2014 Public API contract (1\u20133 days)","text":"<p>Expose a public-only contract that supports UI and research workflows:</p> <ul> <li>Changes feed endpoint</li> <li>filterable by source, date range, and (optionally) URL/group.</li> <li>supports pagination.</li> <li>returns summary + provenance fields (snapshot IDs A/B, timestamps, source, URL/group).</li> <li>Compare endpoint</li> <li>fetch a diff artifact for two snapshots (or a precomputed diff ID).</li> <li>returns:<ul> <li>provenance,</li> <li>summary stats,</li> <li>diff content (renderable).</li> </ul> </li> <li>Page timeline endpoint</li> <li>list captures for a given URL group (or snapshot ID \u2192 group resolution).</li> <li>enables UI selection of A and B.</li> </ul> <p>Deliverables: updated schema docs (Pydantic) and a short API section in the backend README (public endpoints only).</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-3e-frontend-ux-timeline-compare-changes-26-days","title":"Sub-phase 3E \u2014 Frontend UX: timeline, compare, changes (2\u20136 days)","text":"<p>Implement three user-facing surfaces:</p> <p>1) Changes page (<code>/changes</code>)    - \u201cChanges\u201d feed with filters (source + date range).    - Default view should be edition-aware (e.g., \u201cchanges in the latest edition\u201d or \u201cbetween edition A and B\u201d), because the project\u2019s default capture cadence is annual.    - A \u201clast N days\u201d view can exist, but must be labeled as recently archived (capture time), not \u201crecently updated by the source.\u201d    - Each entry shows:      - what changed (descriptive summary),      - capture timestamps (UTC labeling),      - links to compare and to each snapshot. 2) Compare view (<code>/compare</code> or equivalent)    - Clear A/B selection and provenance.    - Diff display with:      - obvious \u201carchived content\u201d banner,      - navigation by changed sections (if available),      - warnings for high-noise pages. 3) Snapshot timeline integration    - On snapshot pages, add \u201cOther captures of this page\u201d (timeline list).    - Allow selecting a second snapshot to compare.</p> <p>Guardrail copy must be present on compare and changes pages (descriptive-only; link to official sources for current guidance).</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-3f-digest-mvp-web-rss-13-days","title":"Sub-phase 3F \u2014 Digest MVP: web + RSS (1\u20133 days)","text":"<p>Start with low-ops digest channels:</p> <ul> <li>Digest index page (<code>/digest</code>)</li> <li>Explains what the digest is (a list of changed pages) and what it is not (guidance).</li> <li>Links to RSS feeds.</li> <li>RSS feeds</li> <li>\u201cGlobal changes\u201d RSS.</li> <li>Optional per-source RSS.</li> </ul> <p>Deliverable: a digest archive concept that doesn\u2019t require email infrastructure yet.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-3g-documentation-and-governance-alignment-121-day","title":"Sub-phase 3G \u2014 Documentation and governance alignment (\u00bd\u20131 day)","text":"<p>Update public-facing docs to match the new capability:</p> <ul> <li>Methods/governance text explaining:</li> <li>what \u201cchange tracking\u201d means,</li> <li>limitations (noise, missing captures, replay limitations),</li> <li>\u201cdescriptive only\u201d stance.</li> <li>Researcher guidance:</li> <li>how to cite a compare view (A and B snapshot IDs + timestamps).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-3h-tests-performance-gates-13-days","title":"Sub-phase 3H \u2014 Tests + performance gates (1\u20133 days)","text":"<p>Add tests that protect the core promise:</p> <ul> <li>Backend:</li> <li>candidate detection logic (hash match skips diff),</li> <li>changes feed pagination and filtering,</li> <li>compare endpoint returns stable provenance,</li> <li>disabled modes (feature flag off) behave predictably.</li> <li>Frontend:</li> <li><code>/changes</code> renders with mocked API data,</li> <li>compare view renders provenance and disclaimer,</li> <li>graceful behavior when API is unavailable (fallback messaging).</li> </ul> <p>Performance gates:</p> <ul> <li>No compare request should trigger heavy diff computation synchronously.</li> <li>Feed endpoints should be index-backed and fast for large datasets.</li> </ul> <p>Status (Phase 3 implementation)</p> <ul> <li>Implemented on 2025-12-22.</li> <li>Added a <code>snapshot_changes</code> table and precomputed diff artifacts.</li> <li>Introduced <code>ha-backend compute-changes</code> for backfill + incremental diffing.</li> <li>Added public APIs: <code>/api/changes</code>, <code>/api/changes/compare</code>, <code>/api/changes/rss</code>, <code>/api/snapshots/{id}/timeline</code>.</li> <li>Enabled/controlled via backend env: <code>HEALTHARCHIVE_CHANGE_TRACKING_ENABLED</code> (API surfaces) and <code>/etc/healtharchive/change-tracking-enabled</code> (systemd timer gate).</li> <li>Added frontend pages <code>/changes</code>, <code>/compare</code>, <code>/digest</code> plus snapshot timeline UX.</li> <li>Updated governance/methods/researcher copy and changelog to reflect change tracking.</li> <li>Added systemd timer templates for scheduled change tracking runs (see <code>docs/deployment/systemd/README.md</code>).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-4-distribution-external-validation-expanded","title":"Phase 4 \u2014 Distribution + External Validation (expanded)","text":""},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#why-this-matters_4","title":"Why this matters","text":"<p>External validation is the \u201ccredibility multiplier\u201d:</p> <ul> <li>It makes the project verifiable to outsiders.</li> <li>It reduces \u201cthis is just a personal project\u201d framing.</li> <li>It provides sustainable feedback loops (what\u2019s useful vs noise).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#deliverables_1","title":"Deliverables","text":"<p>1) Partner target list (10 targets is enough)    - Libraries/archives (digital scholarship)    - Journalism programs/labs    - Public health research groups    - Educator networks (critical appraisal / evidence communication)</p> <p>2) Partner pitch assets    - One-page brief (mission + disclaimers + what they get)    - Screenshot pack (search, snapshot, compare, digest)    - \u201cHow to cite a snapshot\u201d guidance</p> <p>3) Distribution mechanism    - Lowest friction: RSS feeds (digest, per-source changes)    - Next: embed widget for \u201crecent changes\u201d</p> <p>4) Verifier strategy    - One credible person willing to attest to:      - your role,      - the project\u2019s utility,      - and (ideally) how they used it.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#acceptance-criteria_3","title":"Acceptance criteria","text":"<ul> <li>At least one external partner links or embeds.</li> <li>At least one named verifier agrees (with permission) to validate your role and impact.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-4-implementation-plan-detailed-sub-phases","title":"Phase 4 Implementation Plan (Detailed; sub-phases)","text":"<p>Phase 4 is intentionally mostly non-code. The outputs that \u201ccount\u201d are public artifacts and third-party confirmations (links/embeds, emails/letters, citations/mentions) rather than features.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#design-principles-phase-4","title":"Design principles (Phase 4)","text":"<ul> <li>Distribution before complexity. Prefer RSS + \u201clink to <code>/changes</code>\u201d over building bespoke widgets.</li> <li>No medical interpretation. Partners should be distributing an archive/change log, not \u201crecommendations\u201d.</li> <li>Permission + accuracy. Never list a partner or verifier publicly without explicit permission and a reviewed description.</li> <li>Privacy by default. Do not collect or store partner contact details in the repo. Keep outreach tracking private.</li> <li>Evidence-first. Every claimed partnership should have a \u201cproof artifact\u201d you can show (screenshot, published link, email permission).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-4a-define-target-partners-selection-criteria-121-day","title":"Sub-phase 4A \u2014 Define target partners + selection criteria (\u00bd\u20131 day)","text":"<p>Goal: A small, realistic list of targets you can actually contact and close.</p> <p>Decide:</p> <ul> <li>The primary distribution channel for Phase 4 (recommended): the <code>/digest</code> RSS feeds (global + per-source).</li> <li>The default \u201cask\u201d (keep it easy): \u201cPlease link to HealthArchive\u2019s digest/changes page as a resource for reproducibility and auditability.\u201d</li> <li>Partner \u201ctiers\u201d (avoid overpromising):</li> <li>Distribution partner: links to <code>/digest</code> or <code>/changes</code>, or republishes the RSS feed in a resource page.</li> <li>Review partner: provides occasional feedback on governance/scope wording (informal advisory).</li> <li>Research/teaching partner: uses it in a class/lab project and is willing to be named (optional; high value).</li> </ul> <p>Selection criteria for the initial list (use as a filter):</p> <ul> <li>Audience overlap with HealthArchive (research methods, journalism, digital scholarship, evidence communication).</li> <li>Ability to \u201csay yes\u201d quickly (a librarian maintaining a LibGuides page, a lab website admin, a newsletter editor).</li> <li>Comfort with the \u201carchive, not guidance\u201d framing.</li> <li>Willingness to be publicly named and/or verify use.</li> </ul> <p>Deliverables:</p> <ul> <li>A private \u201cTop 10 targets\u201d list with:</li> <li>org name, relevant program/page, role/title to contact,</li> <li>one sentence: why they are a fit,</li> <li>one sentence: what you\u2019re asking them to do,</li> <li>the lowest-friction next step (email vs warm intro vs office hours).</li> </ul> <p>Proof artifacts (later):</p> <ul> <li>Screenshot or archived copy of their page once they link/mention it.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-4b-build-partner-ready-assets-12-days","title":"Sub-phase 4B \u2014 Build partner-ready assets (1\u20132 days)","text":"<p>Goal: Remove friction for adoption and prevent misinterpretation.</p> <p>Create (minimum viable kit):</p> <p>1) One-page brief (PDF or web page)    - \u201cWhat it is\u201d in 2 sentences (archive + timestamped change tracking).    - \u201cWhat it is not\u201d (not medical advice; not current guidance; not affiliated).    - Who it is for (researchers/journalists/educators first).    - Links: <code>/methods</code>, <code>/governance</code>, <code>/status</code>, <code>/impact</code>, <code>/digest</code>, <code>/changes</code>.    - A small \u201cproject snapshot\u201d box: sources tracked, snapshots, latest capture date.</p> <p>2) Screenshot pack (5\u20138 images)    - Home + \u201cWhat this is/isn\u2019t\u201d    - Archive search page    - Snapshot page metadata + report link    - Changes feed (<code>/changes</code>)    - Compare view (<code>/compare?to=&lt;real id&gt;</code>) showing \u201cdescriptive only\u201d    - Digest page (<code>/digest</code>) with RSS links    - Status/Impact pages (optional but persuasive)</p> <p>3) \u201cHow to cite\u201d handout (1 page)    - Snapshot citation format (already present on <code>/researchers</code>; refine into a stable standalone artifact).    - Compare citation format (two snapshot IDs + timestamps + compare URL).    - A clear disclaimer that citations refer to archived content, not current guidance.</p> <p>4) Outreach email templates (plain text)    - One initial email template per partner tier (distribution vs research/teaching).    - A 2\u20133 sentence \u201celevator pitch\u201d that avoids adversarial language (\u201cauditability\u201d rather than \u201caccountability attack\u201d).</p> <p>Rules:</p> <ul> <li>Do not include private emails/phone numbers in repo artifacts.</li> <li>Do not include claims like \u201cwidely used\u201d until you can support them with numbers/partners.</li> </ul> <p>Deliverables location guidance:</p> <ul> <li>Public-facing, non-sensitive assets can live in the repo (docs folder) if you want versioning.</li> <li>Contact lists and outreach logs should stay private (not committed).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-4c-outreach-workflow-tracking-12-weeks-ongoing","title":"Sub-phase 4C \u2014 Outreach workflow + tracking (1\u20132 weeks, ongoing)","text":"<p>Goal: Run outreach like a small operational process, not ad-hoc messages.</p> <p>Suggested workflow:</p> <p>1) Identify 3 \u201cwarmest\u201d targets (fastest to yes). 2) Send the initial email with:    - one-page brief link/attachment,    - link to <code>/digest</code> and <code>/changes</code>,    - one specific ask,    - one low-effort follow-up option (\u201cIf helpful, I can send a 2-minute screencast or hop on a 15-minute call.\u201d). 3) Track each outreach attempt privately:    - date sent, response, follow-up dates, outcome. 4) Follow-up cadence:    - 7 days after initial email,    - 14 days after initial email (final polite close).</p> <p>Example \u201cdistribution ask\u201d (keep it concrete):</p> <ul> <li>\u201cWould you be willing to add HealthArchive.ca to your digital scholarship/public health methods resources page, linking to the digest (<code>/digest</code>) or the changes feed (<code>/changes</code>)? The site is explicitly non-authoritative and is intended for reproducibility and auditability.\u201d</li> </ul> <p>Proof artifacts (save for ABS/verifiers):</p> <ul> <li>Email thread granting permission to be listed (even a short \u201cYes, you can list us\u201d reply).</li> <li>Screenshot of the published link on their site (with date).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-4d-partner-onboarding-12-day-per-partner","title":"Sub-phase 4D \u2014 Partner onboarding (\u00bd day per partner)","text":"<p>Goal: Make \u201cyes\u201d immediately turn into public evidence, with minimal partner burden.</p> <p>Distribution options (in order):</p> <p>1) Link to <code>/digest</code> (RSS + explainer). 2) Link to <code>/changes</code> (edition-aware changes feed). 3) Optional: partner adds the RSS feed to their own page/newsletter tooling.</p> <p>What you provide back:</p> <ul> <li>A short \u201cHow to use this resource\u201d blurb they can paste (includes disclaimers).</li> <li>The exact RSS URL(s) for global/per-source feeds.</li> <li>A suggested citation sentence: \u201cThis is an archival record; verify current guidance on the official source.\u201d</li> </ul> <p>What you avoid:</p> <ul> <li>Anything that looks like endorsement or clinical guidance.</li> <li>Anything requiring ongoing support (custom widgets, per-partner deployments) until you have bandwidth.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-4e-verifier-strategy-verification-packet-121-day","title":"Sub-phase 4E \u2014 Verifier strategy + verification packet (\u00bd\u20131 day)","text":"<p>Goal: Make it easy for one credible person to verify your role and the project\u2019s use/impact.</p> <p>Identify one verifier candidate (examples):</p> <ul> <li>librarian/archivist in digital scholarship,</li> <li>public health researcher supervising a methods project,</li> <li>journalism faculty/editor using the resource in a workflow.</li> </ul> <p>Build a small \u201cverification packet\u201d (shareable link or PDF):</p> <ul> <li>Mission + safety posture (one paragraph).</li> <li>What you built (high-level architecture, no sensitive infrastructure details).</li> <li>What is live now (key pages + APIs).</li> <li>Metrics snapshot (from <code>/status</code> + <code>/impact</code>).</li> <li>What you do operationally (annual capture policy + change tracking cadence).</li> <li>A short role summary (what you personally did; keep it factual).</li> </ul> <p>Ask the verifier explicitly:</p> <ul> <li>\u201cAre you willing to verify that I built and operate this project and that it has been useful for X purpose?\u201d</li> <li>\u201cMay I list your name/title as a verifier (with your preferred wording)?\u201d</li> </ul> <p>Proof artifact:</p> <ul> <li>Written confirmation (email is sufficient) saved privately.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-4f-mentionscitations-capture-ongoing-15-minmonth","title":"Sub-phase 4F \u2014 Mentions/citations capture (ongoing; 15 min/month)","text":"<p>Goal: Build an evidence trail without invasive analytics.</p> <p>Keep a lightweight, public \u201cmentions log\u201d (can be a section in the changelog or impact report):</p> <ul> <li>date, outlet/org, link, one-sentence context.</li> </ul> <p>Ways to find mentions (non-invasive):</p> <ul> <li>manual search for \u201chealtharchive.ca\u201d occasionally,</li> <li>direct partner updates (\u201cwe added you to our resources page\u201d),</li> <li>optional: set up alerts outside the repo (do not commit credentials).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-4g-phase-4-definitions-of-done-make-success-measurable","title":"Sub-phase 4G \u2014 Phase 4 definitions of done (make success measurable)","text":"<p>Minimum viable \u201cPhase 4 complete\u201d:</p> <ul> <li>1 distribution partner publicly links to <code>/digest</code> or <code>/changes</code> (with permission to name them).</li> <li>1 verifier agrees (with permission) to verify your role and describe the project\u2019s utility.</li> <li>1 impact report includes the above as \u201cPartner highlights\u201d with links.</li> </ul> <p>Stretch goals (high value if feasible):</p> <ul> <li>1 course/lab/student project uses the data and is willing to be named.</li> <li>1 external mention in a newsletter/blog post/paper.</li> </ul> <p>Status (Phase 4 assets implemented)</p> <ul> <li>Public one-page brief page: <code>https://www.healtharchive.ca/brief</code> (downloadable Markdown at <code>/partner-kit/healtharchive-brief.md</code>)</li> <li>Public citation guidance page: <code>https://www.healtharchive.ca/cite</code> (downloadable Markdown at <code>/partner-kit/healtharchive-citation.md</code>)</li> <li>Partner kit draft: <code>healtharchive-backend/docs/operations/partner-kit.md</code></li> <li>One-page brief (source copy): <code>healtharchive-backend/docs/operations/one-page-brief.md</code></li> <li>Citation handout (source copy): <code>healtharchive-backend/docs/operations/citation-handout.md</code></li> <li>Outreach templates: <code>healtharchive-backend/docs/operations/outreach-templates.md</code></li> <li>Verification packet outline: <code>healtharchive-backend/docs/operations/verification-packet.md</code></li> <li>Mentions log template: <code>healtharchive-backend/docs/operations/mentions-log-template.md</code></li> </ul> <p>Remaining Phase 4 work is outreach and external validation.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-5-research-grade-outputs-expanded","title":"Phase 5 \u2014 Research-Grade Outputs (expanded)","text":""},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#why-this-matters_5","title":"Why this matters","text":"<p>HealthArchive\u2019s strongest natural audience is research/journalism. Making it \u201cresearch-grade\u201d increases:</p> <ul> <li>citations/mentions,</li> <li>reuse in student projects,</li> <li>and the credibility of the archive as an artifact.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#what-already-exists_2","title":"What already exists","text":"<ul> <li>A public citation page exists (<code>/cite</code>) and is linked from <code>/researchers</code>.</li> <li>Change tracking exists (edition-aware <code>/changes</code>, <code>/compare</code>, <code>/digest</code> with RSS).</li> <li>Backend exposes stable, structured APIs for search, sources, snapshots, timeline, and changes.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#design-principles-phase-5","title":"Design principles (Phase 5)","text":"<ul> <li>Research-first, not \u201cconsumer guidance.\u201d Optimize for reproducibility, auditability, and stable references.</li> <li>No medical interpretation. Outputs are descriptive and methodological; avoid \u201cwhat this means medically.\u201d</li> <li>No personal data. Exports must not include IP addresses, emails, user agents, or raw report submissions.</li> <li>Stable identifiers + clear versioning. Prefer durable URLs and explicit \u201cedition\u201d framing over \u201clatest\u201d unless carefully labeled.</li> <li>Sustainable access. Default to lightweight endpoints and cached/exported files; heavy requests go through a human workflow first.</li> <li>Public methods + limitations. Avoid overclaiming completeness or authoritative status.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-5-implementation-plan-detailed-sub-phases","title":"Phase 5 Implementation Plan (Detailed; sub-phases)","text":"<p>Phase 5 has two parallel goals:</p> <p>1) make HealthArchive easier to cite, reuse, and defend in research/journalism, and 2) create \u201cproof artifacts\u201d that demonstrate sustained, measurable public-interest value.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-5a-citation-guidance-stabilize-make-it-unambiguous","title":"Sub-phase 5A \u2014 Citation guidance (stabilize + make it unambiguous)","text":"<p>Goal: A researcher should be able to cite a snapshot or comparison correctly in under 60 seconds.</p> <p>Deliverables:</p> <ul> <li>Keep <code>/cite</code> as the canonical citation page, and ensure it covers:</li> <li>snapshot citations,</li> <li>compare-view citations (two snapshot IDs + both capture timestamps),</li> <li>a short \u201cfields glossary\u201d (title, original URL, capture timestamp, snapshot URL).</li> <li>Add a \u201cHow to cite\u201d link from relevant surfaces:</li> <li><code>/snapshot/[id]</code> (near metadata)</li> <li><code>/compare</code> (near the comparison header)</li> <li><code>/changes</code> and <code>/digest</code> (as \u201cfor researchers\u201d guidance)</li> <li>Add a lightweight citation \u201cready-to-copy\u201d format (descriptive only):</li> <li>snapshot citation template populated from snapshot metadata</li> <li>compare citation template populated from both snapshot IDs/timestamps</li> </ul> <p>Non-goals / guardrails:</p> <ul> <li>Do not suggest citation formats that imply endorsement or \u201cofficial guidance.\u201d</li> <li>Do not add any user tracking to measure citations; citations are measured by mentions/logs.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>Citation formats match the site\u2019s actual stable URLs and timestamps (UTC).</li> <li>A neutral user can follow the instructions without needing to infer missing fields.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-5b-research-access-pathway-public-how-to-request-expectations","title":"Sub-phase 5B \u2014 Research access pathway (public \u201chow to request\u201d + expectations)","text":"<p>Goal: Make bulk/research access possible without promising infinite bandwidth or completeness.</p> <p>Deliverables:</p> <ul> <li>A public \u201cResearch access\u201d section (could live on <code>/researchers</code> or a new <code>/research</code> page) that states:</li> <li>what data is available today (UI + public API endpoints + exports when available),</li> <li>what isn\u2019t available yet (and why),</li> <li>how to request bulk access or a targeted export (email or a structured request form),</li> <li>sustainability constraints (rate limits, caching, reasonable use).</li> <li>A standardized request checklist (for the requester to provide):</li> <li>source(s) and date range(s),</li> <li>whether they want per-snapshot vs per-page grouping,</li> <li>whether they want \u201cedition-to-edition\u201d changes or within-edition diffs,</li> <li>intended use (paper, class project, journalism piece).</li> </ul> <p>Examples (plain-language):</p> <ul> <li>\u201cI need all Health Canada snapshots between Apr 1\u2013May 1, 2025, plus change events between editions.\u201d</li> <li>\u201cI need diffs for these 25 URLs and the two most recent captures of each.\u201d</li> </ul> <p>Acceptance criteria:</p> <ul> <li>Someone unfamiliar with the codebase can understand how to request research access and what they\u2019ll receive.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-5c-machine-readable-exports-metadata-first-then-diffs","title":"Sub-phase 5C \u2014 Machine-readable exports (metadata first, then diffs)","text":"<p>Goal: Provide research-friendly data without turning the public API into an unbounded bulk download service.</p> <p>Recommended defaults (decisions)</p> <ul> <li>Canonical format: <code>JSON Lines</code> (<code>.jsonl</code>) compressed with <code>gzip</code> (<code>.jsonl.gz</code>)</li> <li>Why: streamable, append-friendly, handles optional fields cleanly, works well for large datasets, and is robust to schema evolution.</li> <li>Convenience format: <code>CSV</code> compressed with <code>gzip</code> (<code>.csv.gz</code>) (supported by the export API; dataset releases currently publish JSONL only)</li> <li>Why: easy for non-programmers (Excel/R), still compact when gzipped.</li> <li>Release packaging: always include a small <code>manifest.json</code> alongside exports</li> <li>Why: makes releases citable and reproducible (schema version, date ranges, source list, row counts, checksums).</li> <li>No raw content by default: exclude full <code>diff_html</code> and raw snapshot HTML from exports</li> <li>Why: keeps exports lightweight and reduces misuse risk; raw content stays accessible via the UI and specific snapshot URLs.</li> </ul> <p>Export tiers (recommended):</p> <ul> <li>Tier 0 (public, lightweight, always on):</li> <li>Snapshot metadata export (no raw HTML content).</li> <li>Change event export (no full diff bodies unless already computed and safe to expose).</li> <li>Clear schema/field definitions (\u201cdata dictionary\u201d).</li> <li>Tier 1 (public, cached files):</li> <li>Pre-generated exports per edition or per month, hosted as static files.</li> <li>Tier 2 (by request):</li> <li>Larger custom exports prepared offline for a study/class/journalism project.</li> </ul> <p>Deliverables:</p> <ul> <li>Define a stable export schema (same field names across <code>.jsonl</code> and <code>.csv</code>):</li> <li>Snapshots export (minimum viable)<ul> <li><code>snapshot_id</code></li> <li><code>source_code</code>, <code>source_name</code></li> <li><code>captured_url</code> (as archived)</li> <li><code>normalized_url_group</code> (canonical grouping key)</li> <li><code>capture_timestamp_utc</code> (ISO-8601, UTC)</li> <li><code>language</code>, <code>status_code</code>, <code>mime_type</code></li> <li><code>title</code> (optional but recommended)</li> <li><code>job_id</code>, <code>job_name</code> (edition anchor)</li> <li><code>snapshot_url</code> (absolute; e.g., <code>https://www.healtharchive.ca/snapshot/123</code>)</li> </ul> </li> <li>Changes export (minimum viable)<ul> <li><code>change_id</code></li> <li><code>source_code</code>, <code>source_name</code></li> <li><code>normalized_url_group</code></li> <li><code>from_snapshot_id</code>, <code>to_snapshot_id</code></li> <li><code>from_capture_timestamp_utc</code>, <code>to_capture_timestamp_utc</code></li> <li><code>from_job_id</code>, <code>to_job_id</code></li> <li><code>change_type</code>, <code>summary</code></li> <li><code>added_sections</code>, <code>removed_sections</code>, <code>changed_sections</code></li> <li><code>added_lines</code>, <code>removed_lines</code>, <code>change_ratio</code></li> <li><code>high_noise</code>, <code>diff_truncated</code></li> <li><code>diff_version</code>, <code>normalization_version</code>, <code>computed_at_utc</code></li> <li><code>compare_url</code> (absolute; e.g., <code>https://www.healtharchive.ca/compare?from=\u2026&amp;to=\u2026</code>)</li> </ul> </li> <li>Add a \u201cdata dictionary\u201d page in docs and link to it from the export endpoint and <code>/researchers</code>.</li> <li>Add clear limitations:</li> <li>coverage depends on scope rules and capture success,</li> <li>replay fidelity varies,</li> <li>change tracking reflects what was captured, not what changed on the source site in real time.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>Exports are stable, documented, and do not expose personal data or admin-only fields.</li> <li>Exports are edition-aware by default, to avoid misleading \u201crecent changes\u201d claims.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-5d-dataset-releases-versioned-citable-artifacts","title":"Sub-phase 5D \u2014 Dataset releases (versioned, citable artifacts)","text":"<p>Goal: Create periodic \u201cresearch objects\u201d that can be cited and referenced over time.</p> <p>Current decisions (implemented)</p> <ul> <li>Where to publish: GitHub Releases in <code>jerdaw/healtharchive-datasets</code></li> <li>Why: stable URLs for citation + straightforward distribution.</li> <li>Release cadence: quarterly (Jan/Apr/Jul/Oct) via GitHub Actions (plus a keepalive workflow to avoid schedule auto-disable)</li> <li>Why: matches annual-edition reality and keeps ops low-touch.</li> <li>Release contents (metadata-only): <code>healtharchive-snapshots.jsonl.gz</code>, <code>healtharchive-changes.jsonl.gz</code>, <code>manifest.json</code>, <code>SHA256SUMS</code> (no CSV)</li> <li>Why: lean artifacts with reproducible integrity metadata (checksums + manifest).</li> <li>Release tags: <code>healtharchive-dataset-YYYY-MM-DD</code></li> <li>Why: date-based tags allow finer cadence later without renaming history.</li> </ul> <p>Deliverables:</p> <ul> <li>A simple release cadence (quarterly) for:</li> <li>snapshot metadata dumps,</li> <li>change event dumps,</li> <li>stable release artifacts with checksums + manifest.</li> <li>Versioning approach:</li> <li><code>healtharchive-dataset-YYYY-MM-DD</code> with checksums.</li> <li>A release checklist:</li> <li>schema version,</li> <li>date range included,</li> <li>source list included,</li> <li>checksums generated,</li> <li>known limitations noted.</li> </ul> <p>Where to publish:</p> <ul> <li>Prefer GitHub Releases (with checksums) or a dedicated static dataset page, depending on file sizes and operational comfort.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>A researcher can cite \u201cDataset release YYYY-MM-DD\u201d and reproduce the exact file they used.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-5e-methods-note-poster-scholarship-output","title":"Sub-phase 5E \u2014 Methods note / poster (scholarship output)","text":"<p>Goal: Produce one scholarly artifact that is methodological and defensible.</p> <p>Recommended topic framing (non-interpretive):</p> <ul> <li>\u201cA provenance-first pipeline for archiving Canadian public health webpages.\u201d</li> <li>\u201cEdition-aware change tracking for public health web guidance: methods and limitations.\u201d</li> </ul> <p>Deliverables:</p> <ul> <li>A methods note outline with:</li> <li>motivation (reproducibility + auditability),</li> <li>capture methodology (WARCs + indexing),</li> <li>provenance labeling policy,</li> <li>change tracking approach (normalization + diff),</li> <li>limitations and non-goals (not guidance, not medical advice),</li> <li>ethics/privacy posture (no PHI; aggregated usage metrics only),</li> <li>a small descriptive results section (counts and examples, not interpretation).</li> <li>A \u201cfigure plan\u201d:</li> <li>architecture diagram (high-level),</li> <li>example change timeline for a single URL,</li> <li>coverage table (sources + capture windows).</li> </ul> <p>Acceptance criteria:</p> <ul> <li>The artifact can be shared publicly without creating medical guidance liability.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-5f-measurability-research-adoption-signals","title":"Sub-phase 5F \u2014 Measurability (research adoption signals)","text":"<p>Goal: Collect defensible evidence that research-grade outputs are used.</p> <p>Deliverables:</p> <ul> <li>A mentions/citations log process (already templated in Phase 4) updated monthly.</li> <li>A \u201cresearch use\u201d section in monthly impact reports:</li> <li>number of research inquiries,</li> <li>number of bulk exports delivered (if any),</li> <li>any public citations/mentions (with links).</li> </ul> <p>Acceptance criteria:</p> <ul> <li>You can point to concrete, verifiable research adoption signals without tracking individuals.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#definition-of-done-phase-5","title":"Definition of done (Phase 5)","text":"<p>Minimum viable \u201cPhase 5 complete\u201d:</p> <ul> <li><code>/cite</code> is canonical and linked from snapshot/compare/changes surfaces.</li> <li>A public research access pathway exists (how to request bulk access + constraints).</li> <li>A v1 export exists (snapshot metadata + change events) with a documented schema.</li> <li>One \u201cmethods note\u201d outline exists and is ready to submit as a poster/preprint/blog-style methods write-up.</li> </ul> <p>Stretch goals:</p> <ul> <li>Versioned dataset releases on a fixed cadence (quarterly) with checksums.</li> <li>At least one external research/journalism project uses an export and can be cited in the mentions log.</li> </ul> <p>Status (Phase 5 assets implemented)</p> <ul> <li>Public export manifest and endpoints: <code>/api/exports</code>, <code>/api/exports/snapshots</code>, <code>/api/exports/changes</code>.</li> <li>Export endpoints support <code>HEAD</code> requests for header inspection (e.g., <code>curl -I</code>).</li> <li>Public data dictionary page: <code>https://www.healtharchive.ca/exports</code> (+ downloadable Markdown).</li> <li><code>/researchers</code> updated with research access workflow and export manifest link.</li> <li><code>/cite</code> linked from <code>/snapshot</code>, <code>/compare</code>, <code>/changes</code>, and <code>/digest</code>.</li> <li>Export schema documented in <code>healtharchive-frontend/public/exports/healtharchive-data-dictionary.md</code> (public <code>/exports</code> page).</li> <li>Methods note outline published: <code>healtharchive-backend/docs/operations/methods-note-outline.md</code>.</li> <li>Export env toggles documented (<code>HEALTHARCHIVE_EXPORTS_ENABLED</code>, defaults, and limits).</li> </ul> <p>Remaining Phase 5 work: keep quarterly dataset releases running + track external research adoption signals.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-6-reliability-sustainability-expanded","title":"Phase 6 \u2014 Reliability + Sustainability (expanded)","text":""},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#why-this-matters_6","title":"Why this matters","text":"<p>You already have strong ops docs and an annual campaign definition. The upgrade here is making:</p> <ul> <li>cadence,</li> <li>constraints,</li> <li>and operational discipline</li> </ul> <p>explicit and sustainable.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#what-already-exists_3","title":"What already exists","text":"<ul> <li>Annual campaign scope and seeds (<code>healtharchive-backend/docs/operations/annual-campaign.md</code>)</li> <li>Single VPS production runbook (<code>healtharchive-backend/docs/deployment/production-single-vps.md</code>)</li> <li>Monitoring and CI checklist (<code>healtharchive-backend/docs/operations/monitoring-and-ci-checklist.md</code>)</li> <li>Optional systemd timers for annual scheduling and verification (<code>healtharchive-backend/docs/deployment/systemd/README.md</code>)</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#deliverables_2","title":"Deliverables","text":"<p>1) Public capture cadence policy    - Annual edition: Jan 01 UTC    - Exceptions: what qualifies for ad-hoc captures    - Why scope is limited (reliability &gt; breadth)</p> <p>2) Ops cadence (internal)    - weekly: health review    - monthly: reliability review + impact report    - quarterly: restore test    - routine dependency patching</p> <p>3) Growth constraints    - Storage budget    - Source cap per year    - Performance budgets    - Explicit posture on replay retention (WARCs must remain available for replay)</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#acceptance-criteria_4","title":"Acceptance criteria","text":"<ul> <li>There is a clearly documented \u201chow we operate\u201d routine that does not require heroics.</li> <li>The public-facing cadence statement matches what you actually do.</li> </ul> <p>Status (Phase 6 assets implemented)</p> <ul> <li>Public cadence language updated in: <code>/methods</code>, <code>/governance</code>, <code>/changes</code>, <code>/digest</code>, <code>/status</code>.</li> <li>Internal ops cadence checklist published: <code>docs/operations/ops-cadence-checklist.md</code>.</li> <li>Growth constraints documented: <code>docs/operations/growth-constraints.md</code>.</li> <li>Restore test procedure + log template published:</li> <li><code>docs/operations/restore-test-procedure.md</code></li> <li><code>docs/operations/restore-test-log-template.md</code></li> <li>Systemd deployment guide updated with Phase 6 enablement guidance.</li> <li>Deploy helper now retries health checks during restarts to avoid transient false negatives: <code>scripts/vps-deploy.sh</code>.</li> </ul> <p>Remaining Phase 6 work: keep automation healthy (verify timers continue running) and keep quarterly restore tests running with logged results.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#phase-6-implementation-plan-detailed-sub-phases","title":"Phase 6 Implementation Plan (Detailed; sub-phases)","text":"<p>Phase 6 is mostly operationalizing what already exists: turning \u201cwe do X\u201d into repeatable routines with lightweight artifacts so you can prove reliability over time without burning out.</p> <p>Key principle: Phase 6 is successful when it reduces cognitive load and removes \u201cheroic memory\u201d from operations.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-6a-baseline-audit-inventory-decisions","title":"Sub-phase 6A \u2014 Baseline audit (inventory + decisions)","text":"<p>Goal: ensure the cadence/scope language you publish is true, and ensure the internal routines match the system as deployed.</p> <p>Tasks:</p> <ul> <li>Inventory current reality (don\u2019t guess):</li> <li>What cadence you actually run today (annual editions + ad-hoc).</li> <li>Which timers/automation are installed and enabled on the VPS (if any).</li> <li>What backups exist and how restore is validated today.</li> <li>What monitoring exists today (external checks, Healthchecks-style pings, logs).</li> <li>Decide what you are comfortable committing to publicly (policy that matches your capacity):</li> <li>Annual edition is the default (\u201cJan 01 UTC\u201d).</li> <li>What counts as an \u201cad-hoc capture\u201d exception (e.g., major event, urgent operational fix).</li> <li>Whether annual scheduling should be fully automated or remain operator-triggered.</li> <li>Decide whether \u201creplay automation\u201d is enabled and what the retention stance is (WARCs must remain available if replay is enabled).</li> </ul> <p>Deliverables:</p> <ul> <li>A short \u201cPhase 6 baseline notes\u201d section appended to the changelog (or a dated internal note) capturing the decisions above.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>You can state \u201cwhat happens when\u201d in one paragraph without contradictions.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-6b-public-capture-cadence-policy-public-facing","title":"Sub-phase 6B \u2014 Public capture cadence policy (public-facing)","text":"<p>Goal: publish a clear, non-misleading cadence statement that matches annual-edition reality and avoids \u201creal-time update\u201d impressions.</p> <p>Tasks:</p> <ul> <li>Update public copy to be explicit and consistent:</li> <li>\u201cAnnual edition captured Jan 01 UTC.\u201d</li> <li>\u201cExceptions may trigger ad-hoc captures; these are explicitly labeled.\u201d</li> <li>\u201cChange tracking is edition-aware; it does not imply real-time monitoring.\u201d</li> <li>Ensure the policy appears where users form expectations:</li> <li><code>/methods</code> (primary place for operational details),</li> <li><code>/governance</code> (policy + scope stance),</li> <li><code>/changes</code> + <code>/digest</code> (short, high-visibility summary),</li> <li>optionally <code>/status</code> (freshness definitions).</li> </ul> <p>Deliverables:</p> <ul> <li>Public cadence policy text (short, stable, linked from the footer or methods/governance).</li> </ul> <p>Acceptance criteria:</p> <ul> <li>A user cannot reasonably interpret \u201cChanges\u201d as \u201cthis is what happened this week in real time\u201d when editions are annual.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-6c-ops-cadence-internal-runbook-routines","title":"Sub-phase 6C \u2014 Ops cadence (internal runbook routines)","text":"<p>Goal: define a boring, repeatable operations cadence that can be followed by you or a backup operator.</p> <p>Recommended cadence (adjust to your capacity):</p> <ul> <li>Weekly: 10\u201315 minute health review</li> <li>API health, worker status, disk usage trend, failed jobs queue.</li> <li>Monthly: reliability review (can be merged into the monthly impact report)</li> <li>top incidents, planned maintenance, search quality checks.</li> <li>Quarterly: restore test (prove backups are usable)</li> <li>restore DB + verify key endpoints or counts.</li> <li>Ongoing: dependency patching routine</li> <li>safe update cadence; avoid surprise upgrades during capture campaigns.</li> </ul> <p>Tasks:</p> <ul> <li>Write a single \u201cOps cadence checklist\u201d doc that includes:</li> <li>what to check,</li> <li>where to look (systemd, logs, DB counts),</li> <li>how to record outcomes (a lightweight log entry).</li> <li>Decide where the \u201cops log\u201d lives:</li> <li>simplest: a dated Markdown file in a private operator notes location (not necessarily in git),</li> <li>or, if public-safe, a minimal \u201cincident log\u201d section on <code>/impact</code> or <code>/changelog</code>.</li> </ul> <p>Deliverables:</p> <ul> <li>Internal ops cadence doc (checklist-style, copy/paste commands, no secrets).</li> </ul> <p>Acceptance criteria:</p> <ul> <li>A future-you can follow the checklist after 3 months away and still operate safely.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-6d-automation-gates-systemd-timers-safe-enablement","title":"Sub-phase 6D \u2014 Automation gates (systemd timers + safe enablement)","text":"<p>Goal: enable automation only where it reduces toil without increasing risk.</p> <p>Tasks:</p> <ul> <li>Validate installed timers and dry-run services (safe-by-default):</li> <li>annual scheduling dry-run,</li> <li>change tracking dry-run,</li> <li>replay reconcile dry-run (if replay enabled),</li> <li>annual search verification (optional).</li> <li>Decide which automations to enable now vs later:</li> <li>Change tracking: typically safe to keep enabled (already capped and edition-aware).</li> <li>Annual scheduling: enable only after confirming job configs, disk headroom, and monitoring.</li> <li>Replay reconcile: enable only if replay is enabled and stable.</li> <li>Use sentinel files to gate automation:</li> <li><code>/etc/healtharchive/automation-enabled</code> (annual scheduling),</li> <li><code>/etc/healtharchive/change-tracking-enabled</code>,</li> <li><code>/etc/healtharchive/replay-automation-enabled</code> (optional).</li> <li>Add \u201ctimer ran\u201d visibility (optional but high leverage):</li> <li>Healthchecks-style pings using a root-owned env file on the VPS (no URLs in git).</li> </ul> <p>Deliverables:</p> <ul> <li>Updated deployment docs indicating which timers are expected to be enabled in production and why.</li> <li>A clear operator \u201cenable automation\u201d checklist (dry-run \u2192 enable timer \u2192 confirm next run).</li> </ul> <p>Acceptance criteria:</p> <ul> <li>Automation does not surprise-run expensive tasks without an explicit enablement step.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-6e-growth-constraints-budgets-boundaries","title":"Sub-phase 6E \u2014 Growth constraints (budgets + boundaries)","text":"<p>Goal: prevent slow \u201cscope creep\u201d from breaking reliability (single-VPS reality).</p> <p>Tasks:</p> <ul> <li>Define and publish (internally) budgets with conservative defaults:</li> <li>storage budget (WARCs + DB + backups),</li> <li>source cap per year (how many new sources you can safely add),</li> <li>performance budget (acceptable API latency and indexing overhead),</li> <li>replay retention stance (WARCs must remain if replay is on).</li> <li>Make the public-facing version \u201cprinciple based\u201d:</li> <li>\u201creliability over breadth,\u201d</li> <li>\u201cscope is constrained by storage and operational capacity,\u201d</li> <li>\u201csources are added deliberately with explicit rules.\u201d</li> </ul> <p>Deliverables:</p> <ul> <li>Internal \u201cbudgets &amp; constraints\u201d doc (numbers and targets).</li> <li>Public-facing summary paragraph in <code>/governance</code> (principles, not sensitive numbers).</li> </ul> <p>Acceptance criteria:</p> <ul> <li>You can answer \u201cwhy not add 50 sources?\u201d with a documented policy, not vibes.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#sub-phase-6f-disaster-recovery-proof-restore-test-discipline","title":"Sub-phase 6F \u2014 Disaster recovery proof (restore test discipline)","text":"<p>Goal: make \u201cwe have backups\u201d verifiable by doing quarterly restore tests and recording results.</p> <p>Tasks:</p> <ul> <li>Define the quarterly restore test procedure:</li> <li>restore DB dump to a temporary/staging DB,</li> <li>run a minimal verification checklist (counts + key endpoints),</li> <li>verify that the system starts cleanly with restored state.</li> <li>Create a public-safe template for recording results (no secrets):</li> <li>date, operator, what was restored, what checks ran, pass/fail, follow-ups.</li> </ul> <p>Deliverables:</p> <ul> <li>Restore test template + documented procedure.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>You can show \u201crestore tests performed\u201d as a reliability artifact (even if the underlying logs live privately).</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#appendix-a-suggested-new-public-routes-conceptual","title":"Appendix A \u2014 Suggested new public routes (conceptual)","text":"<p>These are the most likely frontend routes to be added as part of the roadmap:</p> <ul> <li><code>/governance</code></li> <li><code>/terms</code></li> <li><code>/privacy</code></li> <li><code>/changelog</code></li> <li><code>/report</code> (issue intake)</li> <li><code>/status</code> (status/metrics)</li> <li><code>/changes</code> (changes feed)</li> <li><code>/digest</code> (digest index + archive)</li> <li><code>/page/&lt;id-or-encoded-group&gt;</code> (page timeline; exact URL design is a later decision)</li> <li><code>/compare?...</code> (compare view; exact URL design is a later decision)</li> </ul> <p>Note: the exact routing and URL formats should be designed for stability and citation friendliness.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#appendix-b-monthly-impact-report-template-copy","title":"Appendix B \u2014 Monthly impact report template (copy)","text":"<p>Suggested structure (one page):</p> <ul> <li>Summary: what HealthArchive is, what\u2019s new this month</li> <li>Coverage: sources tracked, snapshots/pages totals, major additions</li> <li>Reliability: uptime, incidents, crawl/index success notes</li> <li>Change tracking: biggest changes (once Phase 3 exists)</li> <li>Distribution: partner highlights, mentions/citations</li> <li>Roadmap: what\u2019s next (one paragraph)</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#appendix-c-statusmetrics-page-data-sources-mapping","title":"Appendix C \u2014 Status/Metrics Page: Data Sources (mapping)","text":"<p>This appendix makes the Phase 2 \u201cStatus/Metrics page\u201d concrete by mapping each suggested display element to existing data sources, so an implementer doesn\u2019t invent new backend endpoints prematurely.</p> <p>Data sources that are safe for the public frontend to call:</p> <ul> <li><code>GET /api/health</code></li> <li>Use for \u201cAPI is up / degraded\u201d and basic counts health.</li> <li>Caveat: it\u2019s a point-in-time check, not uptime history.</li> <li><code>GET /api/stats</code></li> <li>Use for top-level counts: snapshots, pages, sources, latest capture date.</li> <li><code>GET /api/sources</code></li> <li>Use for per-source coverage windows and record counts.</li> <li>Also provides entry points and preview URLs when replay/previews are enabled.</li> </ul> <p>Data sources that must not be used from the public frontend:</p> <ul> <li><code>/api/admin/**</code> and <code>/metrics</code> (admin token-protected; reserved for operators).</li> </ul> <p>Recommended \u201cstatus page\u201d sections and where the data comes from:</p> <ul> <li>Current status: <code>/api/health</code> status + a plain-language summary of what that means.</li> <li>Coverage totals: <code>/api/stats</code> + (optionally) derived counts from <code>/api/sources</code>.</li> <li>Per-source coverage table: <code>/api/sources</code> fields:</li> <li>record count</li> <li>first capture date</li> <li>last capture date</li> <li>entry browse URL (if replay enabled)</li> <li>entry preview URL (if preview cache enabled)</li> </ul> <p>Uptime history note:</p> <p>If you want \u201c99.9% uptime last 30 days,\u201d it should come from an external monitor (UptimeRobot/Healthchecks/etc.) or a deliberately designed internal metric. Don\u2019t invent uptime from <code>/api/health</code> alone.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#appendix-d-changelog-template-example-entries","title":"Appendix D \u2014 Changelog: Template + Example Entries","text":"<p>Keep the changelog boring and structured. Suggested fields per entry:</p> <ul> <li>Date (YYYY-MM-DD)</li> <li>Category tags (examples: <code>scope</code>, <code>governance</code>, <code>ui</code>, <code>search</code>, <code>replay</code>, <code>ops</code>, <code>reliability</code>, <code>data</code>)</li> <li>\u201cWhat changed\u201d (2\u20136 bullets)</li> <li>\u201cWhy it changed\u201d (1\u20132 bullets)</li> <li>\u201cNotes / limitations\u201d (optional)</li> </ul> <p>Example entry (scope + UI):</p> <ul> <li>2026-02-01 \u2014 <code>scope</code>, <code>ui</code></li> <li>What changed: Added CIHR as a tracked source; improved \u201carchived, not current guidance\u201d banner on browse pages.</li> <li>Why: Expand annual campaign scope within single-VPS limits; reduce misinterpretation risk in browse mode.</li> <li>Notes: CIHR capture coverage is early-stage; some interactive content may not replay fully.</li> </ul> <p>Example entry (reliability + ops):</p> <ul> <li>2026-03-01 \u2014 <code>ops</code>, <code>reliability</code></li> <li>What changed: Added monthly search-verification artifact generation; clarified restore test cadence in ops docs.</li> <li>Why: Improve verifiability and reduce \u201csilent failure\u201d risk.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#appendix-e-report-an-issue-categories-fields","title":"Appendix E \u2014 \u201cReport an Issue\u201d: Categories + Fields","text":"<p>This is intentionally designed to be implementable as a simple page + email/GitHub workflow first.</p> <p>Suggested report categories (keep it small):</p> <ul> <li>Broken snapshot viewer (raw HTML fails, replay fails, blank iframe)</li> <li>Wrong metadata (source, capture date, title, URL)</li> <li>Missing content (expected page not present / coverage gap)</li> <li>Corrections request (labeling, context, clarification)</li> <li>Takedown / opt-out request</li> <li>Suggest a source / scope request</li> </ul> <p>What info to ask for (so reports are actionable):</p> <ul> <li>Snapshot link (preferred) or snapshot ID</li> <li>Original URL</li> <li>What you expected to see vs what happened</li> <li>Screenshot (optional)</li> <li>Contact email (optional)</li> <li>\u201cThis is urgent because\u2026\u201d (optional)</li> </ul> <p>Response expectations (examples):</p> <ul> <li>Acknowledge within 7 days.</li> <li>Urgent safety labeling issues: acknowledge within 48 hours.</li> <li>If not fixable (capture didn\u2019t include required assets), respond with a clear explanation.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#appendix-f-governance-page-recommended-outline","title":"Appendix F \u2014 Governance Page: Recommended Outline","text":"<p>This is a suggested structure for <code>/governance</code> that is \u201cpublic-interest service\u201d readable but still accurate to the implementation.</p> <p>1) Mission    - 2\u20133 sentence mission statement.    - Explicit \u201cnot medical advice / not current guidance / independent project.\u201d</p> <p>2) Scope    - What sources are currently included (list source codes and names).    - What is explicitly out of scope (private/user-submitted content, PHI, unrelated domains).    - Why scope is constrained (reliability, storage, single-VPS reality).</p> <p>3) How captures work (high-level)    - Time-stamped crawls produce standards-based web archive formats (WARCs).    - Snapshots are indexed into a database to support search and citation.    - Replay (if enabled) provides higher fidelity browsing but depends on captured assets.</p> <p>4) Provenance commitments (what the project guarantees)    - Capture timestamp and original URL are displayed.    - Snapshots are tied back to WARC-backed storage.    - Content hashes exist to detect change (at minimum, internal integrity checks).</p> <p>5) Corrections    - What can be corrected (metadata, labeling, broken links, UX bugs).    - What cannot always be corrected (missing assets in the captured WARC).    - Response expectations and how to submit.</p> <p>6) Takedown / opt-out    - Who can request (rights holders, site owners, etc.).    - What you need to evaluate a request.    - Possible outcomes (remove access, limit distribution, add context).</p> <p>7) Transparency    - Aggregate reporting: number of takedown requests, number of correction requests, general outcomes.    - Link to changelog and monthly impact reports.</p> <p>8) Advisory    - Advisory charter + cadence + published members (with permission).</p> <p>9) Contact    - Link to <code>/report</code> and <code>/contact</code>.</p>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#appendix-g-change-tracking-implementation-decisions-to-resolve-before-building","title":"Appendix G \u2014 Change Tracking: Implementation Decisions to Resolve (before building)","text":"<p>The Phase 3 work can balloon if decisions aren\u2019t made early. This list is meant to force clarity before coding.</p> <p>Core questions</p> <p>1) What is the unit of change?    - Page group (<code>normalized_url_group</code>) is the natural unit, because it already exists and supports \u201ctimeline\u201d thinking.</p> <p>2) What snapshots are compared?    - \u201cLatest vs previous\u201d per page group is a simple default.    - Consider how to handle missing/failed captures and non-2xx results.</p> <p>3) What constitutes a \u201cmeaningful change\u201d?    - Minimal viable: content hash changed \u2192 record as \u201cchanged\u201d.    - Next: generate a noise-reduced text diff and highlight changed sections.</p> <p>4) Where is diff generation run?    - Must be off the user request path.    - Should be scheduled or triggered after indexing, with throttling and failure handling.</p> <p>5) How will you label confidence/noise?    - Some pages will produce noisy diffs; the UI should communicate that.</p> <p>Outputs you should be able to support</p> <ul> <li>Page timeline: list of capture points for a page group</li> <li>Compare view: two selected captures with a descriptive diff</li> <li>Changes feed: list of recent change events</li> <li>Digest: weekly view of top changes + \u201ccoverage notes\u201d</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#appendix-h-digest-web-rss-content-structure-example","title":"Appendix H \u2014 Digest (Web + RSS): Content Structure Example","text":"<p>Keep it simple, descriptive, and consistent week-to-week.</p> <p>Suggested weekly digest sections:</p> <ul> <li>Summary</li> <li>\u201cDuring this period, X pages changed across Y sources.\u201d</li> <li>\u201cZ new pages were discovered; W pages were removed/redirected.\u201d</li> <li>Top changes</li> <li>A list of the top N changes with:<ul> <li>source</li> <li>page title / URL</li> <li>capture dates compared</li> <li>\u201cchanged sections\u201d count or \u201cdiff size\u201d proxy</li> <li>link to compare view</li> </ul> </li> <li>New pages</li> <li>Pages that appeared for the first time in the archive (new page group)</li> <li>Removed/redirected pages</li> <li>Pages that disappeared or became unreachable (if detectable)</li> <li>Coverage notes</li> <li>\u201cPHAC crawl had partial capture of X path; expect gaps.\u201d</li> </ul> <p>RSS guidance:</p> <ul> <li>Keep RSS item titles descriptive and non-editorial (\u201cPHAC:  changed (2026-02-01 \u2192 2026-02-08)\u201d). <li>Include the \u201cnot medical advice / not current guidance\u201d disclaimer in the RSS item body or feed description.</li>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/","title":"Ops observability + private usage dashboards (Prometheus/Grafana) \u2014 implementation plan","text":"<p>This is a sequential implementation plan for adding an operator-only observability stack (Prometheus + Grafana) and a private stats surface for both ops health and expanded usage aggregates, with an explicit bias toward:</p> <ul> <li>Low maintenance / low toil</li> <li>Strong privacy posture (no per-user tracking)</li> <li>No new public attack surface</li> <li>Reuse of existing backend surfaces (<code>/metrics</code>, <code>/api/admin/**</code>, <code>usage_metrics</code>)</li> </ul> <p>This plan does not implement anything by itself.</p>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#0-executive-summary-decisions-locked-up-front","title":"0) Executive summary (decisions locked up-front)","text":""},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#01-what-we-are-building","title":"0.1 What we are building","text":"<p>1) Private \u201cstats page\u201d = Grafana</p> <ul> <li>Grafana dashboards become the operator-only stats surface.</li> <li>Prometheus scrapes backend + host + DB metrics.</li> <li>Grafana reads from Prometheus (time-series) and Postgres (tables / long-window aggregates).</li> </ul> <p>2) Expanded usage (private)</p> <ul> <li>Continue the project\u2019s existing approach: daily aggregate counters only, stored in Postgres (<code>usage_metrics</code>).</li> <li>Expand the event set over time (still aggregate-only), but do not expose every event publicly.</li> <li>Grafana reads the full internal dataset directly from Postgres.</li> </ul> <p>3) Admin/ops UI</p> <ul> <li>Default: treat Grafana as the \u201cops UI\u201d (tables + links), and continue to use existing admin endpoints for JSON drilldown.</li> <li>Only if justified by real operator pain: consider a bespoke admin console later, starting read-only.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#02-private-access-model-chosen","title":"0.2 Private access model (chosen)","text":"<p>Use Tailscale for private access; publish Grafana via <code>tailscale serve</code>.</p> <p>Rationale:</p> <ul> <li>Keeps the public internet surface unchanged (no new DNS, no new public ports).</li> <li>Fits the project\u2019s current production model (Tailscale-only SSH).</li> <li>Gives a clean, low-friction operator UX compared to SSH port-forwarding.</li> <li>Makes \u201crarely used\u201d admin access less annoying.</li> </ul> <p>Implementation principle:</p> <ul> <li>Grafana listens on loopback; Tailscale proxies it to a tailnet-only HTTPS URL.</li> <li>Prometheus does not need to be operator-facing; keep its UI loopback-only.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#03-non-goals-guardrails","title":"0.3 Non-goals / guardrails","text":"<ul> <li>No third-party browser analytics scripts.</li> <li>No collecting IPs, user IDs, query strings, or referrers into \u201canalytics tables\u201d.</li> <li>No public web routes that expose admin/ops data.</li> <li>No weakening admin token behavior; <code>/metrics</code> and <code>/api/admin/**</code> remain token-gated in prod.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#1-scope-definition-what-private-stats-means","title":"1) Scope definition (what \u201cprivate stats\u201d means)","text":""},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#11-ops-health-questions-we-must-answer-quickly","title":"1.1 Ops health questions we must answer quickly","text":"<ul> <li>Is the API up and serving real responses?</li> <li>Is the DB healthy (connections, storage, latency signals)?</li> <li>Are crawls/indexing progressing or stuck?</li> <li>Are failures spiking (job failures, crawl page failures, search errors)?</li> <li>Are we at risk of outage due to disk pressure (especially <code>/srv/healtharchive</code>) or memory pressure?</li> <li>Are automated timers/services running on schedule?</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#12-usage-questions-we-want-aggregate-only","title":"1.2 Usage questions we want (aggregate-only)","text":"<p>Expanded usage should answer questions like:</p> <ul> <li>How many searches per day/week/month?</li> <li>How many snapshot detail views? raw snapshot views?</li> <li>How many compare / changes / exports requests (if we add those events)?</li> <li>How many issue reports submitted?</li> <li>What\u2019s the seasonal trend and growth rate?</li> </ul> <p>Explicitly not in scope:</p> <ul> <li>\u201cTop search terms\u201d (would require storing query text; do not do this).</li> <li>Per-source/per-page breakdown that requires high-cardinality dimensions.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#2-constraints-and-current-state-inventory","title":"2) Constraints and current state (inventory)","text":""},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#21-existing-backend-surfaces-we-will-reuse","title":"2.1 Existing backend surfaces we will reuse","text":"<ul> <li>Admin endpoints (token-gated): <code>/api/admin/**</code></li> <li>Jobs list/detail/status counts, job snapshots, issue reports list/detail, search debug.</li> <li>Metrics endpoint (token-gated): <code>GET /metrics</code></li> <li>Job status counts, storage totals, snapshot/page totals, crawl page totals.</li> <li>Per-process search metrics: request count, errors, latency histogram buckets.</li> <li>Usage metrics (aggregate-only; stored in DB):</li> <li>Table: <code>usage_metrics (metric_date, event, count)</code>.</li> <li>Public API: <code>GET /api/usage</code> currently returns daily + totals for a rolling window.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#22-data-sensitivity-notes","title":"2.2 Data sensitivity notes","text":"<ul> <li><code>issue_reports</code> can include free text and optional email.</li> <li>Treat as sensitive.</li> <li>Grafana should not display raw free text or emails by default.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#23-production-model-constraints","title":"2.3 Production model constraints","text":"<ul> <li>Single VPS; public ports are 80/443 only.</li> <li>SSH is private-only via Tailscale.</li> <li>Ops artifacts live under <code>/srv/healtharchive/ops/</code> with group-writable permissions.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#3-implementation-phases-sequential","title":"3) Implementation phases (sequential)","text":"<p>Each phase includes:</p> <ul> <li>Deliverables (what must exist after)</li> <li>Steps (procedural checklist)</li> <li>Acceptance criteria (how we know it worked)</li> <li>Rollback (how to back out safely)</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#phase-1-formalize-the-private-observability-contract-documentation-first","title":"Phase 1 \u2014 Formalize the \u201cprivate observability\u201d contract (documentation-first)","text":"<p>Goal: lock the boundaries so we don\u2019t accidentally add public surface or privacy risk.</p> <p>Status: implemented in this repo (see <code>docs/operations/observability-and-private-stats.md</code>). Linked from <code>docs/operations/README.md</code>.</p> <p>Deliverables:</p> <ul> <li>A short internal doc under <code>docs/operations/</code> defining:</li> <li>what is collected (aggregate-only),</li> <li>what is never collected,</li> <li>what is public vs private.</li> </ul> <p>Steps:</p> <ol> <li>Draft <code>docs/operations/observability-and-private-stats.md</code>:</li> <li>\u201cPrivate stats = tailnet-only Grafana.\u201d</li> <li>\u201cPublic stats = <code>/status</code> + <code>/impact</code> pages.\u201d</li> <li>\u201cUsage metrics are aggregate-only daily counters; no identifiers.\u201d</li> <li>Add explicit \u201cdo not expose admin/metrics to public UI\u201d reminder.</li> <li>Link the doc from <code>docs/operations/README.md</code>.</li> </ol> <p>Acceptance criteria:</p> <ul> <li>A new operator can read one doc and understand what is and is not collected/exposed.</li> </ul> <p>Rollback:</p> <ul> <li>Documentation-only; revert the file if needed.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#phase-2-provision-the-ops-stack-host-footprint-directories-users-secrets","title":"Phase 2 \u2014 Provision the ops stack host footprint (directories, users, secrets)","text":"<p>Goal: create a stable, least-privilege filesystem and secrets layout that matches existing project conventions.</p> <p>Status: implemented in this repo (bootstrap script + docs).</p> <p>Deliverables:</p> <ul> <li>Standard directories created under <code>/srv/healtharchive/ops/observability/</code> for:</li> <li>dashboard exports / provisioning (no secrets)</li> <li>public-safe operator notes</li> <li>Root-owned secret file locations under <code>/etc/healtharchive/observability/</code>.</li> </ul> <p>Design choice (low-maintenance default):</p> <ul> <li>Keep Prometheus/Grafana data in distro defaults (<code>/var/lib/prometheus</code>, <code>/var/lib/grafana</code>)   unless you have a strong reason to relocate.</li> </ul> <p>Steps:</p> <ol> <li>Add the internal contract doc under <code>docs/operations/</code> (Phase 1).</li> <li>Add the bootstrap script:</li> <li><code>scripts/vps-bootstrap-observability-scaffold.sh</code></li> <li>Add the playbook:</li> <li><code>docs/operations/playbooks/observability-bootstrap.md</code></li> <li>Standardize secret file locations (created root-only by the script):</li> <li><code>/etc/healtharchive/observability/prometheus_backend_admin_token</code></li> <li><code>/etc/healtharchive/observability/grafana_admin_password</code></li> <li><code>/etc/healtharchive/observability/postgres_grafana_password</code></li> </ol> <p>Acceptance criteria:</p> <ul> <li>Secrets are not present in <code>/srv/healtharchive/ops/</code>.</li> <li>Secrets are not logged by scripts.</li> <li>The bootstrap script is idempotent and safe to re-run.</li> </ul> <p>Rollback:</p> <ul> <li>Remove the new directories and secrets files.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#phase-3-install-and-configure-exporters-host-postgres","title":"Phase 3 \u2014 Install and configure exporters (host + Postgres)","text":"<p>Goal: provide a minimal set of signals that catch real outages: disk, CPU, memory, DB health.</p> <p>Status: implemented in this repo (installer script + playbook); requires running on the VPS.</p> <p>Deliverables:</p> <ul> <li>Node exporter running as a service (private bind).</li> <li>Postgres exporter running as a service (private bind), using a least-privilege role.</li> <li>Role: <code>postgres_exporter</code> with <code>pg_monitor</code>.</li> <li>Credentials: <code>/etc/healtharchive/observability/postgres_exporter.env</code> (root-owned).</li> </ul> <p>Steps:</p> <ol> <li>Add the VPS installer script:</li> <li><code>scripts/vps-install-observability-exporters.sh</code></li> <li>Add the playbook:</li> <li><code>docs/operations/playbooks/observability-exporters.md</code></li> <li>Run on the VPS:</li> <li>dry-run: <code>./scripts/vps-install-observability-exporters.sh</code></li> <li>apply: <code>sudo ./scripts/vps-install-observability-exporters.sh --apply</code></li> <li>Validate endpoints locally:</li> <li><code>curl http://127.0.0.1:9100/metrics | head</code></li> <li><code>curl http://127.0.0.1:9187/metrics | head</code></li> </ol> <p>Acceptance criteria:</p> <ul> <li>Exporters are reachable locally and are not reachable from the public internet.</li> </ul> <p>Rollback:</p> <ul> <li>Stop/disable services; uninstall packages.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#phase-4-prometheus-scrape-config-retention-service-hardening","title":"Phase 4 \u2014 Prometheus (scrape config, retention, service hardening)","text":"<p>Goal: collect time-series metrics in a controlled way that cannot fill disk or overload the backend.</p> <p>Status: implemented in this repo (installer script + playbook); requires running on the VPS.</p> <p>Deliverables:</p> <ul> <li>Prometheus running under systemd.</li> <li>A <code>prometheus.yml</code> that scrapes:</li> <li>backend <code>/metrics</code> via loopback</li> <li>node exporter</li> <li>postgres exporter</li> <li>Explicit retention and disk guards.</li> </ul> <p>Steps:</p> <ol> <li>Add the VPS installer script:</li> <li><code>scripts/vps-install-observability-prometheus.sh</code></li> <li>Add the playbook:</li> <li><code>docs/operations/playbooks/observability-prometheus.md</code></li> <li>Run on the VPS:</li> <li>dry-run: <code>./scripts/vps-install-observability-prometheus.sh</code></li> <li>apply: <code>sudo ./scripts/vps-install-observability-prometheus.sh --apply</code></li> <li>Validate locally:</li> <li>readiness: <code>curl -s http://127.0.0.1:9090/-/ready</code></li> <li>targets: <code>curl -s http://127.0.0.1:9090/api/v1/targets | head</code></li> <li>loopback-only bind: <code>ss -lntp | grep -E ':9090\\b'</code></li> </ol> <p>Acceptance criteria:</p> <ul> <li>Prometheus is stable for 24h with no meaningful CPU spikes and no rapid TSDB growth.</li> <li>Backend remains responsive under scrape load.</li> </ul> <p>Rollback:</p> <ul> <li>Stop Prometheus; remove its data dir.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#phase-5-grafana-private-stats-page-with-tailnet-only-access","title":"Phase 5 \u2014 Grafana (private \u201cstats page\u201d) with tailnet-only access","text":"<p>Goal: create a single operator-only entrypoint for ops health and private usage.</p> <p>Status: implemented in this repo (installer scripts + playbook); requires running on the VPS.</p> <p>Deliverables:</p> <ul> <li>Grafana running.</li> <li>Grafana accessible only via tailnet (preferred: SSH port-forward over Tailscale; optional: Tailscale Serve).</li> <li>Data sources configured:</li> <li>Prometheus</li> <li>Postgres (read-only role)</li> </ul> <p>Steps:</p> <ol> <li>Add the VPS installer scripts:</li> <li><code>scripts/vps-install-observability-grafana.sh</code></li> <li><code>scripts/vps-enable-tailscale-serve-grafana.sh</code></li> <li>Add the playbook:</li> <li><code>docs/operations/playbooks/observability-grafana.md</code></li> <li>Run on the VPS:</li> <li>Grafana install/hardening:<ul> <li>dry-run: <code>./scripts/vps-install-observability-grafana.sh</code></li> <li>apply: <code>sudo ./scripts/vps-install-observability-grafana.sh --apply</code></li> <li>note: if <code>grafana</code> is not in the default apt sources (common on Ubuntu), the script will add the Grafana Labs apt repo automatically.</li> </ul> </li> <li>Operator access (preferred; no Tailscale Serve required):<ul> <li>from an operator machine on the tailnet: <code>ssh -L 3000:127.0.0.1:3000 haadmin@&lt;vps-tailscale-ip-or-name&gt;</code></li> <li>then open: <code>http://127.0.0.1:3000</code></li> </ul> </li> <li>Tailnet-only HTTPS (optional; requires enabling Tailscale HTTPS certificates + Serve for the tailnet):<ul> <li>dry-run: <code>./scripts/vps-enable-tailscale-serve-grafana.sh</code></li> <li>apply: <code>sudo ./scripts/vps-enable-tailscale-serve-grafana.sh --apply</code></li> </ul> </li> <li>Configure data sources in the Grafana UI:</li> <li>Prometheus: <code>http://127.0.0.1:9090</code></li> <li>Postgres: <code>127.0.0.1:5432</code>, DB <code>healtharchive</code>, user <code>grafana_readonly</code></li> </ol> <p>Acceptance criteria:</p> <ul> <li>Grafana is reachable from an operator machine on the tailnet (SSH tunnel or Serve).</li> <li>Grafana is unreachable from the public internet.</li> </ul> <p>Rollback:</p> <ul> <li>Remove <code>tailscale serve</code> config; stop/disable Grafana.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#phase-6-dashboards-ops-health-must-have","title":"Phase 6 \u2014 Dashboards: ops health (must-have)","text":"<p>Goal: ship dashboards that directly reduce operator time during \u201csomething feels wrong\u201d.</p> <p>Status: implemented in this repo (dashboard JSON + provisioning installer + playbook); requires running on the VPS.</p> <p>Deliverables (minimum set):</p> <p>1) Ops overview dashboard:</p> <ul> <li>Backend up (Prometheus <code>up</code> for the backend scrape job)</li> <li>Job status counts</li> <li>Snapshot totals</li> <li>Disk usage for <code>/srv/healtharchive</code></li> <li>DB up + key DB signals</li> <li>Search errors and request volume</li> </ul> <p>2) Pipeline health dashboard:</p> <ul> <li>Crawl pages crawled/failed totals</li> <li>Storage totals (warc/output/tmp)</li> <li>\u201cNo progress\u201d heuristics (e.g., snapshots flatline)</li> </ul> <p>3) Search performance dashboard:</p> <ul> <li><code>healtharchive_search_duration_seconds_*</code> quantiles (using <code>histogram_quantile</code>)</li> <li>Search error rate</li> <li>Mode breakdown (relevance vs fallback vs boolean vs url)</li> </ul> <p>Steps:</p> <ol> <li>Add dashboards (JSON):</li> <li><code>ops/observability/dashboards/healtharchive-ops-overview.json</code></li> <li><code>ops/observability/dashboards/healtharchive-pipeline-health.json</code></li> <li><code>ops/observability/dashboards/healtharchive-search-performance.json</code></li> <li>Add the VPS installer script:</li> <li><code>scripts/vps-install-observability-dashboards.sh</code></li> <li>Add the playbook:</li> <li><code>docs/operations/playbooks/observability-dashboards.md</code></li> <li>Run on the VPS:</li> <li>dry-run: <code>./scripts/vps-install-observability-dashboards.sh</code></li> <li>apply: <code>sudo ./scripts/vps-install-observability-dashboards.sh --apply</code></li> </ol> <p>Acceptance criteria:</p> <ul> <li>Operator can answer the Phase 1 questions (\u201cis it up?\u201d, \u201cis disk dying?\u201d, \u201care jobs stuck?\u201d) in &lt;2 minutes.</li> </ul> <p>Rollback:</p> <ul> <li>Dashboards are config; revert dashboard JSON/provisioning.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#phase-7-dashboards-expanded-private-usage-aggregate-only","title":"Phase 7 \u2014 Dashboards: expanded private usage (aggregate-only)","text":"<p>Goal: provide a long-window, operator-only view of usage without collecting identifiers.</p> <p>Status: implemented in this repo (expanded usage events + dashboard JSON + provisioning installer); requires running on the VPS.</p> <p>Deliverables:</p> <ul> <li>A Grafana \u201cUsage\u201d dashboard backed by Postgres queries against <code>usage_metrics</code>.</li> <li>A \u201cMonthly/Quarterly impact\u201d dashboard that helps generate public-safe reporting.</li> </ul> <p>Steps:</p> <ol> <li>Confirm current <code>usage_metrics</code> event set (aggregate-only; no identifiers):</li> <li><code>search_request</code>, <code>snapshot_detail</code>, <code>snapshot_raw</code>, <code>report_submitted</code></li> <li><code>changes_list</code>, <code>compare_view</code>, <code>timeline_view</code></li> <li><code>exports_download_snapshots</code>, <code>exports_download_changes</code></li> <li>Decide which additional events are worth adding (still aggregate-only), e.g.:</li> <li><code>sources_list</code> (GET /api/sources)</li> <li><code>stats_view</code> (GET /api/stats)</li> </ol> <p>Keep the list small; each event adds ongoing interpretation overhead.</p> <ol> <li>Make a public vs private exposure decision for new events:</li> <li>Public <code>/api/usage</code> should remain aligned with public reporting pages.</li> <li>Private dashboards can see more than public.</li> </ol> <p>Recommended approach:    - Store all events in <code>usage_metrics</code>.    - Expose only a curated subset via public <code>GET /api/usage</code>.    - Optionally create an admin-only endpoint for full usage metrics later.</p> <ol> <li>Add dashboards (JSON):</li> <li><code>ops/observability/dashboards/healtharchive-usage-private.json</code></li> <li> <p><code>ops/observability/dashboards/healtharchive-impact-summary.json</code></p> </li> <li> <p>Install via the same VPS dashboards installer (Phase 6).</p> </li> </ol> <p>Acceptance criteria:</p> <ul> <li>Operator can generate a public-safe \u201cimpact summary\u201d without guessing.</li> <li>No identifiers or raw text are introduced.</li> </ul> <p>Rollback:</p> <ul> <li>No schema changes required for dashboards; remove dashboards.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#phase-8-alerting-strategy-minimal-high-signal","title":"Phase 8 \u2014 Alerting strategy (minimal, high-signal)","text":"<p>Goal: get notified about real outages without creating pager fatigue.</p> <p>Status: implemented in this repo (Prometheus rules + Alertmanager webhook routing).</p> <p>Deliverables:</p> <ul> <li>A small alert set routed to one operator channel.</li> <li>Each alert includes a runbook link.</li> </ul> <p>Steps:</p> <ol> <li>Decide routing:</li> <li>Keep existing external uptime checks for public URLs.</li> <li>Use Prometheus alert rules + Alertmanager for \u201cconfig-as-code\u201d alerting.<ul> <li>Route to a single webhook receiver (Discord/Slack/etc).</li> <li>Avoids Grafana alert provisioning complexity (datasource UIDs, version drift).</li> </ul> </li> <li>Start with only these alerts:</li> <li>Backend scrape down for &gt;5 minutes.</li> <li>Disk usage &gt;80% (warning) and &gt;90% (critical) on <code>/srv/healtharchive</code>.</li> <li>Search error rate non-zero for sustained window.</li> <li>Job failures rising (or <code>failed</code> status count increases unexpectedly).</li> <li>Add runbook links that point to existing playbooks and the production runbook.</li> <li>Test each alert intentionally once.</li> </ol> <p>Implementation notes:</p> <ul> <li>Script: <code>scripts/vps-install-observability-alerting.sh</code></li> <li>Playbook: <code>docs/operations/playbooks/observability-alerting.md</code></li> <li>Secret (webhook URL): <code>/etc/healtharchive/observability/alertmanager_webhook_url</code></li> </ul> <p>Acceptance criteria:</p> <ul> <li>Alerts fire when expected and do not produce daily noise.</li> </ul> <p>Rollback:</p> <ul> <li>Disable alerts; keep dashboards.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#phase-9-adminops-ui-decision-gate-build-only-if-needed","title":"Phase 9 \u2014 Admin/ops UI (decision gate; build only if needed)","text":"<p>Goal: avoid building a bespoke UI unless it clearly reduces toil.</p> <p>Status: implemented in this repo (Grafana \u201cops console\u201d + loopback-only admin proxy; bespoke UI deferred).</p> <p>Decision gate (required):</p> <ul> <li>Use Grafana + existing JSON endpoints for at least 1\u20132 weeks.</li> <li>Track operator friction:</li> <li>\u201cWhat do we still need SSH for?\u201d</li> <li>\u201cWhat do dashboards not answer?\u201d</li> <li>\u201cWhat actions are error-prone as CLI-only?\u201d</li> <li>Log entries using: <code>docs/operations/ops-ui-friction-log-template.md</code></li> </ul> <p>Implementation (low-maintenance default):</p> <ul> <li>Add a Grafana dashboard that covers \u201cread-only ops console\u201d needs via Postgres:</li> <li>Recent non-indexed jobs</li> <li>Redacted issue report metadata</li> <li>Add a loopback-only \u201cadmin proxy\u201d on the VPS to browse admin endpoints in a browser   without manually copying tokens:</li> <li>Proxies <code>GET /api/admin/**</code> and <code>GET /metrics</code></li> <li>Adds the admin token server-side from <code>/etc/healtharchive/observability/prometheus_backend_admin_token</code></li> <li>Intended access is via SSH port-forwarding (tailnet-only SSH), not public exposure.</li> </ul> <p>If (and only if) justified later, implement a bespoke read-only admin UI MVP.</p> <p>Read-only MVP scope (no mutating actions):</p> <ul> <li>Jobs list + filters (maps to <code>GET /api/admin/jobs</code>)</li> <li>Job detail (maps to <code>GET /api/admin/jobs/{id}</code>)</li> <li>Issue report list/detail (maps to <code>GET /api/admin/reports</code>, <code>/api/admin/reports/{id}</code>)</li> <li>Search debug tool (maps to <code>GET /api/admin/search-debug</code>)</li> </ul> <p>Security requirements:</p> <ul> <li>Must be tailnet-only.</li> <li>Must not embed the admin token into a publicly served bundle.</li> <li>Prefer server-side token usage (UI backend calls admin API; browser never sees token).</li> </ul> <p>Acceptance criteria:</p> <ul> <li>Operator can triage jobs and reports without copying tokens into the browser.</li> <li>If using SSH port-forwarding, SSH is used only as the transport (tailnet-only), not as the primary UI.</li> </ul> <p>Rollback:</p> <ul> <li>Disable/remove the admin proxy service; no data migrations required.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#phase-10-drift-proofing-and-docs-updates-make-it-maintainable","title":"Phase 10 \u2014 Drift-proofing and docs updates (make it maintainable)","text":"<p>Goal: ensure the new stack stays reproducible and doesn\u2019t silently degrade.</p> <p>Deliverables:</p> <ul> <li>Production runbook updated with:</li> <li>how to access Grafana (tailnet-only; SSH port-forward)</li> <li>how to restart services</li> <li>where dashboards/config live</li> <li>Baseline drift policy extended to include observability invariants.</li> <li>One ops playbook: \u201cObservability maintenance\u201d.</li> <li>One quick verification script (VPS): <code>scripts/vps-verify-observability.sh</code>.</li> </ul> <p>Steps:</p> <ol> <li>Update <code>docs/deployment/production-single-vps.md</code>:</li> <li>Add a section describing Prometheus/Grafana components.</li> <li>Document tailnet-only access path.</li> <li>Update <code>docs/operations/production-baseline-policy.toml</code>:</li> <li>Assert that Prometheus/Grafana services exist and are enabled.</li> <li>Assert that no new public ports are open.</li> <li>Assert secrets/config files exist with correct ownership/mode.</li> <li>Add <code>docs/operations/playbooks/observability-maintenance.md</code>:</li> <li>upgrade cadence (quarterly)</li> <li>how to export dashboards</li> <li>how to rotate creds</li> <li>how to prune retention</li> <li>Add a simple verification checklist (similar to deploy+verify style):</li> <li>Grafana reachable via tailnet</li> <li>Prometheus targets up</li> <li>Key dashboards load</li> </ol> <p>Acceptance criteria:</p> <ul> <li>A new operator can set up and use the stack with only canonical docs.</li> </ul> <p>Rollback:</p> <ul> <li>Remove baseline assertions and docs; disable services.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#4-sequencing-and-estimated-time","title":"4) Sequencing and estimated time","text":"<p>Conservative estimate (single operator, careful execution):</p> <ul> <li>Phase 1: 0.5\u20131 day</li> <li>Phase 2: 0.5 day</li> <li>Phase 3: 1\u20132 days</li> <li>Phase 4: 1\u20132 days</li> <li>Phase 5: 1\u20132 days</li> <li>Phase 6: 1\u20133 days</li> <li>Phase 7: 1\u20133 days</li> <li>Phase 8: 1\u20132 days</li> <li>Phase 9: defer unless needed</li> <li>Phase 10: 0.5\u20131 day</li> </ul> <p>Total to a useful private stats surface (through Phase 7): ~5\u201312 days elapsed.</p>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#5-operational-checklists-copypaste","title":"5) Operational checklists (copy/paste)","text":""},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#51-did-we-accidentally-make-anything-public-checklist","title":"5.1 \u201cDid we accidentally make anything public?\u201d checklist","text":"<ul> <li> No new DNS records for ops tools.</li> <li> No new Caddy vhosts for ops tools.</li> <li> Hetzner firewall unchanged (still only 80/443 + Tailscale UDP).</li> <li> UFW unchanged except tailnet-only allowances (if any).</li> <li> Grafana/Prometheus/exporters bind to loopback.</li> <li> Access is via Tailscale Serve only.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#52-did-we-accidentally-collect-identifiers-checklist","title":"5.2 \u201cDid we accidentally collect identifiers?\u201d checklist","text":"<ul> <li> No IP/user-agent logging copied into analytics tables.</li> <li> No query strings stored.</li> <li> No user IDs (there are none).</li> <li> Usage metrics remain daily aggregates.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#6-notes-for-future-iterations-do-not-do-on-day-1","title":"6) Notes for future iterations (do not do on day 1)","text":"<ul> <li>If <code>/metrics</code> scraping becomes heavy:</li> <li>Add server-side caching for <code>/metrics</code> output for a short TTL.</li> <li>Or move some DB-derived metrics into a scheduled \u201csnapshot table\u201d updated periodically.</li> <li>If you need \u201cper-source usage\u201d later:</li> <li>Prefer a small, fixed dimension space (e.g., \u201cfiltered vs unfiltered search\u201d), not per-URL breakdown.</li> <li>If true per-source usage is required, implement a dedicated daily aggregation job that computes counts by joining stable DB keys (source_id) rather than recording high-cardinality labels on every request.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#appendix-a-component-port-access-matrix-target-state","title":"Appendix A \u2014 Component / port / access matrix (target state)","text":"<p>This section exists to prevent accidental public exposure.</p> Component Purpose Bind Port Who can access Notes HealthArchive API Public API + admin endpoints <code>127.0.0.1</code> <code>8001</code> Public via Caddy for <code>/api/**</code>; operators for <code>/metrics</code> + <code>/api/admin/**</code> Prometheus scrapes <code>/metrics</code> via loopback with token Caddy Public TLS reverse proxy public <code>443</code> Public Should not proxy Grafana/Prometheus Prometheus Metrics scraping + storage <code>127.0.0.1</code> <code>9090</code> Operators (optional) UI not required; keep loopback by default Grafana Private stats \u201cpage\u201d <code>127.0.0.1</code> <code>3000</code> Operators (tailnet only) Expose via <code>tailscale serve</code> node exporter Host CPU/mem/disk signals <code>127.0.0.1</code> <code>9100</code> Prometheus only Never public postgres exporter Postgres signals <code>127.0.0.1</code> <code>9187</code> (typical) Prometheus only Never public <p>If you choose to bind any of these to a tailnet interface (instead of loopback), limit it explicitly to <code>tailscale0</code> and still do not add any Caddy vhosts.</p>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#appendix-b-prometheus-configuration-skeleton-safe-no-secrets","title":"Appendix B \u2014 Prometheus configuration skeleton (safe, no secrets)","text":"<p>Goal: scrape the backend via loopback and provide the admin token via a credentials file (so it never appears in <code>prometheus.yml</code>).</p> <p>Suggested files:</p> <ul> <li><code>/etc/prometheus/prometheus.yml</code></li> <li><code>/etc/healtharchive/observability/prometheus_backend_admin_token</code> (created root-only by default; later phases may relax for Prometheus)</li> </ul> <p>Example <code>prometheus.yml</code> (adjust ports/paths to your distro packaging):</p> <pre><code>global:\n  scrape_interval: 60s\n  scrape_timeout: 10s\n\nscrape_configs:\n  - job_name: healtharchive_backend\n    metrics_path: /metrics\n    scheme: http\n    static_configs:\n      - targets: [\"127.0.0.1:8001\"]\n    authorization:\n      type: Bearer\n      credentials_file: /etc/healtharchive/observability/prometheus_backend_admin_token\n\n  - job_name: node\n    static_configs:\n      - targets: [\"127.0.0.1:9100\"]\n\n  - job_name: postgres\n    static_configs:\n      - targets: [\"127.0.0.1:9187\"]\n</code></pre> <p>Hardening checklist (Prometheus):</p> <ul> <li>Run Prometheus as a dedicated user (distro packages usually do this).</li> <li>Set retention explicitly (time and/or size) so it cannot fill <code>/</code>.</li> <li>Prefer distro defaults for TSDB (<code>/var/lib/prometheus</code>) to reduce maintenance.</li> </ul> <p>Backend load control:</p> <ul> <li>If <code>/metrics</code> queries prove heavy, increase scrape interval first.</li> <li>Only then consider caching or pre-aggregation in the backend.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#appendix-c-grafana-data-sources-and-least-privilege-postgres-roles","title":"Appendix C \u2014 Grafana data sources and least-privilege Postgres roles","text":"<p>Grafana should have:</p> <p>1) Prometheus data source (time-series) 2) Postgres data source (tables + long-window aggregates)</p>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#c1-postgres-role-grafana-read-only","title":"C.1 Postgres role: Grafana read-only","text":"<p>Create a dedicated DB role for Grafana; do not reuse the app DB user.</p> <p>Example SQL (adjust DB name/schema if needed):</p> <pre><code>-- One-time: create a login role.\nCREATE ROLE grafana_readonly LOGIN PASSWORD '&lt;STRONG_PASSWORD&gt;';\n\n-- Allow connecting.\nGRANT CONNECT ON DATABASE healtharchive TO grafana_readonly;\n\n-- Allow schema usage.\nGRANT USAGE ON SCHEMA public TO grafana_readonly;\n\n-- Allow reading usage aggregates.\nGRANT SELECT ON TABLE usage_metrics TO grafana_readonly;\n\n-- Allow reading jobs/snapshots if you want table panels.\nGRANT SELECT ON TABLE archive_jobs TO grafana_readonly;\nGRANT SELECT ON TABLE sources TO grafana_readonly;\nGRANT SELECT ON TABLE snapshots TO grafana_readonly;\n</code></pre>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#c2-sensitive-data-issue-reports","title":"C.2 Sensitive data: issue reports","text":"<p><code>issue_reports</code> may contain emails + free text. For low-risk dashboards:</p> <ul> <li>Create a redacted view that excludes <code>reporter_email</code>, <code>description</code>, and   <code>internal_notes</code>.</li> <li>Grant Grafana access only to that view.</li> </ul> <p>Example:</p> <pre><code>CREATE VIEW grafana_issue_reports_summary AS\nSELECT\n  id,\n  category,\n  status,\n  created_at,\n  updated_at,\n  snapshot_id,\n  original_url,\n  page_url\nFROM issue_reports;\n\nGRANT SELECT ON grafana_issue_reports_summary TO grafana_readonly;\n</code></pre> <p>If you later build a bespoke admin UI for reports, handle sensitive fields there with explicit operator intent and audit trails.</p>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#appendix-d-expanded-usage-metrics-design-and-implementation-details","title":"Appendix D \u2014 Expanded usage metrics: design and implementation details","text":""},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#d1-current-state","title":"D.1 Current state","text":"<ul> <li>Storage: <code>usage_metrics(metric_date, event, count)</code>.</li> <li>Events today:</li> <li><code>search_request</code></li> <li><code>snapshot_detail</code></li> <li><code>snapshot_raw</code></li> <li><code>report_submitted</code></li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#d2-expansion-strategy-low-maintenance","title":"D.2 Expansion strategy (low-maintenance)","text":"<p>Principles:</p> <ul> <li>Add a few new events that correspond to real user workflows.</li> <li>Avoid any dimension that explodes cardinality.</li> <li>Keep event names stable and documented.</li> </ul> <p>Recommended \u201cnext events\u201d (candidate set):</p> <ul> <li><code>changes_list</code> (human workflow: \u201cwhat changed?\u201d)</li> <li><code>compare_view</code> (human workflow: \u201cwhat changed between captures?\u201d)</li> <li><code>exports_manifest</code> (human workflow: \u201cwhat data can I download?\u201d)</li> <li><code>exports_download_snapshots</code>, <code>exports_download_changes</code> (human workflow: \u201cdownload data\u201d)</li> </ul> <p>Recommended \u201cdo not add\u201d (privacy/toil risk):</p> <ul> <li>Query text capture (top terms).</li> <li>Per-URL/per-path tracking.</li> <li>Per-source breakdown via labels recorded per request.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#d3-public-vs-private-usage-exposure-important","title":"D.3 Public vs private usage exposure (important)","text":"<p>Because the project already uses <code>GET /api/usage</code> to power public reporting, we need a stable policy:</p> <ul> <li>DB can store more than the public API exposes.</li> <li>Public API should return only metrics you\u2019re comfortable publishing.</li> </ul> <p>Implementation plan (backend; later work):</p> <ol> <li>Split events into two lists:</li> <li><code>EVENTS_INTERNAL</code> (everything you store)</li> <li><code>EVENTS_PUBLIC</code> (subset returned by <code>GET /api/usage</code>)</li> <li>Keep the existing response shape stable for the frontend <code>/impact</code> page.</li> <li>If you add new public fields, coordinate frontend changes.</li> <li>For truly private usage panels, Grafana should read from Postgres directly,    not from <code>GET /api/usage</code>.</li> </ol> <p>Testing plan (backend; later work):</p> <ul> <li>Add tests that verify:</li> <li>private-only events do not appear in <code>/api/usage</code> output.</li> <li>public events continue to increment and render as expected.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#d4-excluding-automation-noise","title":"D.4 Excluding \u201cautomation noise\u201d","text":"<p>Do not count:</p> <ul> <li><code>/api/health</code> (it is hit by uptime checks)</li> </ul> <p>Prefer counting:</p> <ul> <li>Search requests, snapshot views, compare views, report submissions\u2014these are   closer to human workflow.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#appendix-e-verification-and-rollback-checklists-operator-run","title":"Appendix E \u2014 Verification and rollback checklists (operator run)","text":""},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#e1-verify-tailnet-only-access","title":"E.1 Verify \u201ctailnet only\u201d access","text":"<ul> <li>From a machine not on your tailnet:</li> <li>Grafana URL should be unreachable.</li> <li>From a tailnet-connected operator machine:</li> <li>Grafana URL loads and prompts for login.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#e2-verify-backend-scrape-is-token-protected","title":"E.2 Verify backend scrape is token-protected","text":"<ul> <li><code>curl -i https://api.healtharchive.ca/metrics</code> should be <code>403</code> without token.</li> <li>Prometheus target for backend <code>/metrics</code> should show <code>UP</code>.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#e3-verify-dashboards-are-answering-the-right-questions","title":"E.3 Verify dashboards are answering the right questions","text":"<ul> <li>Ops overview: backend up, job counts, disk, DB.</li> <li>Usage: daily totals (long-window) match expectations.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-ops-observability-and-private-usage/#e4-rollback-recipe-if-anything-feels-risky","title":"E.4 Rollback recipe (if anything feels risky)","text":"<ul> <li>Remove <code>tailscale serve</code> mapping first (instant private UI removal).</li> <li>Stop Grafana.</li> <li>Stop Prometheus.</li> <li>Leave backend unchanged.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/","title":"HealthArchive sequential implementation plan (implemented 2025-12-24)","text":"<p>This is the historical, step-by-step execution plan that was used to bring HealthArchive to the current production state across:</p> <ul> <li>backend (<code>healtharchive-backend</code>)</li> <li>frontend (<code>healtharchive-frontend</code>)</li> <li>production operations (VPS + Vercel + GitHub settings)</li> </ul> <p>It is intentionally sequential: complete each phase before starting the next.</p> <p>Notes:</p> <ul> <li>This is not the historical \u201c6-phase upgrade roadmap\u201d. That historical doc lives in   <code>2025-12-24-6-phase-upgrade-roadmap-2025.md</code>.</li> <li>This plan is written against current implementation reality. Where code already   exists, phases focus on configuration + verification rather than new development.</li> <li>Keep <code>docs/operations/healtharchive-ops-roadmap.md</code> short; it should reference canonical docs   and <code>docs/roadmaps/roadmap.md</code> rather than duplicating this historical plan.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-0-baseline-inventory-one-time-repeat-after-major-changes","title":"Phase 0 \u2014 Baseline inventory (one-time; repeat after major changes)","text":"<p>Current state: configuration-driven; code supports most features but production state can drift.</p> <p>Best-practice approach: treat baseline as policy (in git) + observed snapshots (generated on VPS) + drift checks.</p> <p>Implementation helpers (repo):</p> <ul> <li>Desired-state policy (in git): <code>docs/operations/production-baseline-policy.toml</code></li> <li>Drift check + snapshot writer (VPS): <code>scripts/check_baseline_drift.py</code></li> <li>Snapshot generator (optional): <code>scripts/baseline_snapshot.py</code></li> <li>Background timer (optional): <code>docs/deployment/systemd/healtharchive-baseline-drift-check.*</code></li> <li> <p>Ops doc: <code>docs/operations/baseline-drift.md</code></p> </li> <li> <p>Record the current values (or \u201cunset\u201d) for:</p> </li> <li>Backend: <code>HEALTHARCHIVE_ENV</code>, <code>HEALTHARCHIVE_DATABASE_URL</code>, <code>HEALTHARCHIVE_ARCHIVE_ROOT</code>,      <code>HEALTHARCHIVE_ADMIN_TOKEN</code>, <code>HEALTHARCHIVE_CORS_ORIGINS</code>, <code>HEALTHARCHIVE_LOG_LEVEL</code>,      <code>HA_SEARCH_RANKING_VERSION</code>, <code>HA_PAGES_FASTPATH</code>,      <code>HEALTHARCHIVE_REPLAY_BASE_URL</code>, <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code>,      <code>HEALTHARCHIVE_USAGE_METRICS_ENABLED</code>, <code>HEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS</code>,      <code>HEALTHARCHIVE_CHANGE_TRACKING_ENABLED</code>,      <code>HEALTHARCHIVE_EXPORTS_ENABLED</code>, <code>HEALTHARCHIVE_EXPORTS_DEFAULT_LIMIT</code>,      <code>HEALTHARCHIVE_EXPORTS_MAX_LIMIT</code>, <code>HEALTHARCHIVE_PUBLIC_SITE_URL</code>.</li> <li>Frontend (Vercel): <code>NEXT_PUBLIC_API_BASE_URL</code>,      <code>NEXT_PUBLIC_SHOW_API_HEALTH_BANNER</code>, <code>NEXT_PUBLIC_LOG_API_HEALTH_FAILURE</code>,      <code>NEXT_PUBLIC_SHOW_API_BASE_HINT</code>.</li> <li>Capture the current \u201clive surface\u201d URLs you consider canonical:</li> <li><code>https://healtharchive.ca</code>, <code>https://www.healtharchive.ca</code></li> <li><code>https://api.healtharchive.ca</code></li> <li><code>https://replay.healtharchive.ca</code> (if replay is enabled)</li> <li>Confirm what environments you actively support:</li> <li>\u201csingle production backend\u201d vs \u201cstaging backend exists\u201d.</li> <li>On the production VPS, generate an observed snapshot and drift report:</li> </ul> <pre><code>cd /opt/healtharchive-backend\n./scripts/check_baseline_drift.py --mode live\n</code></pre> <p>This writes:</p> <ul> <li><code>/srv/healtharchive/ops/baseline/observed-&lt;timestamp&gt;.json</code></li> <li><code>/srv/healtharchive/ops/baseline/drift-report-&lt;timestamp&gt;.txt</code></li> <li> <p>and updates <code>observed-latest.json</code> + <code>drift-report-latest.txt</code>.</p> </li> <li> <p>If drift is detected, fix production or update policy only if the change is intentional.</p> </li> </ul> <p>Exit criteria: <code>./scripts/check_baseline_drift.py --mode live</code> reports PASS and artifacts exist under <code>/srv/healtharchive/ops/baseline/</code>.</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-1-security-and-access-control-must-be-correct-before-scaling-usage","title":"Phase 1 \u2014 Security and access control (must be correct before scaling usage)","text":"<p>Current state: implemented in code; must be correctly configured in production.</p> <p>Implementation helpers (repo):</p> <ul> <li> <p>Security/admin verification script: <code>scripts/verify-security-and-admin.sh</code></p> </li> <li> <p>Configure <code>HEALTHARCHIVE_ENV=production</code> and set a strong <code>HEALTHARCHIVE_ADMIN_TOKEN</code>.</p> </li> <li>Code reference: <code>ha_backend/api/deps.py</code> fails closed in prod/staging if token is missing.</li> <li>Verify:</li> <li><code>/metrics</code> requires auth and cannot be scraped without the token.</li> <li><code>/api/admin/*</code> routes require auth.</li> <li>Ensure secrets posture:</li> <li>tokens/credentials are in server env files / secret manager only, never committed.</li> <li>Confirm HTTPS posture for the API:</li> <li>HTTP\u2192HTTPS redirect</li> <li>HSTS enabled for <code>api.healtharchive.ca</code></li> <li>(documented checklist) <code>docs/deployment/hosting-and-live-server-to-dos.md</code></li> </ul> <p>HSTS implementation note (Caddy):</p> <ul> <li>Prefer setting HSTS at the reverse proxy (Caddy), not in the FastAPI app.</li> <li>Example (adjust to your policy; <code>includeSubDomains</code> is only safe if all subdomains are HTTPS):</li> </ul> <pre><code>api.healtharchive.ca {\n  header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"\n  reverse_proxy 127.0.0.1:8001\n}\n</code></pre> <p>Recommended verification (production):</p> <pre><code>cd /opt/healtharchive-backend\nset -a; source /etc/healtharchive/backend.env; set +a\n./scripts/verify-security-and-admin.sh --api-base https://api.healtharchive.ca --require-hsts\n</code></pre> <p>Exit criteria: admin endpoints are closed to the public, secrets are stored safely, API HTTPS posture is verified.</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-2-ci-enforcement-and-merge-discipline-prevents-regressions","title":"Phase 2 \u2014 CI enforcement and merge discipline (prevents regressions)","text":"<p>Current state: workflow files exist; enforcement is mostly GitHub settings.</p> <p>Implementation helpers (repo):</p> <ul> <li>Backend CI workflow: <code>healtharchive-backend/.github/workflows/backend-ci.yml</code></li> <li>Frontend CI workflow: <code>healtharchive-frontend/.github/workflows/frontend-ci.yml</code></li> <li> <p>GitHub branch protection walkthrough: <code>docs/operations/monitoring-and-ci-checklist.md</code></p> </li> <li> <p>Ensure Actions workflows are enabled for both repos:</p> </li> <li><code>healtharchive-backend/.github/workflows/backend-ci.yml</code></li> <li><code>healtharchive-frontend/.github/workflows/frontend-ci.yml</code></li> <li>Solo-fast mode (recommended while you\u2019re the only committer):</li> <li>allow direct pushes to <code>main</code></li> <li>treat \u201cgreen main\u201d as the deploy gate</li> <li>add local guardrails so you don\u2019t accidentally push broken <code>main</code>:<ul> <li>from the mono-repo root: <code>make check</code></li> <li>optional but recommended:</li> <li><code>healtharchive-backend/scripts/install-pre-push-hook.sh</code></li> <li><code>healtharchive-frontend/scripts/install-pre-push-hook.sh</code></li> </ul> </li> <li>Deploy gate (production):</li> <li>recommended (one command): <code>./scripts/vps-deploy.sh --apply --baseline-mode live</code><ul> <li>includes baseline drift + public-surface verify by default</li> </ul> </li> <li>if you use a local alias like <code>dodeploy</code>, ensure you still run:<ul> <li><code>./scripts/check_baseline_drift.py --mode live</code></li> <li><code>./scripts/verify_public_surface.py</code></li> </ul> </li> <li>Future (tighten later when there are multiple committers):</li> <li>require PR</li> <li>require status checks</li> <li>include administrators + require code owner review</li> <li>track in <code>../roadmap.md</code></li> </ul> <p>Exit criteria: deploys only happen from a green <code>main</code>, and post-deploy verification is routine.</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-3-external-monitoring-site-up-signal-low-noise-alerts","title":"Phase 3 \u2014 External monitoring (site-up signal + low-noise alerts)","text":"<p>Current state: guidance + helper scripts exist; requires operator setup.</p> <p>Implementation helpers (repo):</p> <ul> <li>Monitor setup walkthrough: <code>docs/operations/monitoring-and-ci-checklist.md</code></li> <li> <p>Pre-flight checker: <code>scripts/smoke-external-monitors.sh</code></p> </li> <li> <p>Configure external uptime monitors:</p> </li> <li><code>https://api.healtharchive.ca/api/health</code></li> <li><code>https://www.healtharchive.ca/archive</code> (integration check)</li> <li><code>https://replay.healtharchive.ca/</code> (only if replay is relied upon)</li> <li>Use the local helper to validate from a laptop/VPS:</li> <li><code>healtharchive-backend/scripts/smoke-external-monitors.sh</code></li> <li>Decide alert routing (page vs email) and document the decision.</li> </ul> <p>Exit criteria: monitors exist, are green, and alert routing is confirmed.</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-4-environment-wiring-cors-posture-and-preview-policy","title":"Phase 4 \u2014 Environment wiring, CORS posture, and \u201cpreview\u201d policy","text":"<p>Current state: backend CORS is strict-by-default; frontend can fall back to demo mode.</p> <p>Implementation helpers (repo):</p> <ul> <li>Canonical wiring doc: <code>docs/deployment/environments-and-configuration.md</code></li> <li> <p>Production drift + wiring validation (includes CORS header checks): <code>scripts/check_baseline_drift.py --mode live</code></p> </li> <li> <p>Set canonical API base on Vercel:</p> </li> <li><code>NEXT_PUBLIC_API_BASE_URL=https://api.healtharchive.ca</code></li> <li>Decide branch-preview posture:<ul> <li>Option A (recommended): strict CORS allowlist; branch preview URLs fall back to demo.</li> </ul> </li> <li>Option B: allow additional preview origins (higher risk; more surface).</li> <li>Implement the chosen posture:</li> <li>set <code>HEALTHARCHIVE_CORS_ORIGINS</code> explicitly in production env.</li> <li>Verify:</li> <li>production site uses live API</li> <li>Vercel project domain (<code>healtharchive.vercel.app</code>) behavior matches the chosen posture</li> <li>branch previews behave as expected (demo fallback or live API)</li> </ul> <p>Exit criteria: there are no \u201csurprise demo mode\u201d deployments; behavior is explicit and repeatable.</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-5-end-to-end-baseline-usability-frontend-backend","title":"Phase 5 \u2014 End-to-end baseline usability (frontend \u2194 backend)","text":"<p>Current state: frontend already calls the backend directly and has offline fallbacks.</p> <p>Implementation helpers (repo):</p> <ul> <li> <p>Public-surface verifier (frontend + public API + replay + usage): <code>scripts/verify_public_surface.py</code></p> </li> <li> <p>Verify core public routes against the live backend:</p> </li> <li><code>/archive</code> search + pagination</li> <li><code>/archive/browse-by-source</code></li> <li><code>/snapshot/[id]</code> metadata loads from <code>/api/snapshot/{id}</code></li> <li><code>/report</code> submits to the backend via the frontend forwarder</li> <li>Confirm expected behavior when the backend is down:</li> <li>clear fallback notices</li> <li>demo dataset is used</li> </ul> <p>Exit criteria: <code>scripts/verify_public_surface.py</code> passes and degradations are clear and safe.</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-6-replay-service-full-fidelity-browsing-snapshot-viewer-fidelity","title":"Phase 6 \u2014 Replay service (full-fidelity browsing) + snapshot viewer fidelity","text":"<p>Current state: backend can emit <code>browseUrl</code>; frontend prefers replay when present.</p> <ol> <li>Deploy/maintain replay service (pywb) per:</li> <li><code>docs/deployment/replay-service-pywb.md</code></li> <li>Enable replay URL generation on the backend:</li> <li>set <code>HEALTHARCHIVE_REPLAY_BASE_URL</code></li> <li>confirm API responses include <code>browseUrl</code> where expected</li> <li>Index at least one job into replay:</li> <li><code>ha-backend replay-index-job --id &lt;JOB_ID&gt;</code></li> <li>validate a known-good replay URL loads</li> <li>Confirm frontend behavior:</li> <li><code>/snapshot/[id]</code> embeds replay (<code>browseUrl</code>) when available, otherwise raw HTML fallback.</li> <li>Confirm cleanup posture:</li> <li>do not delete WARCs needed for replay; rely on the cleanup safety checks when replay is enabled.</li> </ol> <p>Exit criteria: replay-backed snapshots work end-to-end for at least one real job and are safe from accidental cleanup.</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-7-usage-metrics-and-public-reporting-pages-status-impact","title":"Phase 7 \u2014 Usage metrics and public reporting pages (<code>/status</code>, <code>/impact</code>)","text":"<p>Current state: backend has usage metrics code + endpoint; frontend already renders enabled/disabled states.</p> <ol> <li>Ensure the DB schema/migrations include usage tables and the feature is enabled:</li> <li>set <code>HEALTHARCHIVE_USAGE_METRICS_ENABLED=1</code> (or decide to keep it off)</li> <li>Verify event recording works (best-effort):</li> <li>search requests</li> <li>snapshot detail views</li> <li>raw snapshot views</li> <li>report submissions</li> <li>Verify <code>/api/usage</code> returns <code>enabled=true</code> and realistic windowed totals.</li> <li>Verify frontend pages render real numbers:</li> <li><code>/status</code></li> <li><code>/impact</code></li> </ol> <p>Exit criteria: public reporting surfaces reflect real aggregate counts (or are explicitly disabled by policy).</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-8-research-exports-dataset-releases-and-researchers-page-accuracy","title":"Phase 8 \u2014 Research exports, dataset releases, and Researchers page accuracy","text":"<p>Current state: export endpoints and the <code>/exports</code> + <code>/researchers</code> pages exist; verify script covers exports and pages.</p> <ol> <li>Confirm exports are enabled and stable:</li> <li><code>HEALTHARCHIVE_EXPORTS_ENABLED=1</code></li> <li><code>/api/exports</code>, <code>/api/exports/snapshots</code>, <code>/api/exports/changes</code></li> <li>Recommended verifier (production):<ul> <li><code>./scripts/verify_public_surface.py</code> (checks exports manifest + export HEADs)</li> </ul> </li> <li>Confirm the public data dictionary + downloadables are correct:</li> <li><code>healtharchive-frontend/public/exports/healtharchive-data-dictionary.md</code></li> <li><code>healtharchive-frontend/public/exports/healtharchive-data-dictionary.fr.md</code> (alpha)</li> <li>Decide and document the dataset release process:</li> <li>cadence (quarterly is the current intent)</li> <li>where releases live (GitHub Releases)</li> <li>checksum policy (<code>SHA256SUMS</code>)</li> <li>validation step (in ops cadence)</li> <li>Implementation helpers (repo):<ul> <li><code>docs/operations/dataset-release-runbook.md</code></li> <li><code>docs/operations/export-integrity-contract.md</code></li> </ul> </li> <li>Implement the Researchers page workflow (avoid \u201cplanned\u201d contradictions):</li> <li>state the current dataset release cadence (or explicitly mark it \u201cnot yet stable\u201d)</li> <li>document the bulk export request workflow (what info to send, constraints, expected response time)</li> <li>keep English canonical and ship French in the same change</li> <li>reference: <code>healtharchive-frontend/docs/development/bilingual-dev-guide.md</code></li> </ol> <p>Exit criteria: <code>/researchers</code> accurately describes how researchers get data today (including datasets + bulk requests), and no \u201cplanned\u201d copy contradicts reality.</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-9-coverage-expansion-cihr-legacy-import-and-new-sources","title":"Phase 9 \u2014 Coverage expansion (CIHR legacy import and new sources)","text":"<p>Current state: CIHR import is documented; helper script exists to normalize/register/index on the VPS.</p> <ol> <li>Complete CIHR legacy import:</li> <li>normalize permissions</li> <li>register job dir for CIHR</li> <li>index job</li> <li>reference: <code>docs/operations/legacy-crawl-imports.md</code></li> <li>Optional helper (VPS): <code>scripts/import-legacy-crawl.sh</code></li> <li>Verify:</li> <li>CIHR appears in <code>/api/sources</code></li> <li>CIHR is searchable in <code>/api/search</code> and visible in the frontend</li> <li>Single command (production):<ul> <li><code>./scripts/verify_public_surface.py --require-source cihr</code></li> </ul> </li> <li>Only after CIHR is stable, consider additional source expansion (annual campaign updates).</li> </ol> <p>Exit criteria: CIHR is a first-class source in search, browse, and snapshot detail.</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-10-search-quality-loop-ranking-rollout-decisions","title":"Phase 10 \u2014 Search quality loop + ranking rollout decisions","text":"<p>Current state: evaluation docs + capture/diff scripts exist; ranking v2 rollout is documented (and currently recommended).</p> <ol> <li>Establish a repeatable evaluation workflow:</li> <li>golden queries + expectations live in:<ul> <li><code>docs/operations/search-golden-queries.md</code></li> <li><code>docs/operations/search-quality.md</code></li> </ul> </li> <li>capture/diff scripts under <code>healtharchive-backend/scripts/</code>:<ul> <li><code>scripts/search-eval-run.sh</code> (one-command v1+v2 capture + diff)</li> </ul> </li> <li>Run evaluations periodically and record outcomes (public-safe).</li> <li>Only after you have signal:</li> <li>decide whether to keep <code>HA_SEARCH_RANKING_VERSION=v1</code> or switch to <code>v2</code></li> <li>follow <code>docs/deployment/search-rollout.md</code> if switching</li> </ol> <p>Exit criteria: search changes are evaluated against a stable query set and rollout/rollback is routine.</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#phase-11-automation-and-sustainability-timers-reconcile-loops-retention","title":"Phase 11 \u2014 Automation and sustainability (timers, reconcile loops, retention)","text":"<p>Current state: systemd templates and ops docs exist; enabling automation is an operator action on the VPS.</p> <p>Implementation helpers (repo):</p> <ul> <li>Systemd templates + enablement steps: <code>docs/deployment/systemd/README.md</code></li> <li>Install/update systemd templates (VPS): <code>scripts/vps-install-systemd-units.sh</code></li> <li>Bootstrap <code>/srv/healtharchive/ops/*</code> dirs (VPS): <code>scripts/vps-bootstrap-ops-dirs.sh</code></li> <li>Automation verifier (VPS): <code>scripts/verify_ops_automation.sh</code></li> <li>Ops cadence docs/templates:</li> <li>restore tests: <code>docs/operations/restore-test-procedure.md</code>, <code>docs/operations/restore-test-log-template.md</code></li> <li>dataset releases: <code>docs/operations/dataset-release-runbook.md</code></li> <li>adoption signals: <code>docs/operations/adoption-signals-log-template.md</code></li> <li>monitoring/timer pings: <code>docs/operations/monitoring-and-ci-checklist.md</code></li> <li>Retention constraints (replay safety): <code>docs/operations/growth-constraints.md</code></li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#111-bootstrap-ops-directories-one-time-vps","title":"11.1 Bootstrap ops directories (one-time; VPS)","text":"<p>If not already created, bootstrap the ops artifact directories so timers and scripts can write artifacts:</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-bootstrap-ops-dirs.sh\n</code></pre>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#112-installupdate-systemd-templates-vps","title":"11.2 Install/update systemd templates (VPS)","text":"<pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker\n</code></pre>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#113-enable-baseline-drift-timer-recommended-vps","title":"11.3 Enable baseline drift timer (recommended; VPS)","text":"<p>Baseline drift checks are low-risk and provide early warning on misconfig (env, perms, unit enablement).</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/baseline-drift-enabled\nsudo systemctl enable --now healtharchive-baseline-drift-check.timer\n</code></pre>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#114-decide-optional-automation-timers-vps","title":"11.4 Decide optional automation timers (VPS)","text":"<p>Keep it boring: enable only what you\u2019re ready to operate. Validate dry-run services before enabling timers:</p> <ul> <li>Annual scheduling (Jan 01 UTC)</li> <li>Validate: <code>sudo systemctl start healtharchive-schedule-annual-dry-run.service</code></li> <li>Enable: create <code>/etc/healtharchive/automation-enabled</code>, then <code>enable --now healtharchive-schedule-annual.timer</code></li> <li>Replay reconcile (daily; optional)</li> <li>Validate: <code>sudo systemctl start healtharchive-replay-reconcile-dry-run.service</code></li> <li>Enable: create <code>/etc/healtharchive/replay-automation-enabled</code>, then <code>enable --now healtharchive-replay-reconcile.timer</code></li> <li>Change tracking (daily; optional)</li> <li>Validate: <code>sudo systemctl start healtharchive-change-tracking-dry-run.service</code></li> <li>Enable: create <code>/etc/healtharchive/change-tracking-enabled</code>, then <code>enable --now healtharchive-change-tracking.timer</code></li> <li>Annual search verification capture (daily timer; captures once per year; optional)</li> <li>Enable: <code>sudo systemctl enable --now healtharchive-annual-search-verify.timer</code></li> <li>Public surface verification (daily; optional, recommended)</li> <li>Enable: create <code>/etc/healtharchive/public-verify-enabled</code>, then <code>enable --now healtharchive-public-surface-verify.timer</code></li> </ul> <p>Full enablement and rollback commands live in: <code>docs/deployment/systemd/README.md</code></p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#115-optional-timer-ran-monitoring-healthchecks-style-vps","title":"11.5 Optional: timer-ran monitoring (Healthchecks-style; VPS)","text":"<p>If you want \u201csilent failure\u201d alerts (timer disabled, unit failing, disk-full refusal, etc.), follow:</p> <ul> <li><code>docs/operations/monitoring-and-ci-checklist.md</code> (Step 4)</li> </ul> <p>This uses a root-owned <code>/etc/healtharchive/healthchecks.env</code> plus <code>scripts/systemd-healthchecks-wrapper.sh</code> so ping URLs are not committed.</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#116-operationalize-the-quarterly-ops-cadence-vps-github-releases","title":"11.6 Operationalize the quarterly ops cadence (VPS + GitHub Releases)","text":"<ul> <li>Quarterly restore test: follow <code>docs/operations/restore-test-procedure.md</code> and write a public-safe log entry under <code>/srv/healtharchive/ops/restore-tests/</code>.</li> <li>Quarterly dataset checksum verification: verify release assets with <code>sha256sum -c SHA256SUMS</code> (see <code>docs/operations/dataset-release-runbook.md</code>).</li> <li>Quarterly adoption signals entry: write a public-safe entry under <code>/srv/healtharchive/ops/adoption/</code> using <code>docs/operations/adoption-signals-log-template.md</code>.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#117-retention-cleanup-posture-vps","title":"11.7 Retention / cleanup posture (VPS)","text":"<ul> <li>If replay is enabled, treat WARC retention as critical state: do not delete WARCs needed for replay.</li> <li>Use only \u201csafe cleanup\u201d modes until you have a cold storage replay plan:</li> <li>reference: <code>docs/operations/growth-constraints.md</code></li> </ul>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#118-verify-phase-11-vps","title":"11.8 Verify Phase 11 (VPS)","text":"<pre><code>cd /opt/healtharchive-backend\n./scripts/verify_ops_automation.sh\n./scripts/check_baseline_drift.py --mode live\n</code></pre> <p>Exit criteria: baseline drift timer is enabled, ops artifact dirs exist, worker priority override is installed, and any optional timers are either enabled or explicitly deferred (with monitoring decisions recorded).</p>"},{"location":"roadmaps/implemented/2025-12-24-sequential-implementation-plan/#remaining-work","title":"Remaining work","text":"<p>Non-code/external work (partners, verifier, mentions/citations log) is tracked in:</p> <ul> <li><code>docs/roadmaps/roadmap.md</code></li> <li><code>docs/operations/healtharchive-ops-roadmap.md</code></li> </ul>"},{"location":"roadmaps/implemented/2025-12-25-compare-live/","title":"Compare-live (snapshot vs live) - implementation plan","text":"<p>Status: implemented (2025-12-25).</p>"},{"location":"roadmaps/implemented/2025-12-25-compare-live/#1-goal","title":"1) Goal","text":"<p>Provide a public compare-to-live workflow that diffs an archived snapshot against the current live page, without caching or persisting live content.</p>"},{"location":"roadmaps/implemented/2025-12-25-compare-live/#2-decisions-locked","title":"2) Decisions (locked)","text":"<ul> <li>Public endpoint (no admin token gating).</li> <li>Always fresh fetch (no caching of live results).</li> <li>Descriptive-only copy; no interpretation or medical advice.</li> <li>Safety controls: timeouts, byte limits, redirect limits, SSRF blocking, and per-process concurrency caps.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-25-compare-live/#3-implementation-summary","title":"3) Implementation summary","text":""},{"location":"roadmaps/implemented/2025-12-25-compare-live/#31-backend","title":"3.1 Backend","text":"<ul> <li>New endpoint: <code>GET /api/snapshots/{snapshot_id}/compare-live</code>.</li> <li>Uses existing diffing pipeline (<code>normalize_html_for_diff</code> + <code>compute_diff</code>).</li> <li>Computes section/line stats and high-noise flag using the same heuristic as change tracking.</li> <li>Response headers set <code>Cache-Control: no-store</code> and <code>X-Robots-Tag: noindex, nofollow</code>.</li> <li>Usage event: <code>compare_live_view</code> recorded into aggregate metrics.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-25-compare-live/#32-frontend","title":"3.2 Frontend","text":"<ul> <li>New route: <code>/compare-live?to=&lt;snapshotId&gt;</code>.</li> <li>Two-step UX: initial landing + explicit \"Fetch live diff\" button (<code>run=1</code>) to prevent prefetch-triggered fetches.</li> <li>Entry points added on snapshot page and archived compare page (prefetch disabled).</li> <li>Compare-live copy warns that the live page is not archived and should not be cited.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-25-compare-live/#33-documentation","title":"3.3 Documentation","text":"<ul> <li>Citation guidance updated to clarify that compare-live is not an archival record.</li> <li>Deployment config docs include compare-live env toggles.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-25-compare-live/#4-config-env","title":"4) Config (env)","text":"<ul> <li><code>HEALTHARCHIVE_COMPARE_LIVE_ENABLED</code> (default <code>1</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_TIMEOUT_SECONDS</code> (default <code>8</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_REDIRECTS</code> (default <code>4</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_BYTES</code> (default <code>2000000</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_ARCHIVE_BYTES</code> (default <code>2000000</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_CONCURRENCY</code> (default <code>4</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_USER_AGENT</code> (default identifies HealthArchive)</li> </ul>"},{"location":"roadmaps/implemented/2025-12-25-compare-live/#5-safety-guardrails","title":"5) Safety guardrails","text":"<ul> <li>Only <code>http</code>/<code>https</code> URLs allowed.</li> <li>Blocks private, loopback, link-local, and reserved IP ranges.</li> <li>Disallows non-80/443 ports and embedded credentials.</li> <li>Redirects are capped and re-validated on every hop.</li> <li>Response body size is capped to avoid memory pressure.</li> </ul>"},{"location":"roadmaps/implemented/2025-12-25-compare-live/#6-testing","title":"6) Testing","text":"<ul> <li>Manual test from a known HTML snapshot:</li> <li><code>/compare-live?to=&lt;id&gt;</code> then \"Fetch live diff\".</li> <li>Verify headers (<code>Cache-Control: no-store</code>), diff output, and live metadata.</li> <li>Non-HTML snapshot should return 422 with a clear error message.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke-hardening/","title":"CI e2e smoke hardening (implementation plan)","text":"<p>Status: implemented (2026-01-03)</p>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke-hardening/#goal","title":"Goal","text":"<p>Improve the end-to-end CI smoke coverage so it is:</p> <ul> <li>Higher-signal (catches \u201c200 but broken/miswired\u201d regressions),</li> <li>Less flaky (no fixed ports, better readiness, robust teardown),</li> <li>Easier to debug (logs uploaded as CI artifacts on failure),</li> <li>Faster (avoid duplicate frontend builds in CI where possible),</li> <li>Bilingual-aware (checks both EN unprefixed routes and <code>/fr/...</code> routes).</li> </ul> <p>This work builds on the existing smoke harness:</p> <ul> <li><code>scripts/ci-e2e-seed.py</code></li> <li><code>scripts/ci-e2e-smoke.sh</code></li> <li><code>scripts/verify_public_surface.py</code></li> <li>CI jobs in both repos.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke-hardening/#scope","title":"Scope","text":""},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke-hardening/#reliability-improvements","title":"Reliability improvements","text":"<ul> <li>Remove fixed ports in the smoke runner; select free ports dynamically.</li> <li>Improve readiness checks:</li> <li>Backend: <code>GET /api/health</code></li> <li>Frontend: <code>GET /archive</code> and <code>GET /fr/archive</code></li> <li>Ensure teardown kills process groups and always prints useful logs on failure.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke-hardening/#higher-signal-assertions","title":"Higher-signal assertions","text":"<p>In <code>scripts/verify_public_surface.py</code>:</p> <ul> <li>Add minimal API contract checks (JSON shape and key invariants).</li> <li>Add minimal frontend HTML assertions:</li> <li>For <code>/archive</code> and <code>/fr/archive</code>, verify a stable <code>&lt;title&gt;</code> marker.</li> <li>For <code>/snapshot/{id}</code> and <code>/fr/snapshot/{id}</code>, verify the page contains the snapshot title     returned by the backend API (seeded deterministically in CI).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke-hardening/#ci-diagnostics-and-runtime","title":"CI diagnostics and runtime","text":"<ul> <li>Upload smoke logs (and optionally the tiny seeded artifacts) as GitHub Actions   artifacts on failure.</li> <li>Reduce duplicated frontend builds:</li> <li>Frontend CI should build once in the main job and reuse that build for the     e2e smoke job (artifact download), rather than building twice.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke-hardening/#guardrails-tests","title":"Guardrails (tests)","text":"<ul> <li>Add a small backend test that prevents accidental regression of EN+FR frontend   page coverage in the verifier.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke-hardening/#non-goals","title":"Non-goals","text":"<ul> <li>Browser automation (Playwright/Cypress).</li> <li>Replay (pywb) service validation in CI (smoke uses <code>--skip-replay</code>).</li> <li>Search relevance quality evaluation (golden-query harness is separate).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke-hardening/#implementation-steps","title":"Implementation steps","text":"<ol> <li>Update <code>scripts/ci-e2e-smoke.sh</code>:</li> <li>Add dynamic port selection.</li> <li>Add <code>--skip-frontend-build</code>.</li> <li>Improve readiness, teardown, and failure log reporting.</li> <li>Update <code>scripts/verify_public_surface.py</code>:</li> <li>Strengthen API contract checks.</li> <li>Add minimal HTML assertions for the public pages.</li> <li>CI workflows:</li> <li>Upload smoke logs as artifacts on failure (backend + frontend repos).</li> <li>Frontend repo: reuse the build output for e2e smoke instead of rebuilding.</li> <li>Add tests:</li> <li>Add a test ensuring verifier includes both EN and FR public pages.</li> <li>Update docs:</li> <li>Backend: <code>docs/operations/monitoring-and-ci-checklist.md</code></li> <li>Backend: <code>docs/development/testing-guidelines.md</code></li> <li>Frontend: <code>healtharchive-frontend/docs/development/bilingual-dev-guide.md</code> (if needed)</li> <li>Run:</li> <li><code>healtharchive-backend: make check</code></li> <li><code>healtharchive-frontend: npm run check</code></li> <li><code>healtharchive-backend: ./scripts/ci-e2e-smoke.sh --frontend-dir ../healtharchive-frontend</code></li> <li>Archive this plan under <code>docs/roadmaps/implemented/</code> and update indices.</li> </ol>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke-hardening/#acceptance-criteria","title":"Acceptance criteria","text":"<ul> <li>The smoke script is robust to port collisions (no fixed ports).</li> <li>The verifier fails on \u201cwrong API / empty snapshot page\u201d even if the HTTP status is 200.</li> <li>CI uploads logs on smoke failures.</li> <li>Frontend CI does not rebuild Next twice just to run e2e smoke.</li> <li>Tests protect EN+FR surface coverage.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke/","title":"CI end-to-end smoke coverage (implementation plan)","text":"<p>Status: implemented (2026-01-03)</p>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke/#goal","title":"Goal","text":"<p>Add a fast, local end-to-end smoke check to CI that exercises the public\u2011critical surfaces:</p> <ul> <li>Frontend: <code>GET /archive</code>, <code>GET /snapshot/{id}</code></li> <li>Backend: <code>GET /api/search</code>, <code>GET /api/sources</code>, <code>GET /api/snapshot/{id}</code></li> </ul> <p>The intent is to catch \u201capp starts but user-critical paths fail\u201d regressions early (miswired env, breaking API contract changes, runtime errors, etc.).</p>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke/#scope","title":"Scope","text":"<ul> <li>Reuse the existing <code>healtharchive-backend/scripts/verify_public_surface.py</code>   verifier by running it against locally started backend + frontend.</li> <li>Seed a tiny SQLite database + tiny WARC so that:</li> <li><code>/api/sources</code> is non-empty,</li> <li><code>/api/search</code> returns at least one snapshot,</li> <li><code>/api/snapshots/raw/{id}</code> can serve real HTML.</li> <li>Add GitHub Actions jobs to run the smoke check in:</li> <li><code>healtharchive-backend</code> CI (checks backend changes against latest frontend <code>main</code>),</li> <li><code>healtharchive-frontend</code> CI (checks frontend changes against latest backend <code>main</code>).</li> <li>Update canonical docs to describe the smoke check and local reproduction steps.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke/#non-goals","title":"Non-goals","text":"<ul> <li>Full browser automation (Playwright/Cypress).</li> <li>Replay (pywb) service validation; the CI smoke should not depend on replay being present.</li> <li>Performance benchmarking or search quality evaluation.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke/#implementation-steps","title":"Implementation steps","text":"<ol> <li>Add a backend script to seed an e2e dataset:</li> <li>Create schema (SQLite).</li> <li>Insert seed sources.</li> <li>Write a tiny gzipped WARC with one HTML record.</li> <li>Insert a <code>Snapshot</code> pointing at that WARC record.</li> <li>Add a backend script to run the e2e smoke:</li> <li>Start backend via <code>uvicorn</code> on a fixed local port.</li> <li>Start frontend via <code>next build</code> + <code>next start</code> on a fixed local port.</li> <li>Run <code>verify_public_surface.py</code> with <code>--api-base</code> and <code>--frontend-base</code>      pointing at the local servers, and <code>--skip-replay</code>.</li> <li>Ensure clean shutdown on success/failure.</li> <li>Wire the CI workflows (backend + frontend) to run the smoke script as a    separate job alongside the existing unit/lint checks.</li> <li>Update docs:</li> <li>Add a short section to <code>docs/operations/monitoring-and-ci-checklist.md</code>      describing what the smoke covers and how to run it locally.</li> <li>Remove this item from <code>docs/roadmaps/roadmap.md</code>.</li> </ol>"},{"location":"roadmaps/implemented/2026-01-03-ci-e2e-smoke/#acceptance-criteria","title":"Acceptance criteria","text":"<ul> <li>Running the smoke locally succeeds:</li> <li><code>healtharchive-backend/scripts/ci-e2e-smoke.sh</code></li> <li>Backend CI includes an \u201ce2e smoke\u201d job that is green on <code>main</code>.</li> <li>Frontend CI includes an \u201ce2e smoke\u201d job that is green on <code>main</code>.</li> <li>Canonical docs mention the smoke check and how to reproduce failures locally.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/","title":"Crawl-safe roadmap batch (UX + datasets + governance + outreach) \u2014 implementation plan","text":"<p>Status: completed (repo changes) (created 2026-01-03; Phase 0\u20133 completed 2026-01-03; Phase 4 scaffolding completed 2026-01-03; archived 2026-01-03)</p> <p>Note: This plan is explicitly selected because it is safe to work on while the annual scrape/crawl is running:</p> <ul> <li>No backend crawler changes are required.</li> <li>No production VPS restarts or DB migrations are required.</li> <li>The only \u201clive\u201d effects are optional: frontend deploys (Vercel) and GitHub Actions changes (datasets release pipeline), which do not touch the running crawl.</li> </ul> <p>If a step would impact production infrastructure (e.g., branch protection rules that block emergency fixes), treat it as opt-in and reversible, and schedule it deliberately.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#goal","title":"Goal","text":"<p>Deliver four roadmap items (selected from <code>docs/roadmaps/roadmap.md</code>) with minimal operational risk:</p> <p>1) Frontend archive \u201cpower controls\u201d (URL-param based) - Make advanced archive/search controls clearly discoverable and reliably shareable via URL parameters:   - <code>view=pages|snapshots</code>   - <code>includeNon2xx=true</code>   - <code>includeDuplicates=true</code></p> <p>2) Dataset release pipeline hardening (healtharchive-datasets) - Make quarterly dataset releases more reproducible and less flaky by enforcing:   - retries/backoff that are explicit and diagnosable,   - manifest validation against the project\u2019s export integrity contract,   - checksum verification as a required pre-publish step.</p> <p>3) Repo governance / merge discipline (future-ready) - Define and implement a branch policy that scales from \u201csolo-fast\u201d to \u201cmultiple committers\u201d without surprises:   - required CI checks as deploy gates,   - PR-only + required checks when/if multiple committers,   - clear backout procedures to avoid lockouts.</p> <p>4) External/IRL outreach + verification - Execute a public-safe, privacy-safe outreach workflow to secure:   - at least 1 distribution partner (permission to name publicly),   - at least 1 verifier (permission to name publicly),   - a lightweight mentions/citations log (links only),   - Healthchecks.io alignment for new/changed timers (ongoing hygiene).</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#why-this-is-next-roadmap-selection","title":"Why this is \u201cnext\u201d (roadmap selection)","text":"<p>This batch is intentionally chosen while the annual crawl is running because it:</p> <ul> <li>Is implementable without touching the running crawler/worker.</li> <li>Improves the project\u2019s usefulness (UX), defensibility (dataset integrity), and maintainability (governance).</li> <li>Uses existing project scaffolding (CI workflows, ops docs, outreach templates) instead of inventing new systems.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#docs-setup-do-first-before-coding","title":"Docs setup (do first, before coding)","text":"<p>This repo separates backlog vs implementation plans vs canonical docs to avoid drift (see <code>docs/documentation-guidelines.md</code>).</p> <p>1) Create this plan doc - File: <code>docs/roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch.md</code> (this document)</p> <p>2) Backlog linkage - Update <code>docs/roadmaps/roadmap.md</code> to link the four selected backlog items to this plan, marked \u201cactive plan\u201d.</p> <p>3) Roadmaps index - Update <code>docs/roadmaps/README.md</code> to list this plan under \u201cImplementation plans (active)\u201d.</p> <p>4) Canonical docs that must remain accurate during/after implementation - Governance/CI guidance: <code>docs/operations/monitoring-and-ci-checklist.md</code> - Dataset integrity contract: <code>docs/operations/export-integrity-contract.md</code> - Dataset verification runbook: <code>docs/operations/dataset-release-runbook.md</code> - Frontend implementation guide: <code>healtharchive-frontend/docs/implementation-guide.md</code> - External work templates:   - <code>docs/operations/outreach-templates.md</code>   - <code>docs/operations/partner-kit.md</code>   - <code>docs/operations/verification-packet.md</code>   - <code>docs/operations/mentions-log-template.md</code></p> <p>Rule: keep canonical docs describing what exists and how to use/operate it; keep this plan describing what we will do and in what order.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#scope-goals-constraints","title":"Scope, goals, constraints","text":""},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#in-scope-outcomes-what-we-will-deliver","title":"In-scope outcomes (what we will deliver)","text":"<p>Frontend - <code>/archive</code> exposes and documents \u201cpower\u201d controls:   - <code>view=pages|snapshots</code> toggles grouping semantics.   - <code>includeNon2xx=true</code> includes non-2xx captures where supported.   - <code>includeDuplicates=true</code> includes duplicate captures where supported (only meaningful for <code>view=snapshots</code>). - All parameters are:   - stable (do not silently change meaning),   - shareable (copy/paste URL reproduces view),   - localized for English/French UI expectations (EN governs).</p> <p>Datasets - The datasets publish workflow fails fast on integrity problems:   - manifest missing required fields,   - <code>truncated=true</code>,   - checksum verification mismatch,   - pagination anomalies (non-advancing IDs, repeated IDs). - Failures are diagnosable:   - actionable error messages,   - artifacts/logs captured in GitHub Actions.</p> <p>Governance - A clear branch policy exists and is enforced appropriately for current reality:   - \u201cSolo-fast\u201d remains possible without excessive friction.   - \u201cMulti-committer\u201d mode is pre-designed and can be enabled quickly (PR-only + required checks).</p> <p>External - A repeatable outreach + verification workflow exists:   - target list creation, outreach cadence, follow-ups,   - public-safe documentation/logging,   - permission capture rules,   - clear \u201cdone\u201d definitions for partner + verifier.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#non-goals-explicitly-out-of-scope","title":"Non-goals (explicitly out of scope)","text":"<ul> <li>No changes to the crawler (<code>archive_tool</code>) or production crawl campaign while it is running.</li> <li>No backend DB migrations or API semantic changes as part of this batch.</li> <li>No re-architecture of dataset publishing (keep GitHub Releases + JSONL + manifest).</li> <li>No \u201cbig redesign\u201d of archive UX; keep changes incremental and reversible.</li> <li>No storing private contact details in git (ever).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#constraints-to-respect-project-resources-policy","title":"Constraints to respect (project resources + policy)","text":"<ul> <li>Small team / solo operator bias:</li> <li>Prefer simple, reversible changes over heavy process.</li> <li>Do not introduce ongoing maintenance burdens without clear benefit.</li> <li>Public-safety and privacy posture:</li> <li>Keep outreach artifacts public-safe (no private emails, no sensitive identifiers).</li> <li>Follow <code>docs/operations/data-handling-retention.md</code>.</li> <li>Operational stability:</li> <li>Avoid configuration changes that can lock you out of your repos or block emergency fixes.</li> <li>Prefer staged rollouts for governance changes (warn \u2192 enforce).</li> <li>Documentation hygiene:</li> <li>Keep one canonical doc per concept; link rather than duplicate.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#current-state-map-what-exists-today-to-build-on","title":"Current-state map (what exists today, to build on)","text":""},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#1-frontend-archive-power-controls","title":"1) Frontend archive \u201cpower controls\u201d","text":"<p>Backlog reference: <code>docs/roadmaps/roadmap.md</code> (\u201cArchive UX (frontend)\u201d).</p> <p>Observed implementation (audit required):</p> <ul> <li><code>/archive</code> route: <code>healtharchive-frontend/src/app/[locale]/archive/page.tsx</code></li> <li>Accepts <code>view</code>, <code>includeNon2xx</code>, <code>includeDuplicates</code> as URL params.</li> <li>Treats <code>includeDuplicates</code> as meaningful only when <code>view=snapshots</code>.</li> <li>Supporting component:</li> <li><code>healtharchive-frontend/src/components/archive/SearchWithinResults.tsx</code> (passes through view + filters)</li> <li>Existing tests:</li> <li><code>healtharchive-frontend/tests/searchWithinResults.test.tsx</code> (component behavior; not parameter semantics)</li> </ul> <p>Interpretation:</p> <ul> <li>This backlog item may be partially or fully implemented already.</li> <li>The work may therefore be: verify semantics + ensure \u201cdiscoverable + shareable\u201d is actually achieved, then update docs and remove/update the backlog bullet.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#2-dataset-release-pipeline-hardening","title":"2) Dataset release pipeline hardening","text":"<p>Backlog reference: <code>docs/roadmaps/roadmap.md</code> (\u201cDataset releases\u201d).</p> <p>Current implementation:</p> <ul> <li>Publish workflow:</li> <li><code>healtharchive-datasets/.github/workflows/publish-dataset-release.yml</code></li> <li>Release builder:</li> <li><code>healtharchive-datasets/scripts/build_release.py</code></li> <li>Contract + runbook (backend docs, canonical policy):</li> <li><code>docs/operations/export-integrity-contract.md</code></li> <li><code>docs/operations/dataset-release-runbook.md</code></li> </ul> <p>Current behavior summary:</p> <ul> <li>Script paginates exports via <code>afterId</code>, writes gzipped JSONL, writes <code>manifest.json</code>, writes <code>SHA256SUMS</code>.</li> <li>Script has basic retries/backoff for HTTP requests, and detects non-advancing pagination.</li> <li>Workflow creates or updates a GitHub Release for a date-based tag.</li> </ul> <p>Interpretation:</p> <ul> <li>The core pieces exist; \u201chardening\u201d should focus on:</li> <li>stricter validation aligned to the integrity contract,</li> <li>ensuring checksum verification is enforced before publishing,</li> <li>improving flake resilience and diagnostics (without overengineering).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#3-repo-governance-merge-discipline","title":"3) Repo governance / merge discipline","text":"<p>Backlog reference: <code>docs/roadmaps/roadmap.md</code> (\u201cRepo governance\u201d).</p> <p>Current scaffolding:</p> <ul> <li>CI exists in each repo and is referenced as the deploy gate (\u201cgreen main\u201d):</li> <li>Backend: <code>healtharchive-backend/CONTRIBUTING.md</code> (pre-push hook, <code>make check</code>)</li> <li>Frontend: <code>healtharchive-frontend/CONTRIBUTING.md</code> (pre-push hook, <code>npm run check</code>)</li> <li>Datasets: <code>healtharchive-datasets/CONTRIBUTING.md</code> (<code>make check</code>)</li> <li>CODEOWNERS + PR templates exist for backend/frontend:</li> <li><code>healtharchive-backend/.github/CODEOWNERS</code></li> <li><code>healtharchive-backend/.github/pull_request_template.md</code></li> <li><code>healtharchive-frontend/.github/CODEOWNERS</code></li> <li><code>healtharchive-frontend/.github/pull_request_template.md</code></li> <li>CI + monitoring guidance already exists:</li> <li><code>docs/operations/monitoring-and-ci-checklist.md</code></li> </ul> <p>Interpretation:</p> <ul> <li>Governance work is primarily:</li> <li>GitHub repo settings (branch protection),</li> <li>small doc updates (when policy changes),</li> <li>optionally adding missing governance artifacts to datasets repo (CODEOWNERS / PR template).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#4-external-irl-outreach-verification","title":"4) External / IRL outreach + verification","text":"<p>Backlog reference: <code>docs/roadmaps/roadmap.md</code> (\u201cExternal / IRL work\u201d).</p> <p>Existing docs/templates:</p> <ul> <li>Outreach templates: <code>docs/operations/outreach-templates.md</code></li> <li>Partner kit (internal guide; points at canonical public assets): <code>docs/operations/partner-kit.md</code></li> <li>Verification packet outline: <code>docs/operations/verification-packet.md</code></li> <li>Mentions log template: <code>docs/operations/mentions-log-template.md</code></li> <li>Adoption signals playbook (quarterly; VPS-stored entries): <code>docs/operations/playbooks/adoption-signals.md</code></li> <li>Data-handling constraints (privacy): <code>docs/operations/data-handling-retention.md</code></li> </ul> <p>Interpretation:</p> <ul> <li>The main \u201cwork\u201d here is execution discipline and public-safe recording.</li> <li>The plan should standardize:</li> <li>where logs live (VPS vs git),</li> <li>what is allowed to be stored in git,</li> <li>how permissions are tracked without leaking private contact info.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#definition-of-done-dod-acceptance-criteria","title":"Definition of Done (DoD) + acceptance criteria","text":""},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#frontend-power-controls","title":"Frontend power controls","text":"<p>1) Behavior - <code>view=pages|snapshots</code> always yields consistent grouping behavior. - <code>includeNon2xx=true</code> is preserved through \u201cSearch within results\u201d flows and does not get lost on pagination. - <code>includeDuplicates=true</code>:   - is honored only when <code>view=snapshots</code>,   - is ignored/cleared when <code>view=pages</code> (no confusing \u201cenabled but ineffective\u201d state),   - round-trips through the UI as expected.</p> <p>2) Discoverability - The <code>/archive</code> UI makes it obvious that:   - there is a \u201cPages vs All snapshots\u201d view,   - \u201cInclude errors\u201d exists,   - \u201cInclude duplicates\u201d exists (and explains the constraint to snapshots view).</p> <p>3) Shareability - Copy/pasting the URL reproduces the same view and filters reliably.</p> <p>4) Docs - The frontend implementation guide documents these query params in the <code>/archive</code> section.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#dataset-release-pipeline-hardening","title":"Dataset release pipeline hardening","text":"<p>1) Integrity enforcement - <code>manifest.json</code> is validated against <code>docs/operations/export-integrity-contract.md</code> before publishing. - Checksum verification is performed and must pass before publishing. - <code>truncated=true</code> results in a hard failure (no publish).</p> <p>2) Reliability - Transient network failures do not cause immediate abort; retries/backoff occur with clear logging. - Workflow avoids accidental double publishes (e.g., concurrency control for scheduled runs).</p> <p>3) Diagnostics - On failure, the workflow provides:   - a clear failure reason,   - enough artifacts/logs to debug without rerunning blindly.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#repo-governance-merge-discipline","title":"Repo governance / merge discipline","text":"<p>1) Policy clarity - It is explicit when the project is in \u201csolo-fast\u201d vs \u201cmulti-committer\u201d mode. - The rules are written down in the appropriate canonical doc(s).</p> <p>2) Enforcement - When \u201cmulti-committer\u201d mode is enabled:   - PR-only merges into <code>main</code>,   - required status checks are enforced,   - code owners review is required where intended.</p> <p>3) Safety - There is a tested \u201cbackout\u201d procedure for branch protection misconfigurations (avoid lockout).</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#external-irl-outreach-verification","title":"External / IRL outreach + verification","text":"<p>1) Execution - At least one distribution partner is secured (permission to name publicly). - At least one verifier is secured (permission to name publicly).</p> <p>2) Public-safe logging - Mentions/citations log exists and is maintained with links only; no private contact data.</p> <p>3) Ongoing hygiene - Healthchecks.io alignment process is defined (what to check, how to keep <code>/etc/healtharchive/healthchecks.env</code> in sync with Healthchecks).</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#risk-register-pre-mortem","title":"Risk register (pre-mortem)","text":""},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#governance-risks","title":"Governance risks","text":"<ul> <li>Risk: Branch protection rules accidentally block urgent fixes or lock out maintainers.</li> <li>Mitigation: stage changes; keep \u201cadmin bypass\u201d on initially; document recovery; test on a non-main branch rule first if possible.</li> <li>Risk: Required checks are flaky \u2192 PRs get stuck.</li> <li>Mitigation: fix flakiness first; require only high-signal checks; keep an escape hatch documented.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#dataset-release-risks","title":"Dataset release risks","text":"<ul> <li>Risk: Publishing updates an existing tag/release and silently changes a \u201cresearch object\u201d.</li> <li>Mitigation: define an immutability policy; constrain \u201callowUpdates\u201d behavior; require explicit operator intent for corrections.</li> <li>Risk: Export endpoint returns incomplete data (truncation, pagination bug) \u2192 dataset release is wrong but published.</li> <li>Mitigation: strict manifest validation; enforce <code>truncated=false</code>; verify monotonic IDs; add sanity thresholds.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#frontend-ux-risks","title":"Frontend UX risks","text":"<ul> <li>Risk: Power controls confuse casual users.</li> <li>Mitigation: sensible defaults; progressive disclosure; small inline help text/tooltips.</li> <li>Risk: URL params become a \u201csoft API contract\u201d that is later broken.</li> <li>Mitigation: treat params as stable once documented; add tests to prevent regressions.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#outreach-risks","title":"Outreach risks","text":"<ul> <li>Risk: Accidentally storing private contact info in git.</li> <li>Mitigation: explicit rules; use a private tracker for contacts; keep public logs link-only; review before committing.</li> <li>Risk: Naming an organization publicly without permission.</li> <li>Mitigation: default to \u201cPending\u201d and do not name until permission is explicit.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-plan-sequential","title":"Phase plan (sequential)","text":"<p>This plan is intentionally sequential. Complete each phase (and its verification) before moving to the next.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-0-lock-scope-evidence-audit-no-code-changes-yet","title":"Phase 0 \u2014 Lock scope + evidence audit (no code changes yet)","text":"<p>Objective: confirm what already exists, define exact deltas, and avoid duplicate/phantom work.</p> <p>0.1 Inventory the exact backlog bullets covered - Source: <code>docs/roadmaps/roadmap.md</code> - Record the four items and treat them as the only scope for this plan.</p> <p>0.2 Perform a reality-check audit for each item - Frontend power controls:   - Confirm <code>/archive</code> supports all target URL params.   - Confirm controls are visible in the UI when the backend is available.   - Confirm the params round-trip through:     - \u201cApply\u201d form submission,     - pagination links,     - \u201cSearch within results\u201d flow. - Dataset pipeline:   - Confirm current workflow actually performs retries/backoff (in script) and how failures surface in Actions.   - Confirm whether checksum verification is currently enforced (likely not).   - Confirm whether manifest fields match <code>docs/operations/export-integrity-contract.md</code>. - Governance:   - Inventory current GitHub repo settings (out of git):     - branch protection on <code>main</code>,     - required checks,     - PR requirements,     - admin bypass. - Outreach:   - Decide where private contact tracking will live (not in git).   - Decide where public-safe logs will live (git vs VPS).</p> <p>0.3 Decide what \u201cdone\u201d means for already-implemented items - If frontend power controls are already fully implemented:   - The work becomes: add missing tests/docs, then remove/update the backlog bullet. - If dataset pipeline already meets integrity contract:   - The work becomes: enforce verification in CI/workflow, and improve diagnostics.</p> <p>Phase exit criteria: - A short audit note (can be in the PR description later) listing:   - what is already done,   - what is missing,   - what will be changed in the next phases.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-0-findings-completed-2026-01-03","title":"Phase 0 findings (completed 2026-01-03)","text":"<p>Summary: this batch is viable and crawl-safe. One of the four items (\u201cpower controls\u201d) appears already implemented in code, and is likely \u201cremaining docs/tests polish\u201d rather than \u201cnew UI build\u201d.</p> <p>Frontend power controls</p> <ul> <li>Current <code>/archive</code> URL params already exist and appear wired end-to-end:</li> <li>Route: <code>healtharchive-frontend/src/app/[locale]/archive/page.tsx</code></li> <li>Supported params observed: <code>view</code>, <code>includeNon2xx</code>, <code>includeDuplicates</code> (plus core <code>q/source/from/to/sort/page/pageSize</code>).</li> <li><code>includeDuplicates</code> is only applied when <code>view=snapshots</code> (both for backend calls and for pagination URL building).</li> <li>\u201cSearch within results\u201d round-trips these filters through hidden inputs:</li> <li>Component: <code>healtharchive-frontend/src/components/archive/SearchWithinResults.tsx</code></li> <li>Existing test covers basic \u201creveals and submits\u201d behavior:<ul> <li><code>healtharchive-frontend/tests/searchWithinResults.test.tsx</code></li> </ul> </li> <li>Likely remaining gaps (to confirm in Phase 3 before changing anything):</li> <li>Documentation: <code>healtharchive-frontend/docs/implementation-guide.md</code> does not currently appear to clearly list the <code>/archive</code> query params as a stable contract.</li> <li>Regression tests: there is no dedicated test coverage asserting param semantics such as \u201cduplicates only in snapshots view\u201d.</li> </ul> <p>Dataset release pipeline hardening</p> <ul> <li>Baseline functionality exists and already includes some hardening:</li> <li>Publish workflow: <code>healtharchive-datasets/.github/workflows/publish-dataset-release.yml</code></li> <li>Builder script:<ul> <li>Retries/backoff exist for JSON fetch and NDJSON streaming.</li> <li>Pagination safety exists (non-advancing IDs fail).</li> <li>Outputs: gzipped JSONL exports + <code>manifest.json</code> + <code>SHA256SUMS</code>.</li> <li>File: <code>healtharchive-datasets/scripts/build_release.py</code></li> </ul> </li> <li>Confirmed remaining hardening needs (Phase 2 scope stays valid):</li> <li>Explicit manifest validation aligned to <code>docs/operations/export-integrity-contract.md</code>.</li> <li>Explicit checksum verification as a required step before publishing (e.g., <code>sha256sum -c SHA256SUMS</code>).</li> <li>Workflow-level hardening such as concurrency control, timeouts, and failure artifacts.</li> <li>Decide/tag immutability posture (workflow currently allows release updates via <code>allowUpdates: true</code>).</li> </ul> <p>Repo governance / merge discipline</p> <ul> <li>The \u201csolo-fast\u201d posture and CI-as-deploy-gate are already documented and supported:</li> <li><code>healtharchive-backend/CONTRIBUTING.md</code></li> <li><code>healtharchive-frontend/CONTRIBUTING.md</code></li> <li>Canonical policy doc: <code>docs/operations/monitoring-and-ci-checklist.md</code></li> <li>CI workflows and stable job IDs exist (see Phase 1 inventory).</li> <li>Unknown (operator-only): current branch protection settings in GitHub UI; audit required before changing anything.</li> </ul> <p>External / IRL outreach + verification</p> <ul> <li>Templates and public-safe scaffolding exist:</li> <li><code>docs/operations/outreach-templates.md</code></li> <li><code>docs/operations/partner-kit.md</code></li> <li><code>docs/operations/verification-packet.md</code></li> <li><code>docs/operations/mentions-log-template.md</code></li> <li>Unknown (operator-only): current real-world status (partners contacted, verifier candidates, existing mentions).</li> <li>Recommendation for execution hygiene:</li> <li>Keep private contact tracking out of git entirely.</li> <li>Create/maintain a public-safe mentions log only once there are public links (and permission to name).</li> </ul> <p>Healthchecks alignment (operator-only; completed)</p> <ul> <li>Healthchecks.io checks were aligned to enabled timers and <code>/etc/healtharchive/healthchecks.env</code> on the VPS.</li> <li>Added daily checks for:</li> <li><code>healtharchive-annual-search-verify</code>, <code>healtharchive-change-tracking</code>,     <code>healtharchive-coverage-guardrails</code>, <code>healtharchive-replay-smoke</code></li> <li>Fixed annual sentinel configuration:</li> <li><code>healtharchive-annual-campaign-sentinel</code> is configured as yearly with sufficient grace.</li> <li>Intentional exception:</li> <li><code>healtharchive-cleanup-automation</code> remains disabled and is not monitored in Healthchecks (see <code>docs/operations/playbooks/healthchecks-parity.md</code>).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-1-repo-governance-merge-discipline-design-staged-enforcement","title":"Phase 1 \u2014 Repo governance / merge discipline (design + staged enforcement)","text":"<p>Objective: align process to project reality and avoid brittle enforcement that harms velocity.</p> <p>1.1 Establish two explicit governance modes</p> <p>Mode A: Solo-fast (current recommended posture) - Direct pushes to <code>main</code> permitted. - CI runs on every push; \u201cgreen main\u201d is the deploy gate. - Local hooks are the primary guardrail:   - backend: <code>./scripts/install-pre-push-hook.sh</code> (runs <code>make check</code>)   - frontend: <code>./scripts/install-pre-push-hook.sh</code> (runs <code>npm run check</code>)</p> <p>Mode B: Multi-committer (future posture) - PR-only merges into <code>main</code>. - Required status checks enforced by branch protection. - Code owner review required where appropriate.</p> <p>Decision trigger (explicit): - Switch to Mode B when there is more than one regular committer, or when you want stricter enforcement than \u201csocial contract + hooks\u201d.</p> <p>1.2 Document the policy (canonical docs) - Primary reference: <code>docs/operations/monitoring-and-ci-checklist.md</code> - Ensure it clearly states:   - what \u201cgreen main\u201d means,   - what checks are required,   - when to switch governance modes,   - how to recover from a misconfiguration.</p> <p>1.3 Audit CI check names and stability (per repo) - Backend:   - Identify the exact GitHub Actions workflow + job names that should be required in Mode B.   - Ensure they are stable (renaming breaks branch protection). - Frontend:   - Same: identify stable check names. - Datasets:   - Identify stable check names from <code>healtharchive-datasets/.github/workflows/datasets-ci.yml</code>.</p> <p>Current check name inventory (as of 2026-01-03; confirm in GitHub UI before locking protections):</p> <ul> <li>Backend repo:</li> <li>Workflow: <code>Backend CI</code> (<code>healtharchive-backend/.github/workflows/backend-ci.yml</code>)</li> <li>Jobs:<ul> <li><code>test</code> (required check name usually appears as <code>Backend CI / test</code>)</li> <li><code>e2e-smoke</code> (required check name usually appears as <code>Backend CI / e2e-smoke</code>)</li> </ul> </li> <li>Frontend repo:</li> <li>Workflow: <code>Frontend CI</code> (<code>healtharchive-frontend/.github/workflows/frontend-ci.yml</code>)</li> <li>Jobs:<ul> <li><code>lint-and-test</code> (required check name usually appears as <code>Frontend CI / lint-and-test</code>)</li> <li><code>e2e-smoke</code> (required check name usually appears as <code>Frontend CI / e2e-smoke</code>)</li> </ul> </li> <li>Datasets repo:</li> <li>Workflow: <code>Datasets CI</code> (<code>healtharchive-datasets/.github/workflows/datasets-ci.yml</code>)</li> <li>Jobs:<ul> <li><code>lint</code> (required check name usually appears as <code>Datasets CI / lint</code>)</li> </ul> </li> </ul> <p>Important:</p> <ul> <li>Do not rename workflow/job IDs after you enable required checks, or GitHub will treat them as missing.</li> <li>Cross-repo e2e checks require a token if the sibling repo is private:</li> <li>Secret: <code>HEALTHARCHIVE_CI_READ_TOKEN</code> (documented in <code>docs/operations/monitoring-and-ci-checklist.md</code>).</li> </ul> <p>1.4 Implement missing governance artifacts (datasets repo) - Add <code>CODEOWNERS</code> and a PR template to datasets repo if absent, consistent with backend/frontend. - Ensure CONTRIBUTING covers pre-push hook guidance if you want \u201csolo-fast\u201d there too.</p> <p>1.5 Stage branch protection changes (operator actions in GitHub UI)</p> <p>Staged rollout recommended:</p> <ul> <li>Stage 1 (safe baseline):</li> <li>Protect <code>main</code> from force-pushes and branch deletion.</li> <li>Enable required status checks for PR merges only (does not block direct pushes).</li> <li>Keep admin bypass enabled initially.</li> <li>Stage 2 (enable Mode B):</li> <li>Require PRs to merge into <code>main</code>.</li> <li>Require status checks to pass before merge.</li> <li>Optionally require code owner review.</li> <li>Optionally require up-to-date branches (only if it doesn\u2019t create constant rebase churn).</li> </ul> <p>Backout plan (must be written before enabling Mode B): - Document how to temporarily relax protections if CI is broken or an emergency fix is required. - Ensure at least one maintainer account can always edit branch protection rules.</p> <p>Phase exit criteria: - Policy is documented. - Datasets repo has parity on governance artifacts (if chosen). - A checklist exists for turning Mode B on/off without lockout.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-1-notes-completed-2026-01-03","title":"Phase 1 notes (completed 2026-01-03)","text":"<ul> <li>Policy documentation updated (canonical): <code>docs/operations/monitoring-and-ci-checklist.md</code></li> <li>Adds Mode A vs Mode B framing, includes datasets in \u201crun checks before push\u201d, and records a check-name inventory.</li> <li>Datasets repo governance artifacts added:</li> <li><code>.github/CODEOWNERS</code>, <code>.github/pull_request_template.md</code></li> <li><code>scripts/install-pre-push-hook.sh</code> + <code>CONTRIBUTING.md</code> pre-push guidance</li> <li>Branch protection changes were intentionally not applied (solo-dev posture); defer Mode B until there are multiple committers.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-2-dataset-release-pipeline-hardening-healtharchive-datasets","title":"Phase 2 \u2014 Dataset release pipeline hardening (healtharchive-datasets)","text":"<p>Objective: make releases defensible, reproducible, and less flaky, using the existing contract.</p> <p>2.1 Convert the integrity contract into executable checks</p> <p>Source of truth: <code>healtharchive-backend/docs/operations/export-integrity-contract.md</code>.</p> <p>Implement (in datasets repo) a validation routine that asserts at minimum:</p> <ul> <li>Manifest required fields exist:</li> <li><code>version</code>, <code>tag</code>, <code>releasedAtUtc</code>, <code>apiBase</code>, <code>exportsManifest</code>, <code>artifacts.snapshots</code>, <code>artifacts.changes</code>.</li> <li>Export artifacts required fields exist (for both snapshots/changes):</li> <li><code>rows</code>, <code>minId</code>, <code>maxId</code>, <code>requestsMade</code>, <code>limitPerRequest</code>, <code>truncated</code>, <code>sha256</code>, <code>filename</code>.</li> <li>Invariants:</li> <li><code>truncated</code> is <code>false</code> (or else fail hard).</li> <li><code>rows</code> is non-negative; if <code>rows==0</code>, treat as suspicious and require explicit operator override.</li> <li><code>minId &lt;= maxId</code> when rows &gt; 0.</li> <li><code>requestsMade &gt;= 1</code> when rows &gt; 0.</li> </ul> <p>2.2 Enforce checksum verification before publish</p> <p>Hard requirement:</p> <ul> <li>Before creating/uploading the GitHub Release, run:</li> <li><code>sha256sum -c SHA256SUMS</code> (or Python equivalent on all platforms).</li> </ul> <p>Additionally recommended:</p> <ul> <li>Smoke-validate gzip integrity:</li> <li>decompress stream (without fully loading into memory),</li> <li>ensure JSONL lines parse and contain required <code>id_field</code>.</li> </ul> <p>2.3 Improve retry/backoff and diagnostics without overengineering</p> <p>Goals:</p> <ul> <li>Reduce transient flake failures (timeouts, 502/503, temporary network issues).</li> <li>Produce logs that explain:</li> <li>which request failed,</li> <li>which attempt number,</li> <li>what the response status/body snippet was (capped).</li> </ul> <p>Recommended improvements:</p> <ul> <li>Add jittered exponential backoff (avoid thundering herd).</li> <li>Special-case retryable HTTP statuses (429, 502, 503, 504).</li> <li>Ensure timeouts are sane for large exports:</li> <li>per-request timeout (already exists),</li> <li>possibly a per-export overall timeout via workflow <code>timeout-minutes</code>.</li> <li>Add a clear summary at the end:</li> <li>row counts, min/max IDs, checksum values.</li> </ul> <p>2.4 Add workflow hardening</p> <p>In <code>healtharchive-datasets/.github/workflows/publish-dataset-release.yml</code>:</p> <ul> <li>Add <code>concurrency</code> to prevent overlapping scheduled runs.</li> <li>Set explicit <code>timeout-minutes</code> for the publish job.</li> <li>Upload <code>dist/</code> as an artifact on failure (so you can inspect partial outputs).</li> <li>Decide release immutability posture:</li> <li>If tags must be immutable:<ul> <li>disable silent updates,</li> <li>or allow updates only with an explicit workflow input \u201callow_update=true\u201d and only when the previous release is flagged incomplete.</li> </ul> </li> </ul> <p>2.5 Update runbooks/docs to match reality</p> <ul> <li>Ensure <code>healtharchive-datasets/README.md</code> and   <code>healtharchive-backend/docs/operations/dataset-release-runbook.md</code> remain accurate:</li> <li>what integrity checks run automatically,</li> <li>what to do on failure,</li> <li>how to verify a release locally (<code>sha256sum -c</code>).</li> </ul> <p>Phase exit criteria: - Dataset publish workflow refuses to publish if validation fails. - Failures are diagnosable from Actions logs + artifacts. - Docs accurately reflect the new safeguards.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-2-notes-completed-2026-01-03","title":"Phase 2 notes (completed 2026-01-03)","text":"<ul> <li>Added release bundle validation (datasets repo):</li> <li><code>healtharchive-datasets/scripts/validate_release_bundle.py</code></li> <li>Enforces required <code>manifest.json</code> fields + invariants (<code>truncated=false</code>), verifies artifact SHA-256, and validates gzip integrity.</li> <li>Hardened the publish workflow (datasets repo):</li> <li><code>healtharchive-datasets/.github/workflows/publish-dataset-release.yml</code></li> <li>Adds checksum verification, runs bundle validation before publishing, adds concurrency + timeout, uploads <code>dist/</code> on failure, and defaults to immutable tags (updates only via manual dispatch override).</li> <li>Updated docs/runbooks:</li> <li><code>healtharchive-datasets/README.md</code></li> <li><code>healtharchive-backend/docs/operations/dataset-release-runbook.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-3-frontend-archive-power-controls-verify-polish-document","title":"Phase 3 \u2014 Frontend archive \u201cpower controls\u201d (verify + polish + document)","text":"<p>Objective: ensure the advanced archive controls are truly discoverable and reliably shareable.</p> <p>3.1 Confirm current behavior against the acceptance criteria</p> <p>Audit checklist (in local dev / review):</p> <ul> <li><code>/archive</code> with no params defaults to <code>view=pages</code>.</li> <li><code>/archive?view=snapshots</code> shows \u201cAll snapshots\u201d.</li> <li><code>/archive?includeNon2xx=true</code>:</li> <li>stays enabled after hitting \u201cApply\u201d,</li> <li>stays enabled after pagination.</li> <li><code>/archive?view=snapshots&amp;includeDuplicates=true</code>:</li> <li>shows \u201cInclude duplicates\u201d enabled,</li> <li>persists through \u201cApply\u201d + pagination.</li> <li>Switching from <code>view=snapshots</code> to <code>view=pages</code> clears the duplicates effect:</li> <li>ideally, the URL also becomes free of <code>includeDuplicates=true</code> to avoid misleading share links.</li> </ul> <p>3.2 Fill the gaps (if any)</p> <p>Potential missing pieces (only do what the audit proves is missing):</p> <ul> <li>UI discoverability improvements:</li> <li>make the \u201cShow: Pages / All snapshots\u201d control visible in a consistent place,</li> <li>add short inline help for what \u201cduplicates\u201d means,</li> <li>ensure controls appear only when meaningful (or are visibly disabled with an explanation).</li> <li>URL canonicalization:</li> <li>keep URLs clean (do not include ineffective flags).</li> </ul> <p>3.3 Add regression tests (frontend)</p> <p>Add tests that lock in URL-param semantics and prevent regressions, e.g.:</p> <ul> <li><code>ArchivePage</code> parsing and canonicalization:</li> <li><code>view</code> parsing defaults and validation,</li> <li><code>includeDuplicates</code> only applying in snapshots view,</li> <li>URL round-trip behavior for \u201cApply\u201d and \u201cSearch within results\u201d.</li> </ul> <p>Constraints:</p> <ul> <li>Avoid adding tests that require a live backend.</li> <li>Keep tests deterministic (use existing mocking patterns).</li> </ul> <p>3.4 Document the parameters (frontend docs)</p> <p>Update <code>healtharchive-frontend/docs/implementation-guide.md</code> to include:</p> <ul> <li>Supported <code>/archive</code> query params:</li> <li><code>q</code>, <code>within</code>, <code>source</code>, <code>from</code>, <code>to</code>, <code>sort</code>, <code>page</code>, <code>pageSize</code>,</li> <li><code>view</code>, <code>includeNon2xx</code>, <code>includeDuplicates</code>.</li> <li>A short explanation of each, including constraints (duplicates only relevant for snapshots view).</li> </ul> <p>Phase exit criteria: - UX is discoverable and stable. - URL params are documented. - Tests exist to prevent regressions.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-3-notes-completed-2026-01-03","title":"Phase 3 notes (completed 2026-01-03)","text":"<ul> <li>Added URL canonicalization so <code>includeDuplicates</code> is removed automatically when <code>view=pages</code> (no-op flag).</li> <li>Implementation: <code>healtharchive-frontend/src/app/[locale]/archive/page.tsx</code></li> <li>Improved UI discoverability with an inline tooltip explaining what \u201cInclude duplicates\u201d means (snapshots view).</li> <li>Implementation: <code>healtharchive-frontend/src/app/[locale]/archive/page.tsx</code></li> <li>Added regression tests covering URL semantics and filter round-trips:</li> <li><code>healtharchive-frontend/tests/archive.test.tsx</code></li> <li><code>healtharchive-frontend/tests/searchWithinResults.test.tsx</code></li> <li>Documented the stable <code>/archive</code> query param contract:</li> <li><code>healtharchive-frontend/docs/implementation-guide.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-4-external-irl-outreach-verification-execution-playbook","title":"Phase 4 \u2014 External / IRL outreach + verification (execution playbook)","text":"<p>Objective: run outreach and verification work with a disciplined, privacy-safe workflow.</p> <p>4.1 Decide where tracking data lives (privacy-first)</p> <p>Hard rules:</p> <ul> <li>Never store private contact details in git.</li> <li>Public-safe logs may be stored in git only if they contain:</li> <li>public links,</li> <li>public organization names (only with permission),</li> <li>no emails/phone numbers/private notes.</li> </ul> <p>Recommended split:</p> <ul> <li>Private contact tracker (not in git): a local notes file, password manager notes, or a private spreadsheet.</li> <li>Public-safe logs:</li> <li>Mentions/citations log: use <code>docs/operations/mentions-log-template.md</code> as the format.</li> <li>Adoption signals entries (quarterly): store on VPS under <code>/srv/healtharchive/ops/adoption/</code> (see <code>docs/operations/playbooks/adoption-signals.md</code>).</li> </ul> <p>4.2 Build a target list (distribution partners + verifiers)</p> <p>Distribution partner candidates (examples; tailor to reality):</p> <ul> <li>University libraries \u201cdigital scholarship\u201d resource pages.</li> <li>Public health methods resource lists.</li> <li>Journalism \u201ctools\u201d pages.</li> <li>Research group reproducibility toolkits.</li> </ul> <p>Verifier candidates:</p> <ul> <li>Librarian (digital scholarship / archives),</li> <li>Researcher in reproducibility / STS / public health communication,</li> <li>Editor/maintainer of a relevant public methods list.</li> </ul> <p>For each candidate, collect privately:</p> <ul> <li>name, role, organization,</li> <li>contact channel,</li> <li>why they are a fit,</li> <li>which template to use (A/B/C).</li> </ul> <p>4.3 Outreach cadence (execute using templates)</p> <p>Use <code>docs/operations/outreach-templates.md</code>:</p> <ul> <li>Send initial outreach (Template A/B/C).</li> <li>Follow-up at 1 week.</li> <li>Final follow-up at 2 weeks.</li> </ul> <p>Record (privately):</p> <ul> <li>date sent,</li> <li>template used,</li> <li>outcome (no response / declined / interested / accepted).</li> </ul> <p>4.4 Partner kit usage (public assets)</p> <p>Reference internal guide: <code>docs/operations/partner-kit.md</code>.</p> <p>Operational checklist:</p> <ul> <li>Ensure <code>/brief</code> and <code>/cite</code> pages are accurate and public-safe.</li> <li>Prepare screenshots per the checklist in the partner kit guide.</li> <li>When a partner agrees to link:</li> <li>confirm permission to name publicly,</li> <li>ask preferred wording (if any),</li> <li>record the public link once it exists.</li> </ul> <p>4.5 Verification workflow</p> <p>Use <code>docs/operations/verification-packet.md</code> as the packet outline:</p> <ul> <li>Fill metrics from <code>/status</code> (public numbers only).</li> <li>Provide the verifier a short list of what verification means, and ask for permission to name them publicly.</li> <li>Record verification outcome privately and (if permitted) in a public-safe mentions log.</li> </ul> <p>4.6 Mentions/citations log (public-safe)</p> <p>Create a real mentions log (not just the template) and keep it current:</p> <ul> <li>Start from <code>docs/operations/mentions-log-template.md</code>.</li> <li>Add entries only when there is a public link and permission to name (or record \u201cPending\u201d).</li> </ul> <p>4.7 Healthchecks.io alignment (ongoing hygiene)</p> <p>Define a quarterly (or per-change) procedure:</p> <ul> <li>Inventory timers/services that ping Healthchecks.</li> <li>Confirm <code>/etc/healtharchive/healthchecks.env</code> matches Healthchecks checks.</li> <li>Create missing checks when new timers are added.</li> </ul> <p>Canonical refs:</p> <ul> <li><code>docs/deployment/systemd/README.md</code></li> <li><code>docs/deployment/production-single-vps.md</code></li> </ul> <p>Phase exit criteria: - At least one distribution partner and verifier secured (with permission). - Public-safe mentions log exists and has real entries. - Healthchecks alignment procedure is documented and followed.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-4-notes-scaffolding-completed-2026-01-03-execution-is-operator-only","title":"Phase 4 notes (scaffolding completed 2026-01-03; execution is operator-only)","text":"<ul> <li>Added an operator playbook to run outreach and verification work in a privacy-safe way:</li> <li><code>docs/operations/playbooks/outreach-and-verification.md</code></li> <li>Created a public-safe mentions log (link-only; do not name without permission):</li> <li><code>docs/operations/mentions-log.md</code></li> <li>Updated ops indexes so the playbook and log are discoverable:</li> <li><code>docs/operations/README.md</code>, <code>docs/operations/playbooks/README.md</code>, <code>docs/operations/ops-cadence-checklist.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-5-closeout-docs-backlog-hygiene","title":"Phase 5 \u2014 Closeout (docs + backlog hygiene)","text":"<p>Objective: ensure the roadmap reflects reality and the work is maintainable.</p> <p>5.1 Update canonical docs - Ensure any modified behavior is documented in the canonical locations (frontend guide, ops runbooks, etc.).</p> <p>5.2 Update the backlog - In <code>docs/roadmaps/roadmap.md</code>:   - remove completed items,   - or rewrite them into a smaller \u201cnext\u201d follow-up item if partial work remains.</p> <p>5.3 Archive the plan - When complete, move this plan to:   - <code>docs/roadmaps/implemented/</code> with a dated filename. - Update <code>docs/roadmaps/implemented/README.md</code> and <code>docs/roadmaps/README.md</code> accordingly.</p> <p>Phase exit criteria: - Backlog is accurate and short. - This plan is archived as history.</p>"},{"location":"roadmaps/implemented/2026-01-03-crawl-safe-roadmap-batch/#phase-5-notes-completed-2026-01-03","title":"Phase 5 notes (completed 2026-01-03)","text":"<ul> <li>Updated backlog (<code>docs/roadmaps/roadmap.md</code>) so completed items are removed and ongoing operator-only work points to playbooks/logs instead of an \u201cactive plan\u201d.</li> <li>Archived this plan and updated roadmap indexes (<code>docs/roadmaps/README.md</code>, <code>docs/roadmaps/implemented/README.md</code>).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verification-json/","title":"Ops automation verification JSON output (implementation plan)","text":"<p>Status: implemented (2026-01-03)</p>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verification-json/#goal","title":"Goal","text":"<p>Make the production posture check <code>./scripts/verify_ops_automation.sh</code> easier to diff and reason about by adding a JSON output mode that summarizes:</p> <ul> <li>which expected timer units exist,</li> <li>which are enabled/active,</li> <li>sentinel file presence (when applicable),</li> <li>worker override presence,</li> <li>expected ops directories presence,</li> <li>and a top-level pass/fail.</li> </ul> <p>This should preserve the current human-readable default output.</p>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verification-json/#scope","title":"Scope","text":"<ul> <li>Add a <code>--json</code> option to <code>scripts/verify_ops_automation.sh</code>.</li> <li>Ensure JSON mode writes JSON to stdout only (human logs go to stderr, or are suppressed).</li> <li>Update ops documentation to mention JSON mode and a suggested diff workflow.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verification-json/#non-goals","title":"Non-goals","text":"<ul> <li>Changing which timers are considered required by default.</li> <li>Adding new timers or altering systemd unit files.</li> <li>Building a separate CI job to run this (it is intended for the VPS).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verification-json/#implementation-steps","title":"Implementation steps","text":"<ol> <li>Extend <code>scripts/verify_ops_automation.sh</code>:</li> <li>Parse <code>--json</code>.</li> <li>Collect per-check results into an in-memory summary.</li> <li>Emit a stable JSON object at the end of the run.</li> <li>On hosts without <code>systemctl</code>, in JSON mode emit a \u201cskipped\u201d JSON payload and exit <code>0</code>.</li> <li>Update canonical ops docs:</li> <li><code>docs/operations/automation-verification-rituals.md</code></li> <li><code>docs/operations/ops-cadence-checklist.md</code></li> <li>Update <code>docs/roadmaps/README.md</code> to list this plan as active.</li> </ol>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verification-json/#acceptance-criteria","title":"Acceptance criteria","text":"<ul> <li><code>./scripts/verify_ops_automation.sh</code> (default) behaves as before.</li> <li><code>./scripts/verify_ops_automation.sh --json</code> prints a single valid JSON object to stdout.</li> <li>Exit codes remain compatible:</li> <li><code>0</code> when all required checks pass,</li> <li><code>1</code> when required checks fail.</li> <li>JSON includes at least:</li> <li><code>timers[]</code> (name, required, unit_present, enabled_state, active_state, sentinel_path, sentinel_present, meets_required),</li> <li><code>worker_override</code> (path, present, required),</li> <li><code>ops_dirs[]</code> (path, present),</li> <li><code>failures</code>, <code>warnings</code>, <code>ok</code>.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verification-json/#operator-usage","title":"Operator usage","text":"<ul> <li>Human mode: <code>./scripts/verify_ops_automation.sh</code></li> <li>JSON mode (diff-friendly): <code>./scripts/verify_ops_automation.sh --json &gt; /srv/healtharchive/ops/automation/posture.json</code></li> <li>Pretty-print (optional): <code>./scripts/verify_ops_automation.sh --json | python3 -m json.tool</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verifier-improvements/","title":"Ops automation verifier improvements (implementation plan)","text":"<p>Status: implemented (2026-01-03)</p>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verifier-improvements/#roadmap-item-single-prioritized","title":"Roadmap item (single, prioritized)","text":"<p>Harden <code>./scripts/verify_ops_automation.sh</code> so it is:</p> <ul> <li>easier to run strictly (one flag instead of many),</li> <li>easier to consume by automation (clean JSON-only mode + stable schema),</li> <li>easier to maintain (single \u201cexpected timers\u201d inventory),</li> <li>and harder to regress (basic CI test for JSON output invariants).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verifier-improvements/#goals","title":"Goals","text":"<ol> <li>Keep human output useful while keeping JSON output truly machine-friendly.</li> <li>Reduce drift by defining expected timers/dirs in one place.</li> <li>Make \u201cstrict posture\u201d checks easy for operators.</li> <li>Add minimal regression coverage for JSON output.</li> </ol>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verifier-improvements/#scope","title":"Scope","text":""},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verifier-improvements/#script-improvements-scriptsverify_ops_automationsh","title":"Script improvements (<code>scripts/verify_ops_automation.sh</code>)","text":"<ul> <li>Add <code>--quiet</code> (suppress all human logs) and <code>--json-only</code> (implies <code>--json --quiet</code>).</li> <li>Add convenience flags:</li> <li><code>--require-all-present</code> (fail if any expected timer unit is missing)</li> <li><code>--require-all-enabled</code> (fail if any expected timer is not enabled; implies <code>--require-all-present</code>)</li> <li>Centralize the expected timers list into a single inventory structure and drive checks from it.</li> <li>Emit a concise end-of-run summary in human mode:</li> <li><code>failures</code>, <code>warnings</code>, <code>missing_optional</code>, <code>disabled_optional</code> (best-effort)</li> <li>Extend JSON with a <code>summary</code> object and (best-effort) <code>unexpected_timers[]</code>.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verifier-improvements/#docs-improvements-ops","title":"Docs improvements (ops)","text":"<ul> <li>Document <code>--json-only</code> and posture snapshot/diff conventions in:</li> <li><code>docs/operations/automation-verification-rituals.md</code></li> <li><code>docs/operations/ops-cadence-checklist.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verifier-improvements/#tests-backend","title":"Tests (backend)","text":"<ul> <li>Add one pytest test that asserts JSON mode invariants:</li> <li>stdout is a single JSON object line</li> <li>parses successfully</li> <li>includes required top-level keys (<code>schema_version</code>, <code>skipped</code>, <code>ok</code>, <code>failures</code>, <code>warnings</code>)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verifier-improvements/#non-goals","title":"Non-goals","text":"<ul> <li>Changing systemd unit file behavior or timer schedules.</li> <li>Expanding the set of production automation units.</li> <li>Adding complex script configuration that would weaken posture checks.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verifier-improvements/#implementation-steps","title":"Implementation steps","text":"<ol> <li>Update <code>scripts/verify_ops_automation.sh</code>:</li> <li>add flags and usage text</li> <li>refactor timers into a single expected inventory</li> <li>implement strict flags + summary + JSON-only output</li> <li>Update ops docs to reflect the new flags and recommended posture snapshot workflow.</li> <li>Add the JSON invariants test under <code>tests/</code>.</li> <li>Run <code>make check</code>.</li> <li>Move this plan to <code>docs/roadmaps/implemented/</code> and update the implemented index.</li> </ol>"},{"location":"roadmaps/implemented/2026-01-03-ops-automation-verifier-improvements/#acceptance-criteria","title":"Acceptance criteria","text":"<ul> <li>Default human output remains readable and exit-code behavior remains compatible.</li> <li><code>--json</code> continues to emit JSON to stdout (logs to stderr).</li> <li><code>--json-only</code> emits only JSON to stdout and nothing else.</li> <li>Strict flags work as intended:</li> <li><code>--require-all-present</code> fails on any missing timer unit</li> <li><code>--require-all-enabled</code> fails on any expected timer not enabled</li> <li>Tests pass in CI: <code>make check</code>.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/","title":"Search ranking + snippet quality iteration (v3) \u2014 implementation plan","text":"<p>Status: Completed (2026-01-18)</p> <p>Note: Do not deploy/implement this plan on the production VPS until the annual scrape/crawl is finished and the campaign jobs are <code>indexed</code>.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#goal","title":"Goal","text":"<p>Iterate on:</p> <ul> <li>Search relevance (especially broad \u201chub intent\u201d queries like <code>covid</code>) using Postgres FTS + lightweight heuristics, without introducing a separate search service.</li> <li>Snippet quality so results \u201cfeel human\u201d (less navigation/cookie/banner boilerplate; more meaningful page content).</li> </ul> <p>Deliver this as:</p> <ul> <li>A new ranking version v3 (opt-in via <code>ranking=v3</code>, later set as default via <code>HA_SEARCH_RANKING_VERSION=v3</code> after evaluation).</li> <li>Index-time extraction improvements for <code>Snapshot.title</code> / <code>Snapshot.snippet</code> / <code>Snapshot.language</code> (and optionally a derived archived flag), with safe refresh/backfill workflows.</li> <li>A repeatable, artifacts-backed evaluation loop using existing golden query tooling.</li> </ul> <p>This plan is intentionally sequential: complete each phase before starting the next.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#why-this-is-next-roadmap-selection","title":"Why this is \u201cnext\u201d (roadmap selection)","text":"<p>This is the next highest-leverage item in <code>docs/roadmaps/roadmap.md</code> that is:</p> <ul> <li>Implementable in git (unlike external/IRL partnership work),</li> <li>High impact for core project purpose (research citations + change tracking depend on discoverability),</li> <li>Compatible with single\u2011VPS constraints and current architecture (Postgres FTS + heuristics),</li> <li>Not \u201cautomation work\u201d (explicitly excluded for now).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#docs-setup-do-first-before-coding","title":"Docs setup (do first, before coding)","text":"<p>This repo separates backlog vs implementation plans vs canonical docs to avoid drift (see <code>../documentation-guidelines.md</code>).</p> <p>1) Create this plan doc - File: <code>docs/roadmaps/2026-01-03-search-ranking-and-snippets-v3.md</code> (this document)</p> <p>2) Backlog linkage - Update <code>docs/roadmaps/roadmap.md</code>:   - Replace the \u201cSearch ranking + snippet quality iteration\u2026\u201d backlog bullet with a link to this plan, marked \u201cin progress\u201d.</p> <p>3) Roadmaps index - Update <code>docs/roadmaps/README.md</code> to list this plan under \u201cImplementation plans (active)\u201d.</p> <p>4) Canonical docs to keep accurate during/after implementation - Ops evaluation:   - <code>docs/operations/search-quality.md</code>   - <code>docs/operations/search-golden-queries.md</code> - Deployment/runbooks:   - <code>docs/deployment/search-rollout.md</code> (extend from v2 \u2192 v3)   - <code>docs/architecture.md</code> (search section remains accurate)</p> <p>Rule: keep canonical docs describing what exists and how to use it; keep this plan describing what we will do and in what order.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#scope-goals-constraints","title":"Scope, goals, constraints","text":""},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#in-scope-outcomes-what-we-will-deliver","title":"In-scope outcomes (what we will deliver)","text":"<ul> <li>Snippets that \u201cfeel human\u201d for common queries:</li> <li>less nav/boilerplate (e.g., \u201cSkip to content \u2026 Menu \u2026 Search \u2026\u201d),</li> <li>more meaningful page body text,</li> <li>stable and reasonably readable across sources.</li> <li>Search relevance improvement for curated golden queries using repeatable captures + diffs.</li> <li>Ranking version v3:</li> <li><code>ranking=v3</code> works for <code>/api/search</code> and <code>/api/admin/search-debug</code>,</li> <li>later make v3 the default via <code>HA_SEARCH_RANKING_VERSION=v3</code> after evaluation.</li> <li>Repeatable evaluation artifacts:</li> <li>captures + diff reports stored outside git (e.g., <code>/srv/healtharchive/ops/search-eval</code>),</li> <li>documented pass/fail rubric for changes.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#non-goals-explicitly-out-of-scope","title":"Non-goals (explicitly out of scope)","text":"<ul> <li>No new search infrastructure (no Elasticsearch/Meilisearch/etc.).</li> <li>No query logging system (golden queries remain curated).</li> <li>No ops automation/timers/healthchecks work in this effort.</li> <li>No major re-architecture of WARC storage/replay retention.</li> <li>No large UX redesign (frontend already supports <code>view=pages|snapshots</code>, <code>includeNon2xx</code>, <code>includeDuplicates</code>).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#constraints-to-respect-project-resources-policy","title":"Constraints to respect (project resources + policy)","text":"<ul> <li>Single-VPS reality + Postgres FTS is the core engine (see <code>docs/operations/search-quality.md</code>).</li> <li>Performance budgets (from <code>docs/operations/growth-constraints.md</code>):</li> <li>Search (view=pages): p95 target &lt; 2s for common queries (regression guardrail).</li> <li>Provenance and trustworthiness:</li> <li>Index-time metadata changes are allowed, but replay fidelity and WARC linkage must remain intact.</li> <li>Minimal operational complexity:</li> <li>Rollout and rollback must be simple and reversible.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#current-state-map-what-exists-today-to-build-on","title":"Current-state map (what exists today, to build on)","text":""},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#search-surfaces","title":"Search surfaces","text":"<ul> <li>Public endpoint: <code>GET /api/search</code></li> <li>Implementation: <code>src/ha_backend/api/routes_public.py</code></li> <li>Admin debug endpoint: <code>GET /api/admin/search-debug</code></li> <li>Implementation: <code>src/ha_backend/api/routes_admin.py</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#ranking-versions-and-configuration","title":"Ranking versions and configuration","text":"<ul> <li>Ranking config and query-mode logic:</li> <li><code>src/ha_backend/search_ranking.py</code> (<code>v1</code>, <code>v2</code>)</li> <li>Default ranking via env var:</li> <li><code>HA_SEARCH_RANKING_VERSION</code> (see <code>docs/deployment/search-rollout.md</code>)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#golden-query-evaluation-tooling","title":"Golden query evaluation tooling","text":"<ul> <li>Evaluation docs:</li> <li><code>docs/operations/search-quality.md</code></li> <li><code>docs/operations/search-golden-queries.md</code></li> <li>Scripts:</li> <li>Capture: <code>scripts/search-eval-capture.sh</code></li> <li>Capture+diff wrapper: <code>scripts/search-eval-run.sh</code></li> <li>Diff: <code>scripts/search-eval-diff.py</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#snippet-extraction-and-indexing-pipeline","title":"Snippet extraction and indexing pipeline","text":"<ul> <li>HTML \u2192 title/text/snippet/language extraction:</li> <li><code>src/ha_backend/indexing/text_extraction.py</code></li> <li>Indexing pipeline uses extraction:</li> <li><code>src/ha_backend/indexing/pipeline.py</code></li> <li>Metadata refresh CLI:</li> <li><code>ha-backend refresh-snapshot-metadata --job-id &lt;ID&gt;</code> (wired in <code>src/ha_backend/cli.py</code>)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#tests-to-extend-existing-coverage","title":"Tests to extend (existing coverage)","text":"<ul> <li>Search API behavior:</li> <li><code>tests/test_api_search_and_snapshot.py</code></li> <li>Admin search debug coverage:</li> <li><code>tests/test_admin_search_debug.py</code></li> <li>Fuzzy search:</li> <li><code>tests/test_search_fuzzy.py</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#definition-of-done-dod-acceptance-criteria","title":"Definition of Done (DoD) + acceptance criteria","text":""},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#relevance-golden-queries","title":"Relevance (golden queries)","text":"<p>For the curated list in <code>docs/operations/search-golden-queries.md</code>:</p> <ul> <li>Broad queries surface expected hub pages in top 10 more consistently under v3 than v2.</li> <li>Anti-results (obvious junk/boilerplate/error surfaces) are less likely to appear in top 20 for broad queries.</li> <li>No major regressions for non\u2011COVID guardrail queries.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#snippet-quality","title":"Snippet quality","text":"<p>On a representative sample of results from golden query captures:</p> <ul> <li>Snippets do not frequently start with navigation boilerplate.</li> <li>Snippets contain at least one meaningful sentence/phrase from the page body for common pages.</li> <li>Language detection remains at least as good as today for en/fr smoke tests.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#performance-safety","title":"Performance + safety","text":"<ul> <li>No meaningful p95 regression for <code>GET /api/search</code> common queries in production.</li> <li>No API schema breaks; the frontend continues working unchanged.</li> <li>Rollback remains a single env-var flip (same operational safety as v2).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#recommendations-for-key-design-decisions-resolve-before-implementation","title":"Recommendations for key design decisions (resolve before implementation)","text":"<p>These are the recommended choices given HealthArchive goals/purposes, single\u2011VPS resources, and modern best practices (explicit derived features + reversible rollouts).</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#1-introduce-snapshotis_archived-recommended-yes-small-schema-change","title":"1) Introduce <code>Snapshot.is_archived</code> (recommended: yes, small schema change)","text":"<p>Recommendation: add a dedicated archived flag column (tri-state: <code>NULL</code> unknown, <code>true</code>, <code>false</code>) and compute it at indexing/refresh time from title + HTML/text signals.</p> <p>Why:</p> <ul> <li>Decouples snippet quality work from ranking quality. We want cleaner snippets, but we also   want archived detection to remain reliable and explainable.</li> <li>Makes ranking cheaper and more stable than query-time <code>ilike()</code> heuristics over snippet text.</li> <li>Supports future UX or research needs (\u201cinclude/exclude archived\u201d) without re-parsing HTML.</li> </ul> <p>Operational approach:</p> <ul> <li>Add <code>snapshots.is_archived</code> via Alembic migration.</li> <li>Update index-time extraction to compute it.</li> <li>Backfill existing rows via:</li> <li>targeted <code>refresh-snapshot-metadata --job-id ...</code> on recent/high-impact jobs first, then expand,</li> <li>or a dedicated backfill CLI if refresh is too slow for full history.</li> <li>In ranking:</li> <li>Prefer <code>is_archived</code> when column exists and non-NULL,</li> <li>Fall back to the existing title/snippet heuristics for rows where <code>is_archived IS NULL</code> during the transition window.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#2-fts-vector-body-text-length-recommended-4kb-of-cleaned-main-content","title":"2) FTS vector body text length (recommended: 4KB of cleaned main content)","text":"<p>Recommendation: feed ~4096 characters of cleaned main-content text into Postgres FTS vectors, in addition to title and URL, while keeping the UI snippet short (~280 chars).</p> <p>Why:</p> <ul> <li>2KB often under-captures key terms on long guidance pages; 4KB improves recall meaningfully.</li> <li>8KB increases backfill CPU + GIN index size and can unintentionally dilute ranking via length normalization on a small VPS.</li> <li>4KB is a conservative, modern default that still leaves headroom to tune later.</li> </ul> <p>Implementation approach (conceptual):</p> <ul> <li>Keep <code>Snapshot.snippet</code> as UI-facing (short).</li> <li>Compute an internal <code>content_text_for_fts</code> from the extraction pipeline (cleaned main content),   then truncate to 4KB before calling <code>to_tsvector</code>.</li> <li>Keep title weight highest; content weight medium; URL weight low (current pattern).</li> </ul> <p>Optional (only if we need flexibility later): add a single env var like <code>HA_SEARCH_VECTOR_BODY_MAX_CHARS</code> defaulting to <code>4096</code>, but avoid config sprawl unless we have a real tuning need.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#3-minimal-snippet-heuristics-for-canadaca-without-overfitting-recommended-generic-first","title":"3) Minimal snippet heuristics for Canada.ca without overfitting (recommended: generic-first)","text":"<p>Recommendation: implement a small, generic, evidence-driven set of heuristics:</p> <p>1) DOM pruning (generic)    - Remove <code>script</code>, <code>style</code>, <code>noscript</code> (already).    - Remove semantic boilerplate containers: <code>nav</code>, <code>header</code>, <code>footer</code>, <code>aside</code>, <code>form</code> (already).    - Add ARIA-role pruning: <code>[role=navigation]</code>, <code>[role=banner]</code>, <code>[role=contentinfo]</code>, <code>[role=search]</code>.</p> <p>2) Content-root selection (generic)    - Prefer <code>&lt;main&gt;</code> / <code>[role=main]</code>.    - Else prefer <code>&lt;article&gt;</code>.    - Else choose the \u201cbest\u201d container by a simple score:      - + text length,      - \u2212 link density penalty,      - \u2212 boilerplate phrase penalty.</p> <p>3) Snippet selection (generic)    - Build snippet from the first \u201cgood\u201d paragraph/sentence chunk that:      - meets a minimum length threshold,      - contains sentence-like punctuation,      - does not match a small bilingual boilerplate phrase list (skip links, cookie consent, \u201cmenu/search\u201d lines),      - is not mostly a list of navigation links.</p> <p>Canada.ca-specific handling should be additive only if needed:</p> <ul> <li>Start generic; evaluate.</li> <li>If Canada.ca pages still leak boilerplate consistently, add a very small optional rule set gated by hostname (endswith <code>canada.ca</code>)   to drop known WET wrapper regions, but only when they are clearly navigation/boilerplate (high link density, low text).</li> </ul> <p>This avoids overfitting while still acknowledging the archive\u2019s heavy Canada.ca coverage.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#phase-1-baseline-capture-problem-inventory-no-behavior-changes-yet","title":"Phase 1 \u2014 Baseline capture + problem inventory (no behavior changes yet)","text":"<p>Objective: establish \u201cbefore\u201d truth for relevance + snippet issues and ensure we can measure improvements.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#11-lock-the-evaluation-protocol-what-we-always-capture","title":"1.1 Lock the evaluation protocol (what we always capture)","text":"<p>Decide and document (in this plan) the exact capture matrix:</p> <ul> <li>Primary user-facing relevance:</li> <li><code>view=pages</code></li> <li><code>sort=relevance</code></li> <li><code>pageSize=20</code> (or 10; pick once and keep stable)</li> <li>Debugging noise / capture-level issues:</li> <li><code>view=snapshots</code></li> <li><code>sort=relevance</code></li> </ul> <p>Decide how diffs key items:</p> <ul> <li>For <code>view=pages</code>: prefer <code>originalUrl</code> (with URL canonicalization).</li> <li>For <code>view=snapshots</code>: prefer <code>id</code> when URL churn makes comparisons confusing.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#12-run-baseline-captures-production","title":"1.2 Run baseline captures (production)","text":"<p>Use the existing scripts; store artifacts outside git.</p> <p>Recommended output directory on the VPS:</p> <ul> <li><code>/srv/healtharchive/ops/search-eval</code></li> </ul> <p>Baseline runs to capture:</p> <ul> <li><code>v1</code> vs <code>v2</code> (explicit <code>ranking=v1</code> and <code>ranking=v2</code>), even if v2 is already default.</li> <li>(Optional) a \u201cdefault\u201d capture (no <code>ranking=</code>) to confirm env wiring.</li> </ul> <p>Artifacts to keep:</p> <ul> <li>Capture dirs: <code>&lt;run-id&gt;-v1/</code>, <code>&lt;run-id&gt;-v2/</code></li> <li>Diff report: <code>&lt;run-id&gt;.diff.txt</code></li> <li>Notes: <code>&lt;run-id&gt;.notes.md</code> summarizing:</li> <li>top obvious improvements/regressions,</li> <li>snippet failure examples,</li> <li>any surprising anti-results.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#13-create-a-snippet-failure-taxonomy-evidence-driven","title":"1.3 Create a snippet failure taxonomy (evidence-driven)","text":"<p>From baseline captures, classify failures with real examples:</p> <ul> <li>Nav/header boilerplate leaks into snippet.</li> <li>Cookie banners / consent text dominates snippet.</li> <li>\u201cArchived page\u201d banner dominates snippet (English/French variants).</li> <li>Empty/near-empty snippet despite content.</li> <li>Garbage whitespace / repeated tokens / broken decoding.</li> </ul> <p>For each category, record 3\u201310 representative URLs (or snapshot IDs) and the observed snippet text.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#14-establish-a-lightweight-performance-baseline","title":"1.4 Establish a lightweight performance baseline","text":"<p>Record at least one timing snapshot for:</p> <ul> <li>broad query: <code>covid</code> (<code>view=pages</code>)</li> <li>medium query: <code>covid vaccine</code></li> <li>French query: <code>grippe</code></li> </ul> <p>Goal: detect \u201cobvious\u201d regressions later, not produce a full benchmark suite.</p> <p>Deliverables:</p> <ul> <li>Baseline capture dirs + diff reports in <code>/srv/healtharchive/ops/search-eval</code></li> <li>Taxonomy + example list in this plan doc (append a short section under Phase 1)</li> <li>A short \u201cbaseline timing\u201d note</li> </ul> <p>Exit criteria: agreement on what \u201cbetter\u201d means and which snippet failures matter most.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#phase-2-design-snippet-extraction-improvements-index-time-minimal-risk","title":"Phase 2 \u2014 Design: snippet extraction improvements (index-time, minimal-risk)","text":"<p>Objective: define the smallest reliable extraction upgrade that improves snippets without destabilizing indexing.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#21-design-principles","title":"2.1 Design principles","text":"<ul> <li>Keep dependencies minimal (prefer improving BeautifulSoup heuristics over adding heavy libraries).</li> <li>Prefer deterministic extraction that behaves similarly across runs.</li> <li>Avoid query-time snippet generation (too expensive unless we store more text).</li> <li>Preserve provenance: snippets are derived metadata; do not alter WARC linkage or replay.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#22-extraction-algorithm-upgrade-target-srcha_backendindexingtext_extractionpy","title":"2.2 Extraction algorithm upgrade (target: <code>src/ha_backend/indexing/text_extraction.py</code>)","text":"<p>Define (before coding) the concrete strategy:</p> <p>1) Pre-clean DOM:    - Remove <code>script</code>, <code>style</code>, <code>noscript</code>.    - Remove semantic boilerplate tags: <code>nav</code>, <code>header</code>, <code>footer</code>, <code>aside</code>, <code>form</code>.    - Remove ARIA boilerplate roles: navigation/banner/contentinfo/search.</p> <p>2) Choose a content root:    - Prefer <code>&lt;main&gt;</code> or <code>[role=main]</code>.    - Else prefer <code>&lt;article&gt;</code>.    - Else score candidate containers (<code>article</code>, <code>section</code>, <code>div</code>) and pick the best:      - text length (positive),      - punctuation density (positive),      - link density (negative),      - boilerplate phrase match (negative).</p> <p>3) Extract candidate blocks:    - Prefer paragraph-like blocks (<code>p</code>, headings + following paragraphs) rather than full-page text.    - Preserve enough context for a meaningful snippet, but avoid dumping navigation lists.</p> <p>4) Snippet selection:    - Choose the first \u201cgood\u201d block meeting criteria; fall back to next best; final fallback to current behavior.    - Ensure the snippet does not start with known boilerplate phrases.    - Keep stable max length (e.g., ~280 chars) and clean whitespace.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#23-archived-detection-decision-commit-to-recommendation","title":"2.3 Archived detection decision (commit to recommendation)","text":"<p>Implement <code>Snapshot.is_archived</code> (Phase 3 will cover details). Snippet extraction should not need to keep archived banner text at the top in order to support ranking.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#24-fts-vector-input-decision-commit-to-recommendation","title":"2.4 FTS vector input decision (commit to recommendation)","text":"<p>Feed ~4KB of cleaned main content text into the search vector computation.</p> <p>Deliverables:</p> <ul> <li>A written extraction spec (heuristics + thresholds + bilingual boilerplate phrase list)</li> <li>A written archived detection spec (signals + conservatism rules)</li> <li>A written FTS text input spec (where content comes from; how it is truncated)</li> </ul> <p>Exit criteria: we can predict how the changes address the Phase\u20111 failures.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#phase-3-implement-test-snippet-extraction-improvements-archived-flag","title":"Phase 3 \u2014 Implement + test: snippet extraction improvements + archived flag","text":"<p>Objective: implement extraction changes safely with strong tests and a refresh/backfill path.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#31-implement-extraction-changes-code","title":"3.1 Implement extraction changes (code)","text":"<p>Files likely involved:</p> <ul> <li><code>src/ha_backend/indexing/text_extraction.py</code></li> <li><code>src/ha_backend/indexing/pipeline.py</code></li> <li><code>src/ha_backend/cli.py</code> (ensure refresh path stays aligned)</li> </ul> <p>Key requirements:</p> <ul> <li>Indexing and <code>refresh-snapshot-metadata</code> must share the same logic path (avoid drift).</li> <li>Snippet quality improves per taxonomy without breaking language detection.</li> <li>Content selection must degrade gracefully for malformed HTML.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#32-add-snapshotis_archived-schema-model-extraction","title":"3.2 Add <code>Snapshot.is_archived</code> (schema + model + extraction)","text":"<p>1) Add Alembic migration adding <code>snapshots.is_archived</code> (nullable boolean). 2) Update <code>src/ha_backend/models.py</code> <code>Snapshot</code> model. 3) Compute <code>is_archived</code> at indexing time based on conservative signals:    - title prefix patterns (e.g., <code>Archived...</code>),    - known bilingual banner phrases in extracted body text (not the UI snippet).</p> <p>Transition strategy:</p> <ul> <li>Until backfill is complete, ranking should treat <code>NULL</code> as \u201cunknown\u201d and fall back   to existing title/snippet heuristics (so we don\u2019t remove archived penalty accidentally).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#33-update-fts-vector-input-postgres","title":"3.3 Update FTS vector input (Postgres)","text":"<p>Ensure FTS vectors include:</p> <ul> <li>Title (high weight),</li> <li>Cleaned main content (medium weight; truncated to 4KB),</li> <li>URL (low weight),</li> </ul> <p>while the UI snippet remains short.</p> <p>Plan for backfill:</p> <ul> <li><code>ha-backend backfill-search-vector --force</code> off-peak, with progress logging.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#34-tests-focused-regression","title":"3.4 Tests (focused + regression)","text":"<p>Add unit tests for extraction using representative HTML fixtures.</p> <p>Assertions should cover:</p> <ul> <li>Boilerplate pruning removes nav/header/footer style text.</li> <li>Content-root selection works when <code>&lt;main&gt;</code> is missing.</li> <li>Snippets are stable, non-empty, and not dominated by boilerplate.</li> <li>Archived detection is conservative and bilingual-aware.</li> </ul> <p>Extend API-level tests to ensure:</p> <ul> <li>Search results include improved snippet text for seeded examples.</li> <li>Archived penalty behavior remains present (via is_archived or heuristic fallback).</li> </ul> <p>Deliverables:</p> <ul> <li>Extraction improvements implemented + tests passing (<code>make check</code>)</li> <li>Migration applied cleanly in dev/test environments</li> <li>Documented refresh/backfill procedure (Phase 7 will finalize canonical docs)</li> </ul> <p>Exit criteria: local tests pass; extraction is measurably better on the Phase\u20111 examples in local/staging.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#phase-4-design-ranking-v3-targeted-explainable-reversible","title":"Phase 4 \u2014 Design: ranking v3 (targeted, explainable, reversible)","text":"<p>Objective: define a small set of ranking changes that improve golden queries without regressions.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#41-use-evidence-not-vibes","title":"4.1 Use evidence (not vibes)","text":"<p>From baseline v1/v2 captures:</p> <ul> <li>Identify where v2 still fails (deep pages outranking hubs, archived pages surfacing too high, etc.).</li> <li>Identify whether snippet/vector improvements change recall in ways that may require coefficient retuning.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#42-define-v3-changes-keep-it-small","title":"4.2 Define v3 changes (keep it small)","text":"<p>Candidate levers (already present in the codebase in some form):</p> <ul> <li>Retune query-mode thresholds (<code>broad</code> / <code>mixed</code> / <code>specific</code>) for better blending.</li> <li>Retune coefficients in <code>src/ha_backend/search_ranking.py</code>.</li> <li>Penalties:</li> <li>querystring penalty and tracking penalty (possibly stronger for broad queries),</li> <li>depth penalty tuning (especially for <code>view=pages</code> grouping keys).</li> <li>Boosts:</li> <li>title token match boost,</li> <li>authority/hubness/pagerank blending when <code>page_signals</code> exists.</li> <li>Archived penalty:</li> <li>prefer <code>Snapshot.is_archived</code> when available and non-NULL (cheaper and more stable than snippet heuristics).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#43-ensure-v3-is-explainable-via-admin-debug-output","title":"4.3 Ensure v3 is explainable via admin debug output","text":"<p><code>/api/admin/search-debug</code> must:</p> <ul> <li>accept <code>ranking=v3</code>,</li> <li>show the same score breakdown components,</li> <li>remain stable enough for operators to reason about changes.</li> </ul> <p>Deliverables:</p> <ul> <li>A v3 scoring spec: formula components, coefficient table, and per-query-mode blends</li> <li>A \u201cv3 target fixes\u201d list mapped to golden queries</li> </ul> <p>Exit criteria: v3 spec is narrow, measurable, and reversible.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#phase-5-implement-test-ranking-v3-tooling-updates","title":"Phase 5 \u2014 Implement + test: ranking v3 + tooling updates","text":"<p>Objective: add v3 without breaking clients; extend tooling to capture/diff v3.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#51-introduce-v3-everywhere-it-must-exist","title":"5.1 Introduce <code>v3</code> everywhere it must exist","text":"<p>Backend surfaces:</p> <ul> <li>Ranking config + parsing:</li> <li><code>src/ha_backend/search_ranking.py</code></li> <li>Query param validation:</li> <li><code>src/ha_backend/api/routes_public.py</code> (<code>/api/search</code>)</li> <li><code>src/ha_backend/api/routes_admin.py</code> (<code>/api/admin/search-debug</code>)</li> </ul> <p>Requirements:</p> <ul> <li>v1 and v2 behavior remains unchanged.</li> <li>v3 is opt-in via <code>ranking=v3</code> until Phase 7.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#52-implement-v3-scoring-logic","title":"5.2 Implement v3 scoring logic","text":"<p>Key requirements:</p> <ul> <li>Works on Postgres (prod) and SQLite (tests/dev).</li> <li>Avoids expensive operations unless strictly necessary.</li> <li>Uses <code>Snapshot.is_archived</code> when present and non-NULL; falls back otherwise.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#53-update-evaluation-scripts","title":"5.3 Update evaluation scripts","text":"<p>Update scripts to allow capturing v3:</p> <ul> <li><code>scripts/search-eval-capture.sh</code> should accept <code>--ranking v3</code>.</li> <li><code>scripts/search-eval-run.sh</code> should:</li> <li>support v2 vs v3 comparisons, or</li> <li>support a 3-way capture mode (v1/v2/v3) if useful.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#54-extend-tests","title":"5.4 Extend tests","text":"<p>Add test fixtures that demonstrate v2 vs v3 differences aligned to the v3 spec.</p> <p>Also extend admin search debug tests to validate:</p> <ul> <li><code>ranking=v3</code> is accepted,</li> <li>score breakdown fields remain present and coherent.</li> </ul> <p>Deliverables:</p> <ul> <li>v3 implemented and test-covered</li> <li>scripts can capture/diff v3 results</li> </ul> <p>Exit criteria: v3 is usable via <code>ranking=v3</code>; tests pass.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#phase-6-evaluation-loop-golden-queries-coefficient-tuning","title":"Phase 6 \u2014 Evaluation loop (golden queries) + coefficient tuning","text":"<p>Objective: iterate until v3 reliably improves curated queries without obvious regressions.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#61-run-repeated-captures-v2-vs-v3","title":"6.1 Run repeated captures (v2 vs v3)","text":"<p>Capture matrix (minimum):</p> <ul> <li><code>view=pages</code> + <code>sort=relevance</code> (primary)</li> <li><code>view=snapshots</code> + <code>sort=relevance</code> (debugging)</li> </ul> <p>Store artifacts under:</p> <ul> <li><code>/srv/healtharchive/ops/search-eval</code></li> </ul> <p>For each iteration:</p> <ul> <li>Keep the diff report.</li> <li>Add a short note describing:</li> <li>which queries improved,</li> <li>which regressed,</li> <li>what coefficient/heuristic change was made.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#62-update-golden-expectations-carefully","title":"6.2 Update golden expectations carefully","text":"<p>Update <code>docs/operations/search-golden-queries.md</code> only when:</p> <ul> <li>archive coverage legitimately changed (new sources/pages),</li> <li>or expectations were incorrect/outdated.</li> </ul> <p>Do not \u201cmove goalposts\u201d to hide ranking regressions.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#63-performance-sanity-checks","title":"6.3 Performance sanity checks","text":"<p>Ensure v3 does not meaningfully regress production query times.</p> <p>If it does:</p> <ul> <li>simplify heuristics,</li> <li>ensure expensive joins happen only when needed,</li> <li>treat index changes as an explicit subtask (avoid scope creep).</li> </ul> <p>Deliverables:</p> <ul> <li>A short v3 evaluation report section in this plan:</li> <li>what improved,</li> <li>what regressed (if anything),</li> <li>final coefficients and rationale.</li> </ul> <p>Exit criteria: v3 is convincingly better on golden queries and not meaningfully slower.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#phase-7-production-rollout-reversible-like-v2-canonical-docs-updates","title":"Phase 7 \u2014 Production rollout (reversible like v2) + canonical docs updates","text":"<p>Objective: ship v3 safely with clear rollback and updated canonical docs.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#71-rollout-strategy-single-vps","title":"7.1 Rollout strategy (single VPS)","text":"<p>1) Deploy code with v3 available, keep default at v2. 2) Run production eval with <code>ranking=v3</code> explicitly (store capture artifacts). 3) Flip default:    - set <code>HA_SEARCH_RANKING_VERSION=v3</code>,    - restart API process.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#72-data-maintenance-tasks-if-required-by-earlier-phases","title":"7.2 Data maintenance tasks (if required by earlier phases)","text":"<p>Depending on what changed:</p> <ul> <li>If search vector inputs changed:</li> <li>run <code>ha-backend backfill-search-vector --force</code> off-peak.</li> <li>If snippet/title extraction changed materially:</li> <li>run <code>ha-backend refresh-snapshot-metadata --job-id ...</code> for recent/high-impact jobs first.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#73-update-canonical-docs-post-implementation","title":"7.3 Update canonical docs (post-implementation)","text":"<ul> <li><code>docs/deployment/search-rollout.md</code>:</li> <li>add v3 rollout + rollback steps,</li> <li>keep it current and reversible.</li> <li><code>docs/operations/search-quality.md</code>:</li> <li>ensure commands mention v3 where relevant.</li> <li><code>docs/architecture.md</code>:</li> <li>reflect v3 existence and any new derived fields (e.g., <code>is_archived</code>).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#74-rollback-plan","title":"7.4 Rollback plan","text":"<p>If v3 looks wrong in production:</p> <p>1) Set <code>HA_SEARCH_RANKING_VERSION=v2</code> (or <code>v1</code> if needed). 2) Restart API.</p> <p>Keep <code>ranking=v3</code> available for investigation even after rollback.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#75-close-out-the-plan","title":"7.5 Close out the plan","text":"<p>When complete:</p> <ul> <li>Move this plan to <code>docs/roadmaps/implemented/</code> with a dated filename.</li> <li>Ensure the backlog item is removed/updated in <code>docs/roadmaps/roadmap.md</code>.</li> </ul> <p>Deliverables:</p> <ul> <li>v3 live (or a documented \u201cno-go\u201d with evidence)</li> <li>canonical docs updated</li> <li>this plan archived under <code>docs/roadmaps/implemented/</code></li> </ul> <p>Exit criteria: production default is v3 and the golden-query eval is documented as having passed.</p>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#risk-register-pre-mortem","title":"Risk register (pre-mortem)","text":"<ul> <li>Risk: snippet cleanup removes signals used for ranking (archived detection).</li> <li>Mitigation: store <code>Snapshot.is_archived</code>; keep heuristic fallback for NULL during transition.</li> <li>Risk: improved FTS input reshuffles ranking too much.</li> <li>Mitigation: stage changes behind v3; evaluate via captures + diffs.</li> <li>Risk: backfills are heavy on a single VPS.</li> <li>Mitigation: batch, off-peak, tmux; prioritize newest jobs first; document stop/resume points.</li> <li>Risk: bilingual content behaves poorly with stemming/tokenization.</li> <li>Mitigation: keep <code>TS_CONFIG='simple'</code>; include French smoke tests in golden queries.</li> <li>Risk: regressions hidden by updated expectations.</li> <li>Mitigation: treat <code>docs/operations/search-golden-queries.md</code> as a contract; update only when coverage changes.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-03-search-ranking-and-snippets-v3/#phase-1-findings-to-be-filled-after-baseline","title":"Phase 1 findings (to be filled after baseline)","text":"<p>Add:</p> <ul> <li>Baseline run IDs and where artifacts were stored.</li> <li>Snippet failure taxonomy summary.</li> <li>Top 10 \u201cworst\u201d snippet examples (URL \u2192 snippet).</li> <li>Any obvious v1 vs v2 relevance observations worth carrying into v3 design.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/","title":"Storage Box / <code>sshfs</code> stale mount incident \u2014 prevention, auto-recovery, and data integrity (2026-01-08) \u2014 implementation plan","text":"<p>Status: implemented (completed 2026-01-16; created 2026-01-08)</p> <p>This plan documents a real production incident and turns it into a concrete, sequenced set of repo changes to:</p> <p>1) prevent recurrence (or at least detect it quickly), 2) automate safe recovery, and 3) preserve data integrity and crawl completeness when it happens anyway.</p> <p>It is intentionally very detailed so future operators can use it as:</p> <ul> <li>an incident postmortem reference,</li> <li>a runbook for manual recovery,</li> <li>and the canonical implementation plan for the code changes.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#executive-summary","title":"Executive summary","text":"<p>What happened: Several job output directories under <code>/srv/healtharchive/jobs/**</code> became unreadable with:</p> <ul> <li><code>OSError: [Errno 107] Transport endpoint is not connected</code></li> </ul> <p>This is a classic FUSE failure mode (commonly seen with <code>sshfs</code>) where a mountpoint stays present but the underlying connection is gone, so basic filesystem operations (<code>stat</code>, <code>is_dir</code>, <code>ls</code>, etc.) fail.</p> <p>Impact: The crawler/worker and related monitoring scripts attempted to <code>stat()</code> files under these mountpoints, threw exceptions, and the system degraded into a \u201clooks alive but isn\u2019t making progress\u201d state:</p> <ul> <li><code>archive_tool</code> crashed on <code>Path.is_file()</code> against a combined log path.</li> <li>The worker loop hit unexpected exceptions while preparing or updating jobs, leaving jobs in confusing states.</li> <li>Crawl metrics timer failed repeatedly because the metrics writer script crashed while probing a job output dir.</li> <li>The annual campaign was blocked: jobs appeared <code>running</code> or ended up <code>failed</code>, with <code>indexed_pages=0</code>.</li> </ul> <p>Recovery: We stopped the worker, identified the stale mountpoints, lazily unmounted them, re-applied the tiering mounts, recovered stale jobs to <code>retryable</code>, re-queued jobs for retry, restarted the worker, and confirmed crawl progress resumed (crawl counters increased; new <code>warc.gz</code> produced; stalled metric stayed 0).</p> <p>Root cause (proximate): job output paths were backed by <code>sshfs</code>/FUSE mounts that became disconnected. \u201cStorage Box mount service active\u201d did not imply these per-job hot paths were healthy. Monitoring and error handling were not robust to this failure mode.</p>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#incident-details-high-fidelity-narrative","title":"Incident details (high-fidelity narrative)","text":""},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#environment-and-components-involved","title":"Environment and components involved","text":"<p>This incident concerns the \u201cproduction single VPS\u201d deployment described in:</p> <ul> <li><code>docs/deployment/production-single-vps.md</code></li> </ul> <p>Key components in play:</p> <ul> <li><code>healtharchive-worker.service</code> (runs the backend worker loop and launches crawls via Docker).</li> <li><code>archive_tool</code> (in-tree crawler CLI under <code>src/archive_tool/</code>).</li> <li><code>healtharchive-crawl-metrics.timer</code> / <code>healtharchive-crawl-metrics.service</code> (writes node_exporter textfile metrics via <code>scripts/vps-crawl-metrics-textfile.py</code>).</li> <li>Storage tiering / mounts (scripts under <code>scripts/</code> + systemd units documented under <code>docs/deployment/systemd/</code>).</li> <li><code>scripts/vps-crawl-status.sh</code> (operator snapshot script used throughout this incident).</li> </ul> <p>The job output directory pattern affected:</p> <ul> <li><code>/srv/healtharchive/jobs/&lt;source&gt;/&lt;job_id_timestamp&gt;__&lt;job_name&gt;</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#observable-symptoms-what-we-saw","title":"Observable symptoms (what we saw)","text":"<p>At the filesystem layer:</p> <ul> <li><code>ls</code> against a job output dir failed with: <code>Transport endpoint is not connected</code></li> <li>directories showed as <code>d?????????</code> when listed (unstat\u2019able)</li> </ul> <p>At the service/script layer:</p> <ul> <li>The crawl metrics writer failed on a path probe:</li> <li><code>scripts/vps-crawl-metrics-textfile.py</code> crashed when calling <code>Path.is_dir()</code> because <code>Path.stat()</code> raised <code>OSError: [Errno 107] Transport endpoint is not connected</code>.</li> <li><code>archive_tool</code> crashed on a combined log probe:</li> <li>stack trace showed <code>src/archive_tool/main.py</code> calling <code>Path.is_file()</code> on the stage combined log path, which raised the same Errno 107.</li> <li>The worker loop logged \u201cUnexpected error in worker iteration\u201d with Errno 107 against per-job output paths.</li> </ul> <p>At the job/state layer (as surfaced by <code>scripts/vps-crawl-status.sh</code> and the worker logs):</p> <ul> <li>campaign jobs became blocked in <code>running</code>/<code>queued</code>/<code>failed</code> states that did not reflect real crawl progress.</li> <li>retries were consumed by infrastructure errors (mount failures), not by true crawl failures.</li> <li>indexing remained at 0 because crawls were not completing successfully.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#timeline-derived-from-the-operator-session-and-systemd-journal","title":"Timeline (derived from the operator session and systemd journal)","text":"<p>This timeline focuses on the causal chain; it is deliberately explicit about each observed step.</p> <p>1) Crawl had been progressing previously    - Earlier snapshots showed job 6 (hc) <code>running</code> with steadily increasing <code>crawled</code> counts and new <code>warc.gz</code> files appearing over time.</p> <p>2) Mountpoints became stale/unreadable    - <code>ls -la</code> under <code>/srv/healtharchive/jobs/hc/...</code> failed with <code>Transport endpoint is not connected</code>.    - The failing directories showed \u201cunknown\u201d metadata (<code>d?????????</code>) indicating <code>stat()</code> failed.</p> <p>3) Metrics writer began failing repeatedly    - <code>healtharchive-crawl-metrics.service</code> exited non-zero because it could not probe a job output dir without crashing.</p> <p>4) Crawl runs began failing in ways that looked like \u201cstalls\u201d    - <code>archive_tool</code> and the worker loop hit hard exceptions, leaving jobs stuck in <code>status=running</code> or <code>status=failed</code> without meaningful progress.</p> <p>5) Operator intervention recovered state    - Worker stopped, stale mountpoints unmounted, tiering re-applied, stale jobs marked retryable, worker restarted.</p>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#root-cause-analysis-rca","title":"Root cause analysis (RCA)","text":""},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#proximate-cause","title":"Proximate cause","text":"<p>One or more <code>sshfs</code>-backed mountpoints under <code>/srv/healtharchive/jobs/**</code> entered a stale/disconnected state where <code>stat()</code> calls failed with:</p> <ul> <li><code>OSError: [Errno 107] Transport endpoint is not connected</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#contributing-factors","title":"Contributing factors","text":"<p>C1) \u201cBase mount healthy\u201d did not imply \u201chot paths healthy\u201d.</p> <ul> <li>The Storage Box mount service could remain \u201cactive\u201d while specific mounted subpaths used by the crawler were broken.</li> <li>Monitoring primarily checked:</li> <li><code>/srv/healtharchive/storagebox</code> reachability, and</li> <li>certain systemd unit health,   but did not validate every hot path we depend on.</li> </ul> <p>C2) Scripts and critical paths were not robust to Errno 107.</p> <ul> <li><code>scripts/vps-crawl-metrics-textfile.py</code> crashed instead of emitting \u201cunhealthy\u201d metrics.</li> <li><code>archive_tool</code> crashed instead of classifying the error as \u201cstorage unavailable\u201d and making it recoverable by automation.</li> </ul> <p>C3) Job lifecycle semantics did not separate infra failures from crawl failures.</p> <ul> <li>Infra errors consumed retry budgets and produced confusing states (<code>running</code> + <code>finished_at</code> inconsistencies; <code>crawl_rc</code>/<code>crawl_status</code> not clearly tied to a specific attempt).</li> </ul> <p>C4) Recovery steps existed but were not packaged as a single safe operation.</p> <ul> <li>Operators could fix it (stop worker \u2192 unmount stale \u2192 reapply mounts \u2192 recover jobs), but the system did not do this automatically and safely.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#impact-assessment","title":"Impact assessment","text":"<p>Primary impact:</p> <ul> <li>Annual campaign blocked (no jobs completed or indexed during the failure window).</li> </ul> <p>Secondary impact:</p> <ul> <li>Monitoring degraded (metrics writer failing), increasing time-to-detection.</li> </ul> <p>Data risk:</p> <ul> <li>Partial writes or truncated <code>warc.gz</code> files are plausible if a mount disconnect occurred mid-write.</li> <li>\u201cCompleteness risk\u201d: if resume state/config is lost and the crawler restarts \u201cfresh\u201d, crawl may recrawl already-covered pages and still miss some queued pages unless we persist resumption state reliably.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#resolution-what-we-did-step-by-step","title":"Resolution (what we did, step-by-step)","text":"<p>This section is both a record and a proto-runbook.</p>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#1-stabilize-services","title":"1) Stabilize services","text":"<ul> <li>Stop worker to prevent repeated failures while repairing storage:</li> <li><code>sudo systemctl stop healtharchive-worker.service</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#2-identify-stale-mountpoints","title":"2) Identify stale mountpoints","text":"<p>Use the playbook:</p> <ul> <li><code>docs/operations/playbooks/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#3-repair-mountpoints-re-apply-tiering","title":"3) Repair mountpoints + re-apply tiering","text":"<p>Preferred:</p> <ul> <li><code>sudo systemctl restart healtharchive-storagebox-sshfs.service</code></li> <li><code>sudo systemctl reset-failed healtharchive-warc-tiering.service</code></li> <li><code>sudo systemctl start healtharchive-warc-tiering.service</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#4-recover-job-state","title":"4) Recover job state","text":"<p>Safest recovery for \u201cstale running\u201d jobs:</p> <ul> <li><code>ha-backend recover-stale-jobs --older-than-minutes 10 --require-no-progress-seconds 3600 --apply</code></li> </ul> <p>Then restart the worker.</p>"},{"location":"roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#implemented-outputs-what-exists-now","title":"Implemented outputs (what exists now)","text":"<p>As of 2026-01-16, this plan is considered implemented; the operational \u201csurface area\u201d is:</p> <ul> <li>Storage hot-path watchdog:</li> <li><code>scripts/vps-storage-hotpath-auto-recover.py</code></li> <li><code>docs/deployment/systemd/healtharchive-storage-hotpath-auto-recover.timer</code></li> <li>sentinel: <code>/etc/healtharchive/storage-hotpath-auto-recover-enabled</code></li> <li>Tiering bind-mount helper:</li> <li><code>scripts/vps-warc-tiering-bind-mounts.sh</code> (supports <code>--repair-stale-mounts</code>)</li> <li><code>docs/deployment/systemd/healtharchive-warc-tiering.service</code> uses <code>--repair-stale-mounts</code></li> <li>Crawl stall recovery:</li> <li><code>scripts/vps-crawl-auto-recover.py</code> (safe-by-default; caps recoveries)</li> <li><code>ha-backend recover-stale-jobs</code> supports <code>--require-no-progress-seconds</code></li> <li>Replay resilience:</li> <li>replay systemd/runbook recommends <code>-v /srv/healtharchive/jobs:/warcs:ro,rshared</code></li> <li>replay smoke tests: <code>healtharchive-replay-smoke.timer</code></li> </ul> <p>Remaining follow-up (not in this plan) is alerting/visibility on the new metrics.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/","title":"Disaster recovery and escalation procedures (v1) \u2014 implementation plan","text":"<p>Status: complete (Implemented Jan 18, 2026)</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#goal","title":"Goal","text":"<p>Create comprehensive documentation for disaster recovery and escalation procedures that are currently missing from the operational documentation:</p> <ul> <li>Disaster Recovery Runbook \u2014 RTO/RPO targets, complete VPS restoration procedures,   storage failure recovery, and DR drill schedule.</li> <li>Escalation Procedures \u2014 on-call responsibilities, escalation contacts, DRI   assignments, and break-glass procedures.</li> </ul> <p>This plan produces documentation only \u2014 no code changes. The deliverables are operational runbooks and policy documents.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#why-this-is-next-roadmap-selection","title":"Why this is \"next\" (roadmap selection)","text":"<p>These documents are high priority because:</p> <ul> <li>Single point of failure \u2014 the project runs on a single VPS; operators need   clear procedures for total failure scenarios.</li> <li>Incident response gap \u2014 recent incidents have postmortems but no escalation   path documentation.</li> <li>Bus factor \u2014 with a single operator, explicit documentation ensures continuity.</li> <li>Best practice \u2014 any production service should have DR procedures documented.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#docs-setup-do-first","title":"Docs setup (do first)","text":"<p>Status: Complete</p> <p>1) Create this plan doc    - File: <code>docs/roadmaps/2026-01-17-disaster-recovery-and-escalation-procedures.md</code> (this document)</p> <p>2) Roadmaps index    - Update <code>docs/roadmaps/README.md</code> to list this plan under \"Implementation plans (active)\".</p> <p>3) New canonical docs to create    - <code>docs/deployment/disaster-recovery.md</code> \u2014 DR runbook    - <code>docs/operations/escalation-procedures.md</code> \u2014 escalation policy and contacts</p> <p>4) Existing docs to update    - <code>docs/operations/incident-response.md</code> \u2014 link to escalation procedures    - <code>docs/deployment/production-single-vps.md</code> \u2014 link to DR runbook    - <code>mkdocs.yml</code> \u2014 add new docs to navigation</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#scope-goals-constraints","title":"Scope, goals, constraints","text":""},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#in-scope-outcomes-what-we-will-deliver","title":"In-scope outcomes (what we will deliver)","text":"<p>Disaster Recovery Runbook: - Defined RTO (Recovery Time Objective) and RPO (Recovery Point Objective) - Complete VPS restoration procedure from NAS backup - Database restoration procedure - Storage/WARC recovery procedure - Archive root reconstruction - Service startup sequence after recovery - Verification checklist post-recovery - DR drill schedule and procedure</p> <p>Escalation Procedures: - On-call responsibilities (even if single operator) - Escalation path by incident severity - Contact information storage (secure, not in git) - DRI (Directly Responsible Individual) assignments - Break-glass procedures for critical failures - Handoff procedures for operator changes</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#non-goals-explicitly-out-of-scope","title":"Non-goals (explicitly out of scope)","text":"<ul> <li>Multi-VPS failover architecture (future consideration)</li> <li>Automated DR (manual procedures are acceptable for current scale)</li> <li>Formal SLA documentation (separate plan)</li> <li>Third-party backup verification (out of scope)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#constraints-to-respect","title":"Constraints to respect","text":"<ul> <li>Single operator reality \u2014 procedures should be executable by one person.</li> <li>Budget constraints \u2014 no additional infrastructure required.</li> <li>Existing backup infrastructure \u2014 use existing NAS + nightly backup.</li> <li>Security \u2014 no secrets or contact info in git-tracked documents.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#current-state-map-what-exists-today","title":"Current-state map (what exists today)","text":""},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#backup-infrastructure","title":"Backup infrastructure","text":"<ul> <li>Nightly backups: <code>pg_dump -Fc</code> to <code>/srv/healtharchive/backups/</code> (14-day retention)</li> <li>Offsite copy: Synology NAS pull over Tailscale</li> <li>Quarterly restore tests: documented in <code>docs/operations/playbooks/restore-test.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#recovery-documentation-gaps","title":"Recovery documentation (gaps)","text":"Document Status Gap Backup procedures Exists None Restore test procedure Exists None Complete VPS restoration Missing Full DR scenario RTO/RPO targets Missing No defined targets Escalation procedures Missing No escalation path Contact information Missing No secure storage"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#related-existing-docs","title":"Related existing docs","text":"<ul> <li><code>docs/deployment/production-single-vps.md</code> \u2014 provisioning (not recovery)</li> <li><code>docs/operations/incident-response.md</code> \u2014 incident handling (no escalation)</li> <li><code>docs/operations/playbooks/restore-test.md</code> \u2014 restore verification (not full DR)</li> <li><code>docs/operations/risk-register.md</code> \u2014 identifies VPS as single point of failure</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#definition-of-done-dod-acceptance-criteria","title":"Definition of Done (DoD) + acceptance criteria","text":""},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#disaster-recovery-runbook","title":"Disaster Recovery Runbook","text":"<ul> <li>RTO and RPO are explicitly documented with rationale</li> <li>Step-by-step VPS restoration procedure is complete and testable</li> <li>Database restoration procedure includes verification steps</li> <li>Archive root reconstruction addresses WARC integrity</li> <li>Service startup sequence is documented with health checks</li> <li>DR drill procedure exists with schedule</li> <li>Linked from production runbook and incident response</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#escalation-procedures","title":"Escalation Procedures","text":"<ul> <li>On-call responsibilities are clearly defined</li> <li>Escalation path covers sev0, sev1, sev2, sev3 incidents</li> <li>Break-glass procedures exist for common critical failures</li> <li>Contact information storage approach is documented (not the contacts themselves)</li> <li>Linked from incident response documentation</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#phase-1-define-rtorpo-targets","title":"Phase 1 \u2014 Define RTO/RPO targets","text":"<p>Status: Complete</p> <p>Objective: Establish and document recovery objectives with rationale.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#11-analyze-service-criticality","title":"1.1 Analyze service criticality","text":"<p>Consider: - User impact of downtime (research access, no real-time critical users) - Data loss tolerance (backups are nightly; up to 24h data loss is acceptable) - Complexity of recovery (single VPS with documented setup)</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#12-define-targets","title":"1.2 Define targets","text":"<p>Recommended targets for HealthArchive:</p> Metric Target Rationale RPO 24 hours Nightly backups; crawl data can be re-crawled RTO 8 hours Manual VPS provisioning + restore is feasible in a work day MTTR 4 hours For partial failures (service restart, DB recovery)"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#13-document-targets","title":"1.3 Document targets","text":"<p>Create section in DR runbook explaining: - What RPO/RTO mean in HealthArchive context - Why these targets are appropriate - When to revisit (scale changes, criticality changes)</p> <p>Deliverables: - RTO/RPO targets documented with rationale</p> <p>Exit criteria: Targets are explicit and justified.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#phase-2-document-complete-vps-restoration-procedure","title":"Phase 2 \u2014 Document complete VPS restoration procedure","text":"<p>Status: Complete</p> <p>Objective: Step-by-step procedure to restore HealthArchive from total VPS loss.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#21-identify-restoration-scenarios","title":"2.1 Identify restoration scenarios","text":"<p>Scenario A: VPS total loss, NAS backup available - Most likely DR scenario - NAS has DB dump + configuration backups</p> <p>Scenario B: VPS total loss, NAS unavailable - Worst case; recover from any available backup - May need to accept data loss</p> <p>Scenario C: VPS corrupted but recoverable - Partial recovery; may not need full restoration</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#22-document-scenario-a-procedure","title":"2.2 Document Scenario A procedure","text":"<p>Prerequisites: - New VPS provisioned (Hetzner or equivalent) - Tailscale access configured - NAS backup accessible</p> <p>Procedure outline: 1. Provision new VPS (reference existing provisioning docs) 2. Install base dependencies 3. Configure Tailscale access 4. Mount NAS or transfer backup files 5. Restore database from <code>pg_dump</code> 6. Restore application configuration 7. Restore archive root (WARCs) 8. Start services in correct order 9. Verify restoration</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#23-document-service-startup-sequence","title":"2.3 Document service startup sequence","text":"<p>Order matters for HealthArchive: 1. Database (PostgreSQL) 2. Backend API (uvicorn) 3. Worker (optional, can delay) 4. Replay service (pywb) 5. Reverse proxy (Caddy)</p> <p>For each service: - Start command - Health check command - Expected behavior - What to check if it fails</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#24-document-verification-checklist","title":"2.4 Document verification checklist","text":"<p>Post-recovery verification: - [ ] Database responds to queries - [ ] <code>/api/health</code> returns 200 - [ ] <code>/api/sources</code> returns expected sources - [ ] Search returns results - [ ] Replay service serves archived pages - [ ] Public URLs resolve correctly - [ ] HTTPS/TLS working - [ ] Monitoring reconnected</p> <p>Deliverables: - Complete VPS restoration procedure - Service startup sequence - Verification checklist</p> <p>Exit criteria: Procedure is detailed enough to follow without prior knowledge.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#phase-3-document-database-restoration-procedure","title":"Phase 3 \u2014 Document database restoration procedure","text":"<p>Status: Complete</p> <p>Objective: Detailed procedure for PostgreSQL database restoration.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#31-document-backup-file-location-and-format","title":"3.1 Document backup file location and format","text":"<ul> <li>Backup location: <code>/srv/healtharchive/backups/</code> (local) + NAS</li> <li>Format: <code>pg_dump -Fc</code> (custom format, compressed)</li> <li>Naming: <code>healtharchive-YYYY-MM-DD.dump</code></li> <li>Retention: 14 days local, longer on NAS</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#32-document-restoration-commands","title":"3.2 Document restoration commands","text":"<pre><code># Create fresh database\nsudo -u postgres createdb healtharchive_restored\n\n# Restore from dump\npg_restore -d healtharchive_restored /path/to/backup.dump\n\n# Verify restoration\npsql healtharchive_restored -c \"SELECT COUNT(*) FROM snapshots;\"\n\n# If verification passes, swap databases\nsudo -u postgres psql -c \"ALTER DATABASE healtharchive RENAME TO healtharchive_old;\"\nsudo -u postgres psql -c \"ALTER DATABASE healtharchive_restored RENAME TO healtharchive;\"\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#33-document-integrity-verification","title":"3.3 Document integrity verification","text":"<p>After restoration: - Row counts match expected ranges - Recent snapshots exist (within RPO) - Foreign key constraints pass - No orphaned records</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#34-document-partial-restoration-scenarios","title":"3.4 Document partial restoration scenarios","text":"<ul> <li>Restore specific tables only</li> <li>Point-in-time recovery (if WAL archiving is enabled)</li> <li>Restore to a different server for verification</li> </ul> <p>Deliverables: - Database restoration procedure - Integrity verification steps - Partial restoration options</p> <p>Exit criteria: DBA-level detail for restoration.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#phase-4-document-archive-root-reconstruction","title":"Phase 4 \u2014 Document archive root reconstruction","text":"<p>Status: Complete</p> <p>Objective: Procedure for recovering WARC files and archive storage.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#41-document-archive-root-structure","title":"4.1 Document archive root structure","text":"<pre><code>/srv/healtharchive/jobs/\n\u251c\u2500\u2500 &lt;source_slug&gt;-&lt;year&gt;-&lt;month&gt;/\n\u2502   \u251c\u2500\u2500 warcs/\n\u2502   \u2502   \u251c\u2500\u2500 manifest.json\n\u2502   \u2502   \u2514\u2500\u2500 warc-000001.warc.gz\n\u2502   \u251c\u2500\u2500 provenance/\n\u2502   \u2502   \u251c\u2500\u2500 archive_state.json\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 logs/\n\u2514\u2500\u2500 tiered/ (mount point if active)\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#42-document-warc-recovery-scenarios","title":"4.2 Document WARC recovery scenarios","text":"<p>Scenario: Local WARCs lost, tiered storage intact - Re-import from tiered storage - Verify against manifests</p> <p>Scenario: Tiered storage unavailable, local intact - Continue operating with local WARCs - Re-tier when storage available</p> <p>Scenario: Both local and tiered lost - Accept data loss - Re-crawl affected sources - Document what was lost</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#43-document-integrity-verification","title":"4.3 Document integrity verification","text":"<ul> <li>Manifest checksum validation</li> <li>WARC file integrity (<code>warcio validate</code>)</li> <li>Database-to-filesystem consistency</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#44-document-re-tiering-procedure","title":"4.4 Document re-tiering procedure","text":"<p>If WARCs need to be re-tiered after recovery: - Verify local WARCs are complete - Run tiering service - Verify tiered copies - Update manifests</p> <p>Deliverables: - Archive root recovery procedure - Integrity verification steps - Re-tiering procedure</p> <p>Exit criteria: Clear path to recover archive data.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#phase-5-document-dr-drill-procedure-and-schedule","title":"Phase 5 \u2014 Document DR drill procedure and schedule","text":"<p>Status: Complete</p> <p>Objective: Establish regular DR testing to validate procedures.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#51-define-drill-types","title":"5.1 Define drill types","text":"<p>Tabletop drill (quarterly): - Walk through DR procedure without execution - Identify gaps in documentation - Update procedures based on findings</p> <p>Partial restore drill (quarterly): - Restore database to temporary instance - Verify data integrity - Document results in ops log</p> <p>Full DR drill (annual): - Provision temporary VPS - Execute full restoration procedure - Verify all services functional - Document total time and issues - Tear down temporary infrastructure</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#52-define-drill-schedule","title":"5.2 Define drill schedule","text":"Drill Type Frequency Next Due Owner Tabletop Quarterly Q1 2026 Operator Partial restore Quarterly Q1 2026 Operator Full DR Annual 2026-06 Operator"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#53-document-drill-procedure","title":"5.3 Document drill procedure","text":"<p>For each drill type: - Prerequisites - Step-by-step procedure - Success criteria - Documentation requirements - Follow-up actions</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#54-document-drill-results-template","title":"5.4 Document drill results template","text":"<pre><code># DR Drill: &lt;type&gt; (&lt;date&gt;)\n\n## Summary\n- Type: Tabletop / Partial / Full\n- Duration: X hours\n- Result: Pass / Fail / Partial\n\n## Procedure Followed\n1. Step...\n\n## Issues Encountered\n- Issue 1...\n\n## Documentation Updates Needed\n- Update X...\n\n## Follow-up Actions\n- [ ] Action 1...\n</code></pre> <p>Deliverables: - DR drill schedule - Drill procedures by type - Results template</p> <p>Exit criteria: Drill schedule established; procedures testable.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#phase-6-document-escalation-procedures","title":"Phase 6 \u2014 Document escalation procedures","text":"<p>Status: Complete</p> <p>Objective: Create escalation policy and contact management approach.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#61-define-escalation-levels","title":"6.1 Define escalation levels","text":"Level Criteria Response Time Actions Sev0 Complete outage, data loss risk Immediate All hands, external comms Sev1 Major degradation, user impact &lt; 1 hour Primary on-call engaged Sev2 Partial degradation, workaround exists &lt; 4 hours Investigate, schedule fix Sev3 Minor issue, no user impact &lt; 24 hours Track, fix in normal flow"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#62-define-escalation-path","title":"6.2 Define escalation path","text":"<p>For single-operator setup: 1. Primary: Operator (self) 2. Backup: Documented break-glass procedures 3. External: Community/support channels (if applicable)</p> <p>For future multi-operator setup: 1. Primary: On-call operator 2. Secondary: Backup operator 3. Escalation: Project lead</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#63-document-dri-assignments","title":"6.3 Document DRI assignments","text":"Area DRI Backup Backend API Operator (self) Worker/Crawls Operator (self) Database Operator (self) Storage/WARC Operator (self) Replay service Operator (self) Infrastructure Operator (self)"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#64-document-contact-information-storage","title":"6.4 Document contact information storage","text":"<p>Approach: Store contacts in secure, non-git location.</p> <p>Recommended: - <code>/etc/healtharchive/contacts.env</code> on VPS (mode 600) - Password manager for personal backup - Not in git repository</p> <p>Contents to store: - Primary operator contact (phone, email) - Backup contacts (if any) - Hosting provider support contacts - Domain registrar contacts</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#65-document-break-glass-procedures","title":"6.5 Document break-glass procedures","text":"<p>Break-glass: API unresponsive 1. SSH to VPS via Tailscale 2. Check service status: <code>systemctl status healtharchive-api</code> 3. Check logs: <code>journalctl -u healtharchive-api -n 100</code> 4. Restart service: <code>sudo systemctl restart healtharchive-api</code> 5. If restart fails, check database connectivity</p> <p>Break-glass: Database unreachable 1. Check PostgreSQL status: <code>systemctl status postgresql</code> 2. Check disk space: <code>df -h</code> 3. Check PostgreSQL logs: <code>journalctl -u postgresql -n 100</code> 4. Restart PostgreSQL: <code>sudo systemctl restart postgresql</code> 5. If restart fails, check for corruption</p> <p>Break-glass: VPS unreachable 1. Check Tailscale status from another device 2. Access Hetzner console 3. Reboot via Hetzner panel 4. If persistent, provision new VPS and restore</p> <p>Deliverables: - Escalation level definitions - Escalation path documentation - DRI assignments - Contact storage approach - Break-glass procedures</p> <p>Exit criteria: Clear escalation path for all severity levels.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#phase-7-integration-and-finalization","title":"Phase 7 \u2014 Integration and finalization","text":"<p>Status: Complete</p> <p>Objective: Integrate new docs into documentation structure and validate.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#71-create-the-canonical-docs","title":"7.1 Create the canonical docs","text":"<p>Create: - <code>docs/deployment/disaster-recovery.md</code> - <code>docs/operations/escalation-procedures.md</code></p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#72-update-navigation","title":"7.2 Update navigation","text":"<p>Add to <code>mkdocs.yml</code>: <pre><code>nav:\n  - Deployment:\n    - ...\n    - Disaster Recovery: deployment/disaster-recovery.md\n  - Operations:\n    - ...\n    - Escalation Procedures: operations/escalation-procedures.md\n</code></pre></p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#73-add-cross-references","title":"7.3 Add cross-references","text":"<p>Update existing docs to link to new procedures: - <code>docs/operations/incident-response.md</code> \u2192 link to escalation - <code>docs/deployment/production-single-vps.md</code> \u2192 link to DR - <code>docs/operations/risk-register.md</code> \u2192 reference DR mitigation</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#74-review-and-validate","title":"7.4 Review and validate","text":"<ul> <li>Read through procedures end-to-end</li> <li>Verify commands are accurate</li> <li>Check all links resolve</li> <li>Ensure no secrets are exposed</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#75-archive-this-plan","title":"7.5 Archive this plan","text":"<p>Move to <code>docs/roadmaps/implemented/</code> when complete.</p> <p>Deliverables: - Canonical docs created - Navigation updated - Cross-references added - Plan archived</p> <p>Exit criteria: Docs are discoverable and integrated; <code>make docs-build</code> passes.</p>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#risk-register-pre-mortem","title":"Risk register (pre-mortem)","text":"<ul> <li>Risk: Procedures become stale if not tested.</li> <li>Mitigation: Quarterly drill schedule ensures regular validation.</li> <li>Risk: Contact information becomes outdated.</li> <li>Mitigation: Review contacts during quarterly ops cadence.</li> <li>Risk: DR procedure is too complex to follow under stress.</li> <li>Mitigation: Keep procedures simple; use checklists; practice with drills.</li> <li>Risk: Single operator bottleneck.</li> <li>Mitigation: Document everything; consider backup operator for future.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#appendix-document-templates","title":"Appendix: Document templates","text":""},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#disaster-recovery-runbook-structure","title":"Disaster Recovery Runbook structure","text":"<pre><code># Disaster Recovery Runbook\n\n## Recovery Objectives\n- RPO: 24 hours\n- RTO: 8 hours\n- MTTR: 4 hours\n\n## Scenarios\n### Scenario A: Complete VPS loss\n### Scenario B: Database corruption\n### Scenario C: Storage failure\n\n## Procedures\n### 1. VPS Provisioning\n### 2. Database Restoration\n### 3. Archive Root Recovery\n### 4. Service Startup\n### 5. Verification\n\n## DR Drills\n### Schedule\n### Procedures\n### Results Log\n\n## References\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#escalation-procedures-structure","title":"Escalation Procedures structure","text":"<pre><code># Escalation Procedures\n\n## Severity Levels\n## Escalation Path\n## DRI Assignments\n## Contact Management\n## Break-Glass Procedures\n## Handoff Procedures\n\n## References\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/","title":"Documentation architecture improvements (v1) \u2014 implementation plan","text":"<p>Status: planned (created 2026-01-17)</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#goal","title":"Goal","text":"<p>Improve the documentation architecture to better follow modern best practices while preserving the project's existing strengths (clear taxonomy, multi-repo boundaries, pointer-based cross-repo linking).</p> <p>Key improvements:</p> <ol> <li>Enhanced discoverability \u2014 Critical docs should be navigable without reading README indices</li> <li>Di\u00e1taxis-aligned structure \u2014 Clear separation of tutorials, how-to guides, reference, and explanation</li> <li>Template isolation \u2014 Move templates to dedicated directory</li> <li>Improved cross-repo linking \u2014 Consistent GitHub URL patterns</li> <li>Navigation hierarchy \u2014 Better use of MkDocs Material's navigation features</li> </ol> <p>This plan produces documentation reorganization only \u2014 no code changes.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#why-this-is-next-roadmap-selection","title":"Why this is \"next\" (roadmap selection)","text":"<p>Documentation improvements support all other work because:</p> <ul> <li>Operator efficiency \u2014 Faster access to runbooks and playbooks during incidents</li> <li>Onboarding \u2014 New contributors can find docs without tribal knowledge</li> <li>Maintenance \u2014 Clear structure reduces drift and duplication</li> <li>Scalability \u2014 Good architecture supports growing documentation</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#current-state-analysis","title":"Current state analysis","text":""},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#strengths-preserve-these","title":"Strengths (preserve these)","text":"<ol> <li>Clear document taxonomy \u2014 Runbooks, playbooks, decision records, policies, logs, templates</li> <li>Multi-repo boundary \u2014 Pointer-based linking avoids duplication and drift</li> <li>README index pattern \u2014 Each directory has a comprehensive index</li> <li>Quality bar defined \u2014 Purpose, audience, preconditions, steps, verification, safety</li> <li>Lifecycle management \u2014 Clear process for outdated docs and roadmap workflow</li> </ol>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#issues-identified","title":"Issues identified","text":"Issue Impact Priority 98 of 121 docs not in navigation Users must know to read READMEs to discover docs High Critical docs hidden Production runbook not directly navigable High Templates mixed with content <code>*-template.md</code> files appear as substantive docs Medium Inconsistent cross-repo links Mix of workspace-relative and GitHub URLs Medium Flat playbooks listing 32 playbooks in one README without categorization Medium No audience-based entry points Operators, developers, researchers have same entry Low"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#navigation-statistics","title":"Navigation statistics","text":"Category Files on disk Files in nav Coverage Operations 50+ 7 14% Deployment 15+ 1 7% Development 5 1 20% Playbooks 32 1 (index) 3% Roadmaps 20 1 (index) 5% Decisions 2 1 (index) 50% Total 121 23 19%"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#scope-goals-constraints","title":"Scope, goals, constraints","text":""},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#in-scope-outcomes-what-we-will-deliver","title":"In-scope outcomes (what we will deliver)","text":"<ul> <li>Expanded navigation \u2014 Key docs directly accessible from sidebar</li> <li>Audience-based entry points \u2014 Operators, developers, and researchers can find relevant docs</li> <li>Template isolation \u2014 Templates moved to <code>_templates/</code> directory</li> <li>Categorized playbooks \u2014 Logical groupings in navigation</li> <li>Consistent cross-repo links \u2014 Standardized on GitHub URLs</li> <li>Updated documentation guidelines \u2014 Reflect new structure</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#non-goals-explicitly-out-of-scope","title":"Non-goals (explicitly out of scope)","text":"<ul> <li>Rewriting doc content (only reorganization)</li> <li>Adding new documentation (covered by other plans)</li> <li>Changing the multi-repo boundary approach (it's working well)</li> <li>Full Di\u00e1taxis compliance (too heavy for current scale)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#constraints-to-respect","title":"Constraints to respect","text":"<ul> <li>Backward compatibility \u2014 Existing links should not break (use redirects if needed)</li> <li>Minimal disruption \u2014 Changes should be incremental and reviewable</li> <li>README index preservation \u2014 Keep READMEs as comprehensive indices</li> <li>Build stability \u2014 <code>make docs-build</code> must pass throughout</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#modern-documentation-best-practices-reference","title":"Modern documentation best practices (reference)","text":""},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#diataxis-framework-adapted-for-healtharchive","title":"Di\u00e1taxis framework (adapted for HealthArchive)","text":"Type Purpose HealthArchive equivalent Tutorials Learning-oriented, step-by-step <code>development/live-testing.md</code>, <code>dev-environment-setup.md</code> How-to guides Task-oriented, problem-solving Playbooks, runbooks Reference Information-oriented, accurate Architecture, API docs, schemas Explanation Understanding-oriented, context Decision records, guidelines"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#navigation-best-practices","title":"Navigation best practices","text":"<ol> <li>7\u00b12 rule \u2014 Top-level sections should be 5-9 items</li> <li>3-click rule \u2014 Users should find any doc within 3 clicks</li> <li>Progressive disclosure \u2014 Start with overview, drill down to details</li> <li>Consistent depth \u2014 Similar content at similar navigation depth</li> <li>Clear labeling \u2014 Navigation labels should be self-explanatory</li> </ol>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#mkdocs-material-features-to-leverage","title":"MkDocs Material features to leverage","text":"<ul> <li><code>navigation.sections</code> \u2014 Group related pages under headers</li> <li><code>navigation.tabs</code> \u2014 Top-level tabs for major sections (already enabled)</li> <li><code>navigation.expand</code> \u2014 Auto-expand sections (optional)</li> <li><code>navigation.indexes</code> \u2014 Section index pages</li> <li><code>navigation.prune</code> \u2014 Hide empty sections</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#phase-1-template-isolation","title":"Phase 1 \u2014 Template isolation","text":"<p>Objective: Move template files to a dedicated directory to prevent confusion.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#11-create-templates-directory","title":"1.1 Create templates directory","text":"<pre><code>docs/_templates/\n\u251c\u2500\u2500 README.md              # How to use templates\n\u251c\u2500\u2500 runbook-template.md\n\u251c\u2500\u2500 playbook-template.md\n\u251c\u2500\u2500 incident-template.md\n\u251c\u2500\u2500 decision-template.md\n\u251c\u2500\u2500 restore-test-log-template.md\n\u251c\u2500\u2500 adoption-signals-log-template.md\n\u251c\u2500\u2500 mentions-log-template.md\n\u2514\u2500\u2500 ops-ui-friction-log-template.md\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#12-move-template-files","title":"1.2 Move template files","text":"<pre><code>mkdir -p docs/_templates\nmv docs/deployment/runbook-template.md docs/_templates/\nmv docs/operations/playbooks/playbook-template.md docs/_templates/\nmv docs/operations/incidents/incident-template.md docs/_templates/\nmv docs/decisions/decision-template.md docs/_templates/\nmv docs/operations/restore-test-log-template.md docs/_templates/\nmv docs/operations/adoption-signals-log-template.md docs/_templates/\nmv docs/operations/mentions-log-template.md docs/_templates/\nmv docs/operations/ops-ui-friction-log-template.md docs/_templates/\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#13-update-references","title":"1.3 Update references","text":"<p>Update all docs that reference templates to use new paths.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#14-exclude-from-build-optional","title":"1.4 Exclude from build (optional)","text":"<p>Templates don't need to be in the published site:</p> <pre><code># mkdocs.yml\nexclude_docs: |\n  frontend/**\n  _templates/**\n</code></pre> <p>Or keep them in build but not in nav (current implicit behavior is fine).</p> <p>Deliverables: - <code>_templates/</code> directory created with README - All template files moved - References updated</p> <p>Exit criteria: <code>make docs-build</code> passes; templates discoverable in one location.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#phase-2-expand-navigation-with-key-operational-docs","title":"Phase 2 \u2014 Expand navigation with key operational docs","text":"<p>Objective: Add critical operational docs to sidebar navigation.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#21-identify-must-have-docs-for-navigation","title":"2.1 Identify must-have docs for navigation","text":"<p>Deployment (add to nav): - <code>production-single-vps.md</code> \u2014 Current production runbook - <code>environments-and-configuration.md</code> \u2014 Cross-repo configuration - <code>hosting-and-live-server-to-dos.md</code> \u2014 Deployment checklist</p> <p>Development (add to nav): - <code>live-testing.md</code> \u2014 Local testing flows - <code>dev-environment-setup.md</code> \u2014 Environment setup - <code>testing-guidelines.md</code> \u2014 Test conventions</p> <p>Operations (add to nav): - <code>monitoring-and-ci-checklist.md</code> \u2014 Monitoring guidance - <code>ops-cadence-checklist.md</code> \u2014 Recurring ops tasks - <code>risk-register.md</code> \u2014 Risk tracking</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#22-update-mkdocsyml-navigation","title":"2.2 Update mkdocs.yml navigation","text":"<pre><code>nav:\n  - Home: README.md\n  - Project: project.md\n  - Operations:\n      - Overview: operations/README.md\n      - Ops Roadmap: operations/healtharchive-ops-roadmap.md\n      - Incidents:\n          - Overview: operations/incidents/README.md\n          - Storage Hotpath: operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount.md\n          - Annual Crawl Stalled: operations/incidents/2026-01-09-annual-crawl-hc-job-stalled.md\n          - PHAC Permission Denied: operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied.md\n          - Replay Smoke 503: operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed.md\n      - Playbooks:\n          - Overview: operations/playbooks/README.md\n          - Operator Responsibilities: operations/playbooks/operator-responsibilities.md\n          - Deploy &amp; Verify: operations/playbooks/deploy-and-verify.md\n          - Incident Response: operations/playbooks/incident-response.md\n      - Monitoring: operations/monitoring-and-ci-checklist.md\n      - Ops Cadence: operations/ops-cadence-checklist.md\n      - Risk Register: operations/risk-register.md\n      - Baseline Drift: operations/baseline-drift.md\n      - Dataset Release: operations/dataset-release-runbook.md\n  - Backend:\n      - Architecture: architecture.md\n      - Decisions: decisions/README.md\n      - Deployment:\n          - Overview: deployment/README.md\n          - Production Runbook: deployment/production-single-vps.md\n          - Configuration: deployment/environments-and-configuration.md\n          - Checklist: deployment/hosting-and-live-server-to-dos.md\n      - Development:\n          - Overview: development/README.md\n          - Live Testing: development/live-testing.md\n          - Dev Setup: development/dev-environment-setup.md\n          - Testing: development/testing-guidelines.md\n      - Guidelines: documentation-guidelines.md\n  - API: api.md\n  - Frontend:\n      - Overview: frontend-external/README.md\n      - I18n: frontend-external/i18n.md\n      - Implementation: frontend-external/implementation-guide.md\n  - Datasets: datasets-external/README.md\n  - Roadmaps:\n      - Overview: roadmaps/README.md\n      - Backlog: roadmaps/roadmap.md\n</code></pre> <p>Deliverables: - Updated mkdocs.yml with expanded navigation - Key docs directly accessible</p> <p>Exit criteria: Critical docs accessible within 2 clicks from homepage.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#phase-3-categorize-playbooks-in-navigation","title":"Phase 3 \u2014 Categorize playbooks in navigation","text":"<p>Objective: Group 32 playbooks into logical categories for easier navigation.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#31-define-playbook-categories","title":"3.1 Define playbook categories","text":"Category Playbooks Purpose Core Operations operator-responsibilities, deploy-and-verify, incident-response Daily operator work Observability monitoring-and-alerting, observability-* (7 files) Monitoring setup/maintenance Crawl &amp; Archive crawl-preflight, crawl-stalls, annual-campaign, cleanup-automation Crawl lifecycle Storage warc-storage-tiering, warc-integrity-verification, storagebox-* Storage management Validation coverage-guardrails, replay-smoke-tests, restore-test, dataset-release Quality assurance External outreach-and-verification, adoption-signals, security-posture External-facing"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#32-update-playbooks-readme-with-categories","title":"3.2 Update playbooks README with categories","text":"<p>Restructure <code>operations/playbooks/README.md</code> to group by category:</p> <pre><code># Ops playbooks (task-oriented)\n\n## Core Operations\n- Operator responsibilities: `operator-responsibilities.md`\n- Deploy &amp; verify: `deploy-and-verify.md`\n- Incident response: `incident-response.md`\n- Admin proxy: `admin-proxy.md`\n\n## Observability Setup\n- Bootstrap: `observability-bootstrap.md`\n- Exporters: `observability-exporters.md`\n- Prometheus: `observability-prometheus.md`\n- Grafana: `observability-grafana.md`\n- Dashboards: `observability-dashboards.md`\n- Alerting: `observability-alerting.md`\n- Maintenance: `observability-maintenance.md`\n- Automation maintenance: `automation-maintenance.md`\n\n## Crawl &amp; Archive Operations\n- Crawl preflight: `crawl-preflight.md`\n- Crawl stalls: `crawl-stalls.md`\n- Annual campaign: `annual-campaign.md`\n- Cleanup automation: `cleanup-automation.md`\n- Replay service: `replay-service.md`\n\n## Storage Management\n- WARC storage tiering: `warc-storage-tiering.md`\n- WARC integrity: `warc-integrity-verification.md`\n- Stale mount recovery: `storagebox-sshfs-stale-mount-recovery.md`\n- Recovery drills: `storagebox-sshfs-stale-mount-drills.md`\n\n## Validation &amp; Testing\n- Coverage guardrails: `coverage-guardrails.md`\n- Replay smoke tests: `replay-smoke-tests.md`\n- Restore test: `restore-test.md`\n- Dataset release: `dataset-release.md`\n- Healthchecks parity: `healthchecks-parity.md`\n\n## External &amp; Outreach\n- Outreach &amp; verification: `outreach-and-verification.md`\n- Adoption signals: `adoption-signals.md`\n- Security posture: `security-posture.md`\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#33-update-navigation-with-category-sections","title":"3.3 Update navigation with category sections","text":"<pre><code>- Playbooks:\n    - Overview: operations/playbooks/README.md\n    - Core:\n        - Operator Responsibilities: operations/playbooks/operator-responsibilities.md\n        - Deploy &amp; Verify: operations/playbooks/deploy-and-verify.md\n        - Incident Response: operations/playbooks/incident-response.md\n    - Observability:\n        - Setup: operations/playbooks/observability-bootstrap.md\n        - Alerting: operations/playbooks/observability-alerting.md\n    - Storage:\n        - WARC Tiering: operations/playbooks/warc-storage-tiering.md\n        - Stale Mount Recovery: operations/playbooks/storagebox-sshfs-stale-mount-recovery.md\n</code></pre> <p>(Include key playbooks; others remain discoverable via README)</p> <p>Deliverables: - Playbooks README restructured with categories - Navigation updated with playbook categories</p> <p>Exit criteria: Playbooks are logically grouped and key ones are directly navigable.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#phase-4-standardize-cross-repo-linking","title":"Phase 4 \u2014 Standardize cross-repo linking","text":"<p>Objective: Ensure consistent GitHub URL patterns for cross-repo references.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#41-define-linking-conventions","title":"4.1 Define linking conventions","text":"<p>Rule: Use GitHub URLs for cross-repo references, not workspace-relative paths.</p> <p>Pattern: <pre><code># Good (stable GitHub URL)\nSee the [bilingual dev guide](https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/development/bilingual-dev-guide.md)\n\n# Avoid (workspace-relative)\nSee `healtharchive-frontend/docs/development/bilingual-dev-guide.md`\n</code></pre></p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#42-identify-files-with-workspace-relative-links","title":"4.2 Identify files with workspace-relative links","text":"<p>Scan roadmap files (especially implemented ones) for patterns like: - <code>healtharchive-frontend/...</code> - <code>healtharchive-datasets/...</code> - <code>../../../healtharchive-frontend/...</code></p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#43-update-links-to-github-urls","title":"4.3 Update links to GitHub URLs","text":"<p>For each workspace-relative link: - Convert to <code>https://github.com/jerdaw/&lt;repo&gt;/blob/main/&lt;path&gt;</code> - Or use docs.healtharchive.ca links for backend docs</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#44-document-convention-in-guidelines","title":"4.4 Document convention in guidelines","text":"<p>Add to <code>documentation-guidelines.md</code>:</p> <pre><code>## Cross-repo link format\n\nWhen linking to other repositories:\n\n- Use GitHub URLs: `https://github.com/jerdaw/&lt;repo&gt;/blob/main/&lt;path&gt;`\n- For backend docs: `https://docs.healtharchive.ca/&lt;path&gt;`\n- Do not use workspace-relative paths like `healtharchive-frontend/...`\n\nGitHub URLs are stable and work in all contexts (browser, IDE, published docs).\n</code></pre> <p>Deliverables: - Workspace-relative links converted to GitHub URLs - Convention documented in guidelines</p> <p>Exit criteria: All cross-repo links use stable URLs.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#phase-5-add-audience-based-entry-points","title":"Phase 5 \u2014 Add audience-based entry points","text":"<p>Objective: Create clear starting points for different audiences.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#51-define-audience-personas","title":"5.1 Define audience personas","text":"Persona Primary needs Entry point Operator Run the service, respond to incidents Operations overview Developer Contribute code, run locally Development overview Researcher Use the archive, cite data Project overview + API"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#52-enhance-readme-index-pages","title":"5.2 Enhance README index pages","text":"<p>Update each major README to include: - \"Start here if you are...\" section - Quick links to most common tasks - Clear pointers to related sections</p> <p>Example for operations/README.md:</p> <pre><code># Operations docs\n\n## Start here\n\n- **New operator?** Start with [Operator Responsibilities](playbooks/operator-responsibilities.md)\n- **Incident in progress?** Go to [Incident Response](playbooks/incident-response.md)\n- **Deploying changes?** See [Deploy &amp; Verify](playbooks/deploy-and-verify.md)\n- **Setting up monitoring?** See [Observability Bootstrap](playbooks/observability-bootstrap.md)\n\n## Quick reference\n\n| Task | Doc |\n|------|-----|\n| Daily checks | [Ops Cadence](ops-cadence-checklist.md) |\n| Weekly tasks | [Ops Cadence](ops-cadence-checklist.md) |\n| Quarterly tasks | [Restore Test](playbooks/restore-test.md), [Adoption Signals](playbooks/adoption-signals.md) |\n\n## All docs\n\n[Full list below...]\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#53-update-home-readme","title":"5.3 Update home README","text":"<p>Enhance <code>docs/README.md</code> with audience-based navigation:</p> <pre><code># HealthArchive Documentation\n\n## Quick start by role\n\n- **Operators**: Start with [Operations Overview](operations/README.md)\n- **Developers**: Start with [Development Guide](development/README.md)\n- **API consumers**: Start with [API Documentation](api.md)\n\n## Key resources\n\n| Need | Doc |\n|------|-----|\n| Architecture overview | [Architecture](architecture.md) |\n| Production runbook | [Production Single-VPS](deployment/production-single-vps.md) |\n| Local development | [Live Testing](development/live-testing.md) |\n| Incident response | [Incident Response](operations/playbooks/incident-response.md) |\n</code></pre> <p>Deliverables: - README pages enhanced with audience-based navigation - Home page includes role-based quick start</p> <p>Exit criteria: Any persona can find their starting point within 1 click.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#phase-6-documentation-guidelines-update","title":"Phase 6 \u2014 Documentation guidelines update","text":"<p>Objective: Update guidelines to reflect new structure and conventions.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#61-add-navigation-policy","title":"6.1 Add navigation policy","text":"<pre><code>## Navigation policy\n\n### What goes in mkdocs.yml nav\n\n- All README index pages\n- Docs that are frequently accessed or critical for operations\n- At least one doc from each major category\n\n### What stays README-only\n\n- Detailed playbooks beyond the core set\n- Historical/archived roadmaps (implemented/)\n- Log files and templates\n- Highly specialized procedures\n\n### Organizing new docs\n\nWhen adding new docs:\n1. Add to appropriate directory\n2. Update the directory's README.md index\n3. If critical or frequently accessed, add to mkdocs.yml nav\n4. Ensure cross-links from related docs\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#62-add-template-usage-section","title":"6.2 Add template usage section","text":"<pre><code>## Using templates\n\nTemplates are stored in `docs/_templates/`. To use:\n\n1. Copy the template to the appropriate directory\n2. Rename with appropriate filename (remove `-template` suffix)\n3. Fill in all sections\n4. Add to directory README index\n5. Add to mkdocs.yml nav if appropriate\n\nAvailable templates:\n- `runbook-template.md` \u2014 For deployment procedures\n- `playbook-template.md` \u2014 For operational tasks\n- `incident-template.md` \u2014 For incident postmortems\n- `decision-template.md` \u2014 For architectural decisions\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#63-add-cross-repo-linking-section","title":"6.3 Add cross-repo linking section","text":"<p>(From Phase 4)</p> <p>Deliverables: - Guidelines updated with navigation policy - Guidelines updated with template usage - Guidelines updated with cross-repo linking convention</p> <p>Exit criteria: Guidelines reflect current structure and conventions.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#phase-7-validation-and-finalization","title":"Phase 7 \u2014 Validation and finalization","text":"<p>Objective: Verify all changes and update indexes.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#71-verify-build","title":"7.1 Verify build","text":"<pre><code>make docs-build\n</code></pre> <p>Ensure no warnings about broken links or missing files.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#72-test-navigation","title":"7.2 Test navigation","text":"<ul> <li>Verify each nav item links to correct file</li> <li>Check that critical docs are accessible within 2-3 clicks</li> <li>Verify playbook categories are intuitive</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#73-update-link-checker","title":"7.3 Update link checker","text":"<p>Run link checker to find any broken cross-references:</p> <pre><code># If using lychee or similar\nlychee docs/**/*.md\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#74-archive-this-plan","title":"7.4 Archive this plan","text":"<p>Move to <code>docs/roadmaps/implemented/</code> when complete.</p> <p>Deliverables: - Build passes - Navigation tested - Links verified - Plan archived</p> <p>Exit criteria: Documentation is reorganized and all links work.</p>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#risk-register-pre-mortem","title":"Risk register (pre-mortem)","text":"<ul> <li>Risk: Navigation becomes too deep/complex.</li> <li>Mitigation: Keep top-level sections to 7\u00b12; use README discovery for deep content.</li> <li>Risk: Moving templates breaks existing workflows.</li> <li>Mitigation: Update all references; keep templates in predictable location.</li> <li>Risk: Cross-repo link changes break external references.</li> <li>Mitigation: Only change internal references; external links are stable GitHub URLs.</li> <li>Risk: Over-engineering the navigation.</li> <li>Mitigation: Focus on high-impact changes; preserve README index pattern.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#summary-of-changes","title":"Summary of changes","text":""},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#file-movements","title":"File movements","text":"From To <code>deployment/runbook-template.md</code> <code>_templates/runbook-template.md</code> <code>operations/playbooks/playbook-template.md</code> <code>_templates/playbook-template.md</code> <code>operations/incidents/incident-template.md</code> <code>_templates/incident-template.md</code> <code>decisions/decision-template.md</code> <code>_templates/decision-template.md</code> <code>operations/*-template.md</code> (4 files) <code>_templates/*-template.md</code>"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#navigation-additions-mkdocsyml","title":"Navigation additions (mkdocs.yml)","text":"Section Added docs Deployment production-single-vps, environments-and-configuration, hosting-and-live-server-to-dos Development live-testing, dev-environment-setup, testing-guidelines Operations monitoring-and-ci-checklist, ops-cadence-checklist, risk-register Playbooks operator-responsibilities, deploy-and-verify, incident-response + category groupings Roadmaps roadmap"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#documentation-updates","title":"Documentation updates","text":"File Change <code>documentation-guidelines.md</code> Navigation policy, template usage, cross-repo linking <code>operations/README.md</code> Audience-based quick start <code>operations/playbooks/README.md</code> Category groupings <code>README.md</code> (home) Role-based quick start"},{"location":"roadmaps/implemented/2026-01-17-documentation-architecture-improvements/#appendix-proposed-mkdocsyml-nav-structure","title":"Appendix: Proposed mkdocs.yml nav structure","text":"<pre><code>nav:\n  - Home: README.md\n  - Project: project.md\n  - Operations:\n      - Overview: operations/README.md\n      - Ops Roadmap: operations/healtharchive-ops-roadmap.md\n      - Monitoring: operations/monitoring-and-ci-checklist.md\n      - Ops Cadence: operations/ops-cadence-checklist.md\n      - Risk Register: operations/risk-register.md\n      - Baseline Drift: operations/baseline-drift.md\n      - Dataset Release: operations/dataset-release-runbook.md\n      - Incidents:\n          - Overview: operations/incidents/README.md\n          - Storage Hotpath: operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount.md\n          - Annual Crawl Stalled: operations/incidents/2026-01-09-annual-crawl-hc-job-stalled.md\n          - PHAC Permission Denied: operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied.md\n          - Replay Smoke 503: operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed.md\n      - Playbooks:\n          - Overview: operations/playbooks/README.md\n          - Core:\n              - Operator Responsibilities: operations/playbooks/operator-responsibilities.md\n              - Deploy &amp; Verify: operations/playbooks/deploy-and-verify.md\n              - Incident Response: operations/playbooks/incident-response.md\n          - Observability:\n              - Bootstrap: operations/playbooks/observability-bootstrap.md\n              - Alerting: operations/playbooks/observability-alerting.md\n          - Storage:\n              - WARC Tiering: operations/playbooks/warc-storage-tiering.md\n  - Backend:\n      - Architecture: architecture.md\n      - Decisions: decisions/README.md\n      - Deployment:\n          - Overview: deployment/README.md\n          - Production Runbook: deployment/production-single-vps.md\n          - Configuration: deployment/environments-and-configuration.md\n          - Checklist: deployment/hosting-and-live-server-to-dos.md\n      - Development:\n          - Overview: development/README.md\n          - Live Testing: development/live-testing.md\n          - Dev Setup: development/dev-environment-setup.md\n          - Testing: development/testing-guidelines.md\n      - Guidelines: documentation-guidelines.md\n  - API: api.md\n  - Frontend:\n      - Overview: frontend-external/README.md\n      - I18n: frontend-external/i18n.md\n      - Implementation: frontend-external/implementation-guide.md\n  - Datasets: datasets-external/README.md\n  - Roadmaps:\n      - Overview: roadmaps/README.md\n      - Backlog: roadmaps/roadmap.md\n</code></pre> <p>This structure: - Keeps top-level sections to 8 items (within 7\u00b12 guideline) - Exposes critical docs directly in navigation - Groups playbooks by category - Preserves README index pattern for deep discovery - Maintains existing strengths while improving discoverability</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/","title":"Operational hardening: tiering alerting + incident follow-ups (v1) \u2014 implementation plan","text":"<p>Status: implemented (2026-01-18)</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#goal","title":"Goal","text":"<p>Close out open operational gaps from recent incidents and enable proactive alerting infrastructure that already exists but is not yet wired:</p> <ul> <li>Enable tiering health metrics + alerting to detect storage/tiering failures   before they cascade into user-visible outages (ref: 2026-01-16 incident).</li> <li>Complete incident follow-ups from 2026-01-09 and 2026-01-16 incidents.</li> <li>Document replay smoke target independence decision (canary replay proposal).</li> </ul> <p>This plan is intentionally small and focused: complete it in a single session before returning to larger feature work.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#why-this-is-next-roadmap-selection","title":"Why this is \"next\" (roadmap selection)","text":"<p>This is the highest-leverage operational work because:</p> <ul> <li>Implementation already exists \u2014 systemd timers and metrics exporters are written;   we just need to enable and wire alerting.</li> <li>Direct incident prevention \u2014 the 2026-01-16 replay smoke + tiering failure could   have been caught earlier with proper alerting.</li> <li>Low risk, high reward \u2014 no code changes to the backend; purely operational.</li> <li>Unblocks other work \u2014 incident follow-ups should be closed before starting new   feature development (good hygiene).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#docs-setup-do-first","title":"Docs setup (do first)","text":"<p>1) Create this plan doc    - File: <code>docs/roadmaps/2026-01-17-ops-tiering-alerting-and-incident-followups.md</code> (this document)</p> <p>2) Backlog linkage    - Update <code>docs/roadmaps/roadmap.md</code>:      - Add link to this plan under \"Ops surface / environments\" section.</p> <p>3) Roadmaps index    - Update <code>docs/roadmaps/README.md</code> to list this plan under \"Implementation plans (active)\".</p> <p>4) Canonical docs to update during/after implementation    - <code>docs/operations/healtharchive-ops-roadmap.md</code> \u2014 mark tiering alerting as done    - <code>docs/operations/observability-alerting.md</code> \u2014 add tiering alert rules    - <code>docs/operations/incidents/2026-01-09-annual-crawl-hc-job-stalled.md</code> \u2014 close action items    - <code>docs/operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed.md</code> \u2014 close action items</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#scope-goals-constraints","title":"Scope, goals, constraints","text":""},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#in-scope-outcomes-what-we-will-deliver","title":"In-scope outcomes (what we will deliver)","text":"<ul> <li>Tiering health alerting enabled on the production VPS:</li> <li><code>healtharchive-tiering-metrics.timer</code> enabled and running.</li> <li>Metrics visible at <code>http://127.0.0.1:9100/metrics</code> (node exporter textfile).</li> <li>Prometheus alert rule wired for <code>healtharchive_tiering_metrics_ok == 0</code>.</li> <li>Alertmanager notification configured (email or webhook).</li> <li>Incident 2026-01-16 follow-ups closed:</li> <li>Tiering alerting enabled (this plan).</li> <li>Decision documented on replay smoke target independence.</li> <li>Incident 2026-01-09 follow-ups closed:</li> <li>Captured repeated URLs from stalled crawl (if still recoverable).</li> <li>Document per-job recovery decision (implement or explicitly defer).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#non-goals-explicitly-out-of-scope","title":"Non-goals (explicitly out of scope)","text":"<ul> <li>No new monitoring infrastructure (use existing Prometheus/Alertmanager stack).</li> <li>No changes to backend code.</li> <li>No new systemd timers (all already exist).</li> <li>No replay service architecture changes (canary is a future decision).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#constraints-to-respect-project-resources-policy","title":"Constraints to respect (project resources + policy)","text":"<ul> <li>Single-VPS reality \u2014 all changes are local to the production VPS.</li> <li>Minimal operational complexity \u2014 enable existing infrastructure, don't add new.</li> <li>Reversible \u2014 disabling a timer or alert rule is trivial.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#current-state-map-what-exists-today","title":"Current-state map (what exists today)","text":""},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#tiering-metrics-infrastructure","title":"Tiering metrics infrastructure","text":"<ul> <li>Timer: <code>healtharchive-tiering-metrics.timer</code> (installed, not enabled)</li> <li>Service: <code>healtharchive-tiering-metrics.service</code></li> <li>Textfile collector output: <code>/var/lib/prometheus/node-exporter/healtharchive_tiering.prom</code></li> <li>Node exporter: reads textfile collector directory</li> <li>Prometheus: scrapes node exporter at <code>:9100</code></li> <li>Alertmanager: configured for HealthArchive alerts</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#incident-notes-open-action-items","title":"Incident notes (open action items)","text":"<p>2026-01-16 (replay smoke 503 + tiering failed): - [x] Re-applied WARC tiering - [x] Restarted replay service - [x] Updated systemd unit for <code>rshared</code> bind propagation - [x] Updated tiering service for stale mount auto-repair - [x] Enabled storage hot-path auto-recovery timer - [ ] Enable tiering health metrics + alerting (this plan) - [ ] Consider \"canary replay\" job for smoke independence (decision needed)</p> <p>2026-01-09 (annual crawl job 6 stalled): - [ ] Capture repeated URLs after recovery - [ ] Assess timeout tuning - [ ] Document per-job recovery without stopping worker</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#definition-of-done-dod-acceptance-criteria","title":"Definition of Done (DoD) + acceptance criteria","text":""},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#tiering-alerting","title":"Tiering alerting","text":"<ul> <li><code>healtharchive-tiering-metrics.timer</code> is enabled and active on VPS.</li> <li><code>curl -s http://127.0.0.1:9100/metrics | grep healtharchive_tiering</code> returns metrics.</li> <li>Prometheus has a firing-capable alert rule for tiering unhealthy.</li> <li>Alert fires correctly in a manual test (simulate failure).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#incident-follow-ups","title":"Incident follow-ups","text":"<ul> <li>2026-01-16 incident note: all action items marked complete or explicitly deferred with rationale.</li> <li>2026-01-09 incident note: all action items marked complete or explicitly deferred with rationale.</li> <li>Canonical ops roadmap reflects current state.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#phase-1-enable-tiering-health-metrics-vps-configuration","title":"Phase 1 \u2014 Enable tiering health metrics (VPS configuration)","text":"<p>Objective: Enable the existing tiering metrics timer and verify metrics are visible.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#11-enable-the-timer","title":"1.1 Enable the timer","text":"<p>On the production VPS:</p> <pre><code>sudo systemctl enable --now healtharchive-tiering-metrics.timer\nsudo systemctl status healtharchive-tiering-metrics.timer\n</code></pre> <p>Verify the service runs successfully:</p> <pre><code>sudo systemctl start healtharchive-tiering-metrics.service\njournalctl -u healtharchive-tiering-metrics.service --no-pager -n 20\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#12-verify-metrics-are-exported","title":"1.2 Verify metrics are exported","text":"<pre><code>curl -s http://127.0.0.1:9100/metrics | grep healtharchive_tiering\n</code></pre> <p>Expected output includes: - <code>healtharchive_tiering_metrics_ok</code> (1 = healthy, 0 = unhealthy) - <code>healtharchive_tiering_last_success_timestamp_seconds</code></p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#13-document-the-enabled-state","title":"1.3 Document the enabled state","text":"<p>Update <code>docs/operations/healtharchive-ops-roadmap.md</code>: - Move \"Enable tiering health metrics + alerting\" from \"Current ops tasks\" to a completed section or remove.</p> <p>Deliverables: - Timer enabled and running - Metrics visible in node exporter output</p> <p>Exit criteria: Metrics export verified; timer survives reboot.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#phase-2-wire-prometheus-alerting-rules","title":"Phase 2 \u2014 Wire Prometheus alerting rules","text":"<p>Objective: Create and deploy alert rules that fire when tiering is unhealthy.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#21-create-alert-rule","title":"2.1 Create alert rule","text":"<p>Add to Prometheus alert rules (typically <code>/etc/prometheus/rules/healtharchive.yml</code>):</p> <pre><code>  - alert: HealthArchiveTieringStale\n    expr: (time() - healtharchive_tiering_metrics_timestamp_seconds) &gt; 7200\n    for: 10m\n    labels:\n      severity: warning\n      service: healtharchive\n    annotations:\n      summary: \"HealthArchive tiering metrics are stale\"\n      description: \"No successful tiering metrics update in over 2 hours. Timer may be failing.\"\n      runbook_url: \"https://github.com/jerdaw/healtharchive-backend/blob/main/docs/operations/playbooks/incident-response.md\"\n\n  # Note: Instead of a summary 'Unhealthy' metric, we use granular metrics\n  # already present in healtharchive-alerts.yml templating:\n  # - healtharchive_storagebox_mount_ok\n  # - healtharchive_tiering_hot_path_ok\n  # - healtharchive_systemd_unit_failed{unit=\"healtharchive-warc-tiering.service\"}\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#22-reload-prometheus","title":"2.2 Reload Prometheus","text":"<pre><code>sudo systemctl reload prometheus\n</code></pre> <p>Verify rules are loaded:</p> <pre><code>curl -s http://127.0.0.1:9090/api/v1/rules | jq '.data.groups[] | select(.name == \"healtharchive-tiering\")'\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#23-test-alert-firing-manual","title":"2.3 Test alert firing (manual)","text":"<p>Temporarily break the tiering metrics to verify alerting works:</p> <pre><code># Create a failure condition (e.g., write unhealthy metric manually)\necho 'healtharchive_tiering_metrics_ok 0' | sudo tee /var/lib/prometheus/node-exporter/healtharchive_tiering_test.prom\n\n# Wait 5+ minutes for alert to fire\n# Check Prometheus alerts UI or Alertmanager\n\n# Clean up test file\nsudo rm /var/lib/prometheus/node-exporter/healtharchive_tiering_test.prom\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#24-document-alert-rules","title":"2.4 Document alert rules","text":"<p>Update <code>docs/operations/observability-alerting.md</code>: - Add tiering alerts to the documented rules list - Include the alert names, thresholds, and what they mean</p> <p>Deliverables: - Alert rules deployed to Prometheus - Alert firing verified in test - Documentation updated</p> <p>Exit criteria: Alert fires correctly when tiering is unhealthy.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#phase-3-close-incident-2026-01-16-follow-ups","title":"Phase 3 \u2014 Close incident 2026-01-16 follow-ups","text":"<p>Objective: Mark all action items complete or explicitly defer with rationale.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#31-tiering-alerting-complete-via-phase-1-2","title":"3.1 Tiering alerting (complete via Phase 1-2)","text":"<p>Update the incident note to mark this complete.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#32-replay-smoke-target-independence-decision","title":"3.2 Replay smoke target independence decision","text":"<p>Decision point: Should we implement a \"canary replay\" job that uses local-only WARCs to distinguish pywb failures from storage mount failures?</p> <p>Options: - A) Implement canary replay now \u2014 adds complexity but improves incident triage. - B) Defer canary replay \u2014 current tiering alerting + auto-recovery is sufficient for now. - C) Document as backlog \u2014 add to roadmap.md for later consideration.</p> <p>Recommendation: Option C (document as backlog). The tiering alerting and auto-recovery mechanisms are now in place, which reduces the urgency. Canary replay is a nice-to-have but not critical.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#33-update-incident-note","title":"3.3 Update incident note","text":"<p>Edit <code>docs/operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed.md</code>: - Mark tiering alerting action item complete - Document the canary replay decision (deferred to backlog with rationale)</p> <p>Deliverables: - Incident note updated with all action items addressed - Canary replay added to roadmap.md if deferred</p> <p>Exit criteria: Incident note shows all items complete or explicitly deferred.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#phase-4-close-incident-2026-01-09-follow-ups","title":"Phase 4 \u2014 Close incident 2026-01-09 follow-ups","text":"<p>Objective: Complete or defer the annual crawl stall follow-ups.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#41-capture-repeated-urls-if-recoverable","title":"4.1 Capture repeated URLs (if recoverable)","text":"<p>Check if the stalled job's state is still available:</p> <pre><code># Check job state\nha-backend show-job --id 6\n\n# If archive_tool state exists, examine for repeated URLs\nls -la /path/to/job6/.archive_state.json\n</code></pre> <p>If state is available, extract and document the repeated URLs. If state was cleaned up, document that the data is no longer recoverable.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#42-assess-timeout-tuning","title":"4.2 Assess timeout tuning","text":"<p>Review the crawl configuration for job 6:</p> <pre><code>ha-backend show-job --id 6 | jq '.config'\n</code></pre> <p>Document whether timeout tuning would have helped: - If yes: add specific tuning recommendation to <code>docs/operations/playbooks/crawl-timeout-tuning.md</code> - If no: document why current settings are appropriate</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#43-per-job-recovery-decision","title":"4.3 Per-job recovery decision","text":"<p>Decision point: Should we implement per-job stall recovery without stopping the entire worker?</p> <p>Options: - A) Implement per-job recovery \u2014 complex; risk of partial state corruption. - B) Keep current behavior \u2014 stopping worker is safe and simple; acceptable blast radius. - C) Document manual procedure \u2014 operator can manually recover specific jobs without code changes.</p> <p>Recommendation: Option C. Document a manual procedure for recovering individual jobs while minimizing worker disruption. Defer code changes until there's a clear pattern of need.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#44-update-incident-note","title":"4.4 Update incident note","text":"<p>Edit <code>docs/operations/incidents/2026-01-09-annual-crawl-hc-job-stalled.md</code>: - Document findings from URL capture (or note data unavailable) - Document timeout tuning assessment - Document per-job recovery decision</p> <p>Deliverables: - Incident note updated with all action items addressed - Any new playbooks or procedures documented</p> <p>Exit criteria: Incident note shows all items complete or explicitly deferred.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#phase-5-final-documentation-updates","title":"Phase 5 \u2014 Final documentation updates","text":"<p>Objective: Ensure all canonical docs reflect the new state.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#51-update-ops-roadmap","title":"5.1 Update ops roadmap","text":"<p>Edit <code>docs/operations/healtharchive-ops-roadmap.md</code>: - Remove tiering alerting from \"Current ops tasks\" - Add any new tasks identified during incident review</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#52-update-future-roadmap-if-needed","title":"5.2 Update future roadmap (if needed)","text":"<p>If canary replay or per-job recovery were deferred, add them to <code>docs/roadmaps/roadmap.md</code> under appropriate sections.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#53-archive-this-plan","title":"5.3 Archive this plan","text":"<p>Move this plan to <code>docs/roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups.md</code> when complete.</p> <p>Deliverables: - Ops roadmap current - Future roadmap updated with deferred items - This plan archived</p> <p>Exit criteria: All docs reflect reality; no stale TODOs.</p>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#risk-register-pre-mortem","title":"Risk register (pre-mortem)","text":"<ul> <li>Risk: Alert fatigue from false positives.</li> <li>Mitigation: Use <code>for: 5m</code> duration before firing; tune thresholds if needed.</li> <li>Risk: Tiering metrics timer fails silently.</li> <li>Mitigation: Added staleness alert (no update in 2+ hours).</li> <li>Risk: Incident follow-up data is no longer recoverable.</li> <li>Mitigation: Document what was available; focus on future prevention.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#appendix-commands-reference","title":"Appendix: Commands reference","text":""},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#enable-tiering-metrics","title":"Enable tiering metrics","text":"<pre><code>sudo systemctl enable --now healtharchive-tiering-metrics.timer\nsudo systemctl status healtharchive-tiering-metrics.timer\ncurl -s http://127.0.0.1:9100/metrics | grep healtharchive_tiering\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#reload-prometheus","title":"Reload Prometheus","text":"<pre><code>sudo systemctl reload prometheus\ncurl -s http://127.0.0.1:9090/api/v1/rules | jq '.data.groups[].name'\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#check-incident-notes","title":"Check incident notes","text":"<pre><code>cat docs/operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed.md\ncat docs/operations/incidents/2026-01-09-annual-crawl-hc-job-stalled.md\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/","title":"SLA and service commitments documentation (v1) \u2014 implementation plan","text":"<p>Status: completed (created 2026-01-17, implemented 2026-01-18)</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#goal","title":"Goal","text":"<p>Create explicit service level documentation that defines what users and operators can expect from HealthArchive:</p> <ul> <li>Service Level Objectives (SLOs) \u2014 measurable targets for availability,   response time, and data freshness.</li> <li>Maintenance Windows \u2014 documented expectations for planned downtime.</li> <li>Communication Commitments \u2014 how and when users are notified of issues.</li> <li>Performance Baselines \u2014 documented performance expectations for key operations.</li> </ul> <p>This plan produces documentation only \u2014 no code changes. The deliverables define expectations rather than enforce them technically.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#why-this-is-next-roadmap-selection","title":"Why this is \"next\" (roadmap selection)","text":"<p>Service level documentation is valuable because:</p> <ul> <li>User expectations \u2014 researchers and users should know what to expect.</li> <li>Operator guidance \u2014 clear targets help prioritize incident response.</li> <li>Accountability \u2014 documented commitments enable honest assessment of service quality.</li> <li>Best practice \u2014 any public service should communicate its reliability posture.</li> </ul> <p>Note: This is lower priority than DR/escalation documentation but completes the operational documentation picture.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#docs-setup-do-first","title":"Docs setup (do first)","text":"<p>1) Create this plan doc    - File: <code>docs/roadmaps/2026-01-17-sla-and-service-commitments.md</code> (this document)</p> <p>2) Roadmaps index    - Update <code>docs/roadmaps/README.md</code> to list this plan under \"Implementation plans (active)\".</p> <p>3) New canonical docs to create    - <code>docs/operations/service-levels.md</code> \u2014 SLOs and commitments</p> <p>4) Existing docs to update    - <code>docs/operations/monitoring-and-ci-checklist.md</code> \u2014 reference SLOs    - <code>mkdocs.yml</code> \u2014 add new doc to navigation</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#scope-goals-constraints","title":"Scope, goals, constraints","text":""},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#in-scope-outcomes-what-we-will-deliver","title":"In-scope outcomes (what we will deliver)","text":"<p>Service Level Objectives (SLOs): - Availability target (e.g., 99.5% monthly) - API response time targets (p50, p95, p99) - Search latency targets - Data freshness commitments (crawl cadence)</p> <p>Maintenance Windows: - Planned maintenance schedule approach - Notification lead time - Maximum maintenance duration</p> <p>Communication Commitments: - Incident notification channels - Status page approach (if any) - Changelog/announcement cadence</p> <p>Performance Baselines: - API endpoint response time baselines - Search query performance baselines - Crawl throughput baselines</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#non-goals-explicitly-out-of-scope","title":"Non-goals (explicitly out of scope)","text":"<ul> <li>Formal contractual SLA (this is documentation, not a contract)</li> <li>Automated SLO enforcement (future work)</li> <li>SLI (Service Level Indicator) instrumentation (existing monitoring is sufficient)</li> <li>Financial penalties or credits (not applicable)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#constraints-to-respect","title":"Constraints to respect","text":"<ul> <li>Single-VPS reality \u2014 availability targets must be realistic for single-node.</li> <li>Solo operator \u2014 response times for incidents and communication depend on operator availability.</li> <li>No 24/7 coverage \u2014 incidents outside business hours may see delayed response.</li> <li>Best-effort service \u2014 HealthArchive is a public good, not a commercial service.</li> <li>Honest commitments \u2014 only commit to what can be delivered.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#current-state-map-what-exists-today","title":"Current-state map (what exists today)","text":""},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#monitoring-infrastructure","title":"Monitoring infrastructure","text":"<ul> <li>External uptime monitoring via UptimeRobot/Healthchecks.io</li> <li>Prometheus metrics for internal monitoring</li> <li>No public status page</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#performance-data-approximate-from-existing-operations","title":"Performance data (approximate, from existing operations)","text":"Metric Current Typical Notes Availability ~99%+ Based on incident history <code>/api/health</code> p95 &lt; 100ms Fast health check <code>/api/search</code> p95 &lt; 2s For common queries Crawl cadence Monthly Per source"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#documentation-gaps","title":"Documentation gaps","text":"<ul> <li>No explicit availability target</li> <li>No documented response time targets</li> <li>No maintenance window policy</li> <li>No communication commitments</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#definition-of-done-dod-acceptance-criteria","title":"Definition of Done (DoD) + acceptance criteria","text":""},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#service-level-documentation","title":"Service Level Documentation","text":"<ul> <li>SLOs are explicit with measurable targets</li> <li>Maintenance window policy is documented</li> <li>Communication approach is documented</li> <li>Performance baselines are documented</li> <li>Document is linked from operations index</li> <li>Document is discoverable via docs site navigation</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#validation","title":"Validation","text":"<ul> <li>SLOs are realistic based on current infrastructure</li> <li>Commitments are achievable with current resources</li> <li>No over-promising beyond single-VPS capabilities</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#phase-1-define-availability-slos","title":"Phase 1 \u2014 Define availability SLOs","text":"<p>Objective: Establish explicit availability targets with rationale.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#11-analyze-historical-availability","title":"1.1 Analyze historical availability","text":"<p>Review: - Incident history from <code>docs/operations/incidents/</code> - Uptime monitoring data - Planned maintenance history</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#12-define-availability-target","title":"1.2 Define availability target","text":"<p>Recommended: 99.5% monthly availability</p> <p>Rationale: - Single VPS means some downtime is inevitable - 99.5% allows ~3.6 hours downtime per month - Realistic for manual operations and maintenance - Higher targets (99.9%) require redundancy not currently in place</p> <p>Breakdown: | Target | Allowed Downtime | Realistic? | |--------|-----------------|------------| | 99% | 7.2 hours/month | Too lenient | | 99.5% | 3.6 hours/month | Appropriate | | 99.9% | 43 min/month | Requires HA |</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#13-define-measurement-approach","title":"1.3 Define measurement approach","text":"<ul> <li>Measured via external uptime monitoring</li> <li>Primary endpoint: <code>/api/health</code></li> <li>Secondary: homepage availability</li> <li>Excludes planned maintenance (with advance notice)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#14-document-availability-slo","title":"1.4 Document availability SLO","text":"<p>Include: - Target percentage - Measurement method - Exclusions (planned maintenance) - Review cadence (semi-annual)</p> <p>Deliverables: - Availability SLO documented with rationale</p> <p>Exit criteria: Clear, measurable availability target.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#phase-2-define-response-time-slos","title":"Phase 2 \u2014 Define response time SLOs","text":"<p>Objective: Establish API response time targets.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#21-identify-key-endpoints","title":"2.1 Identify key endpoints","text":"<p>User-facing (public): - <code>GET /api/health</code> \u2014 health check - <code>GET /api/search</code> \u2014 search queries - <code>GET /api/sources</code> \u2014 source listing - <code>GET /api/snapshot/{id}</code> \u2014 snapshot retrieval - <code>GET /api/changes</code> \u2014 change feed</p> <p>Operator-facing (admin): - <code>GET /api/admin/jobs</code> \u2014 job listing - <code>GET /metrics</code> \u2014 Prometheus metrics</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#22-define-response-time-targets","title":"2.2 Define response time targets","text":"<p>Recommended targets:</p> Endpoint p50 p95 p99 Notes <code>/api/health</code> 50ms 100ms 200ms Fast, minimal work <code>/api/search</code> 500ms 2000ms 5000ms Complex queries <code>/api/sources</code> 100ms 300ms 500ms Cached/lightweight <code>/api/snapshot/{id}</code> 100ms 300ms 500ms Single record <code>/api/changes</code> 200ms 500ms 1000ms Precomputed"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#23-define-degradation-thresholds","title":"2.3 Define degradation thresholds","text":"<p>When to consider the service degraded: - p95 exceeds target by 2x for 5+ minutes - p99 exceeds target by 3x for 5+ minutes - Any endpoint timeout rate &gt; 1%</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#24-document-response-time-slos","title":"2.4 Document response time SLOs","text":"<p>Include: - Per-endpoint targets - Measurement approach (server-side timing) - Degradation criteria - Excluded scenarios (attack traffic, bulk exports)</p> <p>Deliverables: - Response time SLOs documented per endpoint</p> <p>Exit criteria: Clear, measurable latency targets.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#phase-3-define-data-freshness-commitments","title":"Phase 3 \u2014 Define data freshness commitments","text":"<p>Objective: Document expectations for data currency.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#31-define-crawl-cadence-commitments","title":"3.1 Define crawl cadence commitments","text":"<p>Current approach: - Monthly crawls for primary sources (hc, phac) - Annual campaigns for comprehensive coverage - Ad-hoc crawls for specific needs</p> <p>Recommended commitment: - Primary sources: crawled at least annually, with ad-hoc updates as resources permit - New content visibility: within 48 hours of successful crawl, subject to operator availability - Change detection: computed within 24 hours of indexing, subject to operator availability</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#32-define-indexing-freshness","title":"3.2 Define indexing freshness","text":"<ul> <li>Crawl-to-indexed: within 24 hours of crawl completion</li> <li>Indexed-to-searchable: immediate (same transaction)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#33-define-change-tracking-freshness","title":"3.3 Define change tracking freshness","text":"<ul> <li>Changes computed: within 24 hours of new snapshots</li> <li>Change feed updated: on next <code>compute-changes</code> run</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#34-document-data-freshness-commitments","title":"3.4 Document data freshness commitments","text":"<p>Include: - Crawl cadence by source type - Indexing latency expectations - Change tracking latency expectations - Exceptions (manual crawls, emergency updates)</p> <p>Deliverables: - Data freshness commitments documented</p> <p>Exit criteria: Clear expectations for data currency.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#phase-4-define-maintenance-window-policy","title":"Phase 4 \u2014 Define maintenance window policy","text":"<p>Objective: Document planned downtime approach.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#41-define-maintenance-types","title":"4.1 Define maintenance types","text":"<p>Routine maintenance: - Security updates - Dependency updates - Configuration changes - Typically &lt; 15 minutes downtime</p> <p>Major maintenance: - Database migrations - Infrastructure changes - New feature deployments - May require 1-4 hours</p> <p>Emergency maintenance: - Security patches - Critical bug fixes - No advance notice possible</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#42-define-notification-requirements","title":"4.2 Define notification requirements","text":"Type Advance Notice Channel Routine 24 hours Changelog Major 72 hours Changelog + announcement Emergency ASAP (post-hoc if needed) Changelog"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#43-define-maintenance-windows","title":"4.3 Define maintenance windows","text":"<p>Preferred windows: - Weekdays, off-peak hours (early morning or late evening) - Avoid: business hours, weekends, holidays</p> <p>Maximum duration: - Routine: 30 minutes - Major: 4 hours - Emergency: as needed (document afterward)</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#44-document-maintenance-policy","title":"4.4 Document maintenance policy","text":"<p>Include: - Maintenance types and definitions - Notification requirements - Preferred windows - Maximum durations - Post-maintenance verification</p> <p>Deliverables: - Maintenance window policy documented</p> <p>Exit criteria: Clear maintenance expectations.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#phase-5-define-communication-commitments","title":"Phase 5 \u2014 Define communication commitments","text":"<p>Objective: Document how service status is communicated.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#51-define-communication-channels","title":"5.1 Define communication channels","text":"<p>Current channels: - Changelog (public, in docs) - Incident notes (internal, some public-safe) - No dedicated status page</p> <p>Recommended approach: - Continue using changelog for planned changes - Publish public-safe incident summaries per existing policy - Consider simple status page (future, optional)</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#52-define-incident-communication","title":"5.2 Define incident communication","text":"<p>Per existing incident disclosure policy (Option B): - Publish public-safe notes when incidents affect user expectations - Sev0/Sev1: communicate within 48 hours of resolution, or as soon as practical - Sev2/Sev3: include in regular changelog if relevant</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#53-define-changelog-cadence","title":"5.3 Define changelog cadence","text":"<ul> <li>Major changes: immediate entry</li> <li>Minor changes: batched weekly/monthly</li> <li>Security updates: as appropriate (may delay for safety)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#54-document-communication-commitments","title":"5.4 Document communication commitments","text":"<p>Include: - Available channels - Incident communication timeline - Changelog cadence - What is/isn't communicated publicly</p> <p>Deliverables: - Communication commitments documented</p> <p>Exit criteria: Clear communication expectations.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#phase-6-document-performance-baselines","title":"Phase 6 \u2014 Document performance baselines","text":"<p>Objective: Establish documented baselines for key operations.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#61-capture-current-baselines","title":"6.1 Capture current baselines","text":"<p>Run baseline measurements for: - API response times (via curl timing) - Search query latency (sample queries) - Crawl throughput (pages/hour) - Indexing throughput (records/second)</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#62-document-api-baselines","title":"6.2 Document API baselines","text":"Endpoint Baseline p50 Baseline p95 Measured <code>/api/health</code> Xms Xms YYYY-MM-DD <code>/api/search?q=covid</code> Xms Xms YYYY-MM-DD <code>/api/sources</code> Xms Xms YYYY-MM-DD"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#63-document-operational-baselines","title":"6.3 Document operational baselines","text":"Operation Baseline Measured Crawl throughput X pages/hour YYYY-MM-DD Indexing throughput X records/second YYYY-MM-DD Change computation X changes/minute YYYY-MM-DD"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#64-define-baseline-review-cadence","title":"6.4 Define baseline review cadence","text":"<ul> <li>Semi-annual: review baselines against current performance</li> <li>After major changes: re-baseline if needed</li> <li>Document baseline drift if observed</li> </ul> <p>Deliverables: - Performance baselines documented - Baseline review process defined</p> <p>Exit criteria: Current performance is documented and measurable.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#phase-7-integration-and-finalization","title":"Phase 7 \u2014 Integration and finalization","text":"<p>Objective: Create canonical document and integrate into docs structure.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#71-create-the-canonical-doc","title":"7.1 Create the canonical doc","text":"<p>Create <code>docs/operations/service-levels.md</code> with all sections: - Introduction and scope - Availability SLO - Response time SLOs - Data freshness commitments - Maintenance window policy - Communication commitments - Performance baselines - Review and update process</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#72-update-navigation","title":"7.2 Update navigation","text":"<p>Add to <code>mkdocs.yml</code>: <pre><code>nav:\n  - Operations:\n    - ...\n    - Service Levels: operations/service-levels.md\n</code></pre></p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#73-add-cross-references","title":"7.3 Add cross-references","text":"<p>Update: - <code>docs/operations/monitoring-and-ci-checklist.md</code> \u2014 reference SLOs - <code>docs/operations/incident-response.md</code> \u2014 reference communication commitments</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#74-review-and-validate","title":"7.4 Review and validate","text":"<ul> <li>Verify all targets are realistic</li> <li>Confirm measurement approaches are feasible</li> <li>Check for consistency across sections</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#75-archive-this-plan","title":"7.5 Archive this plan","text":"<p>Move to <code>docs/roadmaps/implemented/</code> when complete.</p> <p>Deliverables: - <code>docs/operations/service-levels.md</code> created - Navigation updated - Cross-references added - Plan archived</p> <p>Exit criteria: Service level documentation is complete and discoverable.</p>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#risk-register-pre-mortem","title":"Risk register (pre-mortem)","text":"<ul> <li>Risk: Over-committing to targets that can't be met.</li> <li>Mitigation: Conservative targets based on single-VPS reality.</li> <li>Risk: Targets become stale as service evolves.</li> <li>Mitigation: Quarterly review cadence; update when infrastructure changes.</li> <li>Risk: Users expect contractual SLA from documentation.</li> <li>Mitigation: Clear language that this is best-effort, not contractual.</li> <li>Risk: Performance baselines are not representative.</li> <li>Mitigation: Multiple measurements; document conditions; re-baseline regularly.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-sla-and-service-commitments/#appendix-service-levels-document-template","title":"Appendix: Service levels document template","text":"<pre><code># Service Levels\n\nThis document describes the service level objectives and commitments for\nHealthArchive.ca. These are targets, not contractual guarantees.\n\n## Scope\n\nHealthArchive is a public-interest research archive. These service levels\nreflect best-effort commitments appropriate for the project's resources\nand mission.\n\n## Availability\n\n**Target:** 99.5% monthly availability\n\n- Measured via external monitoring of `/api/health`\n- Excludes planned maintenance with advance notice\n- Reviewed quarterly\n\n## Response Times\n\n| Endpoint | p95 Target |\n|----------|-----------|\n| /api/health | 100ms |\n| /api/search | 2000ms |\n| /api/sources | 300ms |\n| /api/snapshot/{id} | 300ms |\n\n## Data Freshness\n\n- Primary sources crawled monthly\n- New content searchable within 48 hours of crawl\n- Changes computed within 24 hours of indexing\n\n## Maintenance Windows\n\n- Routine: 24 hours notice, &lt; 30 minutes\n- Major: 72 hours notice, &lt; 4 hours\n- Emergency: as needed, documented afterward\n\n## Communication\n\n- Planned changes: documented in changelog\n- Incidents: public-safe summary when user expectations affected\n- No dedicated status page (may add in future)\n\n## Performance Baselines\n\n[See baseline measurements table]\n\n## Review Process\n\n- Annual review of targets vs actuals\n- Update targets when infrastructure changes significantly\n\n---\n\nLast updated: YYYY-MM-DD\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/","title":"Test coverage: critical business logic (v1) \u2014 implementation plan","text":"<p>Status: implemented (completed 2026-01-18)</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#goal","title":"Goal","text":"<p>Add comprehensive unit test coverage to the critical business logic modules that currently lack direct testing:</p> <ul> <li><code>diffing.py</code> \u2014 HTML diff algorithm, normalization, noise filtering</li> <li><code>changes.py</code> \u2014 Snapshot change computation (backfill and incremental)</li> <li><code>archive_storage.py</code> \u2014 WARC consolidation, manifests, storage statistics</li> <li><code>pages.py</code> \u2014 Page grouping SQL generation (PostgreSQL/SQLite dialects)</li> <li><code>archive_tool/strategies.py</code> \u2014 Adaptive crawl strategies and restart logic</li> </ul> <p>These modules contain complex algorithms and conditional logic that should be regression-tested to prevent subtle bugs during refactoring or feature work.</p> <p>This plan is sequential: complete each module's tests before moving to the next.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#why-this-is-next-roadmap-selection","title":"Why this is \"next\" (roadmap selection)","text":"<p>Test coverage improvements are high-leverage because:</p> <ul> <li>Prevent regressions \u2014 these modules are on critical paths (change detection,   search, storage) where bugs have high user impact.</li> <li>Enable confident refactoring \u2014 Search v3 work may touch diffing and text extraction;   tests provide a safety net.</li> <li>Low risk \u2014 adding tests doesn't change production behavior.</li> <li>Fills documented gaps \u2014 project analysis identified these specific modules.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#docs-setup-do-first","title":"Docs setup (do first)","text":"<p>1) Create this plan doc    - File: <code>docs/roadmaps/2026-01-17-test-coverage-critical-business-logic.md</code> (this document)</p> <p>2) Roadmaps index    - Update <code>docs/roadmaps/README.md</code> to list this plan under \"Implementation plans (active)\".</p> <p>3) No canonical docs changes required \u2014 this is internal test coverage work.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#scope-goals-constraints","title":"Scope, goals, constraints","text":""},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#in-scope-outcomes-what-we-will-deliver","title":"In-scope outcomes (what we will deliver)","text":"<ul> <li>Unit tests for <code>diffing.py</code>:</li> <li>HTML normalization (whitespace, case, attribute ordering)</li> <li>Heading extraction</li> <li>Banner/noise detection and filtering</li> <li>Diff algorithm edge cases (empty, identical, completely different)</li> <li> <p>Bilingual content handling (en/fr)</p> </li> <li> <p>Unit tests for <code>changes.py</code>:</p> </li> <li>Backfill computation logic</li> <li>Incremental change detection</li> <li> <p>Edge cases (first snapshot, gaps in timeline, duplicate timestamps)</p> </li> <li> <p>Unit tests for <code>archive_storage.py</code>:</p> </li> <li>WARC consolidation logic</li> <li>Manifest generation and parsing</li> <li>Storage statistics calculation</li> <li> <p>File deduplication detection</p> </li> <li> <p>Unit tests for <code>pages.py</code>:</p> </li> <li>SQL generation for PostgreSQL dialect</li> <li>SQL generation for SQLite dialect</li> <li>URL normalization in SQL context</li> <li> <p>Page grouping logic correctness</p> </li> <li> <p>Unit tests for <code>archive_tool/strategies.py</code>:</p> </li> <li>Restart strategy selection</li> <li>Fallback mechanism triggers</li> <li>Stall detection heuristics</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#non-goals-explicitly-out-of-scope","title":"Non-goals (explicitly out of scope)","text":"<ul> <li>Integration tests (existing API tests provide integration coverage).</li> <li>Performance benchmarks (defer to later if needed).</li> <li>Changes to production code (tests only, unless bugs are discovered).</li> <li>Full mutation testing (good practice but out of scope for this plan).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#constraints-to-respect","title":"Constraints to respect","text":"<ul> <li>Test isolation \u2014 tests must not require external services (use mocks/fixtures).</li> <li>SQLite compatibility \u2014 page grouping tests must work with SQLite in CI.</li> <li>Fast execution \u2014 individual test files should complete in under 30 seconds.</li> <li>Deterministic \u2014 no flaky tests; use fixed seeds for any randomness.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#current-state-map-what-exists-today","title":"Current-state map (what exists today)","text":""},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#existing-test-coverage-related","title":"Existing test coverage (related)","text":"<ul> <li><code>tests/test_api_*.py</code> \u2014 API integration tests (91 tests, good coverage)</li> <li><code>tests/test_search_fuzzy.py</code> \u2014 Fuzzy search unit tests</li> <li><code>tests/test_search_ranking.py</code> \u2014 Ranking algorithm tests</li> <li><code>tests/test_indexing_warc_verify.py</code> \u2014 WARC verification tests</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#modules-lacking-direct-tests","title":"Modules lacking direct tests","text":"Module Lines Complexity Current Tests <code>diffing.py</code> ~400 High (algorithms) None <code>changes.py</code> ~200 Medium None <code>archive_storage.py</code> ~350 Medium None <code>pages.py</code> ~150 Medium (SQL) None <code>archive_tool/strategies.py</code> ~250 High None"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#test-infrastructure","title":"Test infrastructure","text":"<ul> <li>Framework: pytest</li> <li>Database: SQLite in-memory for tests (<code>HEALTHARCHIVE_DATABASE_URL</code>)</li> <li>Fixtures: <code>conftest.py</code> provides DB session, test client, etc.</li> <li>Mocking: <code>unittest.mock</code> or <code>pytest-mock</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#definition-of-done-dod-acceptance-criteria","title":"Definition of Done (DoD) + acceptance criteria","text":""},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#test-coverage-targets","title":"Test coverage targets","text":"<p>For each module: - Minimum 80% line coverage for the module - All public functions have at least one test - Edge cases documented in test names (e.g., <code>test_diff_empty_inputs</code>) - No skipped or xfail tests without documented rationale</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#quality-requirements","title":"Quality requirements","text":"<ul> <li>All tests pass in CI (<code>make ci</code>)</li> <li>Tests are deterministic (no flaky failures)</li> <li>Test execution time &lt; 30 seconds per file</li> <li>Tests are readable and serve as documentation</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#phase-1-test-infrastructure-setup","title":"Phase 1 \u2014 Test infrastructure setup","text":"<p>Objective: Ensure test fixtures and utilities are ready for the new test files.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#11-review-existing-test-infrastructure","title":"1.1 Review existing test infrastructure","text":"<p>Examine <code>tests/conftest.py</code> for reusable fixtures: - Database session management - Test data factories - Cleanup patterns</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#12-create-shared-fixtures-for-new-tests-if-needed","title":"1.2 Create shared fixtures for new tests (if needed)","text":"<p>Consider creating in <code>tests/conftest.py</code> or a new <code>tests/fixtures/</code> directory: - HTML fixture factory (for diffing tests) - Snapshot factory with configurable fields - Mock WARC file generator</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#13-establish-test-file-naming-convention","title":"1.3 Establish test file naming convention","text":"<p>Follow existing pattern: - <code>tests/test_diffing.py</code> - <code>tests/test_changes.py</code> - <code>tests/test_archive_storage.py</code> - <code>tests/test_pages.py</code> - <code>tests/test_archive_tool_strategies.py</code></p> <p>Deliverables: - Any new shared fixtures created - Test file stubs created with proper imports</p> <p>Exit criteria: Test infrastructure ready; <code>make test</code> still passes.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#phase-2-tests-for-diffingpy","title":"Phase 2 \u2014 Tests for <code>diffing.py</code>","text":"<p>Objective: Comprehensive unit tests for the HTML diff and normalization module.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#21-understand-the-module","title":"2.1 Understand the module","text":"<p>Read <code>src/ha_backend/diffing.py</code> and document: - Public functions and their contracts - Normalization steps applied - Noise filtering heuristics - Diff output format</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#22-test-categories","title":"2.2 Test categories","text":"<p>Normalization tests: <pre><code>def test_normalize_whitespace_collapses_multiple_spaces():\n    \"\"\"Multiple whitespace characters should collapse to single space.\"\"\"\n\ndef test_normalize_whitespace_preserves_newlines_in_pre():\n    \"\"\"Whitespace in &lt;pre&gt; blocks should be preserved.\"\"\"\n\ndef test_normalize_case_lowercases_tags():\n    \"\"\"HTML tags should be case-normalized.\"\"\"\n\ndef test_normalize_attributes_sorted():\n    \"\"\"HTML attributes should be sorted for stable comparison.\"\"\"\n</code></pre></p> <p>Heading extraction tests: <pre><code>def test_extract_headings_h1_through_h6():\n    \"\"\"All heading levels should be extracted.\"\"\"\n\ndef test_extract_headings_preserves_hierarchy():\n    \"\"\"Heading hierarchy should be maintained.\"\"\"\n\ndef test_extract_headings_empty_document():\n    \"\"\"Empty document should return empty heading list.\"\"\"\n</code></pre></p> <p>Banner/noise detection tests: <pre><code>def test_detect_archived_banner_english():\n    \"\"\"English archived page banner should be detected.\"\"\"\n\ndef test_detect_archived_banner_french():\n    \"\"\"French archived page banner should be detected.\"\"\"\n\ndef test_detect_cookie_consent_banner():\n    \"\"\"Cookie consent text should be detected as noise.\"\"\"\n\ndef test_noise_filter_removes_navigation():\n    \"\"\"Navigation boilerplate should be filtered.\"\"\"\n</code></pre></p> <p>Diff algorithm tests: <pre><code>def test_diff_identical_documents():\n    \"\"\"Identical documents should produce empty diff.\"\"\"\n\ndef test_diff_empty_inputs():\n    \"\"\"Empty inputs should be handled gracefully.\"\"\"\n\ndef test_diff_completely_different():\n    \"\"\"Completely different documents should show full diff.\"\"\"\n\ndef test_diff_preserves_context():\n    \"\"\"Diff output should include context lines.\"\"\"\n\ndef test_diff_bilingual_content():\n    \"\"\"Mixed English/French content should diff correctly.\"\"\"\n</code></pre></p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#23-create-html-fixtures","title":"2.3 Create HTML fixtures","text":"<p>Create representative HTML fixtures in <code>tests/fixtures/html/</code>: - <code>simple_page.html</code> \u2014 minimal valid page - <code>canada_ca_page.html</code> \u2014 typical Canada.ca structure - <code>archived_page_en.html</code> \u2014 page with English archived banner - <code>archived_page_fr.html</code> \u2014 page with French archived banner - <code>navigation_heavy.html</code> \u2014 page with lots of nav boilerplate</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#24-implement-tests","title":"2.4 Implement tests","text":"<p>Create <code>tests/test_diffing.py</code> with all test categories.</p> <p>Deliverables: - <code>tests/test_diffing.py</code> with 15-25 tests - HTML fixtures for test cases - All tests passing</p> <p>Exit criteria: <code>diffing.py</code> has &gt;80% line coverage; <code>make ci</code> passes.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#phase-3-tests-for-changespy","title":"Phase 3 \u2014 Tests for <code>changes.py</code>","text":"<p>Objective: Unit tests for snapshot change computation logic.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#31-understand-the-module","title":"3.1 Understand the module","text":"<p>Read <code>src/ha_backend/changes.py</code> and document: - Backfill vs incremental computation - How changes are detected between snapshots - Database queries used - Edge case handling</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#32-test-categories","title":"3.2 Test categories","text":"<p>Backfill tests: <pre><code>def test_backfill_computes_changes_for_all_pairs():\n    \"\"\"Backfill should create change records for all adjacent snapshots.\"\"\"\n\ndef test_backfill_respects_max_events_limit():\n    \"\"\"Backfill should stop after max_events.\"\"\"\n\ndef test_backfill_skips_already_computed():\n    \"\"\"Backfill should not recompute existing changes.\"\"\"\n</code></pre></p> <p>Incremental tests: <pre><code>def test_incremental_processes_recent_snapshots():\n    \"\"\"Incremental should only process snapshots within time window.\"\"\"\n\ndef test_incremental_uses_correct_time_window():\n    \"\"\"Default 30-day window should be applied.\"\"\"\n</code></pre></p> <p>Edge case tests: <pre><code>def test_change_first_snapshot_no_previous():\n    \"\"\"First snapshot for a URL should not create a change record.\"\"\"\n\ndef test_change_gaps_in_timeline():\n    \"\"\"Gaps in snapshot timeline should be handled correctly.\"\"\"\n\ndef test_change_duplicate_timestamps():\n    \"\"\"Duplicate timestamps should not cause errors.\"\"\"\n\ndef test_change_same_content_no_change():\n    \"\"\"Identical content should produce 'no change' result.\"\"\"\n</code></pre></p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#33-create-test-fixtures","title":"3.3 Create test fixtures","text":"<p>Use database fixtures to create snapshot pairs with known differences: - Pair with text changes - Pair with structural changes - Pair with no changes - Pair with language change</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#34-implement-tests","title":"3.4 Implement tests","text":"<p>Create <code>tests/test_changes.py</code> with all test categories.</p> <p>Deliverables: - <code>tests/test_changes.py</code> with 12-18 tests - Database fixtures for snapshot pairs - All tests passing</p> <p>Exit criteria: <code>changes.py</code> has &gt;80% line coverage; <code>make ci</code> passes.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#phase-4-tests-for-archive_storagepy","title":"Phase 4 \u2014 Tests for <code>archive_storage.py</code>","text":"<p>Objective: Unit tests for WARC consolidation and storage management.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#41-understand-the-module","title":"4.1 Understand the module","text":"<p>Read <code>src/ha_backend/archive_storage.py</code> and document: - WARC consolidation logic - Manifest file format and parsing - Storage statistics calculation - Deduplication detection</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#42-test-categories","title":"4.2 Test categories","text":"<p>WARC consolidation tests: <pre><code>def test_consolidate_merges_warc_files():\n    \"\"\"Multiple small WARCs should consolidate into larger files.\"\"\"\n\ndef test_consolidate_preserves_record_integrity():\n    \"\"\"Consolidation should not corrupt WARC records.\"\"\"\n\ndef test_consolidate_updates_manifest():\n    \"\"\"Manifest should be updated after consolidation.\"\"\"\n</code></pre></p> <p>Manifest tests: <pre><code>def test_manifest_generation():\n    \"\"\"Manifest should list all WARCs with correct metadata.\"\"\"\n\ndef test_manifest_parsing():\n    \"\"\"Manifest file should parse correctly.\"\"\"\n\ndef test_manifest_checksum_validation():\n    \"\"\"Manifest checksums should validate correctly.\"\"\"\n</code></pre></p> <p>Storage statistics tests: <pre><code>def test_storage_stats_calculates_total_size():\n    \"\"\"Total storage size should be calculated correctly.\"\"\"\n\ndef test_storage_stats_counts_files():\n    \"\"\"File counts should be accurate.\"\"\"\n\ndef test_storage_stats_handles_missing_directory():\n    \"\"\"Missing directory should be handled gracefully.\"\"\"\n</code></pre></p> <p>Deduplication tests: <pre><code>def test_detect_duplicate_warcs():\n    \"\"\"Duplicate WARCs should be detected by content hash.\"\"\"\n\ndef test_dedup_preserves_original():\n    \"\"\"Deduplication should preserve the original file.\"\"\"\n</code></pre></p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#43-create-test-fixtures","title":"4.3 Create test fixtures","text":"<p>Create temporary directory structures with mock WARC files: - Use <code>tmp_path</code> pytest fixture - Create minimal valid WARC files for testing - Create manifest files for parsing tests</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#44-implement-tests","title":"4.4 Implement tests","text":"<p>Create <code>tests/test_archive_storage.py</code> with all test categories.</p> <p>Deliverables: - <code>tests/test_archive_storage.py</code> with 12-16 tests - Temporary file fixtures - All tests passing</p> <p>Exit criteria: <code>archive_storage.py</code> has &gt;80% line coverage; <code>make ci</code> passes.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#phase-5-tests-for-pagespy","title":"Phase 5 \u2014 Tests for <code>pages.py</code>","text":"<p>Objective: Unit tests for SQL generation and page grouping logic.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#51-understand-the-module","title":"5.1 Understand the module","text":"<p>Read <code>src/ha_backend/pages.py</code> and document: - PostgreSQL-specific SQL constructs - SQLite-specific SQL constructs - URL normalization in SQL - Page grouping criteria</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#52-test-categories","title":"5.2 Test categories","text":"<p>SQL dialect tests: <pre><code>def test_sql_generation_postgresql():\n    \"\"\"PostgreSQL dialect should generate valid SQL.\"\"\"\n\ndef test_sql_generation_sqlite():\n    \"\"\"SQLite dialect should generate valid SQL.\"\"\"\n\ndef test_sql_dialect_detection():\n    \"\"\"Correct dialect should be detected from connection.\"\"\"\n</code></pre></p> <p>URL normalization tests: <pre><code>def test_url_normalize_removes_trailing_slash():\n    \"\"\"Trailing slashes should be normalized.\"\"\"\n\ndef test_url_normalize_lowercases_host():\n    \"\"\"Hostname should be lowercased.\"\"\"\n\ndef test_url_normalize_removes_fragments():\n    \"\"\"URL fragments should be removed for grouping.\"\"\"\n\ndef test_url_normalize_sorts_query_params():\n    \"\"\"Query parameters should be sorted for stable grouping.\"\"\"\n</code></pre></p> <p>Page grouping tests: <pre><code>def test_group_pages_by_normalized_url():\n    \"\"\"Snapshots with same normalized URL should group together.\"\"\"\n\ndef test_group_pages_respects_source():\n    \"\"\"Grouping should respect source boundaries.\"\"\"\n\ndef test_group_pages_ordering():\n    \"\"\"Groups should be ordered by specified criteria.\"\"\"\n</code></pre></p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#53-test-both-dialects","title":"5.3 Test both dialects","text":"<p>Ensure tests run with both SQLite (CI default) and can verify PostgreSQL logic through query string inspection (without requiring a real PostgreSQL instance).</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#54-implement-tests","title":"5.4 Implement tests","text":"<p>Create <code>tests/test_pages.py</code> with all test categories.</p> <p>Deliverables: - <code>tests/test_pages.py</code> with 12-15 tests - Tests for both SQL dialects - All tests passing</p> <p>Exit criteria: <code>pages.py</code> has &gt;80% line coverage; <code>make ci</code> passes.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#phase-6-tests-for-archive_toolstrategiespy","title":"Phase 6 \u2014 Tests for <code>archive_tool/strategies.py</code>","text":"<p>Objective: Unit tests for adaptive crawl strategies and restart logic.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#61-understand-the-module","title":"6.1 Understand the module","text":"<p>Read <code>src/archive_tool/strategies.py</code> and document: - Strategy selection criteria - Restart triggers and conditions - Fallback mechanisms - Stall detection heuristics</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#62-test-categories","title":"6.2 Test categories","text":"<p>Strategy selection tests: <pre><code>def test_strategy_selection_fresh_crawl():\n    \"\"\"Fresh crawl should select appropriate strategy.\"\"\"\n\ndef test_strategy_selection_resume():\n    \"\"\"Resume should select continuation strategy.\"\"\"\n\ndef test_strategy_selection_after_failure():\n    \"\"\"After failure should select recovery strategy.\"\"\"\n</code></pre></p> <p>Restart logic tests: <pre><code>def test_restart_trigger_on_stall():\n    \"\"\"Stall detection should trigger restart.\"\"\"\n\ndef test_restart_respects_max_retries():\n    \"\"\"Restart should not exceed max retries.\"\"\"\n\ndef test_restart_preserves_progress():\n    \"\"\"Restart should preserve crawl progress.\"\"\"\n</code></pre></p> <p>Fallback tests: <pre><code>def test_fallback_reduces_workers():\n    \"\"\"Fallback should reduce worker count.\"\"\"\n\ndef test_fallback_adjusts_timeouts():\n    \"\"\"Fallback should adjust timeout settings.\"\"\"\n\ndef test_fallback_chain_order():\n    \"\"\"Fallbacks should apply in correct order.\"\"\"\n</code></pre></p> <p>Stall detection tests: <pre><code>def test_stall_detection_no_progress():\n    \"\"\"No progress for threshold time should detect stall.\"\"\"\n\ndef test_stall_detection_slow_progress():\n    \"\"\"Slow but progressing crawl should not trigger stall.\"\"\"\n\ndef test_stall_detection_log_parsing():\n    \"\"\"Stall detection should parse crawler logs correctly.\"\"\"\n</code></pre></p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#63-mock-external-dependencies","title":"6.3 Mock external dependencies","text":"<p>Use mocks for: - Docker process interaction - File system state - Time-based triggers</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#64-implement-tests","title":"6.4 Implement tests","text":"<p>Create <code>tests/test_archive_tool_strategies.py</code> with all test categories.</p> <p>Deliverables: - <code>tests/test_archive_tool_strategies.py</code> with 12-18 tests - Mocks for external dependencies - All tests passing</p> <p>Exit criteria: <code>strategies.py</code> has &gt;80% line coverage; <code>make ci</code> passes.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#phase-7-coverage-verification-and-documentation","title":"Phase 7 \u2014 Coverage verification and documentation","text":"<p>Objective: Verify overall coverage improvement and document the test suite.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#71-run-coverage-report","title":"7.1 Run coverage report","text":"<pre><code>pytest --cov=src/ha_backend --cov=src/archive_tool --cov-report=html\n</code></pre> <p>Verify that targeted modules have &gt;80% coverage.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#72-document-test-suite","title":"7.2 Document test suite","text":"<p>Add comments to each new test file explaining: - What the module does - Test categories and their purpose - How to run tests in isolation</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#73-update-agentsmd-if-applicable","title":"7.3 Update AGENTS.md (if applicable)","text":"<p>If test patterns or conventions changed, update <code>AGENTS.md</code> testing section.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#74-archive-this-plan","title":"7.4 Archive this plan","text":"<p>Move to <code>docs/roadmaps/implemented/</code> when complete.</p> <p>Deliverables: - Coverage report showing improvement - Test documentation complete - Plan archived</p> <p>Exit criteria: All target modules at &gt;80% coverage; <code>make ci</code> passes.</p>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#risk-register-pre-mortem","title":"Risk register (pre-mortem)","text":"<ul> <li>Risk: Tests reveal bugs in production code.</li> <li>Mitigation: Fix bugs as separate commits; document in test comments.</li> <li>Risk: Tests are slow due to complex fixtures.</li> <li>Mitigation: Use minimal fixtures; mock expensive operations.</li> <li>Risk: SQLite/PostgreSQL behavior differences cause test failures.</li> <li>Mitigation: Test SQL generation separately from execution where needed.</li> <li>Risk: Mocking makes tests brittle.</li> <li>Mitigation: Mock at boundaries, not internals; prefer real objects when cheap.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-17-test-coverage-critical-business-logic/#appendix-test-execution-commands","title":"Appendix: Test execution commands","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test file\npytest tests/test_diffing.py -v\n\n# Run with coverage\npytest --cov=src/ha_backend/diffing tests/test_diffing.py\n\n# Run tests matching pattern\npytest -k \"diff\" -v\n\n# Run tests with verbose output\npytest tests/test_changes.py -v --tb=short\n</code></pre>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/","title":"Annual crawl resiliency hardening (implemented) \u2014 2026-01-19","text":"<p>This is a historical implementation note capturing a production incident follow-up for the 2026 annual campaign.</p>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#problem-statement","title":"Problem statement","text":"<p>During the 2026 annual campaign, we observed:</p> <ul> <li>Retry churn / starvation: jobs could remain <code>retryable</code> and not make forward progress due to queue tie-breaking and long backoff delays.</li> <li>Restart thrash on long annual crawls (notably <code>canada.ca</code>): low timeout thresholds triggered repeated adaptive restarts + long backoffs, reducing throughput.</li> <li>Config error failures (e.g., invalid Zimit args) that consumed time and retries without a clear \u201cthis is a config bug\u201d signal.</li> <li>Limited observability into restart/worker-throttle state without manually inspecting <code>.archive_state.json</code>.</li> </ul> <p>HealthArchive policy is completeness-first archival, so \u201cpage caps\u201d and other early-stop controls are not acceptable for annual crawls.</p>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#goals","title":"Goals","text":"<ul> <li>Make annual crawl defaults more resilient to long-tail timeouts and transient network/protocol noise.</li> <li>Make the annual job pick order deterministic in a single-worker environment.</li> <li>Prevent retry churn on invalid CLI/config failures by classifying them as configuration errors.</li> <li>Improve monitoring to detect restart thrash and state-file health issues.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#work-implemented","title":"Work implemented","text":""},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#safer-annual-defaults","title":"Safer annual defaults","text":"<ul> <li>Annual job tool options now default to:</li> <li><code>max_container_restarts = 20</code></li> <li><code>error_threshold_timeout = 50</code></li> <li><code>error_threshold_http = 50</code></li> <li><code>backoff_delay_minutes = 2</code></li> <li>Files:</li> <li><code>src/ha_backend/job_registry.py</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#deterministic-annual-queue-order","title":"Deterministic annual queue order","text":"<ul> <li><code>ha-backend schedule-annual</code> staggers <code>queued_at</code> timestamps (hc \u2192 phac \u2192 cihr).</li> <li>Files:</li> <li><code>src/ha_backend/cli.py</code></li> <li>Doc note: <code>docs/operations/annual-campaign.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#configuration-error-classification","title":"Configuration error classification","text":"<ul> <li>Persist combined log path when available and classify \u201cinvalid CLI args\u201d failures as <code>infra_error_config</code>.</li> <li>Files:</li> <li><code>src/ha_backend/jobs.py</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#auto-recover-guardrails","title":"Auto-recover guardrails","text":"<ul> <li>Auto-recover now enforces annual minimums for restart budget and thresholds when recovering jobs.</li> <li>Files:</li> <li><code>scripts/vps-crawl-auto-recover.py</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#monitoring-improvements","title":"Monitoring improvements","text":"<ul> <li><code>vps-crawl-metrics-textfile.py</code> now exports <code>.archive_state.json</code> health + counters:</li> <li>state file probe OK/errno</li> <li>parse OK</li> <li>mtime age seconds</li> <li>current workers, reductions, container restarts, VPN rotations, temp dir count</li> <li>Files:</li> <li><code>scripts/vps-crawl-metrics-textfile.py</code></li> <li><code>docs/operations/playbooks/storagebox-sshfs-stale-mount-recovery.md</code></li> <li><code>docs/reference/archive-tool.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#documentation-corrections","title":"Documentation corrections","text":"<ul> <li>Corrected misleading mention of unsupported Zimit page-cap args and clarified that annual crawls should not use caps.</li> <li>Files:</li> <li><code>src/archive_tool/docs/documentation.md</code></li> <li><code>docs/tutorials/debug-crawl.md</code></li> <li><code>docs/architecture.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#tests","title":"Tests","text":"<ul> <li>Added/updated tests for ordering, auto-recover tool option enforcement, config error classification, and state-file metrics.</li> <li>Files:</li> <li><code>tests/test_cli_schedule_annual.py</code></li> <li><code>tests/test_ops_crawl_auto_recover_tool_options.py</code></li> <li><code>tests/test_jobs_persistent.py</code></li> <li><code>tests/test_ops_crawl_metrics_textfile_state.py</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#definition-of-done-checked","title":"Definition of done (checked)","text":"<ul> <li> Annual defaults updated in code.</li> <li> Annual queue ordering made deterministic.</li> <li> Invalid CLI/config failures classified as <code>infra_error_config</code>.</li> <li> Auto-recover enforces annual minimum guardrails.</li> <li> Metrics exporter includes state-file health + restart counters.</li> <li> Docs updated to reflect reality and policy (no annual caps).</li> <li> Tests updated/added and pass in CI gate (<code>make ci</code>).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#follow-ups-not-part-of-this-change","title":"Follow-ups (not part of this change)","text":"<ul> <li>Confirm production deployment and validate dashboards/alerts incorporate the new state-file metrics.</li> <li>Ensure indexing runs after crawl completion and that WARC discovery/counting is consistent between CLI/status output and the indexing pipeline.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening/#references","title":"References","text":"<ul> <li>Decision record:</li> <li><code>docs/decisions/2026-01-19-annual-crawl-resiliency-and-queue-order.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/","title":"Annual crawl throughput and WARC-first artifacts (implemented, 2026-01-23)","text":"<p>Status: implemented</p>"},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#goal","title":"Goal","text":"<p>Increase annual crawl throughput on the single production VPS while staying aligned with campaign values:</p> <ul> <li>completeness-first within explicit scope boundaries</li> <li>accuracy and reproducibility</li> <li>search-first readiness (WARCs indexed ASAP)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#constraints","title":"Constraints","text":"<ul> <li>Production: Hetzner <code>cx33</code> (4 vCPU / 8GB RAM / 80GB SSD)</li> <li>Optional StorageBox is for cold storage/tiering, not crawl hot-path I/O.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#changes-implemented","title":"Changes implemented","text":""},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#1-warc-first-annual-pipeline-skip-optional-zim-build","title":"1) WARC-first annual pipeline (skip optional ZIM build)","text":"<ul> <li>Added <code>archive_tool</code> support for skipping the final <code>--warcs</code> ZIM stage:</li> <li><code>archive-tool --skip-final-build</code></li> <li>Wired through DB job config:</li> <li><code>tool_options.skip_final_build = true</code></li> <li>Annual source defaults now set <code>skip_final_build=true</code> so the crawl exits successfully once WARCs are produced, enabling indexing to start sooner.</li> </ul> <p>Rationale:</p> <ul> <li>The backend indexes WARCs; <code>.zim</code> is an optional artifact and is not required for annual \u201cdone\u201d.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#2-container-devshm-tuning-for-stability","title":"2) Container <code>/dev/shm</code> tuning for stability","text":"<ul> <li>Added <code>archive-tool --docker-shm-size &lt;value&gt;</code> and pass through to <code>docker run --shm-size</code>.</li> <li>Annual defaults set <code>docker_shm_size=\"1g\"</code> for browser-driven crawl stability.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#3-modest-parallelism-on-the-single-vps","title":"3) Modest parallelism on the single VPS","text":"<ul> <li>Annual defaults increased to <code>initial_workers=2</code> for all three v1 sources.</li> <li><code>canada.ca</code> sources default <code>stall_timeout_minutes=60</code> to avoid false-stall recoveries on long-tail pages.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#4-reduce-duplicatetrap-like-url-expansion-on-shared-host-canadaca","title":"4) Reduce duplicate/trap-like URL expansion on shared-host canada.ca","text":"<ul> <li>Hardened the allowlist regexes for <code>hc</code> and <code>phac</code> content paths to exclude querystring/fragment variants (assets remain permissive).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#docs-updated","title":"Docs updated","text":"<ul> <li>Archive tool reference and internals docs for new flags.</li> <li>Annual campaign doc clarified WARC-first/search-first posture and PDF indexing non-goal for v1.</li> <li>Production runbook includes swap recommendation and \u201clocal SSD hot-path\u201d guidance.</li> <li>Decision record captured in <code>docs/decisions/2026-01-23-annual-crawl-throughput-and-artifacts.md</code>.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#verification","title":"Verification","text":"<ul> <li>Repo checks: <code>make ci</code> (ruff, mypy, pytest).</li> <li>Tests updated for:</li> <li>new default tool options</li> <li>canada.ca scope regex expectations</li> <li>auto-recover tool option behavior</li> </ul>"},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#follow-ups-ops","title":"Follow-ups (ops)","text":"<ul> <li>Deploy the updated backend and restart the worker on production.</li> <li>For already-created annual jobs, ensure their <code>tool_options</code> reflect the desired values if you want them to take effect on the next retry/recovery cycle.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#references","title":"References","text":"<ul> <li>Decision record: <code>../decisions/2026-01-23-annual-crawl-throughput-and-artifacts.md</code></li> <li>Annual campaign scope: <code>../operations/annual-campaign.md</code></li> <li>Production runbook: <code>../deployment/production-single-vps.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/","title":"Infra-error retry storms + Storage Box hot-path resilience \u2014 2026-01-24","text":"<p>Status: implemented (completed 2026-01-24)</p> <p>This plan targets a specific failure mode observed during the 2026 annual campaign on the single-VPS deployment: a Storage Box / <code>sshfs</code>-backed mount (or bind mount onto it) became stale and started raising:</p> <ul> <li><code>OSError: [Errno 107] Transport endpoint is not connected</code></li> </ul> <p>The immediate impact was a retry storm (worker tight-looping on a <code>retryable</code> job that always fails instantly) and an operator-visible \u201ceverything stopped\u201d state when the worker ended up stopped and stayed down.</p> <p>This plan focuses on two outcomes:</p> <p>1) Prevention / mitigation (make the system harder to wedge and reduce alert storms). 2) Automated safe recovery (repair stale hot paths and resume work without manual ops).</p> <p>This plan is intentionally detailed so it can be implemented incrementally and verified with high confidence.</p>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#background-what-happened","title":"Background (what happened)","text":"<p>Observed behavior (condensed):</p> <ul> <li>A job output directory under <code>/srv/healtharchive/jobs/**</code> became unreadable (Errno 107).</li> <li>The worker repeatedly selected the same <code>retryable</code> job and immediately failed it as   <code>crawler_status=infra_error</code> without consuming retry budget.</li> <li>This created high log volume and lots of notifications.</li> <li>Separately, the storage hot-path recovery automation ran but did not fully repair all   relevant mountpoints, and the worker remained down until manually started.</li> </ul> <p>Important nuance:</p> <ul> <li><code>findmnt</code> can still show a mountpoint even when it is stale; always probe with a bounded   <code>ls</code>/<code>stat</code> to detect \u201cmounted but unreadable\u201d.</li> </ul> <p>Related historical context:</p> <ul> <li>Storage Box stale-mount recovery baseline plan (implemented): <code>roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> </ul> <p>This plan closes gaps revealed by the 2026-01-24 incident (notably: retry storms and stale hot paths affecting queued/retryable jobs, not just <code>status=running</code> jobs).</p>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#goals","title":"Goals","text":""},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#g1-stop-alert-storms-from-infra-errors","title":"G1) Stop alert storms from infra errors","text":"<p>If a job fails due to infrastructure (<code>crawler_status=infra_error</code>), do not let the worker immediately re-run it in a tight loop.</p>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#g2-recover-automatically-when-the-hot-path-is-stale","title":"G2) Recover automatically when the hot path is stale","text":"<p>If a hot path is stale (Errno 107), automatically:</p> <ul> <li>quiesce only when necessary,</li> <li>unmount the exact stale mountpoint(s),</li> <li>restart the base Storage Box mount if required,</li> <li>re-apply tiering/bind mounts,</li> <li>and resume worker progress.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#g3-preserve-correctness-completeness-integrity","title":"G3) Preserve correctness (completeness + integrity)","text":"<ul> <li>Do not \u201cfix\u201d infra issues by deleting data.</li> <li>Avoid actions that can corrupt in-flight outputs (prefer quiesce before remounting).</li> <li>Ensure recovery is bounded and idempotent.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#non-goals","title":"Non-goals","text":"<ul> <li>Eliminating all Storage Box / network failures (we can only mitigate).</li> <li>Adding new infrastructure (multi-VPS, object storage, etc.) in this plan.</li> <li>Making crawls \u201cfast\u201d by adding page/depth caps (policy: completeness-first).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#constraints-and-invariants-single-vps-discipline","title":"Constraints and invariants (single VPS discipline)","text":"<ul> <li>Worker is a scarce resource; avoid flapping it.</li> <li>Automation must be:</li> <li>idempotent</li> <li>bounded / rate-limited</li> <li>safe-by-default</li> <li>gated by sentinel files</li> <li>observable (metrics + logs)</li> <li>instantly disable-able</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#workstreams","title":"Workstreams","text":""},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#a-worker-resilience-prevent-tight-retry-loops","title":"A) Worker resilience (prevent tight retry loops)","text":""},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#a1-add-a-cooldown-for-recent-infra_error-jobs","title":"A1) Add a cooldown for recent infra_error jobs","text":"<p>Status: implemented in code and merged to <code>main</code>.</p> <p>Change:</p> <ul> <li>The worker skips jobs whose latest <code>crawler_status == \"infra_error\"</code> for a cooldown window   (currently 10 minutes).</li> </ul> <p>Why:</p> <ul> <li>Prevents \u201cpick \u2192 fail \u2192 pick \u2192 fail \u2026\u201d storms when the underlying issue is not job-specific.</li> </ul> <p>Files:</p> <ul> <li><code>src/ha_backend/worker/main.py</code></li> <li><code>tests/test_worker.py</code></li> </ul> <p>Deployment note:</p> <ul> <li>The cooldown only takes effect after <code>healtharchive-worker.service</code> restarts (safe boundary:   no active <code>status=running</code> job).</li> </ul> <p>Acceptance criteria:</p> <ul> <li>A single infra_error job cannot generate high-rate logs/alerts.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#a2-optional-persist-next-attempt-time-in-db","title":"A2) (Optional) Persist next-attempt time in DB","text":"<p>Status: not implemented (candidate improvement).</p> <p>Current implementation uses a query-time filter on <code>updated_at</code>. If we later want explicit auditability, add a field in <code>ArchiveJob.config</code> (no schema change) like:</p> <ul> <li><code>infra_error_next_attempt_utc</code></li> </ul> <p>Then the worker and/or watchdog can share the same semantics explicitly.</p> <p>Decision point:</p> <ul> <li>Only implement if we need better forensic visibility than \u201cupdated_at cutoff\u201d.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#b-storage-hot-path-auto-recovery-hardening","title":"B) Storage hot-path auto-recovery hardening","text":"<p>We already have <code>healtharchive-storage-hotpath-auto-recover.timer</code> + script:</p> <ul> <li><code>scripts/vps-storage-hotpath-auto-recover.py</code></li> </ul> <p>The incident showed two gaps:</p> <p>1) Recovery focused on running job hot paths + a manifest list, but a stale hot path for a    retryable job can still wedge the worker. 2) Recovery could fail downstream (e.g. annual output tiering fails on stale mountpoints not in    the initial probe set), leaving the system \u201cstopped\u201d.</p>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#b1-detect-stale-mountpoints-for-next-jobs-queuedretryable-not-only-running-jobs","title":"B1) Detect stale mountpoints for \u201cnext jobs\u201d (queued/retryable), not only running jobs","text":"<p>Status: implemented.</p> <p>Add to <code>vps-storage-hotpath-auto-recover.py</code>:</p> <ul> <li>Query the DB for the next N jobs by the same ordering the worker uses:</li> <li><code>status IN (queued, retryable)</code></li> <li>order by <code>queued_at</code> then <code>created_at</code></li> <li>limit (suggest: 5\u201310)</li> <li>Probe <code>output_dir</code> readability for those jobs and treat Errno 107 as an eligible stale target.</li> </ul> <p>Safety rules:</p> <ul> <li>Only unmount if:</li> <li>the path is under <code>jobs_root</code> (e.g. <code>/srv/healtharchive/jobs</code>)</li> <li>it is an exact mountpoint target (do not unmount parent mounts)</li> <li>the probe returns Errno 107</li> </ul> <p>Acceptance criteria:</p> <ul> <li>A stale mountpoint for a retryable job is repaired before the worker repeatedly selects it.</li> </ul> <p>Tests:</p> <ul> <li>Unit test covering that queued/retryable job output dirs are included in detection and that   only exact mountpoint targets are eligible.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#b2-make-annual-output-tiering-repairable-during-recovery-runs","title":"B2) Make annual output tiering repairable during recovery runs","text":"<p>Status: implemented.</p> <p>Problem:</p> <ul> <li><code>scripts/vps-annual-output-tiering.py --apply</code> refuses to proceed when it finds a stale   mountpoint unless <code>--repair-stale-mounts</code> is provided.</li> </ul> <p>Fix:</p> <ul> <li>In <code>vps-storage-hotpath-auto-recover.py</code>, when running in apply mode and the worker is already   quiesced (or there are no running jobs), call:</li> <li><code>vps-annual-output-tiering.py --apply --repair-stale-mounts</code></li> </ul> <p>Safety gate:</p> <ul> <li>Only run <code>--repair-stale-mounts</code> when we are certain it will not interrupt an in-flight crawl:</li> <li>either worker is stopped by the script because the running job hot path is stale, or</li> <li>there are no running jobs to begin with.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>A recovery run can repair stale annual job output mounts without failing early.</li> </ul> <p>Tests:</p> <ul> <li>Unit test that the recovery plan selects the \u201crepair\u201d invocation only when the quiesce gate is met.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#b3-dont-stop-the-worker-for-cold-only-issues-when-a-crawl-is-running-and-healthy","title":"B3) Don\u2019t stop the worker for \u201ccold-only\u201d issues when a crawl is running and healthy","text":"<p>Status: implemented.</p> <p>Refine quiesce logic:</p> <ul> <li>If there is a running job and its hot path is healthy, do not stop the worker just to repair   a stale mountpoint for a different (non-running) job.</li> </ul> <p>Rationale:</p> <ul> <li>Stopping the worker terminates the in-flight crawl container; that risks completeness and wastes time.</li> </ul> <p>Acceptance criteria:</p> <ul> <li>If <code>hc</code> crawl is running and healthy, recovery can still fix a stale <code>phac</code> mountpoint without   stopping the <code>hc</code> crawl.</li> </ul> <p>Implementation note (small plan deviation, safety improvement):</p> <ul> <li>When the DB query succeeds and there are no running jobs, the hot-path recovery script now   stops the worker before mount repairs to prevent a race where the worker could start a crawl   mid-repair.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#b4-post-failure-safety-ensure-worker-stopped-cannot-become-a-long-lived-state","title":"B4) Post-failure safety: ensure \u201cworker stopped\u201d cannot become a long-lived state","text":"<p>Status: implemented.</p> <p>Two complementary changes:</p> <p>1) Improve the storage hot-path script so it restarts the worker when safe even if some    secondary steps fail (e.g., tiering metrics write), as long as hot paths are healthy. 2) Add a separate \u201cworker resurrection\u201d timer (Workstream C) so a single failed recovery run    does not strand the system.</p> <p>Acceptance criteria:</p> <ul> <li>A single transient failure cannot keep the worker down for hours.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#c-worker-resurrection-watchdog-safe-auto-start","title":"C) Worker \u201cresurrection\u201d watchdog (safe auto-start)","text":"<p>Status: implemented.</p> <p>Add a lightweight, production-only automation that ensures the worker is running when it should be.</p> <p>Proposed implementation:</p> <ul> <li>New script: <code>scripts/vps-worker-auto-start.py</code></li> <li>New systemd service + timer:</li> <li><code>healtharchive-worker-auto-start.service</code></li> <li><code>healtharchive-worker-auto-start.timer</code> (suggest: every 2\u20135 minutes)</li> <li>Gated by sentinel file:</li> <li><code>/etc/healtharchive/worker-auto-start-enabled</code></li> </ul> <p>Refusal rules (must be conservative):</p> <ul> <li>If deploy lock exists and is not stale \u2192 refuse.</li> <li>If Storage Box mount is not readable \u2192 refuse.</li> <li>If there is any <code>status=running</code> job but worker is inactive \u2192 do not auto-start   (prefer manual investigation; this indicates a possibly mid-flight/partial state).</li> <li>If there are no <code>queued</code>/<code>retryable</code> jobs \u2192 do nothing.</li> </ul> <p>Action:</p> <ul> <li>If worker is inactive AND there are queued/retryable jobs AND storage probes pass \u2192 start worker.</li> </ul> <p>Observability:</p> <ul> <li>Write a small textfile metric:</li> <li>last auto-start attempt timestamp</li> <li>last result (ok/fail/skip + reason)</li> </ul> <p>Acceptance criteria:</p> <ul> <li>After a storage incident is repaired, the system resumes without a human starting the worker.</li> </ul> <p>Tests:</p> <ul> <li>Unit tests for refusal rules and \u201cstart decision\u201d logic (no integration tests).</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#d-alerting-metrics-reduce-noisy-notifications-increase-signal","title":"D) Alerting + metrics (reduce noisy notifications, increase signal)","text":"<p>Status: implemented.</p> <p>Add/adjust alerting so:</p> <ul> <li>We page on \u201cworker down while jobs pending\u201d (high-signal), not on \u201cmany identical infra_error logs\u201d.</li> <li>We page on \u201cstorage hot-path unrecovered for &gt; X minutes\u201d (bounded).</li> </ul> <p>Suggested metrics additions (textfile-based, DB-derived):</p> <ul> <li><code>healtharchive_jobs_infra_error_recent_total{minutes=\"10\"}</code> (count)</li> <li><code>healtharchive_worker_should_be_running</code> (boolean: jobs pending + storage ok)</li> </ul> <p>Acceptance criteria:</p> <ul> <li>One incident produces one actionable alert, not a flood.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#e-documentation-operator-ux","title":"E) Documentation + operator UX","text":"<p>Status: implemented.</p> <p>Required updates when workstreams B/C/D ship:</p> <ul> <li>Update playbook:</li> <li><code>docs/operations/playbooks/storagebox-sshfs-stale-mount-recovery.md</code></li> <li>include \u201ccooldown semantics\u201d and \u201cworker-auto-start semantics\u201d</li> <li>Update systemd docs:</li> <li><code>docs/deployment/systemd/README.md</code></li> <li>add the new worker-auto-start unit(s) and enablement guidance.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#rollout-plan-safe-sequencing","title":"Rollout plan (safe sequencing)","text":"<p>1) Land code changes behind safe defaults (no automation enablement changes yet). 2) Add tests for all new logic and keep them unit-level and fast. 3) Deploy backend code to the VPS with <code>vps-deploy.sh --skip-restart</code> (crawl-safe). 4) At a safe boundary (no <code>status=running</code> job), restart the worker to pick up:    - worker infra_error cooldown (already merged)    - any new worker/automation logic (once implemented) 5) Run automation verification rituals:    - <code>scripts/verify_ops_automation.sh</code> (and optionally <code>--json</code>) 6) Enable new timers only after dry-runs look correct and caps are in place:    - create sentinel files under <code>/etc/healtharchive/*enabled</code></p>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#definition-of-done-this-plan","title":"Definition of done (this plan)","text":"<ul> <li> Worker has an infra_error cooldown (prevents tight retry loops).</li> <li> Storage hot-path recovery detects stale mountpoints for \u201cnext jobs\u201d (queued/retryable).</li> <li> Storage hot-path recovery can repair annual output tiering stale mounts (<code>--repair-stale-mounts</code>) safely.</li> <li> Worker is not stopped to fix unrelated cold paths when a crawl is healthy.</li> <li> Worker can be auto-started safely when it is down and jobs are pending (sentinel-gated).</li> <li> High-signal alerts exist for \u201cworker down while jobs pending\u201d and \u201chot path stale too long\u201d.</li> <li> Playbooks/systemd docs updated to reflect the new behavior.</li> </ul>"},{"location":"roadmaps/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#references","title":"References","text":"<ul> <li>Storage Box stale mount recovery baseline (implemented): <code>roadmaps/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> <li>Annual crawl resiliency hardening (implemented): <code>roadmaps/implemented/2026-01-19-annual-crawl-resiliency-hardening.md</code></li> <li>Ops automation plan (active): <code>operations/automation-implementation-plan.md</code></li> <li>Incident note: <code>operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/","title":"Archive Tool Hardening and Ops Improvements (2026-01-27)","text":"<p>Implementation plan completed 2026-01-27. This work addressed 35+ improvements across 5 phases, hardening the archive_tool crawler and ops automation.</p>"},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#summary","title":"Summary","text":"<p>This plan improved reliability, observability, and code quality across: - <code>archive_tool</code> subpackage (crawler orchestration) - VPS automation scripts (crawl recovery, metrics, storage hotpath) - Prometheus alerting rules - Backend job management</p>"},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phases","title":"Phases","text":""},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phase-1-pre-crawl-critical-fixes","title":"Phase 1: Pre-Crawl Critical Fixes","text":"<ul> <li>Docker memory/CPU limits (configurable via environment)</li> <li>CIHR stall_timeout override in job registry</li> <li>Thread lock for state file operations</li> <li>OSError handling for stale mounts</li> <li>Pre-crawl output directory writability check</li> <li>Exception handling hardening in main.py</li> </ul>"},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phase-2-operational-automation-improvements","title":"Phase 2: Operational Automation Improvements","text":"<ul> <li>Deploy lock check in crawl-auto-recover.py</li> <li>Prometheus textfile metrics for crawl recovery</li> <li>Lock file to prevent concurrent recovery runs</li> <li>fsync for durable state writes</li> <li>OSError handling in log file discovery</li> </ul>"},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phase-3-monitoring-and-observability","title":"Phase 3: Monitoring and Observability","text":"<ul> <li>Per-job error type counters in crawl metrics</li> <li>Search error type breakdown in runtime metrics</li> <li>Alert for slow crawl rate (&lt;5 ppm for 30m)</li> <li>Alert for high infra_error rate (&gt;=3 in 10m)</li> <li>Alert for stale crawl metrics (&gt;10m old)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phase-4-architecture-and-code-quality","title":"Phase 4: Architecture and Code Quality","text":"<ul> <li>Extracted timeout magic numbers to constants.py</li> <li>Extracted CLI defaults to constants.py</li> <li>Moved late imports to module top in utils.py</li> <li>Consolidated stats regex pattern to constants.py</li> <li>Docker resource limits configurable via environment variables:</li> <li><code>HEALTHARCHIVE_DOCKER_MEMORY_LIMIT</code> (default: \"4g\")</li> <li><code>HEALTHARCHIVE_DOCKER_CPU_LIMIT</code> (default: \"1.5\")</li> <li><code>HEALTHARCHIVE_ZIMIT_DOCKER_IMAGE</code> (default: \"ghcr.io/openzim/zimit\")</li> </ul>"},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phase-5-documentation","title":"Phase 5: Documentation","text":"<ul> <li>Comprehensive docstrings for monitor.py key functions</li> <li>Comprehensive docstrings for docker_runner.py key functions</li> <li>Comprehensive docstrings for state.py key functions</li> <li>Inline comments for main.py complex logic</li> <li>Inline comments for state.py persistence logic</li> </ul>"},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#key-files-changed","title":"Key Files Changed","text":"<p>archive_tool subpackage: - <code>src/archive_tool/constants.py</code> - Centralized timeout/CLI/resource constants - <code>src/archive_tool/docker_runner.py</code> - Docker orchestration with resource limits - <code>src/archive_tool/main.py</code> - Main loop with improved comments - <code>src/archive_tool/monitor.py</code> - Log monitoring with docstrings - <code>src/archive_tool/state.py</code> - State persistence with thread safety - <code>src/archive_tool/cli.py</code> - CLI using named constants</p> <p>Backend: - <code>src/ha_backend/jobs.py</code> - Retry cap enforcement - <code>src/ha_backend/job_registry.py</code> - CIHR stall timeout - <code>src/ha_backend/runtime_metrics.py</code> - Error type breakdown - <code>src/ha_backend/infra_errors.py</code> - Network errno handling - <code>src/ha_backend/crawl_stats.py</code> - Error count metrics</p> <p>Automation scripts: - <code>scripts/vps-crawl-auto-recover.py</code> - Deploy lock, metrics, lock file - <code>scripts/vps-crawl-metrics-textfile.py</code> - Error type metrics - <code>scripts/vps-storage-hotpath-auto-recover.py</code> - fsync - <code>scripts/vps-worker-auto-start.py</code> - fsync helper</p> <p>Alerting: - <code>ops/observability/alerting/healtharchive-alerts.yml</code> - New alerts</p>"},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#testing","title":"Testing","text":"<p>All existing tests continue to pass. The improvements focus on runtime resilience rather than new testable features. Future work (deferred): add integration tests for main.py stage loop.</p>"},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li><code>docs/reference/archive-tool.md</code> - Points to constants for resource limits</li> <li>Environment variables documented in implementation plan (this file)</li> </ul>"},{"location":"roadmaps/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#related","title":"Related","text":"<ul> <li>Previous hardening: <code>2026-01-24-infra-error-and-storage-hotpath-hardening.md</code></li> <li>Previous crawl throughput: <code>2026-01-23-annual-crawl-throughput-and-artifacts.md</code></li> </ul>"},{"location":"roadmaps/implemented/2026-01-28-patch-job-config-and-integration-tests/","title":"2026-01-28: patch-job-config Command and Integration Tests","text":"<p>Status: Implemented 2026-01-28 Scope: Phase 2 implementation from hardening backlog</p>"},{"location":"roadmaps/implemented/2026-01-28-patch-job-config-and-integration-tests/#overview","title":"Overview","text":"<p>Implemented two improvements from the post-hardening backlog: 1. <code>patch-job-config</code> CLI command for modifying job tool_options without recreating jobs 2. Integration tests for <code>archive_tool/main.py</code> stage loop orchestration</p>"},{"location":"roadmaps/implemented/2026-01-28-patch-job-config-and-integration-tests/#implementation-details","title":"Implementation Details","text":""},{"location":"roadmaps/implemented/2026-01-28-patch-job-config-and-integration-tests/#1-patch-job-config-command","title":"1. patch-job-config Command","text":"<p>Location: <code>src/ha_backend/cli.py</code></p> <p>Purpose: Enable patching existing annual jobs with new config options (e.g., <code>skip_final_build</code>, <code>docker_shm_size</code>, <code>stall_timeout_minutes</code>) without recreating them.</p> <p>Features: - Type coercion: <code>true</code>/<code>false</code> \u2192 bool, numeric strings \u2192 int, others \u2192 str - Dry-run by default (shows diff), <code>--apply</code> to save changes - Status restrictions: Only <code>queued</code>, <code>retryable</code>, or <code>failed</code> jobs can be patched - Validates against <code>ArchiveToolOptions</code> dataclass fields - Validates tool_options dependencies (e.g., adaptive_workers requires monitoring)</p> <p>Usage: <pre><code># Dry-run (shows changes without applying)\nha-backend patch-job-config --id 42 \\\n  --set-tool-option skip_final_build=true \\\n  --set-tool-option docker_shm_size=2g\n\n# Apply changes\nha-backend patch-job-config --id 42 \\\n  --set-tool-option skip_final_build=true \\\n  --set-tool-option stall_timeout_minutes=60 \\\n  --apply\n</code></pre></p> <p>Tests: <code>tests/test_cli_patch_job.py</code> (15 tests) - Dry-run and apply modes - Type coercion (bool, int, str) - Status validation - Tool options validation - Error handling</p>"},{"location":"roadmaps/implemented/2026-01-28-patch-job-config-and-integration-tests/#2-integration-tests-for-archive_tool-mainpy","title":"2. Integration Tests for archive_tool main.py","text":"<p>Location: <code>tests/test_archive_tool_main_integration.py</code></p> <p>Purpose: Test main.py orchestration logic without requiring real Docker containers.</p> <p>Coverage (13 tests): - Existing ZIM handling: Exit behavior with/without <code>--overwrite</code> - Docker start failures: Exception handling, None returns - Docker availability: Failure when Docker is unavailable - Dry-run mode: No container starts in dry-run - Output directory: Auto-creation of missing dirs - CrawlState: State file creation, adaptation count persistence, temp dir tracking - Temp directory discovery: <code>discover_temp_dirs()</code> utility - Worker count parsing: Passthrough <code>--workers</code> arg parsing</p> <p>Testing approach: - Mocked Docker operations (no real containers needed) - Tests focus on early exit conditions and state management - Full stage loop tests with threading are complex (log drain thread) - deferred</p>"},{"location":"roadmaps/implemented/2026-01-28-patch-job-config-and-integration-tests/#files-changed","title":"Files Changed","text":"<p>Modified: - <code>src/ha_backend/cli.py</code> - Added <code>cmd_patch_job_config</code> and helpers - <code>CLAUDE.md</code> - Added patch-job-config to CLI examples - <code>docs/roadmaps/roadmap.md</code> - Removed completed integration tests item</p> <p>Added: - <code>tests/test_cli_patch_job.py</code> - 15 tests for patch-job-config - <code>tests/test_archive_tool_main_integration.py</code> - 13 integration tests</p>"},{"location":"roadmaps/implemented/2026-01-28-patch-job-config-and-integration-tests/#verification","title":"Verification","text":"<p>CI Status: \u2705 All checks passing - Format check: \u2705 - Lint: \u2705 - Type check: \u2705 - Tests: 183 passed (added 28 new tests)</p> <p>Manual verification: <pre><code># Test patch-job-config help\nha-backend patch-job-config --help\n\n# Test on a dev job (dry-run)\nha-backend patch-job-config --id 1 --set-tool-option skip_final_build=true\n</code></pre></p>"},{"location":"roadmaps/implemented/2026-01-28-patch-job-config-and-integration-tests/#deferred-items-phase-3","title":"Deferred Items (Phase 3)","text":"<p>These items remain in the backlog per the original plan: - Same-day dedupe with provenance preservation - WARC discovery consistency improvements (manifest verification) - Canary replay job (local-only) - Search authority signals tuning (requires measurement first)</p>"},{"location":"roadmaps/implemented/2026-01-28-patch-job-config-and-integration-tests/#related-documentation","title":"Related Documentation","text":"<ul> <li>Implementation plan: This file</li> <li>Roadmap: <code>docs/roadmaps/roadmap.md</code> (updated)</li> <li>CLI reference: <code>CLAUDE.md</code> (updated)</li> <li>Archive tool internals: <code>src/archive_tool/docs/documentation.md</code></li> </ul>"},{"location":"tutorials/architecture-walkthrough/","title":"Architecture Walkthrough","text":"<p>Learn how HealthArchive works by following a web page from crawl to search result.</p> <p>Time: 20-30 minutes Skill Level: Beginner to intermediate Prerequisites: Basic understanding of web applications and databases</p>"},{"location":"tutorials/architecture-walkthrough/#overview-the-big-picture","title":"Overview: The Big Picture","text":"<p>HealthArchive preserves Canadian health government websites through a multi-stage pipeline:</p> <pre><code>graph LR\n    A[Web Pages] --&gt;|Crawl| B[WARCs]\n    B --&gt;|Index| C[Database]\n    C --&gt;|Serve| D[Search API]\n    D --&gt;|Display| E[Public Website]</code></pre> <p>Let's walk through what happens when we archive a page from Health Canada.</p>"},{"location":"tutorials/architecture-walkthrough/#step-1-job-creation","title":"Step 1: Job Creation","text":"<p>Everything starts with creating an ArchiveJob.</p> <pre><code>ha-backend create-job --source hc\n</code></pre> <p>What happens:</p> <ol> <li>Job Registry (<code>ha_backend/job_registry.py:312-332</code>) looks up configuration for source code <code>\"hc\"</code>:</li> <li>Default seeds: <code>[\"https://www.canada.ca/en/health-canada.html\"]</code></li> <li>Initial workers: <code>1</code></li> <li> <p>Tool options: monitoring, adaptive workers, etc.</p> </li> <li> <p>Job Creation (<code>ha_backend/job_registry.py:400-420</code>):</p> </li> <li>Generates job name: <code>hc-20260118</code> (using today's date)</li> <li>Creates output directory: <code>/mnt/nasd/nobak/healtharchive/jobs/hc/20260118T210911Z__hc-20260118</code></li> <li>Inserts <code>ArchiveJob</code> row with <code>status=\"queued\"</code></li> </ol> <p>Database state after creation:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 name       \u2502 status   \u2502 output_dir                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 42 \u2502 hc-20260118\u2502 queued   \u2502 /mnt/.../20260118T...hc-20260118\u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#step-2-worker-picks-up-the-job","title":"Step 2: Worker Picks Up the Job","text":"<p>The worker runs continuously, polling for queued jobs:</p> <pre><code>ha-backend start-worker --poll-interval 30\n</code></pre> <p>Worker Loop (<code>ha_backend/worker/main.py:1087-1140</code>):</p> <pre><code>sequenceDiagram\n    participant W as Worker\n    participant DB as Database\n    participant J as Jobs Module\n\n    loop Every 30 seconds\n        W-&gt;&gt;DB: SELECT next queued job\n        alt Job found\n            DB-&gt;&gt;W: Return job_id=42\n            W-&gt;&gt;J: run_persistent_job(42)\n            J--&gt;&gt;W: Return exit_code\n            alt Exit code != 0\n                W-&gt;&gt;DB: Set status=retryable\n            else Success\n                W-&gt;&gt;DB: Set status=completed\n                W-&gt;&gt;J: index_job(42)\n            end\n        else No jobs\n            W-&gt;&gt;W: Sleep 30 seconds\n        end\n    end</code></pre>"},{"location":"tutorials/architecture-walkthrough/#step-3-running-the-crawler","title":"Step 3: Running the Crawler","text":"<p>Job Execution (<code>ha_backend/jobs.py:439-560</code>):</p> <ol> <li> <p>Load job config from database:    <pre><code>{\n  \"seeds\": [\"https://www.canada.ca/en/health-canada.html\"],\n  \"tool_options\": {\n    \"initial_workers\": 2,\n    \"cleanup\": false,\n    \"enable_monitoring\": false,\n    \"skip_final_build\": true\n  }\n}\n</code></pre></p> </li> <li> <p>Build CLI command:    <pre><code>archive-tool \\\n  --name hc-20260118 \\\n  --output-dir /mnt/.../20260118T...hc-20260118 \\\n  --initial-workers 2 \\\n  https://www.canada.ca/en/health-canada.html\n</code></pre></p> </li> <li> <p>Execute as subprocess (<code>jobs.py:529-542</code>):</p> </li> <li>Spawns <code>archive-tool</code> process</li> <li>Streams output to logs</li> <li>Waits for completion</li> </ol>"},{"location":"tutorials/architecture-walkthrough/#step-4-archive-tool-crawls","title":"Step 4: Archive Tool Crawls","text":"<p>The archive_tool (separate subpackage) orchestrates Docker:</p> <pre><code>graph TD\n    AT[archive-tool] --&gt;|Spawns| D[Docker Container]\n    D --&gt;|Runs| Z[zimit crawler]\n    Z --&gt;|Writes| W[WARC files]\n    Z --&gt;|Builds| ZIM[ZIM file]\n    W --&gt;|Stored in| TD[.tmp_1/ directory]</code></pre> <p>Key files created (see <code>src/archive_tool/docs/documentation.md</code>):</p> <pre><code>output_dir/\n\u251c\u2500\u2500 .archive_state.json          # Tracks crawl state\n\u251c\u2500\u2500 .tmp_1/                      # Temporary crawl artifacts\n\u2502   \u2514\u2500\u2500 collections/\n\u2502       \u2514\u2500\u2500 crawl-20260118.../\n\u2502           \u2514\u2500\u2500 archive/\n\u2502               \u251c\u2500\u2500 rec-00000-20260118.warc.gz\n\u2502               \u251c\u2500\u2500 rec-00001-20260118.warc.gz\n\u2502               \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 zim/\n    \u2514\u2500\u2500 hc-20260118_2026-01-18.zim\n</code></pre> <p>What's in a WARC?</p> <p>WARC (Web ARChive) files contain: - HTTP request/response pairs - Headers and content - Capture timestamps - Record metadata</p> <p>Example WARC record: <pre><code>WARC/1.0\nWARC-Type: response\nWARC-Date: 2026-01-18T21:15:42Z\nWARC-Record-ID: &lt;urn:uuid:12345...&gt;\nContent-Type: application/http; msgtype=response\n\nHTTP/1.1 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 45678\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;&lt;title&gt;Health Canada&lt;/title&gt;&lt;/head&gt;\n  &lt;body&gt;...actual page content...&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"tutorials/architecture-walkthrough/#step-5-crawl-completes","title":"Step 5: Crawl Completes","text":"<p>When <code>archive-tool</code> exits:</p> <ol> <li>Exit code 0 (success):</li> <li>Worker sets <code>job.status = \"completed\"</code></li> <li>Worker sets <code>job.crawler_exit_code = 0</code></li> <li> <p>Worker proceeds to indexing</p> </li> <li> <p>Exit code != 0 (failure):</p> </li> <li>Worker checks <code>retry_count &lt; MAX_CRAWL_RETRIES</code></li> <li>If retries available: set <code>status = \"retryable\"</code></li> <li>Otherwise: set <code>status = \"failed\"</code></li> </ol> <p>Database state after successful crawl:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 name        \u2502 status    \u2502 crawler_exit \u2502 warc_count\u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 42 \u2502 hc-20260118 \u2502 completed \u2502 0            \u2502 NULL      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#step-6-warc-indexing","title":"Step 6: WARC Indexing","text":"<p>The worker automatically calls <code>index_job(42)</code> after successful crawl.</p> <p>Indexing Pipeline (<code>ha_backend/indexing/pipeline.py:743-778</code>):</p> <pre><code>graph TD\n    A[Discover WARCs] --&gt; B[Read WARC Records]\n    B --&gt; C[Extract Text]\n    C --&gt; D[Create Snapshots]\n    D --&gt; E[Save to Database]</code></pre>"},{"location":"tutorials/architecture-walkthrough/#61-warc-discovery","title":"6.1 WARC Discovery","text":"<p>Discovery process (<code>ha_backend/indexing/warc_discovery.py:660-689</code>):</p> <ol> <li>Load crawl state from <code>.archive_state.json</code></li> <li>Get temporary directories from state</li> <li>Find all <code>*.warc.gz</code> files in temp dirs</li> <li>Return list of WARC paths</li> </ol> <pre><code>warc_paths = discover_warcs_for_job(job)\n# Returns: [\n#   Path(\"/mnt/.../rec-00000-20260118.warc.gz\"),\n#   Path(\"/mnt/.../rec-00001-20260118.warc.gz\"),\n# ]\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#62-reading-warc-records","title":"6.2 Reading WARC Records","text":"<p>WARC Reader (<code>ha_backend/indexing/warc_reader.py</code>):</p> <pre><code>for record in iter_html_records(warc_path):\n    # record.url = \"https://www.canada.ca/en/health-canada.html\"\n    # record.capture_timestamp = datetime(2026, 1, 18, 21, 15, 42)\n    # record.body_bytes = b\"&lt;!DOCTYPE html&gt;...\"\n    # record.warc_record_id = \"&lt;urn:uuid:12345...&gt;\"\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#63-text-extraction","title":"6.3 Text Extraction","text":"<p>Text Extraction (<code>ha_backend/indexing/text_extraction.py</code>):</p> <pre><code># Decode HTML\nhtml = record.body_bytes.decode(\"utf-8\", errors=\"replace\")\n\n# Extract metadata\ntitle = extract_title(html)\n# Returns: \"Health Canada - Canada.ca\"\n\ntext = extract_text(html)\n# Returns: \"Health Canada\\nAbout Health Canada\\n...\"\n\nsnippet = make_snippet(text)\n# Returns: \"Health Canada works to help Canadians maintain...\"\n\nlanguage = detect_language(text, record.headers)\n# Returns: \"en\"\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#64-creating-snapshots","title":"6.4 Creating Snapshots","text":"<p>Mapping (<code>ha_backend/indexing/mapping.py:724-739</code>):</p> <pre><code>snapshot = Snapshot(\n    job_id=42,\n    source_id=1,  # hc\n    url=record.url,\n    normalized_url_group=normalize_url(record.url),\n    capture_timestamp=record.capture_timestamp,\n    title=title,\n    snippet=snippet,\n    language=language,\n    warc_path=str(record.warc_path),\n    warc_record_id=record.warc_record_id,\n    mime_type=\"text/html\",\n    status_code=200,\n)\nsession.add(snapshot)\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#65-database-result","title":"6.5 Database Result","text":"<p>After indexing all WARC records:</p> <pre><code>snapshots table:\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 job_id \u2502 source  \u2502 url                          \u2502 title                \u2502 language\u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1  \u2502 42     \u2502 hc      \u2502 https://www.canada.ca/...    \u2502 Health Canada        \u2502 en      \u2502\n\u2502 2  \u2502 42     \u2502 hc      \u2502 https://www.canada.ca/...    \u2502 About Health Canada  \u2502 en      \u2502\n\u2502 3  \u2502 42     \u2502 hc      \u2502 https://www.canada.ca/...    \u2502 Services             \u2502 en      \u2502\n\u2502 ...\u2502 ...    \u2502 ...     \u2502 ...                          \u2502 ...                  \u2502 ...     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\narchive_jobs table:\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 name        \u2502 status  \u2502 warc_count   \u2502 indexed_page_count \u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 42 \u2502 hc-20260118 \u2502 indexed \u2502 245          \u2502 12,347             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#step-7-serving-via-api","title":"Step 7: Serving via API","text":"<p>Now the indexed snapshots are searchable via the public API.</p>"},{"location":"tutorials/architecture-walkthrough/#71-search-request","title":"7.1 Search Request","text":"<pre><code>curl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;sort=relevance\"\n</code></pre> <p>API Route (<code>ha_backend/api/routes_public.py:885-946</code>):</p> <pre><code>sequenceDiagram\n    participant C as Client\n    participant API as FastAPI\n    participant DB as Database\n\n    C-&gt;&gt;API: GET /api/search?q=vaccines\n    API-&gt;&gt;DB: Build search query\n    alt Postgres with FTS\n        DB-&gt;&gt;DB: Use ts_rank_cd + search_vector\n    else SQLite or no FTS\n        DB-&gt;&gt;DB: Substring match on title/snippet\n    end\n    DB--&gt;&gt;API: Return matching snapshots\n    API-&gt;&gt;API: Apply pagination (page=1, size=20)\n    API--&gt;&gt;C: Return SearchResponseSchema</code></pre>"},{"location":"tutorials/architecture-walkthrough/#72-query-building","title":"7.2 Query Building","text":"<p>For Postgres (with full-text search):</p> <pre><code>SELECT *\nFROM snapshots\nWHERE search_vector @@ websearch_to_tsquery('english', 'vaccines')\n  AND source_id = (SELECT id FROM sources WHERE code = 'hc')\n  AND (status_code IS NULL OR status_code BETWEEN 200 AND 299)\nORDER BY ts_rank_cd(search_vector, websearch_to_tsquery('english', 'vaccines')) DESC\nLIMIT 20 OFFSET 0;\n</code></pre> <p>For SQLite (substring matching):</p> <pre><code>SELECT *\nFROM snapshots\nWHERE (\n    LOWER(title) LIKE '%vaccines%'\n    OR LOWER(snippet) LIKE '%vaccines%'\n    OR LOWER(url) LIKE '%vaccines%'\n  )\n  AND source_id = (SELECT id FROM sources WHERE code = 'hc')\n  AND (status_code IS NULL OR status_code BETWEEN 200 AND 299)\nORDER BY\n    CASE\n        WHEN LOWER(title) LIKE '%vaccines%' THEN 3\n        WHEN LOWER(url) LIKE '%vaccines%' THEN 2\n        ELSE 1\n    END DESC,\n    capture_timestamp DESC\nLIMIT 20 OFFSET 0;\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#73-response-format","title":"7.3 Response Format","text":"<pre><code>{\n  \"results\": [\n    {\n      \"id\": 1,\n      \"title\": \"COVID-19 vaccines: Authorization and safety\",\n      \"sourceCode\": \"hc\",\n      \"sourceName\": \"Health Canada\",\n      \"language\": \"en\",\n      \"captureDate\": \"2026-01-18T21:15:42Z\",\n      \"originalUrl\": \"https://www.canada.ca/en/health-canada/services/drugs-health-products/covid19-industry/drugs-vaccines-treatments/vaccines.html\",\n      \"snippet\": \"Health Canada has approved COVID-19 vaccines for use in Canada...\",\n      \"rawSnapshotUrl\": \"/api/snapshots/raw/1\"\n    }\n  ],\n  \"total\": 127,\n  \"page\": 1,\n  \"pageSize\": 20\n}\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#step-8-viewing-archived-content","title":"Step 8: Viewing Archived Content","text":"<p>When a user clicks a search result, the frontend requests the archived HTML:</p> <pre><code>curl \"https://api.healtharchive.ca/api/snapshots/raw/1\"\n</code></pre> <p>Replay Process (<code>ha_backend/indexing/viewer.py:782-799</code>):</p> <pre><code>sequenceDiagram\n    participant C as Client\n    participant API as FastAPI\n    participant DB as Database\n    participant WARC as WARC File\n\n    C-&gt;&gt;API: GET /api/snapshots/raw/1\n    API-&gt;&gt;DB: SELECT snapshot WHERE id=1\n    DB--&gt;&gt;API: Return snapshot with warc_path\n    API-&gt;&gt;WARC: Open WARC file\n    API-&gt;&gt;WARC: Find record by warc_record_id\n    WARC--&gt;&gt;API: Return archived HTML\n    API--&gt;&gt;C: Return HTMLResponse</code></pre> <p>Output: The archived HTML exactly as it was captured, with a HealthArchive banner added for context.</p>"},{"location":"tutorials/architecture-walkthrough/#system-architecture-diagram","title":"System Architecture Diagram","text":"<p>Putting it all together:</p> <pre><code>graph TB\n    subgraph \"Crawl Phase\"\n        CLI[CLI: create-job] --&gt;|Insert| DB1[(Database)]\n        W[Worker Loop] --&gt;|Poll| DB1\n        W --&gt;|Execute| AT[archive-tool]\n        AT --&gt;|Docker| Z[zimit]\n        Z --&gt;|Write| WARC[WARC Files]\n    end\n\n    subgraph \"Index Phase\"\n        W2[Worker] --&gt;|Discover| WARC\n        WARC --&gt;|Read| IDX[Indexer]\n        IDX --&gt;|Extract| TXT[Text Extraction]\n        TXT --&gt;|Create| SNAP[Snapshots]\n        SNAP --&gt;|Insert| DB2[(Database)]\n    end\n\n    subgraph \"Serve Phase\"\n        FE[Frontend] --&gt;|API Request| API[FastAPI]\n        API --&gt;|Query| DB3[(Database)]\n        DB3 --&gt;|Results| API\n        API --&gt;|JSON| FE\n        FE --&gt;|Request Raw| API\n        API --&gt;|Read| WARC2[WARC Files]\n        WARC2 --&gt;|HTML| API\n        API --&gt;|HTML| FE\n    end\n\n    DB1 --&gt; DB2\n    DB2 --&gt; DB3</code></pre>"},{"location":"tutorials/architecture-walkthrough/#key-components-summary","title":"Key Components Summary","text":"Component Location Role Job Registry <code>ha_backend/job_registry.py</code> Source configs, job creation Worker <code>ha_backend/worker/main.py</code> Job polling and execution Jobs Module <code>ha_backend/jobs.py</code> Runs archive-tool subprocess Archive Tool <code>src/archive_tool/</code> Docker orchestration, crawling WARC Discovery <code>ha_backend/indexing/warc_discovery.py</code> Find WARC files WARC Reader <code>ha_backend/indexing/warc_reader.py</code> Stream WARC records Text Extraction <code>ha_backend/indexing/text_extraction.py</code> Extract title/text/snippet Indexer <code>ha_backend/indexing/pipeline.py</code> Orchestrate indexing Public API <code>ha_backend/api/routes_public.py</code> Search, stats, snapshots Admin API <code>ha_backend/api/routes_admin.py</code> Job management Database Models <code>ha_backend/models.py</code> ORM definitions"},{"location":"tutorials/architecture-walkthrough/#data-flow-summary","title":"Data Flow Summary","text":"<ol> <li>Create: CLI creates <code>ArchiveJob</code> \u2192 database</li> <li>Queue: Worker polls \u2192 finds queued job</li> <li>Crawl: Worker runs <code>archive-tool</code> \u2192 writes WARCs</li> <li>Discover: Indexer finds WARCs in output directory</li> <li>Extract: Parse WARCs \u2192 extract text and metadata</li> <li>Store: Create <code>Snapshot</code> rows \u2192 database</li> <li>Search: API queries snapshots \u2192 returns results</li> <li>View: API reads WARC \u2192 serves archived HTML</li> </ol>"},{"location":"tutorials/architecture-walkthrough/#next-steps","title":"Next Steps","text":"<p>Now that you understand the architecture:</p> <ol> <li>Try it locally: Follow Live Testing to run the pipeline</li> <li>Debug a crawl: See Debugging a Failed Crawl</li> <li>Dive deeper: Read the full Architecture Guide</li> <li>Explore the code: Browse the components in the table above</li> </ol>"},{"location":"tutorials/architecture-walkthrough/#further-reading","title":"Further Reading","text":"<ul> <li>Archive Tool Internals: <code>src/archive_tool/docs/documentation.md</code></li> <li>WARC Format: WARC specification</li> <li>Full-Text Search: PostgreSQL <code>tsvector</code> and <code>ts_rank_cd</code> docs</li> <li>FastAPI: FastAPI documentation</li> </ul>"},{"location":"tutorials/architecture-walkthrough/#questions","title":"Questions?","text":"<ul> <li>How do I add a new source? See <code>ha_backend/job_registry.py</code> and add to <code>SOURCE_CONFIGS</code></li> <li>How do I customize crawl options? Modify <code>default_tool_options</code> in source config</li> <li>How do I improve search ranking? Review search logic in <code>routes_public.py:885-946</code></li> <li>Where are WARCs stored long-term? In the job's output directory under <code>archive_root</code></li> </ul> <p>Still have questions? Check the How-To Guides or ask in GitHub Discussions.</p>"},{"location":"tutorials/debug-crawl/","title":"Debugging a Failed Crawl","text":"<p>A practical tutorial for diagnosing and fixing common crawl failures.</p> <p>Scenario: You created a crawl job, but it failed. Now what?</p> <p>Time: 15-30 minutes Prerequisites: Basic command line skills, access to the backend server</p>"},{"location":"tutorials/debug-crawl/#step-1-identify-the-failed-job","title":"Step 1: Identify the Failed Job","text":"<p>First, find the job ID and understand what went wrong.</p>"},{"location":"tutorials/debug-crawl/#check-job-status","title":"Check Job Status","text":"<pre><code>ha-backend list-jobs\n</code></pre> <p>Example output: <pre><code>ID  Name            Source  Status  Queued              Started             Finished            Pages\n42  hc-20260118     hc      failed  2026-01-18 20:00:00 2026-01-18 20:05:00 2026-01-18 20:45:00 0\n</code></pre></p>"},{"location":"tutorials/debug-crawl/#get-detailed-job-info","title":"Get Detailed Job Info","text":"<pre><code>ha-backend show-job --id 42\n</code></pre> <p>Look for these key fields: <pre><code>{\n  \"id\": 42,\n  \"name\": \"hc-20260118\",\n  \"status\": \"failed\",\n  \"crawler_exit_code\": 1,\n  \"crawler_status\": \"failed\",\n  \"pages_crawled\": 147,\n  \"pages_total\": 500,\n  \"pages_failed\": 12,\n  \"output_dir\": \"/mnt/nasd/nobak/healtharchive/jobs/hc/20260118T200500Z__hc-20260118\",\n  \"combined_log_path\": \"/mnt/.../archive_crawl_20260118T200511Z.combined.log\",\n  \"retry_count\": 0\n}\n</code></pre></p> <p>Key indicators: - <code>crawler_exit_code != 0</code> \u2192 Archive tool process failed - <code>crawler_status = \"failed\"</code> \u2192 Crawl did not complete successfully - <code>retry_count</code> \u2192 How many times we've already retried</p>"},{"location":"tutorials/debug-crawl/#step-2-check-the-crawl-logs","title":"Step 2: Check the Crawl Logs","text":"<p>Logs are your best friend for debugging. Let's examine them systematically.</p>"},{"location":"tutorials/debug-crawl/#find-the-log-file","title":"Find the Log File","text":"<p>The <code>combined_log_path</code> from <code>show-job</code> tells you where to look:</p> <pre><code>JOB_ID=42\nOUTPUT_DIR=$(ha-backend show-job --id $JOB_ID | jq -r '.outputDir')\nLOG_PATH=$(ha-backend show-job --id $JOB_ID | jq -r '.combinedLogPath')\n\n# View the log\nless \"$LOG_PATH\"\n</code></pre>"},{"location":"tutorials/debug-crawl/#common-log-patterns","title":"Common Log Patterns","text":""},{"location":"tutorials/debug-crawl/#1-permission-denied","title":"1. Permission Denied","text":"<pre><code>ERROR: Permission denied: '/mnt/nasd/nobak/healtharchive/jobs/hc/...'\n</code></pre> <p>Diagnosis: Output directory has wrong permissions</p> <p>Fix: <pre><code># Check permissions\nls -la \"$(dirname \"$OUTPUT_DIR\")\"\n\n# Fix ownership (if needed)\nsudo chown -R healtharchive:healtharchive \"$OUTPUT_DIR\"\nsudo chmod -R 755 \"$OUTPUT_DIR\"\n</code></pre></p> <p>Root cause: Often happens after manual operations as root user</p>"},{"location":"tutorials/debug-crawl/#2-docker-not-running","title":"2. Docker Not Running","text":"<pre><code>ERROR: Cannot connect to the Docker daemon at unix:///var/run/docker.sock\n</code></pre> <p>Diagnosis: Docker service is down</p> <p>Fix: <pre><code># Check Docker status\nsudo systemctl status docker\n\n# Start Docker if stopped\nsudo systemctl start docker\n\n# Verify Docker works\ndocker ps\n</code></pre></p> <p>Prevention: Enable Docker to start on boot: <pre><code>sudo systemctl enable docker\n</code></pre></p>"},{"location":"tutorials/debug-crawl/#3-out-of-disk-space","title":"3. Out of Disk Space","text":"<pre><code>ERROR: No space left on device\n</code></pre> <p>Diagnosis: Disk full</p> <p>Fix: <pre><code># Check disk usage\ndf -h /mnt/nasd/nobak\n\n# Find large directories\ndu -sh /mnt/nasd/nobak/healtharchive/jobs/* | sort -rh | head -10\n\n# Clean up old jobs (carefully!)\nha-backend cleanup-job --id OLD_JOB_ID --mode temp\n\n# Or manually remove old temp directories\nrm -rf /mnt/nasd/nobak/healtharchive/jobs/hc/*/.tmp_*\n</code></pre></p> <p>See: <code>operations/playbooks/manage-warc-cleanup.md</code></p>"},{"location":"tutorials/debug-crawl/#4-network-timeout","title":"4. Network Timeout","text":"<pre><code>WARNING: Request timeout for https://www.canada.ca/...\nERROR: Max retries exceeded\n</code></pre> <p>Diagnosis: Network connectivity issues or slow responses</p> <p>Fix: <pre><code># Test connectivity\ncurl -I https://www.canada.ca/en/health-canada.html\n\n# Check DNS\ndig www.canada.ca\n\n# If VPN is enabled, check VPN status\ntailscale status\n</code></pre></p> <p>Workaround: Increase timeouts in job config (see Step 5)</p>"},{"location":"tutorials/debug-crawl/#5-crawl-stalled","title":"5. Crawl Stalled","text":"<pre><code>INFO: Pages crawled: 147/500 (29%)\n... [no new log entries for 30+ minutes]\n</code></pre> <p>Diagnosis: Crawl made progress but stopped advancing</p> <p>Possible causes: - Site became unresponsive - Workers all blocked on slow pages - Memory issues in Docker container</p> <p>Fix: <pre><code># Check if Docker container is still running\ndocker ps\n\n# Check Docker logs\ndocker logs $(docker ps -q --filter ancestor=ghcr.io/openzim/zimit)\n\n# Check system resources\nhtop\n\n# If stalled, kill and retry\ndocker stop $(docker ps -q --filter ancestor=ghcr.io/openzim/zimit)\nha-backend retry-job --id 42\n</code></pre></p> <p>See: Real incident report: operations/incidents/2026-01-09-annual-crawl-hc-job-stalled.md</p>"},{"location":"tutorials/debug-crawl/#6-zimit-errors","title":"6. Zimit Errors","text":"<pre><code>zimit: error: unrecognized arguments: --bad-flag\n</code></pre> <p>Diagnosis: Invalid passthrough arguments to zimit</p> <p>Fix: Check job config: <pre><code>ha-backend show-job --id 42 | jq '.config.zimit_passthrough_args'\n</code></pre></p> <p>Remove invalid flags and recreate job with correct config</p> <p>Reference: See zimit documentation for valid flags</p>"},{"location":"tutorials/debug-crawl/#step-3-inspect-the-job-directory","title":"Step 3: Inspect the Job Directory","text":"<p>Sometimes logs don't tell the full story. Let's check the filesystem.</p>"},{"location":"tutorials/debug-crawl/#navigate-to-output-directory","title":"Navigate to Output Directory","text":"<pre><code>cd \"$OUTPUT_DIR\"\nls -lah\n</code></pre> <p>Expected structure: <pre><code>.archive_state.json           # Crawl state tracking\n.tmp_1/                       # Temporary crawl artifacts\n\u251c\u2500\u2500 collections/\n\u2502   \u2514\u2500\u2500 crawl-20260118.../\n\u2502       \u2514\u2500\u2500 archive/\n\u2502           \u251c\u2500\u2500 rec-00000-20260118.warc.gz\n\u2502           \u2514\u2500\u2500 ...\narchive_crawl_....log         # Individual stage logs\narchive_crawl_....combined.log # Aggregated log\nzim/                          # ZIM output (if built)\n</code></pre></p>"},{"location":"tutorials/debug-crawl/#check-crawl-state","title":"Check Crawl State","text":"<pre><code>cat .archive_state.json | jq '.'\n</code></pre> <p>Key fields: <pre><code>{\n  \"run_mode\": \"Fresh\",\n  \"temp_dir_counter\": 1,\n  \"temp_dirs\": [\".tmp_1\"],\n  \"current_stage\": \"crawl\",\n  \"is_complete\": false\n}\n</code></pre></p> <p>Indicators: - <code>is_complete: false</code> \u2192 Crawl didn't finish - <code>current_stage</code> \u2192 What stage failed</p>"},{"location":"tutorials/debug-crawl/#check-warc-files","title":"Check WARC Files","text":"<pre><code># Count WARC files\nfind .tmp_1 -name \"*.warc.gz\" | wc -l\n\n# Check sizes\nfind .tmp_1 -name \"*.warc.gz\" -exec ls -lh {} \\; | head -5\n\n# Verify WARCs are readable\nwarcio check .tmp_1/collections/*/archive/rec-00000*.warc.gz\n</code></pre> <p>Red flags: - 0 WARC files \u2192 Crawl never started - Very small WARCs (&lt; 1KB) \u2192 Likely corrupt - WARC validation errors \u2192 Damaged files</p>"},{"location":"tutorials/debug-crawl/#step-4-check-system-resources","title":"Step 4: Check System Resources","text":"<p>Resource exhaustion is a common cause of failures.</p>"},{"location":"tutorials/debug-crawl/#memory","title":"Memory","text":"<pre><code># Current memory usage\nfree -h\n\n# Memory usage during crawl (if still running)\ndocker stats\n</code></pre> <p>Fix for memory issues: - Reduce <code>initial_workers</code> in job config - Add swap space - Upgrade server RAM</p>"},{"location":"tutorials/debug-crawl/#cpu","title":"CPU","text":"<pre><code># CPU load\nuptime\n\n# Top processes\nhtop\n</code></pre> <p>Fix for high CPU: - Reduce worker count - Check for competing processes - Consider time-based scheduling</p>"},{"location":"tutorials/debug-crawl/#disk-io","title":"Disk I/O","text":"<pre><code># Check I/O wait\niostat -x 1 5\n\n# Disk usage\ndf -h\n\n# Inode usage (can be exhausted even with space available)\ndf -i\n</code></pre>"},{"location":"tutorials/debug-crawl/#step-5-retry-with-adjustments","title":"Step 5: Retry with Adjustments","text":"<p>Now that you've identified the issue, let's fix it and retry.</p>"},{"location":"tutorials/debug-crawl/#simple-retry-no-changes","title":"Simple Retry (No Changes)","text":"<p>If the issue was transient (network blip, temporary resource exhaustion):</p> <pre><code>ha-backend retry-job --id 42\n</code></pre> <p>This sets <code>status = \"retryable\"</code> and the worker will pick it up.</p>"},{"location":"tutorials/debug-crawl/#retry-with-modified-config","title":"Retry with Modified Config","text":"<p>If you need to change job settings, create a new job with overrides:</p> <pre><code>ha-backend create-job --source hc \\\n  --override '{\"tool_options\": {\"initial_workers\": 2, \"enable_monitoring\": true, \"stall_timeout_minutes\": 60}}'\n</code></pre> <p>Common overrides: - <code>initial_workers: 2</code> \u2192 More parallelism (or <code>1</code> if resource-constrained) - <code>enable_monitoring: true</code> \u2192 Enable stall detection - <code>stall_timeout_minutes: 60</code> \u2192 Abort if no progress for 60 mins - <code>error_threshold_timeout: 50</code> \u2192 Tolerate more timeouts before adaptations - <code>error_threshold_http: 50</code> \u2192 Tolerate more HTTP/network errors before adaptations - <code>backoff_delay_minutes: 2</code> \u2192 Shorten post-adaptation sleep on single-worker hosts - <code>page_limit: 1000</code> \u2192 Limit crawl scope for development/testing (avoid for annual campaign completeness)</p>"},{"location":"tutorials/debug-crawl/#resume-existing-crawl","title":"Resume Existing Crawl","text":"<p>Archive tool can resume from existing state:</p> <pre><code># The output_dir still exists, so just retry\nha-backend retry-job --id 42\n</code></pre> <p>Archive tool will detect <code>.archive_state.json</code> and resume.</p> <p>Resume behavior (see <code>src/archive_tool/docs/documentation.md</code>): - <code>run_mode: \"Resume\"</code> if state indicates incomplete crawl - Reuses existing WARCs - Continues from last checkpoint</p>"},{"location":"tutorials/debug-crawl/#step-6-verify-the-fix","title":"Step 6: Verify the Fix","text":"<p>After retrying, monitor the job closely.</p>"},{"location":"tutorials/debug-crawl/#watch-job-progress","title":"Watch Job Progress","text":"<pre><code># Poll job status\nwatch -n 30 'ha-backend show-job --id 42 | jq \".status, .pagesCrawled, .pagesTotal\"'\n</code></pre>"},{"location":"tutorials/debug-crawl/#tail-the-logs","title":"Tail the Logs","text":"<pre><code>tail -f \"$LOG_PATH\"\n</code></pre> <p>Look for: - <code>INFO: Pages crawled: X/Y</code> \u2192 Progress increasing - <code>INFO: Crawl stage completed successfully</code> \u2192 Success - No new ERROR lines</p>"},{"location":"tutorials/debug-crawl/#check-metrics","title":"Check Metrics","text":"<p>If you have Prometheus metrics enabled:</p> <pre><code>curl -H \"X-Admin-Token: $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  https://api.healtharchive.ca/metrics | grep healtharchive_jobs\n</code></pre>"},{"location":"tutorials/debug-crawl/#step-7-post-mortem-for-serious-failures","title":"Step 7: Post-Mortem (For Serious Failures)","text":"<p>If this was a significant failure (e.g., production annual crawl), document it.</p>"},{"location":"tutorials/debug-crawl/#create-incident-note","title":"Create Incident Note","text":"<pre><code>cp docs/_templates/incident-template.md \\\n   docs/operations/incidents/$(date +%Y-%m-%d)-brief-description.md\n</code></pre> <p>Fill in: - Timeline of events - Impact (pages missed, data loss) - Root cause - Resolution steps - Preventive measures</p> <p>Example: operations/incidents/2026-01-09-annual-crawl-hc-job-stalled.md</p>"},{"location":"tutorials/debug-crawl/#update-runbooks","title":"Update Runbooks","text":"<p>If you discovered a new failure mode or solution:</p> <ol> <li>Update the relevant playbook or this tutorial</li> <li>Add to troubleshooting FAQ</li> <li>Submit a PR</li> </ol>"},{"location":"tutorials/debug-crawl/#common-failure-scenarios-solutions","title":"Common Failure Scenarios &amp; Solutions","text":"Symptom Likely Cause Fix Prevention <code>exit_code: 1</code>, \"Permission denied\" Wrong file permissions <code>chown</code>/<code>chmod</code> output dir Use dedicated user, avoid sudo <code>exit_code: 125</code>, \"Docker not found\" Docker not running <code>systemctl start docker</code> Enable Docker on boot \"No space left\" in logs Disk full Cleanup old jobs Monitor disk usage, automate cleanup Crawl stalled, no progress Network issues or slow site Enable monitoring, retry Use <code>stall_timeout_minutes</code> 0 WARCs created Crawl failed immediately Check seeds, Docker logs Validate seeds before creating job WARCs exist but index fails WARC corruption Re-crawl or skip corrupt files Verify WARC integrity post-crawl <code>retry_count: 3</code>, still failing Persistent issue Manual intervention needed Review config, escalate"},{"location":"tutorials/debug-crawl/#debugging-checklist","title":"Debugging Checklist","text":"<p>Use this checklist for systematic debugging:</p> <ul> <li> Identify job ID and get detailed status (<code>show-job</code>)</li> <li> Read crawl logs (<code>combined_log_path</code>)</li> <li> Check for common error patterns (permissions, Docker, disk, network)</li> <li> Inspect job directory structure and files</li> <li> Verify WARC files exist and are valid</li> <li> Check system resources (memory, CPU, disk)</li> <li> Identify root cause</li> <li> Apply fix (permissions, config, cleanup)</li> <li> Retry job with adjustments</li> <li> Monitor retry for success</li> <li> Document incident if significant</li> <li> Update runbooks/playbooks with learnings</li> </ul>"},{"location":"tutorials/debug-crawl/#advanced-debugging","title":"Advanced Debugging","text":""},{"location":"tutorials/debug-crawl/#enable-debug-logging","title":"Enable Debug Logging","text":"<p>Create job with verbose logging:</p> <pre><code>ha-backend create-job --source hc \\\n  --override '{\"tool_options\": {\"log_level\": \"DEBUG\"}}'\n</code></pre>"},{"location":"tutorials/debug-crawl/#run-archive-tool-manually","title":"Run Archive Tool Manually","text":"<p>For deep debugging, run archive-tool outside the backend:</p> <pre><code>archive-tool \\\n  --name debug-crawl \\\n  --output-dir /tmp/debug-crawl \\\n  --initial-workers 1 \\\n  --log-level DEBUG \\\n  https://www.canada.ca/en/health-canada.html\n</code></pre>"},{"location":"tutorials/debug-crawl/#inspect-docker-container","title":"Inspect Docker Container","text":"<p>If the container is still running:</p> <pre><code># Get container ID\nCONTAINER_ID=$(docker ps -q --filter ancestor=ghcr.io/openzim/zimit)\n\n# Check logs\ndocker logs $CONTAINER_ID\n\n# Exec into container\ndocker exec -it $CONTAINER_ID /bin/bash\n\n# Inside container:\n# - Check /output/\n# - Review zimit logs\n# - Inspect environment\n</code></pre>"},{"location":"tutorials/debug-crawl/#check-database-state","title":"Check Database State","text":"<pre><code># Connect to database\nsqlite3 healtharchive.db  # or psql for Postgres\n\n# Check job status\nSELECT id, name, status, crawler_exit_code, pages_crawled, pages_total\nFROM archive_jobs\nWHERE id = 42;\n\n# Check if any snapshots were indexed\nSELECT COUNT(*) FROM snapshots WHERE job_id = 42;\n</code></pre>"},{"location":"tutorials/debug-crawl/#getting-help","title":"Getting Help","text":"<p>If you're still stuck:</p> <ol> <li>Check existing incidents: operations/incidents/</li> <li>Review playbooks: operations/playbooks/</li> <li>Search GitHub issues: github.com/jerdaw/healtharchive-backend/issues</li> <li>Ask for help: Open a new issue with:</li> <li>Job ID and status output</li> <li>Relevant log excerpts</li> <li>Steps you've already tried</li> <li>Consult archive-tool docs: <code>src/archive_tool/docs/documentation.md</code></li> </ol>"},{"location":"tutorials/debug-crawl/#related-resources","title":"Related Resources","text":"<ul> <li>Architecture Guide: architecture.md</li> <li>Archive Tool Documentation: <code>src/archive_tool/docs/documentation.md</code></li> <li>Incident Response: operations/playbooks/incident-response.md</li> </ul>"},{"location":"tutorials/debug-crawl/#conclusion","title":"Conclusion","text":"<p>Most crawl failures fall into a few categories: - Permissions: Fix ownership and modes - Resources: Free up disk, memory, or adjust worker count - Configuration: Correct invalid options or seeds - Network: Retry or adjust timeouts</p> <p>With systematic debugging, you can identify and fix most issues quickly. Document significant failures so the next operator can benefit from your learnings!</p>"},{"location":"tutorials/first-contribution/","title":"Your First Contribution","text":"<p>This tutorial walks you through making your first code contribution to HealthArchive, from setup to merged pull request.</p> <p>Time: 30-45 minutes Prerequisites: - Git installed - Python 3.11+ installed - Basic command line knowledge - GitHub account</p>"},{"location":"tutorials/first-contribution/#step-1-fork-and-clone","title":"Step 1: Fork and Clone","text":"<ol> <li>Fork the repository on GitHub:</li> <li>Visit github.com/jerdaw/healtharchive-backend</li> <li> <p>Click \"Fork\" in the top-right corner</p> </li> <li> <p>Clone your fork:    <pre><code>git clone https://github.com/YOUR-USERNAME/healtharchive-backend.git\ncd healtharchive-backend\n</code></pre></p> </li> <li> <p>Add upstream remote (to sync with the main repo later):    <pre><code>git remote add upstream https://github.com/jerdaw/healtharchive-backend.git\n</code></pre></p> </li> <li> <p>Verify remotes:    <pre><code>git remote -v\n# Should show:\n# origin    https://github.com/YOUR-USERNAME/healtharchive-backend.git (fetch)\n# origin    https://github.com/YOUR-USERNAME/healtharchive-backend.git (push)\n# upstream  https://github.com/jerdaw/healtharchive-backend.git (fetch)\n# upstream  https://github.com/jerdaw/healtharchive-backend.git (push)\n</code></pre></p> </li> </ol>"},{"location":"tutorials/first-contribution/#step-2-set-up-your-development-environment","title":"Step 2: Set Up Your Development Environment","text":"<ol> <li>Create a virtual environment and install dependencies:    <pre><code>make venv\n</code></pre></li> </ol> <p>This command:    - Creates <code>.venv/</code> directory    - Installs all Python dependencies    - Installs development tools (pytest, ruff, mypy, pre-commit)</p> <ol> <li> <p>Activate the virtual environment:    <pre><code>source .venv/bin/activate\n</code></pre></p> </li> <li> <p>Copy the example environment file:    <pre><code>cp .env.example .env\n</code></pre></p> </li> <li> <p>Edit <code>.env</code> with your local paths (optional, defaults work for most cases):    <pre><code># Example local settings\nHEALTHARCHIVE_DATABASE_URL=sqlite:///$(pwd)/.dev-healtharchive.db\nHEALTHARCHIVE_ARCHIVE_ROOT=$(pwd)/.dev-archive-root\nHEALTHARCHIVE_LOG_LEVEL=INFO\n</code></pre></p> </li> <li> <p>Source the environment:    <pre><code>source .env\n</code></pre></p> </li> <li> <p>Run database migrations:    <pre><code>alembic upgrade head\n</code></pre></p> </li> <li> <p>Seed initial data:    <pre><code>ha-backend seed-sources\n</code></pre></p> </li> </ol>"},{"location":"tutorials/first-contribution/#step-3-verify-your-setup","title":"Step 3: Verify Your Setup","text":"<p>Run the fast CI checks to ensure everything works:</p> <pre><code>make ci\n</code></pre> <p>This runs: - <code>ruff check</code> (linting) - <code>mypy</code> (type checking) - <code>pytest</code> (tests)</p> <p>Expected output: All checks should pass \u2705</p> <p>If anything fails, review the error messages and check: - Python version is 3.11+ - Virtual environment is activated - Dependencies installed correctly</p>"},{"location":"tutorials/first-contribution/#step-4-find-a-good-first-issue","title":"Step 4: Find a Good First Issue","text":"<ol> <li>Browse issues labeled <code>good first issue</code>:</li> <li> <p>Visit github.com/jerdaw/healtharchive-backend/issues?q=is:issue+is:open+label:\"good+first+issue\"</p> </li> <li> <p>Pick an issue that interests you:</p> </li> <li>Look for clear descriptions</li> <li>Check if anyone is already working on it</li> <li> <p>Comment on the issue to claim it</p> </li> <li> <p>No good first issues available? Try:</p> </li> <li>Fix a typo in documentation</li> <li>Add a test for an existing function</li> <li>Improve error messages or log output</li> </ol> <p>For this tutorial, we'll add a simple CLI command as an example.</p>"},{"location":"tutorials/first-contribution/#step-5-create-a-feature-branch","title":"Step 5: Create a Feature Branch","text":"<ol> <li> <p>Sync with upstream (ensure you have latest changes):    <pre><code>git checkout main\ngit pull upstream main\n</code></pre></p> </li> <li> <p>Create a new branch (use a descriptive name):    <pre><code>git checkout -b add-version-command\n</code></pre></p> </li> </ol> <p>Branch naming conventions:    - <code>add-*</code> for new features    - <code>fix-*</code> for bug fixes    - <code>docs-*</code> for documentation changes    - <code>refactor-*</code> for code refactoring</p>"},{"location":"tutorials/first-contribution/#step-6-make-your-change","title":"Step 6: Make Your Change","text":"<p>Let's add a simple <code>--version</code> command to the CLI.</p> <ol> <li> <p>Open <code>src/ha_backend/cli/main.py</code> in your editor</p> </li> <li> <p>Add a version command (example change):    <pre><code>@cli.command()\ndef version():\n    \"\"\"Display version information.\"\"\"\n    import ha_backend\n    from ha_backend.logging_config import setup_logging\n\n    setup_logging()\n    logger = logging.getLogger(__name__)\n\n    # You would import from a version module in a real implementation\n    version_string = \"0.1.0\"  # Placeholder\n    logger.info(f\"HealthArchive Backend version {version_string}\")\n    print(f\"HealthArchive Backend v{version_string}\")\n</code></pre></p> </li> <li> <p>Test your change manually:    <pre><code>ha-backend version\n# Should output: HealthArchive Backend v0.1.0\n</code></pre></p> </li> </ol>"},{"location":"tutorials/first-contribution/#step-7-write-tests","title":"Step 7: Write Tests","text":"<p>Every code change needs tests. Let's write a test for our new command.</p> <ol> <li>Create or update test file: <code>tests/test_cli_version.py</code></li> </ol> <pre><code>\"\"\"Tests for version CLI command.\"\"\"\nimport subprocess\n\n\ndef test_version_command():\n    \"\"\"Test that version command runs and outputs version info.\"\"\"\n    result = subprocess.run(\n        [\"ha-backend\", \"version\"],\n        capture_output=True,\n        text=True,\n    )\n\n    assert result.returncode == 0\n    assert \"HealthArchive Backend\" in result.stdout\n    assert \"0.1.0\" in result.stdout\n</code></pre> <ol> <li> <p>Run the test:    <pre><code>pytest tests/test_cli_version.py -v\n</code></pre></p> </li> <li> <p>Verify it passes:    <pre><code>tests/test_cli_version.py::test_version_command PASSED\n</code></pre></p> </li> </ol>"},{"location":"tutorials/first-contribution/#step-8-run-all-checks","title":"Step 8: Run All Checks","text":"<p>Before submitting, ensure all checks pass:</p> <pre><code>make ci\n</code></pre> <p>This runs: 1. Format check: <code>ruff format --check .</code> 2. Lint: <code>ruff check .</code> 3. Type check: <code>mypy .</code> 4. Tests: <code>pytest</code></p> <p>Fix any failures before proceeding.</p> <p>Common fixes: - Formatting issues: Run <code>make format</code> to auto-fix - Linting issues: Fix manually or run <code>ruff check --fix .</code> - Type errors: Add type hints or fix type mismatches - Test failures: Fix the code or update tests</p>"},{"location":"tutorials/first-contribution/#step-9-commit-your-changes","title":"Step 9: Commit Your Changes","text":"<ol> <li> <p>Stage your changes:    <pre><code>git add src/ha_backend/cli/main.py tests/test_cli_version.py\n</code></pre></p> </li> <li> <p>Commit with a clear message:    <pre><code>git commit -m \"feat: add version command to CLI\n\n- Add 'ha-backend version' command\n- Display version information\n- Add test coverage for version command\n\nCloses #123\"\n</code></pre></p> </li> </ol> <p>Commit message conventions:    - First line: <code>type: short description</code> (50 chars or less)    - Types: <code>feat</code>, <code>fix</code>, <code>docs</code>, <code>test</code>, <code>refactor</code>, <code>chore</code>    - Body: Explain what and why (not how)    - Footer: Reference issues with <code>Closes #123</code></p>"},{"location":"tutorials/first-contribution/#step-10-push-and-create-pull-request","title":"Step 10: Push and Create Pull Request","text":"<ol> <li> <p>Push your branch to your fork:    <pre><code>git push origin add-version-command\n</code></pre></p> </li> <li> <p>Create a pull request on GitHub:</p> </li> <li>Visit your fork on GitHub</li> <li>Click \"Compare &amp; pull request\" button</li> <li> <p>Fill in the PR template:</p> <ul> <li>Title: Clear, concise description</li> <li>Description: What changed and why</li> <li>Testing: How you verified it works</li> <li>Checklist: Complete all items</li> </ul> </li> <li> <p>Example PR description:    <pre><code>## Summary\nAdds a `--version` command to display backend version information.\n\n## Changes\n- Added `version` command to CLI\n- Added test coverage in `tests/test_cli_version.py`\n\n## Testing\n- \u2705 Manual testing: `ha-backend version` outputs correct version\n- \u2705 All CI checks pass\n- \u2705 New tests added and passing\n\n## Related Issues\nCloses #123\n</code></pre></p> </li> <li> <p>Wait for CI checks to run on your PR (takes ~5 minutes)</p> </li> <li> <p>Address review feedback if requested</p> </li> </ol>"},{"location":"tutorials/first-contribution/#step-11-respond-to-review-feedback","title":"Step 11: Respond to Review Feedback","text":"<p>When maintainers review your PR, they may request changes:</p> <ol> <li> <p>Make the requested changes:    <pre><code># Still on your feature branch\nvim src/ha_backend/cli/main.py\n</code></pre></p> </li> <li> <p>Commit the changes:    <pre><code>git add .\ngit commit -m \"fix: address review feedback\n\n- Improve version output formatting\n- Add docstring details\"\n</code></pre></p> </li> <li> <p>Push updates:    <pre><code>git push origin add-version-command\n</code></pre></p> </li> </ol> <p>The PR will update automatically!</p> <ol> <li>Reply to review comments to acknowledge feedback</li> </ol>"},{"location":"tutorials/first-contribution/#step-12-merge-and-celebrate","title":"Step 12: Merge and Celebrate! \ud83c\udf89","text":"<p>Once approved:</p> <ol> <li>A maintainer will merge your PR</li> <li>Your contribution is now part of HealthArchive!</li> <li>Your GitHub profile will show the contribution</li> </ol> <p>Optional: Clean up your local branches: <pre><code>git checkout main\ngit pull upstream main\ngit branch -d add-version-command\n</code></pre></p>"},{"location":"tutorials/first-contribution/#tips-for-success","title":"Tips for Success","text":""},{"location":"tutorials/first-contribution/#code-quality","title":"Code Quality","text":"<ul> <li>\u2705 Follow existing code style and patterns</li> <li>\u2705 Add type hints to all functions</li> <li>\u2705 Write clear docstrings</li> <li>\u2705 Keep changes focused and small</li> </ul>"},{"location":"tutorials/first-contribution/#testing","title":"Testing","text":"<ul> <li>\u2705 Write tests for all new code</li> <li>\u2705 Ensure tests are deterministic (no flaky tests)</li> <li>\u2705 Use fixtures for test data</li> <li>\u2705 Test edge cases and error conditions</li> </ul>"},{"location":"tutorials/first-contribution/#communication","title":"Communication","text":"<ul> <li>\u2705 Ask questions if unclear</li> <li>\u2705 Keep PR descriptions detailed</li> <li>\u2705 Respond to feedback promptly</li> <li>\u2705 Be respectful and collaborative</li> </ul>"},{"location":"tutorials/first-contribution/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>\u274c Don't commit directly to <code>main</code></li> <li>\u274c Don't include unrelated changes in one PR</li> <li>\u274c Don't skip writing tests</li> <li>\u274c Don't ignore CI failures</li> </ul>"},{"location":"tutorials/first-contribution/#next-steps","title":"Next Steps","text":"<p>After your first contribution:</p> <ol> <li>Tackle more issues: Graduate to <code>help wanted</code> issues</li> <li>Learn the architecture: Read the Architecture Walkthrough</li> <li>Improve the docs: Documentation PRs are always welcome</li> <li>Review others' PRs: Great way to learn and help the community</li> </ol>"},{"location":"tutorials/first-contribution/#getting-help","title":"Getting Help","text":"<ul> <li>Questions about the code? Ask in the issue thread</li> <li>CI failures? Check the Testing Guidelines</li> <li>Architecture questions? Read the Architecture Guide</li> <li>Still stuck? Open a discussion on GitHub</li> </ul>"},{"location":"tutorials/first-contribution/#resources","title":"Resources","text":"<ul> <li>Development Setup</li> <li>Testing Guidelines</li> <li>Architecture Guide</li> <li>Documentation Guidelines</li> </ul> <p>Welcome to the HealthArchive community! \ud83d\ude80</p>"}]}