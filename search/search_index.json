{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HealthArchive Documentation","text":"<p>This documentation portal covers the HealthArchive backend and links to frontend and datasets documentation.</p>"},{"location":"#quick-start-by-role","title":"Quick Start by Role","text":"<p>Choose your path:</p> <ul> <li>\ud83d\udc64 Operators: Start with Operations Overview \u2192 Operator Responsibilities</li> <li>\ud83d\udcbb Developers: Start with Development Guide \u2192 Live Testing</li> <li>\ud83d\udd27 Deploying: Start with Production Runbook</li> <li>\ud83d\udcca API consumers: Start with API Documentation</li> <li>\ud83d\udcda Researchers: Start with Project Overview \u2192 Datasets</li> </ul>"},{"location":"#key-resources","title":"Key Resources","text":"Need Documentation Architecture overview Architecture Production deployment Production Runbook Local development setup Dev Setup Incident response Incident Response Search API API Documentation Monitoring setup Monitoring"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>This docs portal is built from the backend repo only. Frontend and datasets docs are canonical in their repos and are linked-to from this site:</p> <ul> <li>Frontend pointers: <code>frontend-external/README.md</code></li> <li>Datasets pointers: <code>datasets-external/README.md</code></li> </ul>"},{"location":"#recommended-reading-order","title":"Recommended reading order","text":"<ol> <li>Project docs portal (multi-repo navigation)</li> <li><code>project.md</code></li> <li>Architecture &amp; implementation (how the code works)</li> <li><code>architecture.md</code></li> <li>Documentation guidelines (how docs stay sane)</li> <li><code>documentation-guidelines.md</code></li> <li><code>documentation-process-audit.md</code> (audit of doc processes; 2026-01-09)</li> <li><code>decisions/README.md</code> (decision records for high-stakes choices)</li> <li>Local development / live testing (how to run it locally)</li> <li><code>development/live-testing.md</code></li> <li><code>development/dev-environment-setup.md</code> (local setup + local vs VPS guidance)</li> <li><code>development/testing-guidelines.md</code> (backend test expectations)</li> <li>Deployment (how to run it on a server)</li> <li><code>deployment/production-single-vps.md</code> (current production runbook)</li> <li><code>deployment/systemd/README.md</code> (systemd units: annual scheduler, crawl monitoring + auto-recovery, baseline drift, replay reconcile + smoke tests, change tracking, annual search verify, coverage guardrails, cleanup automation, worker priority)</li> <li><code>deployment/replay-service-pywb.md</code> (pywb replay service for full-fidelity browsing)</li> <li><code>deployment/search-rollout.md</code> (enable v2 search + rollback)</li> <li><code>deployment/pages-table-rollout.md</code> (pages table backfill + browse fast path)</li> <li><code>deployment/hosting-and-live-server-to-dos.md</code> (deployment checklist + Vercel wiring)</li> <li><code>deployment/environments-and-configuration.md</code> (cross\u2011repo env vars + host matrix)</li> <li><code>deployment/production-rollout-checklist.md</code> (generic production checklist)</li> <li><code>deployment/staging-rollout-checklist.md</code> (optional future staging)</li> <li>Operations (how to keep it healthy)</li> <li><code>operations/README.md</code> (index of ops docs)</li> <li>Roadmaps and implementation plans</li> <li><code>planning/README.md</code></li> <li><code>roadmap-process.md</code> (short pointer)</li> </ol>"},{"location":"#notes","title":"Notes","text":"<ul> <li>No secrets live in this repo. Any token/password values shown in docs must be   placeholders.</li> <li>The <code>archive_tool</code> crawler has its own internal documentation at   <code>src/archive_tool/docs/documentation.md</code>.</li> </ul>"},{"location":"api-consumer-guide/","title":"API Consumer Guide","text":"<p>A practical guide for researchers, journalists, and developers who want to programmatically access the HealthArchive.</p> <p>Audience: API consumers, data researchers, integration developers Time: 20-30 minutes to read, lifetime to master Prerequisites: Basic HTTP/REST knowledge, command line or programming experience</p>"},{"location":"api-consumer-guide/#quick-start","title":"Quick Start","text":"<p>Base URL: <code>https://api.healtharchive.ca</code></p> <p>Authentication: None required for public endpoints (search, stats, snapshots)</p> <p>Try it now: <pre><code>curl \"https://api.healtharchive.ca/api/stats\"\n</code></pre></p>"},{"location":"api-consumer-guide/#api-overview","title":"API Overview","text":"<p>HealthArchive provides a RESTful JSON API for searching and retrieving archived Canadian health government content.</p>"},{"location":"api-consumer-guide/#public-endpoints","title":"Public Endpoints","text":"Endpoint Purpose Auth Required <code>GET /api/health</code> Health check No <code>GET /api/stats</code> Archive statistics No <code>GET /api/sources</code> List archived sources No <code>GET /api/search</code> Search snapshots No <code>GET /api/snapshot/{id}</code> Get snapshot metadata No <code>GET /api/snapshots/raw/{id}</code> View archived HTML No"},{"location":"api-consumer-guide/#admin-endpoints","title":"Admin Endpoints","text":"<p>(For operators only, require token)</p> Endpoint Purpose <code>GET /api/admin/jobs</code> List crawl jobs <code>GET /api/admin/jobs/{id}</code> Get job details <code>GET /metrics</code> Prometheus metrics <p>This guide focuses on public endpoints.</p>"},{"location":"api-consumer-guide/#interactive-api-documentation","title":"Interactive API Documentation","text":"<p>Explore the API interactively:</p> <p>Swagger UI: https://api.healtharchive.ca/docs</p> <p>Features: - Try requests directly in browser - See request/response examples - View full schema definitions</p>"},{"location":"api-consumer-guide/#core-concepts","title":"Core Concepts","text":""},{"location":"api-consumer-guide/#sources","title":"Sources","text":"<p>A Source represents a content origin (e.g., Health Canada, PHAC).</p> <p>Available sources: - <code>hc</code> - Health Canada - <code>phac</code> - Public Health Agency of Canada</p>"},{"location":"api-consumer-guide/#snapshots","title":"Snapshots","text":"<p>A Snapshot is a single captured web page at a specific point in time.</p> <p>Key attributes: - <code>url</code>: Original web address - <code>captureDate</code>: When it was archived - <code>title</code>: Page title - <code>snippet</code>: Text preview - <code>language</code>: Content language (<code>en</code> or <code>fr</code>)</p>"},{"location":"api-consumer-guide/#views","title":"Views","text":"<p>Search results can be returned in two views:</p> <ol> <li>Snapshots view (<code>view=snapshots</code>, default): Returns individual captures</li> <li>Pages view (<code>view=pages</code>): Returns only the latest capture per page</li> </ol>"},{"location":"api-consumer-guide/#api-versioning-response-headers","title":"API Versioning &amp; Response Headers","text":""},{"location":"api-consumer-guide/#versioning-strategy","title":"Versioning Strategy","text":"<p>The HealthArchive API uses header-based versioning for forward compatibility:</p> <ul> <li>Current Version: <code>1</code> (major version only)</li> <li>Header: <code>X-API-Version: 1</code></li> <li>Stability: Version 1 is stable; breaking changes will increment to version 2</li> </ul> <p>Version Header (returned on all responses): <pre><code>X-API-Version: 1\n</code></pre></p> <p>Versioning Policy: - Major version changes (1 \u2192 2): Breaking changes to request/response format, removed endpoints - Minor updates (within v1): Additive only (new fields, new optional parameters, new endpoints) - Clients should: Inspect <code>X-API-Version</code> header to detect version; log warnings if unexpected</p> <p>Deprecation: If breaking changes are needed, we will: 1. Announce deprecation at least 6 months in advance 2. Run both versions in parallel during transition 3. Provide migration guide in this documentation</p>"},{"location":"api-consumer-guide/#standard-response-headers","title":"Standard Response Headers","text":"<p>All API responses include these headers:</p> Header Purpose Example <code>X-API-Version</code> API major version <code>1</code> <code>X-Request-Id</code> Request correlation ID <code>a3f2e1d0-...</code> <code>X-Content-Type-Options</code> Security: prevent MIME sniffing <code>nosniff</code> <code>X-Frame-Options</code> Security: clickjacking protection <code>SAMEORIGIN</code> <code>Referrer-Policy</code> Privacy: control referrer info <code>strict-origin-when-cross-origin</code> <p>Security headers (all responses):</p> Header Purpose Value <code>Content-Security-Policy</code> XSS/injection prevention See CSP section below <code>Strict-Transport-Security</code> Enforce HTTPS <code>max-age=31536000; includeSubDomains</code> <code>Permissions-Policy</code> Disable sensitive browser features <code>geolocation=(), microphone=(), camera=()</code> <p>Rate-limited endpoints also include:</p> Header Purpose Example <code>X-RateLimit-Limit</code> Maximum requests allowed in window <code>60</code> <code>X-RateLimit-Remaining</code> Requests remaining in current window <code>57</code> <p>Using Request IDs: - Include <code>X-Request-Id</code> from response when reporting issues - Pass custom <code>X-Request-Id</code> in request to trace across systems - IDs are UUIDv4 format and logged server-side for debugging</p>"},{"location":"api-consumer-guide/#content-security-policy-csp","title":"Content Security Policy (CSP)","text":"<p>The API implements Content Security Policy headers to prevent XSS and code injection attacks.</p> <p>For JSON endpoints (most of the API): <pre><code>Content-Security-Policy: default-src 'none'; frame-ancestors 'none'\n</code></pre> - Blocks all resource loading by default - Prevents the API from being embedded in iframes</p> <p>For HTML replay endpoints (<code>/api/snapshots/raw/*</code>): <pre><code>Content-Security-Policy: default-src 'none'; script-src 'unsafe-inline' 'unsafe-eval';\n  style-src 'unsafe-inline' *; img-src * data: blob:; font-src * data:;\n  connect-src *; media-src *; object-src 'none'; frame-src *;\n  base-uri 'self'; form-action 'self'\n</code></pre> - Allows inline scripts/styles (required for archived HTML) - Allows external resources (images, fonts, media) - Still blocks dangerous features (object/embed tags)</p> <p>Why this matters for API consumers: - CSP headers are informational for JSON API consumers (your code isn't affected) - If you're embedding <code>/api/snapshots/raw/*</code> in iframes, CSP allows it with proper sandboxing - CSP is automatically relaxed for archived content replay while maintaining security for JSON endpoints</p>"},{"location":"api-consumer-guide/#request-size-limits","title":"Request Size Limits","text":"<p>The API enforces size limits to prevent abuse and ensure system stability:</p> Limit Default Max Configurable Request body size 1 MB 10 MB Yes Query string length 8 KB 64 KB Yes <p>Error Responses: - <code>413 Payload Too Large</code>: Request body exceeds size limit - <code>414 URI Too Long</code>: Query string exceeds length limit</p> <p>Example 413 response: <pre><code>{\n  \"error\": \"Payload Too Large\",\n  \"detail\": \"Request body exceeds maximum size of 1048576 bytes\"\n}\n</code></pre></p> <p>Example 414 response: <pre><code>{\n  \"error\": \"URI Too Long\",\n  \"detail\": \"Query string exceeds maximum length of 8192 characters\"\n}\n</code></pre></p> <p>Best Practices: - Keep issue reports concise (under 1MB) - Use pagination for large result sets instead of increasing page size - Filter search queries to reduce result count rather than fetching everything - The 1MB body limit is sufficient for all standard API operations</p>"},{"location":"api-consumer-guide/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api-consumer-guide/#1-get-archive-statistics","title":"1. Get Archive Statistics","text":"<p>Use case: Display total archive size, latest capture, etc.</p> <pre><code>curl \"https://api.healtharchive.ca/api/stats\"\n</code></pre> <p>Response: <pre><code>{\n  \"snapshotsTotal\": 45678,\n  \"pagesTotal\": 12345,\n  \"sourcesTotal\": 2,\n  \"latestCaptureDate\": \"2026-01-18\",\n  \"latestCaptureAgeDays\": 0\n}\n</code></pre></p> <p>Fields: - <code>snapshotsTotal</code>: Total captures across all sources - <code>pagesTotal</code>: Unique pages (excluding duplicates) - <code>sourcesTotal</code>: Number of sources - <code>latestCaptureDate</code>: Most recent capture timestamp - <code>latestCaptureAgeDays</code>: Days since latest capture</p>"},{"location":"api-consumer-guide/#2-list-all-sources","title":"2. List All Sources","text":"<p>Use case: Understand what's in the archive</p> <pre><code>curl \"https://api.healtharchive.ca/api/sources\"\n</code></pre> <p>Response: <pre><code>[\n  {\n    \"sourceCode\": \"hc\",\n    \"sourceName\": \"Health Canada\",\n    \"recordCount\": 30123,\n    \"firstCapture\": \"2024-06-01T00:00:00Z\",\n    \"lastCapture\": \"2026-01-18T21:15:42Z\",\n    \"latestRecordId\": 12345\n  },\n  {\n    \"sourceCode\": \"phac\",\n    \"sourceName\": \"Public Health Agency of Canada\",\n    \"recordCount\": 15555,\n    \"firstCapture\": \"2024-06-01T00:00:00Z\",\n    \"lastCapture\": \"2026-01-18T20:30:15Z\",\n    \"latestRecordId\": 12346\n  }\n]\n</code></pre></p>"},{"location":"api-consumer-guide/#3-search-for-content","title":"3. Search for Content","text":"<p>Use case: Find pages about a specific topic</p>"},{"location":"api-consumer-guide/#basic-keyword-search","title":"Basic Keyword Search","text":"<pre><code>curl \"https://api.healtharchive.ca/api/search?q=covid vaccines\"\n</code></pre> <p>Response: <pre><code>{\n  \"results\": [\n    {\n      \"id\": 1,\n      \"title\": \"COVID-19 vaccines: Authorization and safety\",\n      \"sourceCode\": \"hc\",\n      \"sourceName\": \"Health Canada\",\n      \"language\": \"en\",\n      \"captureDate\": \"2026-01-18T21:15:42Z\",\n      \"originalUrl\": \"https://www.canada.ca/en/health-canada/services/drugs-health-products/covid19-industry/drugs-vaccines-treatments/vaccines.html\",\n      \"snippet\": \"Health Canada has approved several COVID-19 vaccines for use in Canada...\",\n      \"rawSnapshotUrl\": \"/api/snapshots/raw/1\"\n    }\n  ],\n  \"total\": 127,\n  \"page\": 1,\n  \"pageSize\": 20\n}\n</code></pre></p>"},{"location":"api-consumer-guide/#filter-by-source","title":"Filter by Source","text":"<pre><code>curl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;source=phac\"\n</code></pre> <p>Only returns results from PHAC.</p>"},{"location":"api-consumer-guide/#sort-by-date-newest-first","title":"Sort by Date (Newest First)","text":"<pre><code>curl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;sort=newest\"\n</code></pre> <p>Sort options: - <code>relevance</code> (default when <code>q</code> is present): Best match first - <code>newest</code>: Most recent captures first</p>"},{"location":"api-consumer-guide/#filter-by-date-range","title":"Filter by Date Range","text":"<pre><code># Captures from 2025 only\ncurl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;from=2025-01-01&amp;to=2025-12-31\"\n</code></pre>"},{"location":"api-consumer-guide/#include-non-2xx-http-status","title":"Include Non-2xx HTTP Status","text":"<p>By default, only successful (200-299) responses are returned. To include redirects, errors, etc.:</p> <pre><code>curl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;includeNon2xx=true\"\n</code></pre>"},{"location":"api-consumer-guide/#pagination","title":"Pagination","text":"<pre><code># Get page 2, 50 results per page\ncurl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;page=2&amp;pageSize=50\"\n</code></pre> <p>Limits: - <code>page</code>: Min 1 (default: 1) - <code>pageSize</code>: Min 1, Max 100 (default: 20)</p>"},{"location":"api-consumer-guide/#4-advanced-search-syntax","title":"4. Advanced Search Syntax","text":""},{"location":"api-consumer-guide/#boolean-operators","title":"Boolean Operators","text":"<pre><code># AND (both terms must appear)\ncurl \"https://api.healtharchive.ca/api/search?q=covid+AND+vaccine\"\n\n# OR (either term)\ncurl \"https://api.healtharchive.ca/api/search?q=covid+OR+coronavirus\"\n\n# NOT (exclude term)\ncurl \"https://api.healtharchive.ca/api/search?q=vaccine+NOT+flu\"\n\n# Parentheses for grouping\ncurl \"https://api.healtharchive.ca/api/search?q=(covid+OR+coronavirus)+AND+vaccine\"\n</code></pre>"},{"location":"api-consumer-guide/#field-specific-search","title":"Field-Specific Search","text":"<pre><code># Search only in title\ncurl \"https://api.healtharchive.ca/api/search?q=title:vaccines\"\n\n# Search only in snippet (text content)\ncurl \"https://api.healtharchive.ca/api/search?q=snippet:mRNA\"\n\n# Search only in URL\ncurl \"https://api.healtharchive.ca/api/search?q=url:health-canada\"\n</code></pre>"},{"location":"api-consumer-guide/#url-lookup","title":"URL Lookup","text":"<p>Find all captures of a specific page:</p> <pre><code>curl \"https://api.healtharchive.ca/api/search?q=url:https://www.canada.ca/en/health-canada.html\"\n</code></pre> <p>Or use the <code>url:</code> prefix:</p> <pre><code>curl \"https://api.healtharchive.ca/api/search?q=url:canada.ca/en/health-canada\"\n</code></pre>"},{"location":"api-consumer-guide/#5-browse-pages-latest-captures-only","title":"5. Browse Pages (Latest Captures Only)","text":"<p>Use case: Get a list of unique pages, not all captures</p> <pre><code>curl \"https://api.healtharchive.ca/api/search?view=pages&amp;source=hc&amp;sort=newest\"\n</code></pre> <p>Difference from snapshots view: - <code>view=snapshots</code>: Returns all captures (same page may appear multiple times) - <code>view=pages</code>: Returns only the most recent capture per page</p> <p>Response includes <code>pageSnapshotsCount</code>: <pre><code>{\n  \"results\": [\n    {\n      \"id\": 12345,\n      \"title\": \"Health Canada\",\n      \"pageSnapshotsCount\": 15,\n      ...\n    }\n  ]\n}\n</code></pre></p> <p><code>pageSnapshotsCount</code> tells you how many times this page was captured.</p>"},{"location":"api-consumer-guide/#6-get-snapshot-metadata","title":"6. Get Snapshot Metadata","text":"<p>Use case: Retrieve full details for a specific snapshot</p> <pre><code>curl \"https://api.healtharchive.ca/api/snapshot/12345\"\n</code></pre> <p>Response: <pre><code>{\n  \"id\": 12345,\n  \"title\": \"COVID-19 vaccines\",\n  \"sourceCode\": \"hc\",\n  \"sourceName\": \"Health Canada\",\n  \"language\": \"en\",\n  \"captureDate\": \"2026-01-18T21:15:42Z\",\n  \"originalUrl\": \"https://www.canada.ca/en/health-canada/services/drugs-health-products/covid19-industry/drugs-vaccines-treatments/vaccines.html\",\n  \"mimeType\": \"text/html\",\n  \"statusCode\": 200,\n  \"snippet\": \"Health Canada has approved...\",\n  \"rawSnapshotUrl\": \"/api/snapshots/raw/12345\"\n}\n</code></pre></p>"},{"location":"api-consumer-guide/#7-view-archived-html","title":"7. View Archived HTML","text":"<p>Use case: Retrieve the actual archived page content</p> <pre><code>curl \"https://api.healtharchive.ca/api/snapshots/raw/12345\"\n</code></pre> <p>Response: HTML page with HealthArchive header banner</p> <p>In browser: Visit <code>https://api.healtharchive.ca/api/snapshots/raw/12345</code> to see rendered page</p> <p>Note: This is the archived content exactly as it was captured, plus a small HealthArchive navigation bar.</p>"},{"location":"api-consumer-guide/#language-support","title":"Language Support","text":"<p>HealthArchive indexes content in English and French.</p>"},{"location":"api-consumer-guide/#search-by-language","title":"Search by Language","text":"<pre><code># English content only\ncurl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;language=en\"\n\n# French content only\ncurl \"https://api.healtharchive.ca/api/search?q=vaccins&amp;language=fr\"\n</code></pre> <p>Tip: HealthArchive auto-detects language, but some pages may be incorrectly classified.</p>"},{"location":"api-consumer-guide/#pagination-performance","title":"Pagination &amp; Performance","text":""},{"location":"api-consumer-guide/#response-times","title":"Response Times","text":"<ul> <li>Search: ~100-500ms (depending on query complexity)</li> <li>Stats: ~50ms (heavily cached)</li> <li>Sources: ~100ms</li> <li>Snapshot metadata: ~50ms</li> <li>Raw HTML: ~200-500ms (reads WARC from disk)</li> </ul>"},{"location":"api-consumer-guide/#rate-limiting","title":"Rate Limiting","text":"<p>Rate limits are enforced per client IP address to ensure fair resource allocation and prevent abuse.</p> Endpoint Limit Window <code>POST /api/reports</code> 5 requests per minute <code>GET /api/exports/*</code> 10 requests per minute <code>GET /api/search</code> 60 requests per minute All other endpoints 120 requests per minute <p>Rate limit headers (included in responses to limited endpoints): <pre><code>X-RateLimit-Limit: 60\nX-RateLimit-Remaining: 57\n</code></pre></p> <p>When limits are exceeded: - HTTP status: <code>429 Too Many Requests</code> - Response includes <code>Retry-After</code> header (seconds until limit resets) - Example error response: <pre><code>{\n  \"error\": \"Rate limit exceeded\",\n  \"detail\": \"60 per 1 minute\"\n}\n</code></pre></p> <p>Best practices: - Monitor <code>X-RateLimit-Remaining</code> header to avoid hitting limits - Implement exponential backoff when you receive 429 responses - Cache responses when appropriate to reduce request volume - Use <code>pageSize</code> wisely (larger pages = slower but fewer requests) - For bulk exports, use the <code>/api/exports/*</code> endpoints instead of paginating search - Contact the maintainers if you need higher limits for legitimate research</p>"},{"location":"api-consumer-guide/#pagination-best-practices","title":"Pagination Best Practices","text":"<p>For complete datasets: <pre><code>import requests\n\nbase_url = \"https://api.healtharchive.ca/api/search\"\npage = 1\npage_size = 100  # Max allowed\nall_results = []\n\nwhile True:\n    response = requests.get(base_url, params={\n        \"q\": \"vaccines\",\n        \"page\": page,\n        \"pageSize\": page_size\n    })\n    data = response.json()\n\n    all_results.extend(data[\"results\"])\n\n    if page * page_size &gt;= data[\"total\"]:\n        break\n\n    page += 1\n\nprint(f\"Retrieved {len(all_results)} results\")\n</code></pre></p>"},{"location":"api-consumer-guide/#code-examples","title":"Code Examples","text":""},{"location":"api-consumer-guide/#python","title":"Python","text":"<pre><code>import requests\n\ndef search_healtharchive(query, source=None, sort=\"relevance\", page=1):\n    \"\"\"Search HealthArchive API.\"\"\"\n    url = \"https://api.healtharchive.ca/api/search\"\n    params = {\n        \"q\": query,\n        \"sort\": sort,\n        \"page\": page,\n        \"pageSize\": 20\n    }\n    if source:\n        params[\"source\"] = source\n\n    response = requests.get(url, params=params)\n    response.raise_for_status()\n    return response.json()\n\n# Example usage\nresults = search_healtharchive(\"covid vaccines\", source=\"hc\")\nfor snapshot in results[\"results\"]:\n    print(f\"{snapshot['title']} - {snapshot['captureDate']}\")\n</code></pre>"},{"location":"api-consumer-guide/#javascript-nodejs","title":"JavaScript (Node.js)","text":"<pre><code>const fetch = require('node-fetch');\n\nasync function searchHealthArchive(query, options = {}) {\n    const baseUrl = 'https://api.healtharchive.ca/api/search';\n    const params = new URLSearchParams({\n        q: query,\n        sort: options.sort || 'relevance',\n        page: options.page || 1,\n        pageSize: options.pageSize || 20,\n        ...(options.source &amp;&amp; { source: options.source })\n    });\n\n    const response = await fetch(`${baseUrl}?${params}`);\n    return response.json();\n}\n\n// Example usage\n(async () =&gt; {\n    const results = await searchHealthArchive('covid vaccines', { source: 'hc' });\n    results.results.forEach(snapshot =&gt; {\n        console.log(`${snapshot.title} - ${snapshot.captureDate}`);\n    });\n})();\n</code></pre>"},{"location":"api-consumer-guide/#r","title":"R","text":"<pre><code>library(httr)\nlibrary(jsonlite)\n\nsearch_healtharchive &lt;- function(query, source = NULL, sort = \"relevance\", page = 1) {\n  base_url &lt;- \"https://api.healtharchive.ca/api/search\"\n\n  params &lt;- list(\n    q = query,\n    sort = sort,\n    page = page,\n    pageSize = 20\n  )\n\n  if (!is.null(source)) {\n    params$source &lt;- source\n  }\n\n  response &lt;- GET(base_url, query = params)\n  stop_for_status(response)\n\n  content(response, as = \"parsed\")\n}\n\n# Example usage\nresults &lt;- search_healtharchive(\"covid vaccines\", source = \"hc\")\nfor (snapshot in results$results) {\n  cat(sprintf(\"%s - %s\\n\", snapshot$title, snapshot$captureDate))\n}\n</code></pre>"},{"location":"api-consumer-guide/#shell-curl-jq","title":"Shell (curl + jq)","text":"<pre><code>#!/bin/bash\n\n# Search and format results\ncurl -s \"https://api.healtharchive.ca/api/search?q=vaccines&amp;source=hc\" | \\\n  jq -r '.results[] | \"\\(.title) - \\(.captureDate)\"'\n\n# Get total count\ncurl -s \"https://api.healtharchive.ca/api/search?q=vaccines\" | \\\n  jq '.total'\n\n# Download all snapshot URLs\ncurl -s \"https://api.healtharchive.ca/api/search?q=vaccines&amp;pageSize=100\" | \\\n  jq -r '.results[] | .originalUrl' &gt; urls.txt\n</code></pre>"},{"location":"api-consumer-guide/#research-workflows","title":"Research Workflows","text":""},{"location":"api-consumer-guide/#1-historical-analysis","title":"1. Historical Analysis","text":"<p>Goal: Track how Health Canada's COVID-19 vaccine page changed over time</p> <pre><code>import requests\nfrom datetime import datetime\n\nurl_to_track = \"https://www.canada.ca/en/health-canada/services/drugs-health-products/covid19-industry/drugs-vaccines-treatments/vaccines.html\"\n\nresponse = requests.get(\n    \"https://api.healtharchive.ca/api/search\",\n    params={\n        \"q\": f\"url:{url_to_track}\",\n        \"view\": \"snapshots\",  # Get all captures\n        \"sort\": \"newest\",\n        \"pageSize\": 100\n    }\n)\n\nsnapshots = response.json()[\"results\"]\n\nprint(f\"Found {len(snapshots)} captures of this page\")\n\nfor snapshot in snapshots:\n    capture_date = datetime.fromisoformat(snapshot[\"captureDate\"].replace(\"Z\", \"+00:00\"))\n    print(f\"{capture_date.strftime('%Y-%m-%d')}: {snapshot['title']}\")\n</code></pre>"},{"location":"api-consumer-guide/#2-comparative-analysis","title":"2. Comparative Analysis","text":"<p>Goal: Compare coverage of a topic across sources</p> <pre><code>import requests\n\ntopic = \"vaccination\"\n\nfor source in [\"hc\", \"phac\"]:\n    response = requests.get(\n        \"https://api.healtharchive.ca/api/search\",\n        params={\"q\": topic, \"source\": source, \"pageSize\": 1}\n    )\n    total = response.json()[\"total\"]\n    print(f\"{source.upper()}: {total} snapshots mention '{topic}'\")\n</code></pre>"},{"location":"api-consumer-guide/#3-bulk-download-metadata","title":"3. Bulk Download Metadata","text":"<p>Goal: Export all metadata for offline analysis</p> <pre><code>import requests\nimport json\n\nall_snapshots = []\npage = 1\n\nwhile True:\n    response = requests.get(\n        \"https://api.healtharchive.ca/api/search\",\n        params={\n            \"q\": \"\",  # Empty query = browse all\n            \"page\": page,\n            \"pageSize\": 100,\n            \"sort\": \"newest\"\n        }\n    )\n    data = response.json()\n    all_snapshots.extend(data[\"results\"])\n\n    if page * 100 &gt;= data[\"total\"]:\n        break\n\n    page += 1\n\n# Save to JSON\nwith open(\"healtharchive_metadata.json\", \"w\") as f:\n    json.dump(all_snapshots, f, indent=2)\n\nprint(f\"Exported {len(all_snapshots)} snapshots\")\n</code></pre>"},{"location":"api-consumer-guide/#citation-attribution","title":"Citation &amp; Attribution","text":""},{"location":"api-consumer-guide/#citing-healtharchive","title":"Citing HealthArchive","text":"<p>When using HealthArchive data in research:</p> <pre><code>HealthArchive. (2026). Archive of Canadian Health Government Websites.\nRetrieved [Date] from https://healtharchive.ca\n</code></pre>"},{"location":"api-consumer-guide/#citing-specific-snapshots","title":"Citing Specific Snapshots","text":"<pre><code>Health Canada. (2026, January 18). COVID-19 vaccines: Authorization and safety.\nArchived by HealthArchive. Retrieved from https://api.healtharchive.ca/api/snapshots/raw/12345\nOriginal URL: https://www.canada.ca/en/health-canada/services/drugs-health-products/covid19-industry/drugs-vaccines-treatments/vaccines.html\n</code></pre>"},{"location":"api-consumer-guide/#data-access-datasets","title":"Data Access &amp; Datasets","text":""},{"location":"api-consumer-guide/#bulk-data-downloads","title":"Bulk Data Downloads","text":"<p>For large-scale research, consider using dataset releases:</p> <p>Datasets Repository: github.com/jerdaw/healtharchive-datasets</p> <p>Benefits: - Pre-packaged metadata exports - Checksums for integrity verification - Version-controlled releases - Citable DOIs (future)</p>"},{"location":"api-consumer-guide/#api-vs-datasets","title":"API vs Datasets","text":"Use Case Use API Use Dataset Real-time search \u2705 \u274c Small queries (&lt; 1000 results) \u2705 \u274c Complete metadata export \u274c \u2705 Reproducible research ~ \u2705 Offline analysis \u274c \u2705"},{"location":"api-consumer-guide/#error-handling","title":"Error Handling","text":""},{"location":"api-consumer-guide/#common-http-status-codes","title":"Common HTTP Status Codes","text":"Code Meaning Action 200 Success Process response 404 Snapshot/resource not found Check ID, may have been deleted 422 Validation error Fix query parameters 500 Server error Retry with exponential backoff 503 Service unavailable Maintenance, retry later"},{"location":"api-consumer-guide/#example-error-response","title":"Example Error Response","text":"<pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\"query\", \"page\"],\n      \"msg\": \"ensure this value is greater than or equal to 1\",\n      \"type\": \"value_error.number.not_ge\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api-consumer-guide/#robust-error-handling-python","title":"Robust Error Handling (Python)","text":"<pre><code>import requests\nimport time\n\ndef search_with_retry(query, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            response = requests.get(\n                \"https://api.healtharchive.ca/api/search\",\n                params={\"q\": query},\n                timeout=10\n            )\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code &gt;= 500:\n                # Server error, retry with backoff\n                wait = 2 ** attempt\n                print(f\"Server error, retrying in {wait}s...\")\n                time.sleep(wait)\n            else:\n                # Client error, don't retry\n                raise\n        except requests.exceptions.Timeout:\n            print(f\"Timeout, retrying...\")\n            time.sleep(2 ** attempt)\n\n    raise Exception(f\"Failed after {max_retries} retries\")\n</code></pre>"},{"location":"api-consumer-guide/#api-limits-fair-use","title":"API Limits &amp; Fair Use","text":""},{"location":"api-consumer-guide/#current-limits","title":"Current Limits","text":"<ul> <li>Rate limiting: None (subject to change)</li> <li>Query complexity: No hard limits, but very broad queries may timeout</li> <li>Page size: Max 100 results per page</li> </ul>"},{"location":"api-consumer-guide/#fair-use-guidelines","title":"Fair Use Guidelines","text":"<p>To keep the API available for everyone:</p> <ol> <li>Cache aggressively: Don't request the same data repeatedly</li> <li>Use appropriate page sizes: Don't always use <code>pageSize=100</code> if you only need 20</li> <li>Implement backoff: Retry with exponential backoff on errors</li> <li>Consider datasets: For bulk access, use dataset releases instead of paginating through API</li> <li>Report issues: If you encounter consistent errors, let us know</li> </ol>"},{"location":"api-consumer-guide/#future-changes","title":"Future Changes","text":"<p>We may introduce: - Rate limiting (per IP or API key) - API keys for higher limits - Tiered access (free vs. paid)</p> <p>Stay informed: Monitor github.com/jerdaw/healtharchive-backend for announcements</p>"},{"location":"api-consumer-guide/#faq","title":"FAQ","text":"<p>Q: Is there an API key or authentication? A: Public endpoints require no authentication. Admin endpoints require a token.</p> <p>Q: Can I download the entire archive? A: Use dataset releases for bulk access. API is designed for queries, not full dumps.</p> <p>Q: How often is the archive updated? A: Annual full crawls, with potential ad-hoc crawls for significant events.</p> <p>Q: What if a snapshot I need is missing? A: Check the capture dates via <code>/api/sources</code>. We can only provide what was archived.</p> <p>Q: Can I request a specific page be archived? A: Currently no on-demand archiving. Future feature under consideration.</p> <p>Q: How long are snapshots retained? A: Indefinitely, subject to storage constraints. See Data Handling Policy.</p> <p>Q: Is there a GraphQL API? A: Not yet. REST/JSON only for now.</p> <p>Q: Can I embed archived pages in my site? A: Yes, use <code>&lt;iframe src=\"https://api.healtharchive.ca/api/snapshots/raw/{id}\"&gt;&lt;/iframe&gt;</code>. Attribute HealthArchive.</p>"},{"location":"api-consumer-guide/#support-contact","title":"Support &amp; Contact","text":"<ul> <li>Technical issues: GitHub Issues</li> <li>General questions: GitHub Discussions</li> <li>API documentation: Interactive docs</li> </ul>"},{"location":"api-consumer-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API: Try interactive documentation</li> <li>Download datasets: Visit healtharchive-datasets</li> <li>Read the architecture: Architecture Guide</li> <li>Stay updated: Watch the backend repo for changes</li> </ul> <p>Happy researching! \ud83d\udcca</p>"},{"location":"api/","title":"API Documentation","text":"<p>This page provides interactive documentation for the HealthArchive Backend API. You can explore the available endpoints, view schemas, and test requests directly from this interface.</p> <p></p>"},{"location":"architecture/","title":"HealthArchive Backend \u2013 Architecture &amp; Implementation Guide","text":"<p>This document is an in\u2011depth walkthrough of the HealthArchive.ca backend (<code>healtharchive-backend</code> repo). It covers:</p> <ul> <li>How the backend is structured.</li> <li>How it integrates with the <code>archive_tool</code> crawler subpackage.</li> <li>The data model and job lifecycle.</li> <li>The indexing pipeline (WARCs \u2192 snapshots).</li> <li>HTTP APIs (public + admin) and metrics.</li> <li>Worker loop, retries, and cleanup/retention (future).</li> </ul> <p>For <code>archive_tool</code> internals (log parsing, Docker orchestration, run modes), see <code>src/archive_tool/docs/documentation.md</code>. For a shorter, task\u2011oriented overview of common commands and local testing flows, see <code>development/live-testing.md</code>. For deployment\u2011oriented configuration (staging/prod env vars, DNS, Vercel), see <code>deployment/hosting-and-live-server-to-dos.md</code>.</p>"},{"location":"architecture/#1-highlevel-architecture","title":"1. High\u2011level architecture","text":""},{"location":"architecture/#11-components","title":"1.1 Components","text":"<ul> <li>archive_tool (internal subpackage under <code>src/archive_tool/</code>):</li> <li>CLI wrapper around <code>zimit</code> + Docker.</li> <li>Manages temporary output dirs, WARCs, and final ZIM build.</li> <li>Tracks persistent state in <code>.archive_state.json</code> + <code>.tmp*</code> directories.</li> <li> <p>Implements stall/error detection, adaptive worker reductions, and VPN     rotation (when enabled).</p> </li> <li> <p>Backend package (<code>src/ha_backend/</code>):</p> </li> <li>Orchestrates crawl jobs using <code>archive_tool</code> as a subprocess.</li> <li>Stores job and snapshot metadata in a relational database via SQLAlchemy.</li> <li>Indexes WARCs into <code>Snapshot</code> rows.</li> <li>Exposes HTTP APIs via FastAPI.</li> <li>Provides a worker loop to process queued jobs.</li> <li> <p>Offers CLI commands for admins (job creation, status, retry, cleanup).</p> </li> <li> <p>External dependencies:</p> </li> <li>Docker &amp; <code>ghcr.io/openzim/zimit</code> image.</li> <li>Database (SQLite by default; Postgres recommended in production).</li> <li>Optional VPN client/command for rotation (e.g., <code>nordvpn</code>).</li> </ul>"},{"location":"architecture/#12-data-flow-overview","title":"1.2 Data flow overview","text":"<ol> <li>Job creation:</li> <li>Admin runs <code>ha-backend create-job --source hc</code>.</li> <li> <p>Backend:</p> <ul> <li>Ensures a <code>Source</code> row exists.</li> <li>Uses <code>SourceJobConfig</code> to build seeds, tool options, and <code>output_dir</code>.</li> <li>Inserts an <code>ArchiveJob</code> with <code>status=\"queued\"</code>.</li> </ul> </li> <li> <p>Crawl (archive_tool):</p> </li> <li>Worker or CLI runs <code>run_persistent_job(job_id)</code>:<ul> <li>Builds <code>archive_tool</code> CLI args from <code>ArchiveJob.config</code> and <code>output_dir</code>.</li> <li>Runs <code>archive_tool</code> as a subprocess (no in\u2011process calls).</li> <li>Marks job <code>running</code> \u2192 <code>completed</code> or <code>failed</code> with <code>crawler_exit_code</code>    and <code>crawler_status</code>.</li> </ul> </li> <li> <p><code>archive_tool</code>:</p> <ul> <li>Validates Docker.</li> <li>Determines run mode (Fresh/Resume/New\u2011with\u2011Consolidation/Overwrite).</li> <li>Spawns <code>docker run ghcr.io/openzim/zimit zimit ...</code>.</li> <li>Tracks temp dirs and state, discovers WARCs, and optionally runs a    final ZIM build (depending on its configuration).</li> </ul> </li> <li> <p>Indexing (WARCs \u2192 Snapshot):</p> </li> <li>Worker calls <code>index_job(job_id)</code> when crawl succeeds.</li> <li> <p>Backend:</p> <ul> <li>Uses <code>CrawlState</code> + <code>find_all_warc_files</code> to locate WARCs under    <code>output_dir</code>.</li> <li>Streams WARC records, extracts HTML, text, language, etc.</li> <li>Writes <code>Snapshot</code> rows for each captured page.</li> <li>Marks job <code>indexed</code> with <code>indexed_page_count</code>.</li> </ul> </li> <li> <p>Change tracking (Snapshot \u2192 Change events):</p> </li> <li>A background task (<code>ha-backend compute-changes</code>) computes precomputed      change events between adjacent captures of the same <code>normalized_url_group</code>.</li> <li>Outputs <code>SnapshotChange</code> rows with:<ul> <li>provenance (from/to snapshot IDs, timestamps),</li> <li>summary stats (sections/lines changed),</li> <li>and a renderable diff artifact when available.</li> </ul> </li> <li> <p>This work is intentionally off the request path to keep APIs fast.</p> </li> <li> <p>Serving:</p> </li> <li> <p>FastAPI app:</p> <ul> <li><code>GET /api/search</code> queries <code>Snapshot</code> for search results.</li> <li><code>GET /api/stats</code> provides lightweight public archive totals for frontend metrics.</li> <li><code>GET /api/sources</code> summarises captures per <code>Source</code>.</li> <li><code>GET /api/snapshot/{id}</code> returns metadata for a single snapshot.</li> <li><code>GET /api/snapshots/raw/{id}</code> replays archived HTML from a WARC.</li> <li><code>GET /api/changes</code> and <code>GET /api/changes/compare</code> expose change feeds and diffs.</li> <li><code>GET /api/snapshots/{id}/timeline</code> returns a capture timeline for a page group.</li> </ul> </li> <li> <p>Admin &amp; cleanup:</p> </li> <li>Admin API:<ul> <li><code>GET /api/admin/jobs</code> / <code>{id}</code> for job status and config.</li> <li><code>GET /metrics</code> for Prometheus\u2011style metrics.</li> </ul> </li> <li>CLI:<ul> <li><code>ha-backend retry-job</code> to reattempt failed jobs.</li> <li><code>ha-backend cleanup-job</code> to delete temp dirs/state for indexed jobs,    updating <code>cleanup_status</code>.</li> </ul> </li> </ol>"},{"location":"architecture/#2-configuration-environment","title":"2. Configuration &amp; environment","text":""},{"location":"architecture/#21-config-module-ha_backendconfigpy","title":"2.1 Config module (<code>ha_backend/config.py</code>)","text":"<p>Key roles:</p> <ul> <li>Locate the archive root (<code>--output-dir</code> base) and <code>archive_tool</code> command.</li> <li>Read the database URL.</li> </ul> <p>Admin\u2011related configuration is handled separately in <code>ha_backend/api/deps.py</code>, which reads <code>HEALTHARCHIVE_ADMIN_TOKEN</code> from the environment. When this token is unset, admin and metrics endpoints are effectively open and should only be used in local development. In staging and production you should always set <code>HEALTHARCHIVE_ADMIN_TOKEN</code> to a long, random value and treat it as a secret.</p>"},{"location":"architecture/#archivetoolconfig","title":"ArchiveToolConfig","text":"<pre><code>@dataclass\nclass ArchiveToolConfig:\n    archive_root: Path = DEFAULT_ARCHIVE_ROOT\n    archive_tool_cmd: str = DEFAULT_ARCHIVE_TOOL_CMD\n\n    def ensure_archive_root(self) -&gt; None:\n        self.archive_root.mkdir(parents=True, exist_ok=True)\n</code></pre> <p>Defaults:</p> <ul> <li><code>DEFAULT_ARCHIVE_ROOT</code> = <code>/mnt/nasd/nobak/healtharchive/jobs</code></li> <li><code>DEFAULT_ARCHIVE_TOOL_CMD</code> = <code>\"archive-tool\"</code></li> </ul> <p>Env overrides:</p> <ul> <li><code>HEALTHARCHIVE_ARCHIVE_ROOT</code> \u2192 archive root.</li> <li><code>HEALTHARCHIVE_TOOL_CMD</code> \u2192 CLI to call (e.g., <code>archive-tool</code>, <code>python run_archive.py</code>).</li> </ul>"},{"location":"architecture/#databaseconfig","title":"DatabaseConfig","text":"<pre><code>@dataclass\nclass DatabaseConfig:\n    database_url: str = DEFAULT_DATABASE_URL\n</code></pre> <p>Defaults:</p> <ul> <li><code>DEFAULT_DATABASE_URL = \"sqlite:///healtharchive.db\"</code> in the repo root.</li> </ul> <p>Env override:</p> <ul> <li><code>HEALTHARCHIVE_DATABASE_URL</code>.</li> </ul>"},{"location":"architecture/#22-logging-ha_backendlogging_configpy","title":"2.2 Logging (<code>ha_backend/logging_config.py</code>)","text":"<p>Centralized logging configuration:</p> <ul> <li>Reads <code>HEALTHARCHIVE_LOG_LEVEL</code> (default <code>INFO</code>).</li> <li>On first call, uses <code>logging.basicConfig(...)</code> with:</li> <li>Format: <code>\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"</code>.</li> <li>Adjusts noisy loggers:</li> <li><code>sqlalchemy.engine</code> \u2192 <code>WARNING</code>.</li> <li><code>uvicorn.access</code> \u2192 <code>INFO</code>.</li> </ul> <p>Used in:</p> <ul> <li><code>ha_backend.api.__init__</code> (API startup).</li> <li><code>ha_backend.cli.main</code> (CLI entrypoint).</li> </ul>"},{"location":"architecture/#3-data-model-sqlalchemy-orm","title":"3. Data model (SQLAlchemy ORM)","text":"<p>Defined in <code>src/ha_backend/models.py</code>, with <code>Base</code> from <code>ha_backend.db</code>.</p>"},{"location":"architecture/#31-source","title":"3.1 Source","text":"<p>Represents a logical content origin (e.g., Health Canada, PHAC).</p> <p>Important fields:</p> <ul> <li><code>id: int</code> (PK)</li> <li><code>code: str</code> \u2013 short code (<code>\"hc\"</code>, <code>\"phac\"</code>) \u2013 unique, indexed.</li> <li><code>name: str</code> \u2013 human\u2011readable name.</li> <li><code>base_url: str | None</code></li> <li><code>description: str | None</code></li> <li><code>enabled: bool</code></li> <li>Timestamps: <code>created_at</code>, <code>updated_at</code></li> </ul> <p>Relationships:</p> <ul> <li><code>jobs: List[ArchiveJob]</code> \u2013 all jobs for this source.</li> <li><code>snapshots: List[Snapshot]</code> \u2013 all snapshots for this source.</li> </ul>"},{"location":"architecture/#32-archivejob","title":"3.2 ArchiveJob","text":"<p>Represents a single <code>archive_tool</code> run (or family of runs) for a source.</p> <p>Key fields:</p> <ul> <li>Identity:</li> <li><code>id: int</code> (PK)</li> <li><code>source_id: int | None</code> \u2192 FK to <code>sources.id</code></li> <li><code>name: str</code> \u2013 must match <code>--name</code> for <code>archive_tool</code>; used in ZIM naming.</li> <li> <p><code>output_dir: str</code> \u2013 host path used as <code>--output-dir</code> for <code>archive_tool</code>.</p> </li> <li> <p>Lifecycle/status:</p> </li> <li><code>status: str</code> \u2013 high\u2011level state; typical values:<ul> <li><code>queued</code></li> <li><code>running</code></li> <li><code>retryable</code></li> <li><code>failed</code></li> <li><code>completed</code> (crawl succeeded)</li> <li><code>indexing</code></li> <li><code>indexed</code></li> <li><code>index_failed</code></li> </ul> </li> <li><code>queued_at</code>, <code>started_at</code>, <code>finished_at</code>: timestamps.</li> <li> <p><code>retry_count: int</code> \u2013 number of times the worker retried the crawl.</p> </li> <li> <p>Configuration:</p> </li> <li> <p><code>config: JSON | None</code> \u2013 \u201copaque\u201d config used to reconstruct the CLI:</p> <pre><code>{\n  \"seeds\": [\"https://...\"],\n  \"zimit_passthrough_args\": [\"--profile\", \"foo\"],\n  \"tool_options\": {\n    \"cleanup\": false,\n    \"overwrite\": false,\n    \"skip_final_build\": false,\n    \"enable_monitoring\": false,\n    \"enable_adaptive_workers\": false,\n    \"enable_vpn_rotation\": false,\n    \"initial_workers\": 2,\n    \"log_level\": \"INFO\",\n    \"...\": \"...\"\n  }\n}\n</code></pre> </li> <li> <p>Crawl metrics:</p> </li> <li><code>crawler_exit_code: int | None</code> \u2013 exit code from the <code>archive_tool</code> process.</li> <li><code>crawler_status: str | None</code> \u2013 summarised status (e.g. <code>\"success\"</code>, <code>\"failed\"</code>).</li> <li><code>crawler_stage: str | None</code> \u2013 last known stage (not heavily used yet).</li> <li><code>last_stats_json: JSON | None</code> \u2013 parsed crawl stats from the latest combined log, when available.</li> <li> <p><code>pages_crawled</code>, <code>pages_total</code>, <code>pages_failed</code>: simple integer metrics derived from <code>last_stats_json</code> (best-effort).</p> </li> <li> <p>WARC/ZIM counts:</p> </li> <li><code>warc_file_count: int</code> \u2013 number of WARCs discovered for this job.</li> <li> <p><code>indexed_page_count: int</code> \u2013 number of <code>Snapshot</code>s created during indexing.</p> </li> <li> <p>Filesystem paths:</p> </li> <li><code>final_zim_path: str | None</code> \u2013 if a ZIM is produced by <code>archive_tool</code> or manual <code>warc2zim</code>.</li> <li><code>combined_log_path: str | None</code> \u2013 path to the latest combined log, used for stats/debugging.</li> <li> <p><code>state_file_path: str | None</code> \u2013 path to <code>.archive_state.json</code> within <code>output_dir</code> (may be <code>None</code> after cleanup).</p> </li> <li> <p>Cleanup state (future):</p> </li> <li><code>cleanup_status: str</code> \u2013 describes whether any cleanup has occurred:<ul> <li><code>\"none\"</code> (default) \u2013 temp dirs &amp; state still present (or never existed).</li> <li><code>\"temp_cleaned\"</code> \u2013 <code>cleanup-job</code> or an equivalent operation removed temp dirs/state.</li> <li>Future values could represent more aggressive cleanup.</li> </ul> </li> <li><code>cleaned_at: datetime | None</code> \u2013 when cleanup was performed.</li> </ul> <p>Relationships:</p> <ul> <li><code>source: Source | None</code> \u2013 parent source.</li> <li><code>snapshots: List[Snapshot]</code> \u2013 all snapshots produced by this job.</li> </ul>"},{"location":"architecture/#33-snapshot","title":"3.3 Snapshot","text":"<p>Represents a single captured web page (an HTML response) extracted from a WARC.</p> <p>Key fields:</p> <ul> <li>Identity:</li> <li><code>id: int</code> (PK)</li> <li><code>job_id: int | None</code> \u2192 FK to <code>archive_jobs.id</code></li> <li> <p><code>source_id: int | None</code> \u2192 FK to <code>sources.id</code></p> </li> <li> <p>URL &amp; grouping:</p> </li> <li><code>url: str</code> \u2013 full URL of the capture (including query string).</li> <li> <p><code>normalized_url_group: str | None</code> \u2013 optional canonicalised URL for grouping (e.g., removing query or anchors).</p> </li> <li> <p>Timing:</p> </li> <li> <p><code>capture_timestamp: datetime</code> \u2013 from <code>WARC-Date</code> or HTTP headers.</p> </li> <li> <p>HTTP &amp; content:</p> </li> <li><code>mime_type: str | None</code></li> <li><code>status_code: int | None</code></li> <li><code>title: str | None</code> \u2013 extracted from <code>&lt;title&gt;</code> or headings.</li> <li><code>snippet: str | None</code> \u2013 short preview text.</li> <li> <p><code>language: str | None</code> \u2013 ISO language (e.g. <code>\"en\"</code>, <code>\"fr\"</code>).</p> </li> <li> <p>Storage / replay:</p> </li> <li><code>warc_path: str</code> \u2013 path to the <code>.warc.gz</code> file on disk.</li> <li><code>warc_record_id: str | None</code> \u2013 WARC record identifier or offset (see <code>indexing.viewer</code>).</li> <li><code>raw_snapshot_path: str | None</code> \u2013 optional path to a static HTML export, if you create such stubs.</li> <li><code>content_hash: str | None</code> \u2013 hash of the HTML body for deduplication.</li> </ul> <p>Relationships:</p> <ul> <li><code>job: ArchiveJob | None</code></li> <li><code>source: Source | None</code></li> </ul>"},{"location":"architecture/#4-job-registry--creation-ha_backendjob_registrypy","title":"4. Job registry &amp; creation (<code>ha_backend/job_registry.py</code>)","text":"<p>The job registry defines default behavior and seeds for each source code (<code>\"hc\"</code>, <code>\"phac\"</code>).</p>"},{"location":"architecture/#41-sourcejobconfig","title":"4.1 SourceJobConfig","text":"<pre><code>@dataclass\nclass SourceJobConfig:\n    source_code: str\n    name_template: str\n    default_seeds: List[str]\n    default_zimit_passthrough_args: List[str]\n    default_tool_options: Dict[str, Any]\n    schedule_hint: Optional[str] = None\n</code></pre> <p>Examples:</p> <ul> <li> <p><code>hc</code> (Health Canada):</p> </li> <li> <p><code>name_template = \"hc-{date:%Y%m%d}\"</code></p> </li> <li><code>default_seeds = [\"https://www.canada.ca/en/health-canada.html\"]</code></li> <li> <p><code>default_tool_options</code>:</p> <ul> <li><code>cleanup = False</code></li> <li><code>overwrite = False</code></li> <li><code>enable_monitoring = True</code> (required for adaptive strategies)</li> <li><code>enable_adaptive_workers = True</code></li> <li><code>enable_adaptive_restart = True</code></li> <li><code>enable_vpn_rotation = False</code> (disabled by default)</li> <li><code>initial_workers = 2</code></li> <li><code>stall_timeout_minutes = 60</code></li> <li><code>docker_shm_size = \"1g\"</code></li> <li><code>skip_final_build = True</code> (annual campaign: search/indexing uses WARCs)</li> <li><code>error_threshold_timeout = 50</code></li> <li><code>error_threshold_http = 50</code></li> <li><code>backoff_delay_minutes = 2</code></li> <li><code>max_container_restarts = 20</code></li> <li><code>log_level = \"INFO\"</code></li> </ul> </li> <li> <p><code>phac</code> (Public Health Agency of Canada) is similar with a PHAC home page seed.</p> </li> </ul>"},{"location":"architecture/#42-job-name-and-output-dir","title":"4.2 Job name and output dir","text":"<ul> <li><code>generate_job_name(source_cfg, now)</code>:</li> <li>Renders <code>name_template</code> using <code>{date:%Y%m%d}</code> from UTC timestamp.</li> <li> <p>E.g. <code>hc-20251209</code>.</p> </li> <li> <p><code>build_output_dir_for_job(source_code, job_name, archive_root, now)</code>:</p> </li> </ul> <pre><code>&lt;archive_root&gt;/&lt;source_code&gt;/&lt;YYYYMMDDThhmmssZ&gt;__&lt;job_name&gt;\n</code></pre> <p>Example:</p> <pre><code>/mnt/nasd/nobak/healtharchive/jobs/hc/20251209T210911Z__hc-20251209\n</code></pre>"},{"location":"architecture/#43-job-config-json","title":"4.3 Job config JSON","text":"<ul> <li><code>build_job_config(source_cfg, extra_seeds=None, overrides=None)</code>:</li> <li>Merges <code>default_seeds</code> + extra seeds.</li> <li>Copies <code>default_zimit_passthrough_args</code>.</li> <li>Copies and updates <code>default_tool_options</code> with any <code>overrides</code>.</li> <li> <p>Performs basic validation of <code>tool_options</code> to fail fast on     misconfiguration:</p> <ul> <li>If <code>enable_adaptive_workers=True</code> but <code>enable_monitoring</code> is not <code>True</code>,   a <code>ValueError</code> is raised.</li> <li>If <code>enable_vpn_rotation=True</code> but <code>enable_monitoring</code> is not <code>True</code>,   a <code>ValueError</code> is raised.</li> <li>If <code>enable_vpn_rotation=True</code> but <code>vpn_connect_command</code> is missing or   empty, a <code>ValueError</code> is raised.</li> </ul> </li> </ul> <p>Result structure:</p> <pre><code>{\n  \"seeds\": [\"https://...\", \"...\"],\n  \"zimit_passthrough_args\": [],\n  \"tool_options\": {\n    \"cleanup\": false,\n    \"overwrite\": false,\n    \"skip_final_build\": true,\n    \"enable_monitoring\": true,\n    \"enable_adaptive_workers\": true,\n    \"enable_adaptive_restart\": true,\n    \"enable_vpn_rotation\": false,\n    \"initial_workers\": 2,\n    \"stall_timeout_minutes\": 60,\n    \"docker_shm_size\": \"1g\",\n    \"error_threshold_timeout\": 50,\n    \"error_threshold_http\": 50,\n    \"backoff_delay_minutes\": 2,\n    \"max_container_restarts\": 20,\n    \"log_level\": \"INFO\"\n  }\n}\n</code></pre>"},{"location":"architecture/#44-create_job_for_source","title":"4.4 create_job_for_source","text":"<pre><code>def create_job_for_source(\n    source_code: str,\n    *,\n    session: Session,\n    overrides: Optional[Dict[str, Any]] = None,\n) -&gt; ORMArchiveJob:\n</code></pre> <p>Steps:</p> <ol> <li>Look up <code>SourceJobConfig</code> for <code>source_code</code>.</li> <li>Ensure a <code>Source</code> row with that code exists (or raise).</li> <li>Resolve <code>archive_root</code> from config.</li> <li>Generate <code>job_name</code> and <code>output_dir</code>.</li> <li>Build <code>job_config</code>.</li> <li>Insert an <code>ArchiveJob</code>:</li> <li><code>status=\"queued\"</code>, <code>queued_at=now</code>, <code>config=job_config</code>.</li> </ol> <p>The CLI command <code>ha-backend create-job --source hc</code> is a thin wrapper around this.</p>"},{"location":"architecture/#5-archive_tool-integration--job-runner-ha_backendjobspy","title":"5. archive_tool integration &amp; job runner (<code>ha_backend/jobs.py</code>)","text":""},{"location":"architecture/#51-runtimearchivejob","title":"5.1 RuntimeArchiveJob","text":"<p><code>RuntimeArchiveJob</code> is a small helper for ad\u2011hoc runs (<code>ha-backend run-job</code>) that:</p> <ul> <li>Holds just a <code>name</code> and <code>seeds: list[str]</code>.</li> <li>Creates a timestamped job directory under the archive root (unless overridden).</li> <li>Builds the <code>archive_tool</code> CLI command.</li> <li>Executes it via <code>subprocess.run(...)</code>.</li> </ul> <p>This path is used by:</p> <ul> <li><code>ha-backend run-job</code> \u2013 direct, non\u2011persistent jobs.</li> </ul>"},{"location":"architecture/#52-run_persistent_job--db-backed-jobs","title":"5.2 run_persistent_job \u2013 DB\u2011backed jobs","text":"<pre><code>def run_persistent_job(job_id: int) -&gt; int:\n    ...\n</code></pre> <p>Responsibilities:</p> <ol> <li> <p>Load job and mark running:</p> </li> <li> <p>Using <code>get_session()</code>:</p> <ul> <li>Fetch <code>ArchiveJob</code> by ID.</li> <li>Validate <code>status in (\"queued\", \"retryable\")</code>.</li> <li>Extract <code>config</code>, splitting into:</li> <li><code>tool_options</code></li> <li><code>zimit_passthrough_args</code></li> <li><code>seeds</code></li> <li>Validate that <code>seeds</code> is non\u2011empty.</li> <li>Record <code>output_dir</code> and <code>name</code>.</li> <li>Set:</li> <li><code>status = \"running\"</code></li> <li><code>started_at = now</code></li> </ul> </li> <li> <p>Build CLI options from tool_options:</p> </li> <li> <p>Core:</p> <pre><code>initial_workers = int(tool_options.initial_workers)\ncleanup = bool(tool_options.cleanup)\noverwrite = bool(tool_options.overwrite)\nlog_level = str(tool_options.log_level)\n</code></pre> </li> <li> <p>Monitoring options:</p> <p>Only if <code>enable_monitoring</code> is <code>True</code>:</p> <ul> <li>Adds <code>--enable-monitoring</code>.</li> <li>Optionally:</li> <li><code>monitor_interval_seconds</code> \u2192 <code>--monitor-interval-seconds N</code></li> <li><code>stall_timeout_minutes</code> \u2192 <code>--stall-timeout-minutes N</code></li> <li><code>error_threshold_timeout</code> \u2192 <code>--error-threshold-timeout N</code></li> <li><code>error_threshold_http</code> \u2192 <code>--error-threshold-http N</code></li> </ul> </li> <li> <p>Adaptive workers:</p> <p>Only if both <code>enable_monitoring</code> and <code>enable_adaptive_workers</code> are <code>True</code>:</p> <ul> <li>Adds <code>--enable-adaptive-workers</code>.</li> <li>Optionally:</li> <li><code>min_workers</code> \u2192 <code>--min-workers N</code></li> <li><code>max_worker_reductions</code> \u2192 <code>--max-worker-reductions N</code></li> </ul> </li> <li> <p>VPN rotation:</p> <p>Only if <code>enable_monitoring</code>, <code>enable_vpn_rotation</code>, and <code>vpn_connect_command</code>  are all present:</p> <ul> <li>Adds:</li> </ul> <pre><code>--enable-vpn-rotation\n--vpn-connect-command \"&lt;vpn_connect_command&gt;\"\n</code></pre> <ul> <li>Optionally:</li> <li><code>max_vpn_rotations</code> \u2192 <code>--max-vpn-rotations N</code></li> <li><code>vpn_rotation_frequency_minutes</code> \u2192 <code>--vpn-rotation-frequency-minutes N</code></li> </ul> </li> <li> <p>Backoff:</p> <p>Only when monitoring is enabled and <code>backoff_delay_minutes</code> is set:</p> <ul> <li><code>--backoff-delay-minutes N</code>.</li> </ul> </li> <li> <p>Zimit passthrough:</p> <ul> <li><code>zimit_passthrough_args</code> are appended directly (no explicit <code>\"--\"</code>    separator is required): <code>archive_tool</code> uses <code>argparse.parse_known_args()</code>    and passes unknown args through to <code>zimit</code>.</li> <li>For <code>ha-backend run-job</code>, a leading <code>\"--\"</code> is accepted and stripped for    convenience when passing through flags interactively.</li> </ul> </li> <li> <p>The final <code>extra_args</code> passed to <code>RuntimeArchiveJob.run(...)</code> look like:</p> <pre><code>[archive_tool_flags..., zimit_passthrough_args...]\n</code></pre> </li> <li> <p>Execute archive_tool:</p> </li> <li> <p>Instantiates <code>RuntimeArchiveJob(name, seeds)</code>.</p> </li> <li> <p>Calls:</p> <pre><code>rc = runtime_job.run(\n    initial_workers=initial_workers,\n    cleanup=cleanup,\n    overwrite=overwrite,\n    log_level=log_level,\n    extra_args=full_extra_args,\n    stream_output=True,\n    output_dir_override=Path(output_dir_str),\n)\n</code></pre> </li> <li> <p><code>output_dir_override</code> ensures a specific job directory under the archive      root (matching the DB record) is used, and created if needed.</p> </li> <li> <p>Update job status:</p> </li> <li> <p>After the subprocess returns:</p> <ul> <li><code>crawler_exit_code = rc</code></li> <li><code>finished_at = now</code></li> <li><code>combined_log_path</code> is recorded best-effort (newest <code>archive_*.combined.log</code>)</li> <li><code>status = \"completed\"</code> and <code>crawler_status = \"success\"</code> if <code>rc == 0</code></li> <li>Otherwise:</li> <li><code>status = \"retryable\"</code>, <code>crawler_status = \"infra_error\"</code> for storage/mount failures</li> <li><code>status = \"failed\"</code>, <code>crawler_status = \"infra_error_config\"</code> for CLI/config/runtime errors      (e.g., invalid <code>zimit_passthrough_args</code>)</li> <li><code>status = \"failed\"</code>, <code>crawler_status = \"failed\"</code> for normal crawl failures</li> </ul> </li> </ol> <p>The worker uses <code>run_persistent_job(job_id)</code> for each queued job.</p>"},{"location":"architecture/#53-maintaining-the-archive_tool-integration","title":"5.3 Maintaining the archive_tool integration","text":"<p>The backend and <code>archive_tool</code> share a small but important contract:</p> <ul> <li> <p>Configuration JSON:</p> </li> <li> <p><code>ArchiveJob.config</code> stores a dict that is the serialised form of     <code>ArchiveJobConfig</code> from <code>ha_backend.archive_contract</code>:</p> <pre><code>{\n  \"seeds\": [\"https://...\", \"...\"],\n  \"zimit_passthrough_args\": [\"--scopeType\", \"host\"],\n  \"tool_options\": {\n    \"cleanup\": false,\n    \"overwrite\": false,\n    \"skip_final_build\": true,\n    \"enable_monitoring\": true,\n    \"enable_adaptive_workers\": true,\n    \"enable_adaptive_restart\": true,\n    \"enable_vpn_rotation\": false,\n    \"initial_workers\": 2,\n    \"log_level\": \"INFO\",\n    \"relax_perms\": true,\n    \"stall_timeout_minutes\": 60,\n    \"docker_shm_size\": \"1g\",\n    \"error_threshold_timeout\": 50,\n    \"error_threshold_http\": 50,\n    \"max_container_restarts\": 20,\n    \"backoff_delay_minutes\": 2\n  }\n}\n</code></pre> </li> <li> <p><code>SourceJobConfig.default_tool_options</code> in <code>ha_backend.job_registry</code> is the     source of truth for defaults; overrides are merged via     <code>build_job_config(...)</code> which uses <code>ArchiveToolOptions</code> +     <code>validate_tool_options(...)</code> to enforce invariants that mirror     <code>archive_tool.cli</code> (e.g. monitoring required for adaptive/VPN).</p> </li> <li> <p>CLI construction:</p> </li> <li> <p><code>ha_backend.jobs.run_persistent_job</code> is the only place that maps     <code>tool_options</code> fields to <code>archive_tool</code> CLI flags. It expects the argument     model described in <code>src/archive_tool/docs/documentation.md</code> and     <code>archive_tool/cli.py</code>.</p> </li> <li> <p>If you add or rename CLI options in <code>archive_tool</code>:</p> <ul> <li>Extend <code>ArchiveToolOptions</code> and <code>ArchiveJobConfig</code> to carry the new   fields.</li> <li>Update <code>run_persistent_job</code> to add/remove the corresponding flags.</li> <li>Adjust tests under <code>tests/test_job_registry.py</code>,   <code>tests/test_archive_contract.py</code>, and <code>tests/test_jobs_persistent.py</code>   that assert config and CLI behaviour.</li> </ul> </li> <li> <p>Stats and logs:</p> </li> <li> <p><code>archive_tool</code> writes combined logs     <code>archive_&lt;stage_name&gt;_*.combined.log</code> under each job's <code>output_dir</code> and     emits <code>\"Crawl statistics\"</code> JSON lines that     <code>archive_tool.utils.parse_last_stats_from_log</code> can parse.</p> </li> <li> <p><code>ha_backend.crawl_stats.update_job_stats_from_logs</code>:</p> <ul> <li>Locates the latest combined log for a job.</li> <li>Calls <code>parse_last_stats_from_log(log_path)</code> to obtain a stats dict.</li> <li>Stores it in <code>ArchiveJob.last_stats_json</code>.</li> <li>Updates <code>pages_crawled</code>, <code>pages_total</code>, <code>pages_failed</code>, and   <code>combined_log_path</code> as a best-effort summary.</li> </ul> </li> <li> <p><code>/metrics</code> exposes these page counters via:</p> <ul> <li><code>healtharchive_jobs_pages_crawled_total</code></li> <li><code>healtharchive_jobs_pages_failed_total</code></li> <li>per-source variants, backed by the <code>pages_*</code> fields on <code>ArchiveJob</code>.</li> </ul> </li> <li> <p>WARC discovery and cleanup:</p> </li> <li> <p><code>ha_backend.indexing.warc_discovery.discover_warcs_for_job</code> relies on     <code>archive_tool.state.CrawlState</code> and <code>archive_tool.utils.find_all_warc_files</code>     / <code>find_latest_temp_dir_fallback</code> for WARC discovery and temp dir     tracking.</p> </li> <li><code>ha_backend.cli.cmd_cleanup_job</code> uses <code>CrawlState</code> and     <code>archive_tool.utils.cleanup_temp_dirs</code> to remove <code>.tmp*</code> directories and     <code>.archive_state.json</code> safely once jobs are indexed.</li> </ul> <p>If you change log formats, state layout, or directory structure in <code>archive_tool</code>, update the corresponding backend helpers (<code>ArchiveJobConfig</code>, <code>run_persistent_job</code>, <code>update_job_stats_from_logs</code>, WARC discovery, and cleanup) and their tests to keep the contract in sync.</p>"},{"location":"architecture/#6-indexing-pipeline-ha_backendindexing","title":"6. Indexing pipeline (<code>ha_backend/indexing/*</code>)","text":"<p>The indexing pipeline converts the WARCs produced by <code>archive_tool</code> into structured <code>Snapshot</code> rows.</p>"},{"location":"architecture/#61-warc-discovery-warc_discoverypy","title":"6.1 WARC discovery (<code>warc_discovery.py</code>)","text":"<pre><code>from archive_tool.state import CrawlState\nfrom archive_tool.utils import find_all_warc_files, find_latest_temp_dir_fallback\n</code></pre> <pre><code>def discover_warcs_for_job(\n    job: ArchiveJob,\n    *,\n    allow_fallback: bool = True,\n) -&gt; List[Path]:\n</code></pre> <p>Steps:</p> <ol> <li>Resolve <code>host_output_dir = Path(job.output_dir).resolve()</code>.</li> <li>Instantiate <code>CrawlState(host_output_dir, initial_workers=1)</code>:</li> <li>This loads <code>.archive_state.json</code> if present.</li> <li>Get <code>temp_dirs = state.get_temp_dir_paths()</code>:</li> <li>Returns only existing directories and prunes missing ones from state.</li> <li>If <code>temp_dirs</code> is empty and <code>allow_fallback</code>:</li> <li>Use <code>find_latest_temp_dir_fallback(host_output_dir)</code> to scan for <code>.tmp*</code>      directories.</li> <li>If still empty \u2192 return <code>[]</code>.</li> <li>Call <code>find_all_warc_files(temp_dirs)</code>:</li> <li>Returns a de\u2011duplicated list of <code>*.warc.gz</code> files under each      <code>collections/crawl-*/archive</code> directory.</li> </ol> <p>This ensures the backend uses exactly the same WARC discovery logic as <code>archive_tool</code> itself.</p>"},{"location":"architecture/#62-warc-reading-warc_readerpy","title":"6.2 WARC reading (<code>warc_reader.py</code>)","text":"<p>Wraps <code>warcio</code> to stream HTML response records from a <code>.warc.gz</code> file.</p> <p>Exports a generator like:</p> <pre><code>def iter_html_records(warc_path: Path) -&gt; Iterator[ArchiveRecord]:\n    ...\n</code></pre> <p>Where <code>ArchiveRecord</code> provides:</p> <ul> <li><code>url: str</code></li> <li><code>capture_timestamp: datetime</code></li> <li><code>headers: dict[str, str]</code></li> <li><code>body_bytes: bytes</code></li> <li><code>warc_path: Path</code></li> <li><code>warc_record_id: str | None</code></li> </ul>"},{"location":"architecture/#63-text-extraction-text_extractionpy","title":"6.3 Text extraction (<code>text_extraction.py</code>)","text":"<p>Helpers:</p> <ul> <li><code>extract_title(html: str) -&gt; str</code> \u2013 heuristics over <code>&lt;title&gt;</code> / headings.</li> <li><code>extract_text(html: str) -&gt; str</code> \u2013 uses BeautifulSoup to pull visible text.</li> <li><code>make_snippet(text: str) -&gt; str</code> \u2013 short preview (~N chars/words).</li> <li><code>detect_language(text: str, headers: dict) -&gt; str</code> \u2013 simple language detection,   leveraging headers or heuristics (kept basic for now).</li> </ul>"},{"location":"architecture/#64-mapping-records-to-snapshot-mappingpy","title":"6.4 Mapping records to Snapshot (<code>mapping.py</code>)","text":"<p><code>record_to_snapshot(job, source, rec, title, snippet, language)</code>:</p> <ul> <li>Takes:</li> <li><code>ArchiveJob</code></li> <li><code>Source</code></li> <li><code>ArchiveRecord</code> from <code>iter_html_records</code></li> <li><code>title</code>, <code>snippet</code>, <code>language</code> from text extraction</li> <li>Produces a new <code>Snapshot</code> instance with:</li> <li><code>job_id</code>, <code>source_id</code></li> <li><code>url</code>, <code>normalized_url_group</code></li> <li><code>capture_timestamp</code></li> <li><code>mime_type</code>, <code>status_code</code></li> <li><code>title</code>, <code>snippet</code>, <code>language</code></li> <li><code>warc_path</code>, <code>warc_record_id</code></li> <li><code>content_hash</code> (if computed)</li> </ul>"},{"location":"architecture/#65-orchestration-pipelinepy","title":"6.5 Orchestration (<code>pipeline.py</code>)","text":"<pre><code>def index_job(job_id: int) -&gt; int:\n</code></pre> <p>Steps:</p> <ol> <li>Load <code>ArchiveJob</code> by ID, ensure:</li> <li><code>job.source</code> is not <code>None</code>.</li> <li><code>job.status in (\"completed\", \"index_failed\", \"indexed\")</code>.</li> <li>Validate <code>output_dir</code> exists.</li> <li>Discover WARCs:</li> <li><code>warc_paths = discover_warcs_for_job(job)</code>.</li> <li>Sets <code>job.warc_file_count = len(warc_paths)</code>.</li> <li>If no WARCs found:<ul> <li>Logs warning.</li> <li>Sets <code>job.status = \"index_failed\"</code> and returns <code>1</code>.</li> </ul> </li> <li>Clear previous snapshots for this job:</li> <li><code>DELETE FROM snapshots WHERE job_id = :job_id</code>.</li> <li>Mark job as indexing:</li> <li><code>job.indexed_page_count = 0</code>, <code>job.status = \"indexing\"</code>.</li> <li>For each WARC path:</li> <li>Iterate <code>iter_html_records(warc_path)</code>.</li> <li>Decode <code>html = rec.body_bytes.decode(\"utf-8\", errors=\"replace\")</code>.</li> <li>Use text extraction functions to get <code>title</code>, <code>text</code>, <code>snippet</code>, <code>language</code>.</li> <li>Call <code>record_to_snapshot(...)</code> to construct a <code>Snapshot</code>.</li> <li><code>session.add(snapshot)</code>; flush every 500 additions.</li> <li>Count snapshots in <code>n_snapshots</code>.</li> <li>On per\u2011record errors, log and continue.</li> <li>On success:</li> <li>Set <code>job.indexed_page_count = n_snapshots</code>.</li> <li>Set <code>job.status = \"indexed\"</code>.</li> <li>Return <code>0</code>.</li> <li>On unexpected error:</li> <li>Log at error level.</li> <li>Set <code>job.status = \"index_failed\"</code>.</li> <li>Return <code>1</code>.</li> </ol>"},{"location":"architecture/#7-viewer-helper-ha_backendindexingviewerpy","title":"7. Viewer helper (<code>ha_backend/indexing/viewer.py</code>)","text":"<p>The viewer helper is used by <code>GET /api/snapshots/raw/{id}</code> to reconstruct the HTML for a snapshot from its WARC.</p> <p>Design:</p> <ul> <li>Either:</li> <li>Use <code>warc_record_id</code> to seek directly to a known record, or</li> <li>Fallback to scanning <code>warc_path</code> for the first matching URL + timestamp.</li> </ul> <p>The API route:</p> <ul> <li>Validates that <code>Snapshot</code> and its <code>warc_path</code> exist.</li> <li>Calls <code>find_record_for_snapshot(snapshot)</code>:</li> <li>Returns an <code>ArchiveRecord</code> or <code>None</code>.</li> <li>Decodes <code>record.body_bytes</code> as UTF\u20118 with replacement.</li> <li>Writes <code>HTMLResponse(content=html, media_type=\"text/html\")</code>.</li> </ul> <p>This is used by the Next.js frontend for the embedded snapshot viewer.</p>"},{"location":"architecture/#8-http-api-ha_backendapi","title":"8. HTTP API (<code>ha_backend/api/*</code>)","text":""},{"location":"architecture/#81-public-schemas-schemaspy","title":"8.1 Public schemas (<code>schemas.py</code>)","text":"<p>Public Pydantic models:</p> <ul> <li><code>SourceSummarySchema</code> \u2013 used by <code>/api/sources</code>:</li> </ul> <pre><code>sourceCode: str\nsourceName: str\nrecordCount: int\nfirstCapture: str\nlastCapture: str\nlatestRecordId: Optional[int]\n</code></pre> <ul> <li> <p><code>SnapshotSummarySchema</code> \u2013 used by <code>/api/search</code>:</p> </li> <li> <p><code>id</code>, <code>title</code>, <code>sourceCode</code>, <code>sourceName</code>, <code>language</code>, <code>captureDate</code>,     <code>originalUrl</code>, <code>snippet</code>, <code>rawSnapshotUrl</code>.</p> </li> <li> <p><code>SearchResponseSchema</code>:</p> </li> <li> <p><code>results: List[SnapshotSummarySchema]</code>, <code>total</code>, <code>page</code>, <code>pageSize</code>.</p> </li> <li> <p><code>ArchiveStatsSchema</code> \u2013 used by <code>/api/stats</code>:</p> </li> <li> <p><code>snapshotsTotal</code>, <code>pagesTotal</code>, <code>sourcesTotal</code>, <code>latestCaptureDate</code>, <code>latestCaptureAgeDays</code>.</p> </li> <li> <p><code>SnapshotDetailSchema</code> \u2013 used by <code>/api/snapshot/{id}</code>:</p> </li> <li> <p>Contains metadata for a single snapshot including <code>mimeType</code> and     <code>statusCode</code>, plus <code>rawSnapshotUrl</code>.</p> </li> </ul>"},{"location":"architecture/#82-public-routes-routes_publicpy","title":"8.2 Public routes (<code>routes_public.py</code>)","text":"<ul> <li> <p><code>GET /api/health</code>:</p> </li> <li> <p>Returns JSON with:</p> <pre><code>{\n  \"status\": \"ok\",\n  \"checks\": {\n    \"db\": \"ok\",\n    \"jobs\": {\n      \"queued\": 1,\n      \"indexed\": 5,\n      ...\n    },\n    \"snapshots\": {\n      \"total\": 12345\n    }\n  }\n}\n</code></pre> </li> <li> <p>If the DB connectivity check fails, returns HTTP 500 with     <code>{\"status\": \"error\", \"checks\": {\"db\": \"error\"}}</code>.</p> </li> <li> <p><code>GET /api/stats</code>:</p> </li> <li> <p>Returns lightweight, cacheable archive totals used by the frontend:</p> <pre><code>{\n  \"snapshotsTotal\": 12345,\n  \"pagesTotal\": 6789,\n  \"sourcesTotal\": 2,\n  \"latestCaptureDate\": \"2025-04-19\",\n  \"latestCaptureAgeDays\": 3\n}\n</code></pre> </li> <li> <p><code>GET /api/sources</code>:</p> </li> <li> <p>Aggregates <code>Snapshot</code> by <code>source_id</code>:</p> <ul> <li>Counts, first/last capture dates, latest snapshot ID.</li> </ul> </li> <li> <p><code>GET /api/search</code>:</p> </li> <li> <p>Query params:</p> <ul> <li><code>q: str | None</code> \u2013 keyword.</li> <li><code>source: str | None</code> \u2013 source code (e.g. <code>\"hc\"</code>).</li> <li><code>sort: \"relevance\" | \"newest\" | None</code> \u2013 ordering mode.</li> <li><code>view: \"snapshots\" | \"pages\" | None</code> \u2013 results grouping mode.</li> <li><code>includeNon2xx: bool</code> \u2013 include non\u20112xx HTTP status captures (defaults to <code>false</code>).</li> <li><code>from: YYYY-MM-DD | None</code> \u2013 filter captures from this UTC date, inclusive.</li> <li><code>to: YYYY-MM-DD | None</code> \u2013 filter captures up to this UTC date, inclusive.</li> <li><code>page: int</code> \u2013 1\u2011based page index (default <code>1</code>, must be <code>&gt;= 1</code>).</li> <li><code>pageSize: int</code> \u2013 results per page (default <code>20</code>, minimum <code>1</code>, maximum <code>100</code>).</li> </ul> </li> <li>Filters:<ul> <li><code>Source.code == source.lower()</code> when <code>source</code> set.</li> <li>By default (<code>includeNon2xx=false</code>), filters out snapshots with a known non\u20112xx   <code>status_code</code> (keeps <code>status_code IS NULL</code> and <code>200\u2013299</code>).</li> <li>Keyword filter / query intent:</li> <li>URL lookup: when <code>q</code> looks like a URL (or starts with <code>url:</code>), treat it as     a page lookup and filter by the normalized URL group (with a small set of     common scheme/<code>www.</code> variants).</li> <li>Boolean/field syntax: when <code>q</code> contains <code>AND</code>/<code>OR</code>/<code>NOT</code>, parentheses, <code>-term</code>,     or <code>title:</code>/<code>snippet:</code>/<code>url:</code> prefixes, parse it and apply a boolean filter     using case-insensitive substring matching.</li> <li>Plain text:<ul> <li>On Postgres with <code>sort=\"relevance\"</code>: full\u2011text search (FTS) against   <code>snapshots.search_vector</code>.</li> <li>If FTS yields no results, fall back to tokenized substring matching.</li> <li>If that still yields no results and <code>pg_trgm</code> is available, fall back to     pg_trgm word-level trigram similarity for fuzzy matching (misspellings).</li> <li>Otherwise: tokenized substring matching on <code>title</code>, <code>snippet</code>, and <code>url</code>.</li> </ul> </li> </ul> </li> <li>Ordering:<ul> <li>Default sort:</li> <li>When <code>q</code> is present: <code>sort=\"relevance\"</code>.</li> <li>When <code>q</code> is absent: <code>sort=\"newest\"</code>.<ul> <li><code>sort=\"relevance\"</code> (when <code>q</code> present):</li> <li>On Postgres: uses FTS (<code>websearch_to_tsquery</code> + <code>ts_rank_cd</code>) against     <code>snapshots.search_vector</code>, with small heuristics (phrase-in-title boost,     URL depth/querystring penalties) and an optional authority boost from     <code>page_signals.inlink_count</code> (when available).</li> <li>On SQLite/other DBs: uses a DB\u2011agnostic match score (title &gt; URL &gt; snippet),     then (when available) a small authority tie-break from <code>page_signals</code>,     then recency.</li> </ul> </li> <li><code>sort=\"newest\"</code>: orders by recency.</li> <li>When <code>includeNon2xx=true</code>, 2xx snapshots are still prioritised ahead of 3xx,   unknown, and 4xx/5xx captures.</li> </ul> </li> <li>Grouping:<ul> <li>Default view: <code>view=\"snapshots\"</code> (returns individual captures; <code>total</code> counts snapshots).</li> <li><code>view=\"pages\"</code> returns only the latest snapshot for each page group   (<code>normalized_url_group</code>, falling back to <code>url</code> with query/fragment stripped), and   <code>total</code> counts page groups.</li> <li>When <code>view=\"pages\"</code> is used for browse (no <code>q</code> and no date range), the API can optionally   use the <code>pages</code> table as a fast path (controlled by <code>HA_PAGES_FASTPATH</code>). This is a   metadata-only optimization and does not affect replay fidelity.</li> <li>When available, <code>pageSnapshotsCount</code> is included on <code>view=\"pages\"</code> results to show the   number of captures for that page group.</li> </ul> </li> <li> <p>Pagination semantics:</p> <ul> <li><code>total</code> is the total number of matching items across all pages (snapshots   for <code>view=\"snapshots\"</code>, page groups for <code>view=\"pages\"</code>).</li> <li><code>results</code> contains at most <code>pageSize</code> snapshots for the requested <code>page</code>   (in <code>view=\"pages\"</code>, these are the latest snapshots for each page group).</li> <li>Requesting a page past the end of the result set returns <code>200 OK</code> with <code>results: []</code> and <code>total</code> unchanged.</li> <li>Supplying an invalid <code>page</code> (<code>&lt; 1</code>) or <code>pageSize</code> (<code>&lt; 1</code> or <code>&gt; 100</code>) yields <code>422 Unprocessable Entity</code> from FastAPI\u2019s validation.</li> </ul> </li> <li> <p><code>GET /api/snapshot/{id}</code>:</p> </li> <li> <p>Loads <code>Snapshot</code> + <code>Source</code>.</p> </li> <li>Returns <code>SnapshotDetailSchema</code>.</li> <li> <p>404 if snapshot or source missing.</p> </li> <li> <p><code>GET /api/snapshots/raw/{id}</code>:</p> </li> <li> <p>Validates <code>Snapshot</code> exists and <code>warc_path</code> points to an existing file.</p> </li> <li>Uses <code>find_record_for_snapshot(snapshot)</code> to get a WARC record.</li> <li>Returns an HTML page via <code>HTMLResponse</code> that includes the reconstructed archived HTML     plus a lightweight HealthArchive top bar (navigation links + disclaimer) so it can be     viewed standalone.</li> </ul>"},{"location":"architecture/#83-admin-auth-depspy","title":"8.3 Admin auth (<code>deps.py</code>)","text":"<p><code>require_admin</code> is a FastAPI dependency used to protect admin and metrics endpoints.</p> <p>Behavior:</p> <ul> <li>Reads <code>HEALTHARCHIVE_ENV</code> and <code>HEALTHARCHIVE_ADMIN_TOKEN</code> from the   environment.</li> <li>If <code>HEALTHARCHIVE_ENV</code> is <code>\"production\"</code> or <code>\"staging\"</code> and   <code>HEALTHARCHIVE_ADMIN_TOKEN</code> is unset:</li> <li>Admin and metrics endpoints fail closed with HTTP 500 and a clear     error detail (<code>\"Admin token not configured for this environment\"</code>).</li> <li>In other environments (or when <code>HEALTHARCHIVE_ENV</code> is unset) and the admin   token is unset:</li> <li>Admin endpoints are open (dev mode convenience).</li> <li>When <code>HEALTHARCHIVE_ADMIN_TOKEN</code> is set:</li> <li>Requires the same token via either:<ul> <li><code>Authorization: Bearer &lt;token&gt;</code> header, or</li> <li><code>X-Admin-Token: &lt;token&gt;</code> header.</li> </ul> </li> <li>On mismatch/missing token \u2192 <code>HTTP 403</code>.</li> </ul>"},{"location":"architecture/#84-admin-schemas-schemas_adminpy","title":"8.4 Admin schemas (<code>schemas_admin.py</code>)","text":"<p>Key models:</p> <ul> <li> <p><code>JobSummarySchema</code> \u2013 used for lists:</p> </li> <li> <p>Contains the key job fields plus:</p> <pre><code>cleanupStatus: str\ncleanedAt: Optional[datetime]\n</code></pre> </li> <li> <p><code>JobDetailSchema</code> \u2013 extended view for a single job:</p> </li> <li> <p>Includes status, worker counters, pages, WARC counts, ZIM/log/state paths,     <code>config</code> (JSON), and <code>lastStats</code> (JSON, reserved).</p> </li> <li> <p>Also includes <code>cleanupStatus</code> and <code>cleanedAt</code>.</p> </li> <li> <p><code>JobSnapshotSummarySchema</code> \u2013 minimal <code>Snapshot</code> view in a job context.</p> </li> <li> <p><code>JobListResponseSchema</code> \u2013 wrapper for job list results.</p> </li> <li> <p><code>JobStatusCountsSchema</code> \u2013 dictionary of <code>{status: count}</code>.</p> </li> </ul>"},{"location":"architecture/#85-admin-routes-routes_adminpy","title":"8.5 Admin routes (<code>routes_admin.py</code>)","text":"<p>All routes are under <code>/api/admin</code> and use <code>require_admin</code> for auth. They are intended for internal operator tooling (CLI or a future admin console), not for the public web UI.</p> <ul> <li><code>GET /api/admin/jobs</code> \u2192 <code>JobListResponseSchema</code>:</li> <li>Filters:<ul> <li><code>source: str | None</code> \u2013 by source code.</li> <li><code>status: str | None</code> \u2013 by job status.</li> <li><code>limit</code> (1\u2013500, default 50), <code>offset</code> (\u22650).</li> </ul> </li> <li> <p>Joins <code>ArchiveJob</code> with <code>Source</code> (outer join).</p> </li> <li> <p><code>GET /api/admin/jobs/{job_id}</code> \u2192 <code>JobDetailSchema</code>:</p> </li> <li>Joins <code>ArchiveJob</code> with <code>Source</code>.</li> <li> <p>404 if job not found.</p> </li> <li> <p><code>GET /api/admin/jobs/status-counts</code> \u2192 <code>JobStatusCountsSchema</code>:</p> </li> <li> <p>SQL: <code>SELECT status, COUNT(*) FROM archive_jobs GROUP BY status</code>.</p> </li> <li> <p><code>GET /api/admin/jobs/{job_id}/snapshots</code> \u2192 <code>List[JobSnapshotSummarySchema]</code>:</p> </li> <li>Lists snapshots for a given job with pagination (<code>limit</code>, <code>offset</code>).</li> </ul>"},{"location":"architecture/#86-metrics-prometheusstyle","title":"8.6 Metrics (Prometheus\u2011style)","text":"<p>Defined directly in <code>ha_backend.api.__init__</code>:</p> <ul> <li><code>GET /metrics</code>:</li> <li>Protected by <code>require_admin</code> (same token behavior) and intended for     scrape\u2011only use by monitoring systems (e.g., Prometheus) and internal     tooling.</li> <li>Computes:<ul> <li><code>healtharchive_jobs_total{status=\"...\"}</code></li> <li><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"...\"}</code></li> <li><code>healtharchive_snapshots_total</code></li> <li><code>healtharchive_snapshots_total{source=\"hc\"}</code>, etc.</li> </ul> </li> </ul>"},{"location":"architecture/#87-cors","title":"8.7 CORS","text":"<ul> <li>CORS is enabled on the public API routes. Allowed origins are derived from   <code>HEALTHARCHIVE_CORS_ORIGINS</code> (comma-separated). Defaults cover local dev and   production (<code>http://localhost:3000</code>, <code>http://localhost:5173</code>,   <code>https://healtharchive.ca</code>, <code>https://www.healtharchive.ca</code>).</li> <li>Admin and metrics routes remain token-gated even when CORS allows browser   access to public routes.</li> </ul> <p>Typical environment setups:</p> <ul> <li>Local development:</li> </ul> <pre><code># often no override needed; defaults already include localhost:3000/5173\nexport HEALTHARCHIVE_DATABASE_URL=sqlite:///$(pwd)/.dev-healtharchive.db\nexport HEALTHARCHIVE_ARCHIVE_ROOT=$(pwd)/.dev-archive-root\n# Optional CORS override if your frontend runs on a different origin:\n# export HEALTHARCHIVE_CORS_ORIGINS=http://localhost:3000\n</code></pre> <ul> <li>Staging (example):</li> </ul> <pre><code># frontend served from https://healtharchive.vercel.app\nexport HEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.vercel.app\n</code></pre> <ul> <li>Production (example):</li> </ul> <pre><code># frontend served from https://healtharchive.ca and https://www.healtharchive.ca\nexport HEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.ca,https://www.healtharchive.ca\n</code></pre> <p>In all cases, CORS affects only the browser\u2019s ability to call public routes; admin and metrics endpoints still require the admin token when configured.</p>"},{"location":"architecture/#9-worker-loop-ha_backendworkermainpy","title":"9. Worker loop (<code>ha_backend/worker/main.py</code>)","text":"<p>The worker processes jobs end\u2011to\u2011end: crawl and index.</p>"},{"location":"architecture/#91-selection","title":"9.1 Selection","text":"<p><code>_select_next_crawl_job(session)</code>:</p> <ul> <li>Query:</li> </ul> <pre><code>session.query(ArchiveJob) \\\n  .join(Source) \\\n  .filter(ArchiveJob.status.in_([\"queued\", \"retryable\"])) \\\n  .order_by(ArchiveJob.queued_at.asc().nullsfirst(),\n            ArchiveJob.created_at.asc()) \\\n  .first()\n</code></pre> <ul> <li>Chooses the oldest queued/retryable job, preferring jobs with the earliest   <code>queued_at</code>.</li> </ul>"},{"location":"architecture/#92-processing-a-single-job","title":"9.2 Processing a single job","text":"<p><code>_process_single_job()</code>:</p> <ol> <li>Select a job \u2192 get <code>job_id</code>.</li> <li>Run <code>run_persistent_job(job_id)</code>:</li> <li>Executes <code>archive_tool</code> and returns a process exit code.</li> <li>Reload job in a new session and apply retry semantics:</li> <li>If <code>crawl_rc != 0</code> or <code>job.status == \"failed\"</code>:<ul> <li>If <code>job.retry_count &lt; MAX_CRAWL_RETRIES</code>:</li> <li>Increment <code>job.retry_count</code>.</li> <li>Set <code>job.status = \"retryable\"</code>.</li> <li>Else:</li> <li>Log error; job remains in <code>failed</code>.</li> </ul> </li> <li>Else (crawl succeeded):<ul> <li>Log that indexing will start.</li> </ul> </li> <li>If crawl succeeded:</li> <li>Run <code>index_job(job_id)</code>.</li> <li>Log success/failure for indexing.</li> </ol> <p>Returns <code>True</code> if a job was processed, <code>False</code> if no jobs were found.</p>"},{"location":"architecture/#93-main-loop","title":"9.3 Main loop","text":"<p><code>run_worker_loop(poll_interval=30, run_once=False)</code>:</p> <ul> <li>Logs startup with the given interval and <code>run_once</code>.</li> <li>In a loop:</li> <li>Calls <code>_process_single_job()</code>.</li> <li>If <code>run_once</code> \u2192 break after first iteration.</li> <li>If no job processed:<ul> <li>Logs and sleeps for <code>poll_interval</code> seconds.</li> </ul> </li> <li>Handles <code>KeyboardInterrupt</code> gracefully.</li> </ul>"},{"location":"architecture/#10-cleanup-retention-future","title":"10. Cleanup &amp; retention (future)","text":"<p>Job\u2011level cleanup is focused on removing temporary crawl artifacts (<code>.tmp*</code> dirs and <code>.archive_state.json</code>) after indexing is complete.</p>"},{"location":"architecture/#101-cleanup-flags-on-archivejob","title":"10.1 Cleanup flags on ArchiveJob","text":"<p>New fields:</p> <ul> <li><code>cleanup_status: str</code>:</li> <li><code>\"none\"</code> \u2013 no cleanup performed (default).</li> <li><code>\"temp_cleaned\"</code> \u2013 temporary dirs and state file have been deleted.</li> <li>Future values could represent more aggressive cleanup modes.</li> <li><code>cleaned_at: datetime | None</code> \u2013 when cleanup occurred.</li> </ul> <p>These fields are exposed through:</p> <ul> <li>Admin schemas (<code>JobSummarySchema</code>, <code>JobDetailSchema</code>).</li> <li>Metrics (<code>healtharchive_jobs_cleanup_status_total</code>).</li> </ul>"},{"location":"architecture/#102-cli-command-cleanup-job","title":"10.2 CLI command: cleanup-job","text":"<p><code>ha-backend cleanup-job --id JOB_ID [--mode temp] [--force]</code></p> <p>Implementation notes:</p> <ul> <li>Currently supports only <code>--mode temp</code>:</li> <li> <p>Any other mode \u2192 error.</p> </li> <li> <p>Behavior:</p> </li> <li> <p>Load the <code>ArchiveJob</code> by ID.</p> </li> <li>If job is missing \u2192 error, exit 1.</li> <li>If replay is enabled globally (<code>HEALTHARCHIVE_REPLAY_BASE_URL</code> is set) and      <code>--force</code> is not provided:<ul> <li>Refuse cleanup and exit 1.</li> <li>Rationale: <code>--mode temp</code> can delete WARCs required for replay.</li> </ul> </li> <li>If <code>job.status</code> is not one of:<ul> <li><code>\"indexed\"</code> \u2013 indexing completed successfully, or</li> <li><code>\"index_failed\"</code> \u2013 indexing failed and you have decided not to retry,  then refuse cleanup and exit 1.</li> <li>This ensures we don\u2019t delete temp dirs while a job might still be    resumed or indexing is in progress.</li> </ul> </li> <li>Validate <code>output_dir</code> exists and is a directory.</li> <li>Use <code>archive_tool.state.CrawlState(output_dir, initial_workers=1)</code> to      instantiate state and locate the state file.</li> <li>Use <code>state.get_temp_dir_paths()</code> to get known temp dirs; fall back to      <code>find_latest_temp_dir_fallback</code> if none are tracked.</li> <li>If neither temp dirs nor the state file exist:<ul> <li>Print a message that there is nothing to clean up and do not change    <code>cleanup_status</code> or <code>cleaned_at</code>.</li> </ul> </li> <li>Otherwise (if temp dirs and/or state file exist):<ul> <li>Call <code>cleanup_temp_dirs(temp_dirs, state.state_file_path)</code>:</li> <li>Deletes <code>.tmp*</code> directories and the <code>.archive_state.json</code>.</li> <li>Update job:<ul> <li><code>cleanup_status = \"temp_cleaned\"</code></li> <li><code>cleaned_at = now</code></li> <li><code>state_file_path = None</code></li> </ul> </li> </ul> </li> </ul> <p>Operational warning:</p> <ul> <li><code>cleanup-job --mode temp</code> will delete WARCs if they live under the job\u2019s   <code>.tmp*</code> directory (common for legacy imports and some crawl layouts).   If you intend to serve the job via replay (pywb), do not run cleanup for that   job \u2014 replay depends on WARCs remaining on disk.   If replay is enabled globally, you must pass <code>--force</code> to run cleanup; treat   this as an emergency override.</li> </ul> <p>Caution: This cleanup removes WARCs stored under <code>.tmp*</code> directories, consistent with <code>archive_tool</code>\u2019s own <code>--cleanup</code> behavior. In v1 you should only run it once you have: - Indexed the job successfully (<code>status=\"indexed\"</code>), and - Verified any desired ZIM or exports derived from these WARCs.</p>"},{"location":"architecture/#103-metrics-for-cleanup","title":"10.3 Metrics for cleanup","text":"<p><code>/metrics</code> includes:</p> <ul> <li><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"none\"}</code></li> <li><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"temp_cleaned\"}</code></li> </ul> <p>This gives a quick overview of how many jobs still have temp artifacts versus those that have been cleaned.</p>"},{"location":"architecture/#11-cli-commands-summary","title":"11. CLI commands summary","text":"<p>All commands are available via the <code>ha-backend</code> entrypoint.</p> <ul> <li>Environment / connectivity:</li> <li><code>check-env</code> \u2013 show archive root and ensure it exists.</li> <li><code>check-archive-tool</code> \u2013 run <code>archive-tool --help</code>.</li> <li> <p><code>check-db</code> \u2013 simple DB connectivity check.</p> </li> <li> <p>Direct, non\u2011persistent job:</p> </li> <li> <p><code>run-job</code> \u2013 run <code>archive_tool</code> immediately with explicit <code>--name</code>, <code>--seeds</code>,     <code>--initial-workers</code>, etc.</p> </li> <li> <p>Persistent jobs (DB\u2011backed):</p> </li> <li><code>create-job --source CODE</code> \u2013 create <code>ArchiveJob</code> using registry defaults.</li> <li><code>run-db-job --id ID</code> \u2013 run <code>archive_tool</code> for an existing job.</li> <li><code>index-job --id ID</code> \u2013 index an existing job\u2019s WARCs into snapshots.</li> <li><code>register-job-dir --source CODE --output-dir PATH [--name NAME]</code> \u2013     attach a DB <code>ArchiveJob</code> to an existing archive_tool output directory     (useful when a crawl has already been run and you want to index its     WARCs).</li> <li> <p>Job configs default to <code>relax_perms=True</code> for dev (adds <code>--relax-perms</code> so     temp WARCs are chmod\u2019d readable on the host after a crawl).</p> </li> <li> <p>Seeding:</p> </li> <li> <p><code>seed-sources</code> \u2013 insert baseline <code>Source</code> rows for <code>hc</code>, <code>phac</code>.</p> </li> <li> <p>Admin / introspection:</p> </li> <li><code>list-jobs</code> \u2013 list recent jobs with basic fields.</li> <li><code>show-job --id ID</code> \u2013 detailed job info including config.</li> <li><code>retry-job --id ID</code> \u2013 mark:<ul> <li><code>failed</code> jobs as <code>retryable</code> (for another crawl).</li> <li><code>index_failed</code> jobs as <code>completed</code> (for re-indexing).</li> </ul> </li> <li><code>cleanup-job --id ID [--mode temp] [--force]</code> \u2013 cleanup temp dirs/state for jobs in     status <code>indexed</code> or <code>index_failed</code>.</li> <li><code>replay-index-job --id ID</code> \u2013 create/refresh the pywb collection + CDX index     for a job (so snapshots can be browsed via replay).</li> <li><code>start-worker [--poll-interval N] [--once]</code> \u2013 start the worker loop.</li> </ul>"},{"location":"architecture/#12-testing-development","title":"12. Testing &amp; development","text":"<ul> <li>Tests are written with <code>pytest</code> and live under <code>tests/</code>.</li> <li>To run checks:</li> </ul> <pre><code>make venv\nmake check\n</code></pre> <ul> <li>Many tests configure a temporary SQLite DB by:</li> <li>Setting <code>HEALTHARCHIVE_DATABASE_URL</code> to a temp file.</li> <li>Resetting <code>db_module._engine</code> and <code>_SessionLocal</code>.</li> <li>Calling <code>Base.metadata.drop_all()</code> / <code>create_all()</code> to fully reset the schema.</li> </ul> <p>This allows development and CI to run in isolated environments without touching real data.</p>"},{"location":"architecture/#13-relationship-to-archive_tool-and-the-frontend","title":"13. Relationship to archive_tool and the frontend","text":"<ul> <li>archive_tool:</li> <li>Lives under <code>src/archive_tool/</code> and is maintained as part of this repo.     It originated as an earlier standalone crawler project but is now the     in-tree crawler/orchestrator subpackage for the backend.</li> <li>The backend calls it strictly via the CLI (<code>archive-tool</code>) as a subprocess.</li> <li> <p>Its internal behavior (Docker orchestration, run modes, monitoring,     adaptive strategies) is documented in <code>src/archive_tool/docs/documentation.md</code>.</p> </li> <li> <p>Frontend (healtharchive-frontend):</p> </li> <li>Next.js 16 app using the backend\u2019s HTTP APIs:<ul> <li><code>/api/health</code></li> <li><code>/api/sources</code></li> <li><code>/api/search</code></li> <li><code>/api/snapshot/{id}</code></li> <li><code>/api/snapshots/raw/{id}</code></li> </ul> </li> <li>The frontend currently still supports a demo dataset, but is gradually     being wired to these real APIs.</li> </ul> <p>Together, the backend + <code>archive_tool</code> + frontend form a pipeline from:</p> <p>Web \u2192 crawl (Docker + <code>zimit</code>) \u2192 WARCs \u2192 Snapshots in DB \u2192 searchable archive UI at HealthArchive.ca.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>The canonical contribution guide for this repo lives at the repository root:</p> <ul> <li>CONTRIBUTING.md</li> </ul> <p>Related docs in this site:</p> <ul> <li>Quickstart</li> <li>Documentation Guidelines</li> </ul>"},{"location":"documentation-guidelines/","title":"Documentation Guidelines (internal)","text":"<p>Keep documentation accurate, minimal, and easy to maintain across repos.</p>"},{"location":"documentation-guidelines/#canonical-sources","title":"Canonical sources","text":"<ul> <li>Docs portal (published): https://docs.healtharchive.ca</li> <li>Docs portal (local): Run <code>make docs-serve</code> in the backend repo root.</li> <li>Navigation config: <code>mkdocs.yml</code> (source of truth for sidebar structure).</li> <li>Cross-repo environment wiring: <code>docs/deployment/environments-and-configuration.md</code></li> <li>Ops roadmap/todo: <code>docs/operations/healtharchive-ops-roadmap.md</code></li> <li>Future roadmap backlog (not-yet-implemented work): <code>docs/planning/roadmap.md</code></li> <li>Implemented plans archive (historical records): <code>docs/planning/implemented/</code></li> <li>Frontend documentation (canonical): https://github.com/jerdaw/healtharchive-frontend/tree/main/docs</li> <li>Datasets documentation (canonical): https://github.com/jerdaw/healtharchive-datasets</li> </ul>"},{"location":"documentation-guidelines/#multi-repo-boundary-avoid-bleed","title":"Multi-repo boundary (avoid bleed)","text":"<p>This documentation site is built from the backend repo only.</p> <ul> <li>Frontend docs are canonical in the frontend repo (<code>docs/**</code>) and should be linked-to, not copied into this site.</li> <li>Datasets docs are canonical in the datasets repo and should be linked-to, not copied into this site.</li> <li>Frontend PRs should not break backend docs builds (and vice versa).</li> </ul>"},{"location":"documentation-guidelines/#cross-repo-linking-avoid-drift","title":"Cross-repo linking (avoid drift)","text":"<p>When referencing another repo from docs in this repo:</p> <ul> <li> <p>For documentation references: Use GitHub URLs   <pre><code># Good\nSee the [frontend i18n guide](https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/i18n.md)\n\n# Avoid\nSee `healtharchive-frontend/docs/i18n.md`\n</code></pre></p> </li> <li> <p>For command examples: Workspace-relative paths are fine   <pre><code># This is appropriate in a development guide\ncd ../healtharchive-frontend &amp;&amp; npm ci\n</code></pre></p> </li> <li> <p>For project names in prose: Use simple names   <pre><code>The healtharchive-frontend repository handles the public UI.\n</code></pre></p> </li> <li> <p>Treat cross-repo references as pointers. Do not copy text across repos unless it is an intentional public-safe excerpt.</p> </li> <li>Links to backend docs can use relative paths within this repo or <code>docs.healtharchive.ca</code> URLs.</li> </ul>"},{"location":"documentation-guidelines/#external-pointer-pages","title":"External pointer pages","text":"<p>If you want another repo\u2019s docs to be discoverable from the docs portal, add a small pointer page under <code>docs/*-external/</code> and add it to <code>mkdocs.yml</code> <code>nav</code>. Do not mirror the other repo\u2019s docs into this site.</p>"},{"location":"documentation-guidelines/#navigation-policy","title":"Navigation policy","text":""},{"location":"documentation-guidelines/#what-goes-in-mkdocsyml-nav","title":"What goes in mkdocs.yml nav","text":"<ul> <li>All README index pages</li> <li>Docs that are frequently accessed or critical for operations</li> <li>At least one representative doc from each major category</li> <li>Core playbooks (operator responsibilities, deploy &amp; verify, incident response)</li> </ul>"},{"location":"documentation-guidelines/#what-stays-readme-only","title":"What stays README-only","text":"<ul> <li>Detailed playbooks beyond the core set (discoverable via playbooks/README.md)</li> <li>Historical/archived roadmaps (implemented/)</li> <li>Log files and templates</li> <li>Highly specialized procedures</li> </ul>"},{"location":"documentation-guidelines/#organizing-new-docs","title":"Organizing new docs","text":"<p>When adding new docs:</p> <ol> <li>Add to the appropriate directory</li> <li>Update the directory's <code>README.md</code> index</li> <li>If critical or frequently accessed, add to <code>mkdocs.yml</code> nav</li> <li>Ensure cross-links from related docs</li> </ol>"},{"location":"documentation-guidelines/#using-templates","title":"Using templates","text":"<p>Templates are stored in <code>docs/_templates/</code>. To use:</p> <ol> <li>Copy the template to the appropriate directory</li> <li>Rename with appropriate filename (remove <code>-template</code> suffix)</li> <li>Fill in all sections</li> <li>Add to directory README index</li> <li>Add to <code>mkdocs.yml</code> nav if appropriate</li> </ol> <p>Available templates:</p> <ul> <li><code>_templates/runbook-template.md</code> \u2014 For deployment procedures</li> <li><code>_templates/playbook-template.md</code> \u2014 For operational tasks</li> <li><code>_templates/incident-template.md</code> \u2014 For incident postmortems</li> <li><code>_templates/decision-template.md</code> \u2014 For architectural decisions</li> <li><code>_templates/restore-test-log-template.md</code> \u2014 For quarterly restore test logs</li> <li><code>_templates/adoption-signals-log-template.md</code> \u2014 For quarterly adoption signals</li> <li><code>_templates/mentions-log-template.md</code> \u2014 For mentions log entries</li> <li><code>_templates/ops-ui-friction-log-template.md</code> \u2014 For internal friction logging</li> </ul>"},{"location":"documentation-guidelines/#when-adding-or-changing-docs","title":"When adding or changing docs","text":"<ul> <li>Prefer one canonical source. Use pointers elsewhere instead of copying text.</li> <li>Keep docs close to the code they describe.</li> <li>Registry: New critical docs should be added to the <code>nav</code> section of <code>mkdocs.yml</code>. All docs should be added to their directory's <code>README.md</code> index.</li> <li>Use MkDocs Material features like Admonitions (<code>!!! note</code>), Tabs, and Mermaid diagrams.</li> <li>Documentation should be English-only; do not duplicate it in other languages.</li> <li>Avoid \"phase\" labels or other implementation-ordering labels outside <code>docs/planning/roadmap.md</code> and <code>docs/planning/implemented/</code>. The order that something was implemented in is not something that needs documentation; rather documentation should focus on key elements of what was implemented, how it was implemented, and how it is to be used.</li> <li>Keep public copy public-safe (no secrets, private emails, or internal IPs).</li> <li>If you sync your workspace via Syncthing, treat <code>.stignore</code> as \"sync ignore\" (like <code>.gitignore</code>) and ensure it excludes build artifacts and machine-local dev artifacts (e.g., <code>.venv/</code>, <code>node_modules/</code>, <code>.dev-archive-root/</code>). Secrets may sync via Syncthing, but must remain git-ignored.</li> </ul>"},{"location":"documentation-guidelines/#documentation-framework-diataxis","title":"Documentation framework (Di\u00e1taxis)","text":"<p>HealthArchive documentation follows the Di\u00e1taxis framework for clarity and user-centered organization. Di\u00e1taxis divides documentation into four types based on user needs:</p>"},{"location":"documentation-guidelines/#four-documentation-types","title":"Four Documentation Types","text":"Type Purpose User Action Examples Tutorials Learning-oriented Following steps to gain skills First contribution guide, architecture walkthrough How-To Guides Task-oriented Solving specific problems Playbooks, runbooks, checklists Reference Information-oriented Looking up details API docs, CLI reference, data model Explanation Understanding-oriented Understanding concepts Architecture, decisions, guidelines <p>Key principle: Keep these types separate. Don't mix tutorials with reference material, or how-to guides with explanations.</p> <p>Learn more: diataxis.fr</p>"},{"location":"documentation-guidelines/#mapping-to-our-taxonomy","title":"Mapping to Our Taxonomy","text":"<p>Our existing document types map to Di\u00e1taxis categories:</p> <p>Tutorials (Learning): - Lives under <code>docs/tutorials/</code> - Examples: <code>tutorials/first-contribution.md</code>, <code>tutorials/architecture-walkthrough.md</code>, <code>tutorials/debug-crawl.md</code> - Characteristics: Step-by-step, hands-on, designed for learning</p> <p>How-To Guides (Tasks): - Runbooks: Deployment procedures in <code>docs/deployment/</code> (template: <code>_templates/runbook-template.md</code>) - Playbooks: Operational tasks in <code>docs/operations/playbooks/</code> or <code>docs/development/playbooks/</code> (template: <code>_templates/playbook-template.md</code>) - Checklists: Minimal verification lists - Characteristics: Goal-oriented, assume some knowledge, focused on results</p> <p>Reference (Information): - Lives under <code>docs/reference/</code> or specialized files (<code>api.md</code>, etc.) - Examples: <code>reference/data-model.md</code>, <code>reference/cli-commands.md</code>, <code>reference/archive-tool.md</code> - Also: API documentation (<code>api.md</code>), Architecture sections - Characteristics: Factual, precise, structured for lookup</p> <p>Explanation (Understanding): - Decision records: In <code>docs/decisions/</code> (template: <code>_templates/decision-template.md</code>) - Policies/contracts: Invariants and boundaries - Guidelines: This file, <code>documentation-process-audit.md</code> - Architecture: <code>architecture.md</code> (blends reference and explanation) - Characteristics: Background, context, \"why\" not \"how\"</p>"},{"location":"documentation-guidelines/#additional-document-types","title":"Additional Document Types","text":"<p>These support but don't replace the four main types:</p> <ul> <li>Index (<code>README.md</code>): Navigation only; points to canonical docs</li> <li>Log/record: Dated, append-only operational evidence (restore tests, adoption signals)</li> <li>Template: Scaffolds in <code>docs/_templates/</code></li> <li>Pointer: Short files linking to canonical docs (e.g., <code>frontend-external/</code>)</li> </ul>"},{"location":"documentation-guidelines/#document-types-detailed-taxonomy","title":"Document types (detailed taxonomy)","text":"<p>Use consistent doc types so people know what to expect:</p>"},{"location":"documentation-guidelines/#quality-bar-definition-of-done","title":"Quality bar (definition of done)","text":"<p>For anything procedural (runbook/playbook/checklist), include:</p> <ul> <li>Purpose: why this doc exists and what it covers.</li> <li>Audience + access: who should run it, and from where (local vs VPS; <code>haadmin</code> vs <code>root</code>).</li> <li>Preconditions: required state and inputs (paths, env vars, service names).</li> <li>Steps: explicit commands (prefer stable scripts), ordered, with \u201cwhat this changes\u201d.</li> <li>Verification: what \u201cdone\u201d means (health checks, drift check, smoke tests).</li> <li>Safety: common footguns, irreversible actions, and rollback/recovery notes.</li> <li>References: links to canonical docs, incident notes, or roadmaps.</li> </ul> <p>For anything public-facing (policy pages, changelog, partner kit):</p> <ul> <li>Keep it public-safe (no secrets/emails/internal hostnames; avoid sensitive incident details).</li> <li>Prefer stable claims tied to stable artifacts (URLs, tags, filenames, commit SHAs).</li> <li>Record meaningful changes in the public changelog:</li> <li>Process: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md</li> </ul>"},{"location":"documentation-guidelines/#lifecycle-avoid-drift","title":"Lifecycle (avoid drift)","text":"<p>Docs should reflect current reality. If something is intentionally outdated:</p> <ul> <li>Put a short note at the top: what changed, and where the new canonical doc lives.</li> <li>Prefer updating the doc over adding a second \u201cnew doc\u201d (avoid forks).</li> <li>For long historical artifacts, move them under <code>docs/planning/implemented/</code> (dated).</li> </ul> <p>Suggested cadence (keep it lightweight):</p> <ul> <li>After any production change: update the relevant runbook/playbook and keep deploy/verify steps accurate.</li> <li>After sev0/sev1 incidents: ensure recovery steps are captured and follow-ups exist (roadmap or TODOs).</li> <li>Quarterly: skim the production runbook + incident response playbook and fix any drift discovered during routine ops.</li> </ul>"},{"location":"documentation-guidelines/#roadmap-workflow","title":"Roadmap workflow","text":"<p>This project separates backlog vs implementation plans vs canonical docs to reduce drift.</p> <ul> <li>Short pointer (for new contributors): <code>roadmap-process.md</code></li> <li><code>docs/planning/roadmap.md</code> is the single backlog of not-yet-implemented items.</li> <li>When you start work, create a focused implementation plan under <code>docs/planning/</code>.</li> <li>When the work is done, update canonical docs (deployment/ops/dev) so the result is maintainable.</li> <li>Then move the implementation plan into <code>docs/planning/implemented/</code> with a dated filename.</li> </ul> <p>Rule of thumb: documentation should describe what exists and how to use/operate it, not the order it was implemented.</p>"},{"location":"documentation-guidelines/#implementation-plan-lifecycle","title":"Implementation plan lifecycle","text":"<p>Implementation plans follow a specific lifecycle to prevent documentation sprawl:</p>"},{"location":"documentation-guidelines/#1-active-planning","title":"1. Active planning","text":"<p>Create in <code>docs/planning/</code> with dated filename: <code>YYYY-MM-DD-&lt;short-slug&gt;.md</code></p>"},{"location":"documentation-guidelines/#2-during-implementation","title":"2. During implementation","text":"<p>Update with progress notes as needed. Keep the focus on outcomes and decisions.</p>"},{"location":"documentation-guidelines/#3-after-completion","title":"3. After completion","text":"<ul> <li>Update all canonical docs (deployment/operations/development) with outcomes</li> <li>Compress the plan to summary format (see below)</li> <li>Move to <code>docs/planning/implemented/</code></li> </ul>"},{"location":"documentation-guidelines/#4-compressed-format-for-implemented-plans","title":"4. Compressed format for implemented plans","text":"<p>Completed plans &gt;200 lines should be compressed to this format (~40-80 lines):</p> <pre><code># [Title] (Implemented YYYY-MM-DD)\n\n**Status:** Implemented | **Scope:** [1-2 sentences describing what was done]\n\n## Outcomes\n- [Bullet list of what was delivered]\n\n## Canonical Docs Updated\n- [Links to docs that now contain this information]\n\n## Decisions Created (if any)\n- [Links to decision records]\n\n## Historical Context\n[Brief note that detailed implementation history is preserved in git]\n</code></pre> <p>Why compress? Detailed phase-by-phase narratives become historical journals that duplicate content now in canonical docs. Compression: - Reduces maintenance burden - Prevents documentation sprawl - Preserves outcomes while keeping the archive scannable</p> <p>Detailed history: Git preserves the full implementation narrative for anyone who needs it.</p>"},{"location":"documentation-guidelines/#naming-and-organization","title":"Naming and organization","text":"<ul> <li>Use descriptive filenames (<code>runbook</code>, <code>checklist</code>, <code>guidelines</code>) and avoid phase prefixes.</li> <li>File titles and filenames should reflect the document\u2019s actual purpose and content. If the purpose or content changes, rename the file and update links as needed.</li> <li>Put roadmaps and active implementation plans in <code>docs/planning</code>.</li> <li>Move completed implementation plans into <code>docs/planning/implemented/</code> (dated).</li> <li>Put operational procedures in <code>docs/operations</code>.</li> <li>Put incident notes / lightweight postmortems in <code>docs/operations/incidents/</code> (template: <code>docs/_templates/incident-template.md</code>).</li> <li>Put ops playbooks (task-oriented checklists) in <code>docs/operations/playbooks/</code>.</li> <li>Put deployment/runbooks in <code>docs/deployment</code>.</li> <li>Put developer workflows (local setup, testing, debugging) in <code>docs/development</code>.</li> <li>Put dev playbooks (task workflows) in <code>docs/development/playbooks/</code>.</li> </ul>"},{"location":"documentation-process-audit/","title":"Documentation process audit (2026-01-09)","text":"<p>Scope: HealthArchive project documentation processes and subprocesses across:</p> <ul> <li><code>healtharchive-backend</code> (ops, runbooks, incident notes, canonical internal docs)</li> <li><code>healtharchive-frontend</code> (public policy/reporting surfaces, UX copy, changelog)</li> <li><code>healtharchive-datasets</code> (dataset release documentation and integrity posture)</li> <li>The local \u201cworkspace of sibling repos\u201d convention used in <code>/home/jer/LocalSync/healtharchive/</code></li> </ul> <p>Goal: assess whether the documentation system is well-designed, maintainable, and aligned with modern best practices (docs-as-code + operational excellence), and identify concrete upgrades.</p>"},{"location":"documentation-process-audit/#executive-summary","title":"Executive summary","text":"<p>Overall: the project\u2019s documentation system is already unusually strong for its size. It is structured around a clear \u201cdocs-as-code\u201d posture, high-signal operational procedures, and drift-resistant separation of backlog vs implementation plans vs canonical docs.</p> <p>Key strengths (high confidence):</p> <ul> <li>Drift prevention by design: single canonical sources + pointer docs, and explicit backlog/plan/canonical separation (<code>docs/planning/**</code> vs <code>docs/**</code>).</li> <li>Operations maturity: production runbook, playbooks, cadence checklists, monitoring/CI setup guidance, and safety posture are explicit and actionable.</li> <li>Incident management: severity rubric + incident template + operator response playbook + at least one real incident note showing good practice.</li> <li>Public vs private boundaries: explicit contracts for privacy-preserving usage metrics, issue-report retention, and non-public admin/metrics access.</li> <li>Reproducibility: dataset release integrity rules (checksums + manifest invariants) documented and operationalized.</li> </ul> <p>Primary remaining gaps (fixable, low risk):</p> <ul> <li>Templates / consistency: you had strong examples, but no standard templates for new runbooks/playbooks, and changelog updates were not documented as an SOP.</li> <li>Lifecycle + review cadence: docs avoided duplication well, but \u201chow we keep docs correct over time\u201d could be more explicit (lightweight review + deprecation pattern).</li> <li>Public communication integration: incident notes were solid, but the \u201cwhen do we update <code>/changelog</code> and/or <code>/status</code>?\u201d expectation wasn\u2019t explicit enough.</li> </ul>"},{"location":"documentation-process-audit/#inventory-of-documentation-process-surfaces","title":"Inventory of documentation \u201cprocess surfaces\u201d","text":"<p>This is the set of documents that define how documentation is produced, maintained, and used (not every domain-specific doc).</p>"},{"location":"documentation-process-audit/#governance-doc-architecture","title":"Governance / doc architecture","text":"<ul> <li>Canonical documentation policy and source-of-truth rules:</li> <li><code>healtharchive-backend/docs/documentation-guidelines.md</code></li> <li>Index structure (discoverability):</li> <li><code>healtharchive-backend/docs/README.md</code></li> <li><code>healtharchive-backend/docs/operations/README.md</code></li> <li><code>healtharchive-backend/docs/operations/playbooks/README.md</code></li> <li><code>healtharchive-backend/docs/planning/README.md</code></li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/README.md</li> </ul>"},{"location":"documentation-process-audit/#planning-change-management","title":"Planning / change management","text":"<ul> <li>Backlog and implementation plan workflow:</li> <li><code>healtharchive-backend/docs/roadmap-process.md</code></li> <li><code>healtharchive-backend/docs/planning/roadmap.md</code></li> <li><code>healtharchive-backend/docs/planning/implemented/</code></li> </ul>"},{"location":"documentation-process-audit/#incidents-and-post-incident-learning","title":"Incidents and post-incident learning","text":"<ul> <li>Incident SOP and artifacts:</li> <li><code>healtharchive-backend/docs/operations/incidents/README.md</code></li> <li><code>healtharchive-backend/docs/operations/incidents/incident-template.md</code></li> <li><code>healtharchive-backend/docs/operations/incidents/severity.md</code></li> <li><code>healtharchive-backend/docs/operations/playbooks/core/incident-response.md</code></li> </ul>"},{"location":"documentation-process-audit/#operations-and-reliability-subprocesses-repeatable-routines","title":"Operations and reliability subprocesses (repeatable routines)","text":"<ul> <li>Cadence and routines:</li> <li><code>healtharchive-backend/docs/operations/ops-cadence-checklist.md</code></li> <li>Monitoring/CI and deploy gating:</li> <li><code>healtharchive-backend/docs/operations/monitoring-and-ci-checklist.md</code></li> <li><code>healtharchive-backend/docs/operations/playbooks/core/deploy-and-verify.md</code></li> <li><code>healtharchive-backend/docs/operations/baseline-drift.md</code></li> <li>Backup/restore validation:</li> <li><code>healtharchive-backend/docs/operations/restore-test-procedure.md</code></li> <li><code>healtharchive-backend/docs/operations/restore-test-log-template.md</code></li> <li>Data handling and privacy posture:</li> <li><code>healtharchive-backend/docs/operations/data-handling-retention.md</code></li> <li><code>healtharchive-backend/docs/operations/observability-and-private-stats.md</code></li> </ul>"},{"location":"documentation-process-audit/#public-facing-reporting-surfaces-documentation-for-users","title":"Public-facing reporting surfaces (documentation for users)","text":"<ul> <li>Changelog content lives in code, but is effectively \u201cpublic documentation\u201d:</li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/src/content/changelog.ts</li> <li>(process) https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md</li> <li>Status/impact pages are public reporting surfaces (operational transparency):</li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/src/app/%5Blocale%5D/status/page.tsx</li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/src/app/%5Blocale%5D/impact/page.tsx</li> </ul>"},{"location":"documentation-process-audit/#release-reproducibility-subprocesses","title":"Release + reproducibility subprocesses","text":"<ul> <li>Dataset releases and integrity expectations:</li> <li>https://github.com/jerdaw/healtharchive-datasets/blob/main/README.md</li> <li><code>healtharchive-backend/docs/operations/export-integrity-contract.md</code></li> <li><code>healtharchive-backend/docs/operations/dataset-release-runbook.md</code></li> </ul>"},{"location":"documentation-process-audit/#evaluation-against-modern-best-practices","title":"Evaluation against modern best practices","text":"<p>This section uses a simple \u201cGreen / Yellow / Red\u201d maturity signal.</p>"},{"location":"documentation-process-audit/#1-discoverability-information-architecture-green","title":"1) Discoverability &amp; information architecture \u2014 Green","text":"<p>Evidence:</p> <ul> <li>Dedicated indices exist for backend docs, ops docs, playbooks, and roadmaps.</li> <li>File naming is descriptive and stable (runbook, checklist, playbook).</li> <li>Cross-repo \u201ccanonical doc\u201d pointers exist (env wiring, partner kit, data dictionary).</li> </ul> <p>Residual risks:</p> <ul> <li>Some \u201cproject-level\u201d navigation depends on having sibling repos locally (fine for operators, less ideal for single-repo readers on GitHub).</li> </ul>"},{"location":"documentation-process-audit/#2-single-source-of-truth-drift-control-green","title":"2) Single source of truth / drift control \u2014 Green","text":"<p>Evidence:</p> <ul> <li>Explicit canonical sources + pointer strategy.</li> <li>Explicit separation:</li> <li>backlog (<code>planning/roadmap.md</code>)</li> <li>active plans (<code>docs/planning/*.md</code>)</li> <li>canonical docs (deployment/ops/dev)</li> </ul> <p>Residual risks:</p> <ul> <li>Any duplicated non-git copies (e.g., ops roadmap) are inherently drift-prone; currently mitigated by explicit \u201ckeep synced\u201d guidance.</li> </ul>"},{"location":"documentation-process-audit/#3-operational-excellence-runbooks-playbooks-verification-green","title":"3) Operational excellence (runbooks, playbooks, verification) \u2014 Green","text":"<p>Evidence:</p> <ul> <li>Production runbook is explicit about topology, security posture, and setup steps:</li> <li><code>healtharchive-backend/docs/deployment/production-single-vps.md</code></li> <li>Deploy is treated as a verified procedure with a defined gate (\u201cgreen main\u201d + VPS verification).</li> <li>Baseline drift is operationalized as policy+observed+diff.</li> <li>Restore tests and dataset verification have explicit SOPs and templates.</li> </ul> <p>Residual risks:</p> <ul> <li>As more workflows accumulate, templates become important to prevent playbooks/runbooks diverging in structure/quality.</li> </ul>"},{"location":"documentation-process-audit/#4-incident-response-learning-system-green-with-small-upgrades","title":"4) Incident response &amp; learning system \u2014 Green (with small upgrades)","text":"<p>Evidence:</p> <ul> <li>Incident notes have a clear SOP, a severity rubric, and a good template.</li> <li>The template includes: impact, detection, timeline, root cause, recovery, verification, action items.</li> <li>The repo has at least one high-quality real incident note, with follow-ups tied to a roadmap.</li> </ul> <p>Residual risks:</p> <ul> <li>Public communication expectations (status/changelog) were implicit; now explicit guidance exists, but you may still want to decide a project stance:</li> <li>\u201cWe always publish a public-safe note for sev0/sev1\u201d vs \u201conly when it changes user expectations\u201d.</li> </ul>"},{"location":"documentation-process-audit/#5-public-transparency-user-facing-documentation-yellow","title":"5) Public transparency &amp; user-facing documentation \u2014 Yellow","text":"<p>Evidence:</p> <ul> <li>The site includes <code>/governance</code>, <code>/terms</code>, <code>/privacy</code>, <code>/changelog</code>, <code>/report</code>, <code>/status</code>, <code>/impact</code>.</li> <li>Copy inventory and disclaimer matrices exist to keep safety posture coherent.</li> </ul> <p>Gaps:</p> <ul> <li>The changelog is a core public accountability surface, but without an explicit SOP it risks becoming stale or inconsistent (especially across EN/FR).</li> </ul>"},{"location":"documentation-process-audit/#6-security-privacy-documentation-posture-green","title":"6) Security + privacy documentation posture \u2014 Green","text":"<p>Evidence:</p> <ul> <li>Clear \u201cno secrets in git\u201d posture across docs.</li> <li>Admin/metrics are explicitly private-only; tailnet access model is documented.</li> <li>Data retention and PHI risk are explicitly addressed for issue reports and logs.</li> </ul> <p>Residual risks:</p> <ul> <li>If the project ever adds more operators, formalize \u201cwho has access to what\u201d and credential rotation as explicit operator subprocesses.</li> </ul>"},{"location":"documentation-process-audit/#7-reproducibility-and-research-integrity-green","title":"7) Reproducibility and research integrity \u2014 Green","text":"<p>Evidence:</p> <ul> <li>Export endpoints have defined ordering/pagination invariants.</li> <li>Dataset releases are immutable objects with checksum verification and manifest invariants.</li> <li>Corrections are expected to be documented rather than silently rewriting history.</li> </ul>"},{"location":"documentation-process-audit/#improvements-implemented-in-this-audit-2026-01-09","title":"Improvements implemented in this audit (2026-01-09)","text":"<p>These are low-risk upgrades that make doc creation and maintenance more consistent:</p> <ul> <li>Docs reference sanity checks (broken links/path refs):</li> <li>Backend: <code>healtharchive-backend/scripts/check_docs_references.py</code> (wired into <code>healtharchive-backend/Makefile</code>)</li> <li>Frontend: https://github.com/jerdaw/healtharchive-frontend/blob/main/scripts/check-doc-references.mjs (wired into <code>package.json</code>)</li> <li>Datasets: https://github.com/jerdaw/healtharchive-datasets/blob/main/scripts/check_docs_references.py (wired into <code>Makefile</code>)</li> <li>Standardized templates:</li> <li><code>healtharchive-backend/docs/deployment/runbook-template.md</code></li> <li><code>healtharchive-backend/docs/operations/playbooks/playbook-template.md</code></li> <li>Decision records mechanism:</li> <li><code>healtharchive-backend/docs/decisions/README.md</code></li> <li><code>healtharchive-backend/docs/decisions/decision-template.md</code></li> <li>Clearer doc taxonomy, quality bar, and lifecycle guidance:</li> <li><code>healtharchive-backend/docs/documentation-guidelines.md</code></li> <li>Public changelog SOP (source of truth, format, localization rules):</li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md</li> <li>Stronger \u201cincident \u2192 public-safe note\u201d expectation (optional but recommended for sev0/sev1):</li> <li><code>healtharchive-backend/docs/operations/incidents/README.md</code></li> <li><code>healtharchive-backend/docs/operations/incidents/incident-template.md</code></li> <li><code>healtharchive-backend/docs/operations/incidents/severity.md</code></li> <li><code>healtharchive-backend/docs/operations/ops-cadence-checklist.md</code></li> <li>Process nudges in PR templates:</li> <li><code>healtharchive-backend/.github/pull_request_template.md</code></li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/.github/pull_request_template.md</li> </ul>"},{"location":"documentation-process-audit/#recommendations-next-steps","title":"Recommendations (next steps)","text":""},{"location":"documentation-process-audit/#p0-high-value-low-effort","title":"P0 (high value, low effort)","text":"<ul> <li>Decide an explicit public incident disclosure posture:</li> <li>Option A: always add a public-safe <code>/changelog</code> entry for sev0/sev1 incidents.</li> <li>Option B: only add a public-safe entry when it changes user expectations (outage, integrity risk, policy change).</li> <li>Make doc maintenance part of normal ops:</li> <li>During the quarterly cadence, skim the production runbook + incident response playbook and fix drift discovered during real operations.</li> </ul>"},{"location":"documentation-process-audit/#p1-medium-value-moderate-effort","title":"P1 (medium value, moderate effort)","text":"<ul> <li>(Implemented) Docs link/path sanity checks + decision records are now in place.</li> </ul>"},{"location":"documentation-process-audit/#p2-later-if-team-grows","title":"P2 (later / if team grows)","text":"<ul> <li>If/when there are multiple regular committers:</li> <li>switch to PR-only merges (branch protection required checks),</li> <li>introduce CODEOWNERS for high-risk areas (deployment/ops/policy pages),</li> <li>require review for public-policy copy changes.</li> </ul>"},{"location":"documentation-process-audit/#top-notch-principles-to-keep","title":"\u201cTop notch\u201d principles to keep","text":"<ul> <li>Prefer stable, scripted entrypoints over fragile shell snippets.</li> <li>Keep internal docs public-safe by default (assume they may be shared).</li> <li>Separate \u201cwhat exists and how to operate it\u201d from \u201chow we got here\u201d (planning/implemented plans).</li> <li>Treat verification as first-class: every operational procedure should define what \u201cdone\u201d means.</li> </ul>"},{"location":"project/","title":"HealthArchive Documentation Hub","text":"<p>HealthArchive is a multi-repo project that archives Canadian health government websites for research and accountability. This page helps you navigate the documentation across all repositories.</p>"},{"location":"project/#quick-start-by-role","title":"\ud83d\ude80 Quick Start by Role","text":"<p>Choose your entry point based on what you want to do:</p>"},{"location":"project/#im-an-operator","title":"\ud83d\udc64 I'm an Operator","text":"<p>Goal: Deploy, monitor, and maintain the production system</p> <p>Start Here: 1. Production Runbook - Complete production setup guide 2. Operator Responsibilities - Must-do checklist 3. Deploy &amp; Verify - Safe deployment process 4. Incident Response - Emergency procedures</p> <p>Key Resources: - Ops Cadence Checklist - Daily/weekly/quarterly tasks - Monitoring Checklist - Set up alerts and checks - All Playbooks - 30+ operational procedures</p>"},{"location":"project/#im-a-developer","title":"\ud83d\udcbb I'm a Developer","text":"<p>Goal: Contribute code, fix bugs, add features</p> <p>Start Here: 1. Quick Start Guide - Get running in 5 minutes 2. Your First Contribution - Step-by-step tutorial 3. Dev Environment Setup - Detailed local setup 4. Live Testing Guide - Run the full pipeline locally</p> <p>Key Resources: - Architecture Walkthrough - Visual guide to how it all works - Architecture Deep Dive - Complete technical reference - Testing Guidelines - How to write and run tests - Contributing Guide - Code standards and workflow</p>"},{"location":"project/#im-an-api-consumer-researcher","title":"\ud83d\udd27 I'm an API Consumer / Researcher","text":"<p>Goal: Search the archive and retrieve historical snapshots</p> <p>Start Here: 1. API Consumer Guide - Complete API walkthrough with examples 2. Interactive API Docs - Try the API in your browser 3. API Reference - Full OpenAPI specification</p> <p>Quick API Test: <pre><code>curl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;source=hc\"\n</code></pre></p> <p>Key Resources: - Dataset Downloads: healtharchive-datasets - Bulk metadata exports - Data Handling Policy - Retention and privacy - Live Site: healtharchive.ca - Web interface</p>"},{"location":"project/#im-a-student-new-to-the-project","title":"\ud83d\udcda I'm a Student / New to the Project","text":"<p>Goal: Learn how HealthArchive works</p> <p>Recommended Reading Order: 1. Quick Start - High-level overview 2. Architecture Walkthrough - Follow a page from crawl to search 3. Architecture Reference - Deep technical details 4. Documentation Guidelines - How docs stay organized</p> <p>Tutorials: - Your First Contribution - Hands-on coding tutorial - Debugging a Failed Crawl - Practical troubleshooting - Live Testing - Run it yourself locally</p>"},{"location":"project/#multi-repo-architecture","title":"\ud83d\udce6 Multi-Repo Architecture","text":"<p>HealthArchive is split across three repositories with clear boundaries:</p>"},{"location":"project/#backend-this-repo","title":"\ud83d\udd19 Backend (This Repo)","text":"<p>Purpose: API, crawler, database, operations, and all internal infrastructure</p> <p>Location: github.com/jerdaw/healtharchive-backend</p> <p>Documentation: docs.healtharchive.ca (you are here)</p> <p>What Lives Here: - \u2705 Crawler (archive_tool) and job orchestration - \u2705 Database models and indexing pipeline - \u2705 RESTful JSON API (FastAPI) - \u2705 Operations runbooks and playbooks - \u2705 Deployment guides and systemd units - \u2705 Architecture and developer docs - \u2705 Decision records and incident notes</p> <p>Tech Stack: Python, FastAPI, SQLAlchemy, PostgreSQL, Docker, systemd</p>"},{"location":"project/#frontend","title":"\ud83c\udf10 Frontend","text":"<p>Purpose: Public-facing website and user interface</p> <p>Location: github.com/jerdaw/healtharchive-frontend</p> <p>Live Site: healtharchive.ca</p> <p>What Lives Here: - \u2705 Next.js web application (search UI, snapshot viewer) - \u2705 Public content (status page, impact statement, changelog) - \u2705 Internationalization (i18n) - English and French - \u2705 UI/UX documentation</p> <p>Tech Stack: Next.js 16, React, TypeScript, Tailwind CSS</p> <p>Frontend Docs in This Repo (pointers only): - Frontend Overview - I18n Guide - Implementation Guide</p> <p>Canonical Frontend Docs: frontend/docs/</p>"},{"location":"project/#datasets","title":"\ud83d\udcca Datasets","text":"<p>Purpose: Versioned, citable metadata-only dataset releases</p> <p>Location: github.com/jerdaw/healtharchive-datasets</p> <p>What Lives Here: - \u2705 Snapshot metadata exports (JSON/CSV) - \u2705 Checksums and integrity manifests - \u2705 Dataset release documentation - \u2705 Dataset integrity policies</p> <p>Why Separate?: Enables versioned, citable releases independent of code changes</p> <p>Datasets Docs in This Repo: datasets-external/README.md (pointer only)</p> <p>Canonical Datasets Docs: datasets/README.md</p>"},{"location":"project/#where-things-live-source-of-truth-map","title":"\ud83d\uddfa\ufe0f Where Things Live (Source of Truth Map)","text":"Content Type Lives In Link Operations &amp; Runbooks Backend repo docs.healtharchive.ca/operations Architecture &amp; Dev Guides Backend repo docs.healtharchive.ca/architecture API Documentation Backend repo docs.healtharchive.ca/api Public Changelog Frontend repo github.com/jerdaw/healtharchive-frontend/.../changelog-process.md Status Page Frontend repo (code) github.com/jerdaw/healtharchive-frontend/.../status/page.tsx Impact Statement Frontend repo (code) github.com/jerdaw/healtharchive-frontend/.../impact/page.tsx Dataset Releases Datasets repo github.com/jerdaw/healtharchive-datasets I18n Guidelines Frontend repo github.com/jerdaw/healtharchive-frontend/.../i18n.md <p>Principle: Each doc has one canonical source. Other repos link to it.</p>"},{"location":"project/#cross-repo-linking","title":"\ud83d\udd17 Cross-Repo Linking","text":""},{"location":"project/#in-github-issuesprs","title":"In GitHub Issues/PRs","text":"<p>Use full GitHub URLs: <pre><code>See the [production runbook](https://github.com/jerdaw/healtharchive-backend/blob/main/docs/deployment/production-single-vps.md)\n</code></pre></p>"},{"location":"project/#in-documentation","title":"In Documentation","text":"<p>For docs users: Use the docs site URLs: <pre><code>See the [Production Runbook](https://docs.healtharchive.ca/deployment/production-single-vps/)\n</code></pre></p> <p>For cross-repo references: Use full GitHub URLs: <pre><code>Frontend changelog process: [changelog-process.md](https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md)\n</code></pre></p>"},{"location":"project/#local-development-multi-repo-workspace","title":"Local Development (Multi-Repo Workspace)","text":"<p>If you have all repos cloned as siblings: <pre><code>/home/user/healtharchive/\n\u251c\u2500\u2500 healtharchive-backend/\n\u251c\u2500\u2500 healtharchive-frontend/\n\u2514\u2500\u2500 healtharchive-datasets/\n</code></pre></p> <p>Some docs use workspace paths like <code>healtharchive-frontend/...</code> for convenience.</p> <p>Note: These paths only work in local workspaces, not on GitHub.</p>"},{"location":"project/#system-architecture-high-level","title":"\ud83c\udfd7\ufe0f System Architecture (High Level)","text":"<pre><code>graph TB\n    subgraph \"HealthArchive Backend\"\n        CLI[CLI Commands] --&gt;|Create| Jobs[(Database)]\n        Worker[Worker Process] --&gt;|Poll| Jobs\n        Worker --&gt;|Execute| Crawler[Archive Tool]\n        Crawler --&gt;|Docker| Zimit[Zimit Crawler]\n        Zimit --&gt;|Write| WARC[WARC Files]\n        Worker --&gt;|Index| WARC\n        WARC --&gt;|Extract| Snapshots[(Snapshots)]\n        API[FastAPI API] --&gt;|Query| Snapshots\n    end\n\n    subgraph \"HealthArchive Frontend\"\n        UI[Next.js UI] --&gt;|API Calls| API\n        API --&gt;|JSON| UI\n        UI --&gt;|Display| Users[End Users]\n    end\n\n    subgraph \"HealthArchive Datasets\"\n        Export[Export Script] --&gt;|Read| Snapshots\n        Export --&gt;|Write| Datasets[Dataset Files]\n        Researchers --&gt;|Download| Datasets\n    end</code></pre> <p>Data Flow: 1. Crawl: CLI creates job \u2192 Worker runs crawler \u2192 Docker writes WARCs 2. Index: Worker parses WARCs \u2192 Extracts text \u2192 Stores snapshots in DB 3. Serve: API queries DB \u2192 Returns JSON \u2192 Frontend displays results 4. Export: Scripts export metadata \u2192 Version as datasets \u2192 Researchers download</p> <p>See: Architecture Walkthrough for detailed data flow</p>"},{"location":"project/#documentation-structure","title":"\ud83d\udcd6 Documentation Structure","text":"<p>This documentation follows the Di\u00e1taxis framework for clarity:</p> Type Purpose Where Tutorials Learning-oriented, step-by-step tutorials/ How-To Guides Task-oriented, problem-solving operations/playbooks/, development/ Reference Information-oriented, lookup api.md, architecture.md, reference/ Explanation Understanding-oriented, concepts documentation-guidelines.md, decisions/, operations/ <p>Navigation: Use the sidebar to explore by category</p>"},{"location":"project/#getting-help","title":"\ud83c\udd98 Getting Help","text":""},{"location":"project/#by-issue-type","title":"By Issue Type","text":"Issue Where to Go API questions API Consumer Guide \u2192 API Docs Deployment problems Production Runbook \u2192 Playbooks Code questions Architecture Guide \u2192 GitHub Discussions Bugs or feature requests GitHub Issues Operational incidents Incident Response"},{"location":"project/#community","title":"Community","text":"<ul> <li>GitHub Discussions: backend | frontend</li> <li>Issues: backend | frontend | datasets</li> <li>Contributor Guide: contributing.md</li> </ul>"},{"location":"project/#documentation-updates","title":"\ud83d\udd04 Documentation Updates","text":"<p>Found something wrong? Documentation lives in git and accepts pull requests!</p> <ol> <li>Backend docs: Edit files in <code>docs/</code></li> <li>Frontend docs: Edit files in frontend <code>docs/</code></li> <li>Datasets docs: Edit datasets README</li> </ol> <p>Guidelines: Documentation Guidelines</p>"},{"location":"project/#project-status","title":"\ud83d\udcca Project Status","text":"Metric Value Details Snapshots Archived Check /api/stats Live count Sources 2 (Health Canada, PHAC) /api/sources Crawl Frequency Annual + ad-hoc Ops Roadmap API Status Production Health Check Frontend Status Production healtharchive.ca <p>Latest Incidents: See operations/incidents/</p>"},{"location":"project/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<p>Based on your role, here's what to do next:</p>"},{"location":"project/#operators","title":"Operators","text":"<ol> <li>\u2705 Review Production Runbook</li> <li>\u2705 Complete Monitoring Checklist</li> <li>\u2705 Bookmark Incident Response</li> </ol>"},{"location":"project/#developers","title":"Developers","text":"<ol> <li>\u2705 Complete Quick Start</li> <li>\u2705 Follow Your First Contribution</li> <li>\u2705 Read Architecture Walkthrough</li> </ol>"},{"location":"project/#researchers","title":"Researchers","text":"<ol> <li>\u2705 Read API Consumer Guide</li> <li>\u2705 Try Interactive API Docs</li> <li>\u2705 Explore Datasets</li> </ol>"},{"location":"project/#essential-documentation-index","title":"\ud83d\udcda Essential Documentation Index","text":"<p>Getting Started: - Quick Start - Project Overview (you are here)</p> <p>For Operators: - Production Runbook - All Playbooks - Ops Cadence</p> <p>For Developers: - First Contribution - Architecture Guide - Dev Setup</p> <p>For Researchers: - API Guide - API Reference - Datasets</p> <p>Reference: - Documentation Guidelines - Decision Records - Roadmaps</p>"},{"location":"project/#about-this-documentation","title":"\ud83d\udca1 About This Documentation","text":"<p>This documentation portal is built with MkDocs Material and deployed to docs.healtharchive.ca.</p> <p>Source: docs/ Build: <code>make docs-build</code> Serve Locally: <code>make docs-serve</code></p> <p>Last Updated: Auto-generated on every push to <code>main</code></p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get up and running with HealthArchive in 5 minutes.</p>"},{"location":"quickstart/#what-is-healtharchive","title":"What is HealthArchive?","text":"<p>HealthArchive is a web archiving service that preserves Canadian health government sources (Health Canada, PHAC). It crawls, indexes, and makes searchable snapshots of public health information for research and accountability.</p>"},{"location":"quickstart/#choose-your-path","title":"Choose Your Path","text":"<p>Pick the guide that matches your role:</p>"},{"location":"quickstart/#im-an-operator","title":"\ud83d\udc64 I'm an Operator","text":"<p>Goal: Deploy, monitor, and maintain the production system.</p> <ol> <li>Read the Production Runbook for deployment setup</li> <li>Review Operator Responsibilities for your must-do checklist</li> <li>Bookmark Incident Response for emergencies</li> </ol> <p>Quick Deploy: <pre><code># On the VPS\ncd /opt/healtharchive-backend\n./scripts/vps-deploy.sh --apply --baseline-mode live\n</code></pre></p>"},{"location":"quickstart/#im-a-developer","title":"\ud83d\udcbb I'm a Developer","text":"<p>Goal: Contribute code, fix bugs, add features.</p> <ol> <li> <p>Clone and setup:    <pre><code>git clone https://github.com/jerdaw/healtharchive-backend.git\ncd healtharchive-backend\nmake venv\n</code></pre></p> </li> <li> <p>Configure environment:    <pre><code>cp .env.example .env\nsource .env\n</code></pre></p> </li> <li> <p>Run database migrations:    <pre><code>alembic upgrade head\n</code></pre></p> </li> <li> <p>Start the API:    <pre><code>uvicorn ha_backend.api:app --reload --port 8001\n</code></pre></p> </li> <li> <p>Run tests:    <pre><code>make ci\n</code></pre></p> </li> </ol> <p>Next: Follow the Architecture Walkthrough tutorial to understand how everything fits together.</p>"},{"location":"quickstart/#im-an-api-consumer-researcher","title":"\ud83d\udd27 I'm an API Consumer / Researcher","text":"<p>Goal: Search the archive and retrieve historical snapshots.</p> <p>API Base URL: <code>https://api.healtharchive.ca</code></p> <p>Quick Examples:</p> <pre><code># Search for content about vaccines\ncurl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;sort=relevance\"\n\n# Get archive stats\ncurl \"https://api.healtharchive.ca/api/stats\"\n\n# List all sources\ncurl \"https://api.healtharchive.ca/api/sources\"\n\n# Get a specific snapshot\ncurl \"https://api.healtharchive.ca/api/snapshot/42\"\n</code></pre> <p>Interactive API Docs: api.healtharchive.ca (OpenAPI/Swagger UI)</p> <p>Next: Read the API Consumer Guide for detailed examples and use cases.</p>"},{"location":"quickstart/#multi-repo-project","title":"Multi-Repo Project","text":"<p>HealthArchive uses a multi-repo architecture:</p> <ul> <li>Backend (this repo): API, crawler, database, operations</li> <li>GitHub: jerdaw/healtharchive-backend</li> <li> <p>Docs: docs.healtharchive.ca</p> </li> <li> <p>Frontend: Public website UI</p> </li> <li>GitHub: jerdaw/healtharchive-frontend</li> <li> <p>Live Site: healtharchive.ca</p> </li> <li> <p>Datasets: Versioned data releases</p> </li> <li>GitHub: jerdaw/healtharchive-datasets</li> </ul> <p>See the Project Overview for detailed navigation.</p>"},{"location":"quickstart/#common-tasks","title":"Common Tasks","text":"Task Command Run all checks <code>make ci</code> Start API server <code>uvicorn ha_backend.api:app --reload --port 8001</code> Start worker <code>ha-backend start-worker --poll-interval 30</code> Create a crawl job <code>ha-backend create-job --source hc</code> Run a job <code>ha-backend run-db-job --id 42</code> Index WARCs <code>ha-backend index-job --id 42</code> List jobs <code>ha-backend list-jobs</code> Serve docs locally <code>make docs-serve</code>"},{"location":"quickstart/#need-help","title":"Need Help?","text":"<ul> <li>Architecture Deep Dive: Architecture Guide</li> <li>Local Development: Live Testing</li> <li>API Reference: API Documentation</li> <li>Troubleshooting: Check the How-To Guides</li> <li>Report Issues: GitHub Issues</li> </ul>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":""},{"location":"quickstart/#for-operators","title":"For Operators","text":"<ol> <li>Complete production deployment</li> <li>Set up monitoring and alerts</li> <li>Review the Ops Cadence Checklist</li> </ol>"},{"location":"quickstart/#for-developers","title":"For Developers","text":"<ol> <li>Complete Your First Contribution tutorial</li> <li>Read the Architecture Walkthrough</li> <li>Review Testing Guidelines</li> </ol>"},{"location":"quickstart/#for-researchers","title":"For Researchers","text":"<ol> <li>Explore the API Documentation</li> <li>Download datasets from healtharchive-datasets</li> <li>Read about Data Handling</li> </ol>"},{"location":"roadmap-process/","title":"Roadmap process (pointer)","text":"<p>This repo separates:</p> <ul> <li>the backlog (what is not implemented),</li> <li>active implementation plans (what we are currently doing),</li> <li>and canonical docs (what exists and how to run/operate it),</li> </ul> <p>to reduce documentation drift.</p> <p>Canonical guidance:</p> <ul> <li>Roadmap workflow: <code>documentation-guidelines.md</code></li> <li>Roadmaps index: <code>planning/README.md</code></li> <li>Backlog: <code>planning/roadmap.md</code></li> <li>Implemented plans archive: <code>planning/implemented/README.md</code></li> </ul>"},{"location":"_templates/","title":"Documentation Templates","text":"<p>This directory contains templates for creating consistent documentation across the project.</p>"},{"location":"_templates/#available-templates","title":"Available Templates","text":"Template Purpose Destination <code>runbook-template.md</code> Deployment/operational procedures <code>docs/deployment/</code> <code>playbook-template.md</code> Task-oriented checklists <code>docs/operations/playbooks/</code> or <code>docs/development/playbooks/</code> <code>incident-template.md</code> Incident postmortems <code>docs/operations/incidents/</code> <code>decision-template.md</code> Architectural/policy decisions <code>docs/decisions/</code> <code>restore-test-log-template.md</code> Quarterly restore test logs <code>/srv/healtharchive/ops/restore-tests/</code> (VPS) <code>adoption-signals-log-template.md</code> Quarterly adoption signals <code>/srv/healtharchive/ops/adoption/</code> (VPS) <code>mentions-log-template.md</code> Public mentions log entries <code>docs/operations/mentions-log.md</code> <code>ops-ui-friction-log-template.md</code> Internal friction tracking Local ops notes (not git)"},{"location":"_templates/#how-to-use","title":"How to Use","text":"<ol> <li>Copy the appropriate template to the destination directory</li> <li>Rename the file (remove <code>-template</code> suffix)</li> <li>Fill in all sections with your content</li> <li>Update the directory's <code>README.md</code> index to include the new doc</li> <li>Add to navigation in <code>mkdocs.yml</code> if the doc is critical/frequently accessed</li> </ol>"},{"location":"_templates/#template-conventions","title":"Template Conventions","text":""},{"location":"_templates/#runbooks","title":"Runbooks","text":"<ul> <li>Purpose: Step-by-step operational procedures</li> <li>Audience: Operators with appropriate access</li> <li>Structure: Purpose, Scope, Preconditions, Architecture, Procedure, Verification, Rollback, Troubleshooting, References</li> </ul>"},{"location":"_templates/#playbooks","title":"Playbooks","text":"<ul> <li>Purpose: Short task-oriented checklists</li> <li>Audience: Operators performing recurring work</li> <li>Structure: Purpose, Preconditions, Steps, Verification, Safety, References</li> </ul>"},{"location":"_templates/#incident-notes","title":"Incident Notes","text":"<ul> <li>Purpose: Lightweight postmortems for operational learning</li> <li>Audience: Internal operators</li> <li>Structure: Metadata, Timeline, Impact, Root Cause, Resolution, Follow-ups, References</li> </ul>"},{"location":"_templates/#decision-records","title":"Decision Records","text":"<ul> <li>Purpose: Document high-stakes architectural/policy choices</li> <li>Audience: All contributors</li> <li>Structure: Context, Decision, Rationale, Alternatives, Consequences, Verification, References</li> </ul>"},{"location":"_templates/#maintenance","title":"Maintenance","text":"<ul> <li>Templates should be updated when patterns evolve</li> <li>Keep templates minimal and focused on structure</li> <li>Avoid prescriptive content that changes frequently</li> <li>Templates are excluded from the published docs site navigation but remain in the repo</li> </ul>"},{"location":"_templates/#references","title":"References","text":"<ul> <li>Documentation guidelines: <code>../documentation-guidelines.md</code></li> <li>Quality bar requirements: See \"Definition of Done\" in guidelines</li> </ul>"},{"location":"_templates/adoption-signals-log-template/","title":"Adoption signals log (template; private)","text":"<p>Keep this log public-safe:</p> <ul> <li>No emails, names, or personal details.</li> <li>Links + aggregate counts only.</li> </ul> <p>Recommended cadence: quarterly.</p> <p>Suggested location on the VPS:</p> <ul> <li><code>/srv/healtharchive/ops/adoption/</code></li> <li>One file per quarter (e.g., <code>YYYY-QN.md</code>)</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#adoption-signals-yyyy-qn-utc","title":"Adoption signals \u2014 YYYY-QN (UTC)","text":"<ul> <li>Quarter (UTC): YYYY-QN</li> <li>Operator:</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#dataset-releases","title":"Dataset releases","text":"<ul> <li>Release tags published this quarter:</li> <li><code>healtharchive-dataset-YYYY-MM-DD</code></li> <li>Notes (what changed / anything notable):</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#downloads-usage-aggregate","title":"Downloads / usage (aggregate)","text":"<ul> <li>GitHub release downloads: (if you check them; optional)</li> <li>snapshots: (count)</li> <li>changes: (count)</li> <li>Site usage metrics (aggregate): (optional)</li> <li>notes:</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#public-mentions-citations-links-only","title":"Public mentions / citations (links only)","text":"<ul> <li>(none)</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#research-reuse-signals-public-safe","title":"Research reuse signals (public-safe)","text":"<ul> <li>\u201cUsed in class/lab/project\u201d (link if public; otherwise omit).</li> <li>\u201cReferenced in newsletter/blog\u201d (link).</li> <li>\u201cCited in paper/preprint\u201d (link).</li> </ul>"},{"location":"_templates/adoption-signals-log-template/#follow-ups","title":"Follow-ups","text":"<ul> <li>(none)</li> </ul>"},{"location":"_templates/decision-template/","title":"Decision:  (YYYY-MM-DD) <p>Status: draft | accepted | superseded</p>","text":""},{"location":"_templates/decision-template/#context","title":"Context","text":"<ul> <li>What problem are we solving?</li> <li>What constraints matter (security, privacy, ops capacity, reproducibility)?</li> <li>What triggered the decision (incident, planned change, external request)?</li> </ul>"},{"location":"_templates/decision-template/#decision","title":"Decision","text":"<p>State the decision clearly in 1\u20133 bullets.</p> <ul> <li>We will\u2026</li> <li>We will not\u2026</li> </ul>"},{"location":"_templates/decision-template/#rationale","title":"Rationale","text":"<p>Why this is the right tradeoff for HealthArchive right now.</p>"},{"location":"_templates/decision-template/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Option A \u2014 (why rejected)</li> <li>Option B \u2014 (why rejected)</li> </ul>"},{"location":"_templates/decision-template/#consequences","title":"Consequences","text":""},{"location":"_templates/decision-template/#positive","title":"Positive","text":"<ul> <li>\u2026</li> </ul>"},{"location":"_templates/decision-template/#negative-risks","title":"Negative / risks","text":"<ul> <li>\u2026</li> </ul>"},{"location":"_templates/decision-template/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>How do we confirm the decision is correctly implemented (checks, scripts, policy)?</li> <li>What is the rollback plan if it proves unsafe?</li> </ul>"},{"location":"_templates/decision-template/#references","title":"References","text":"<ul> <li>Related canonical docs:</li> <li>Related playbooks/runbooks:</li> <li>Related incident notes:</li> <li>PRs / issues / external links:</li> </ul>"},{"location":"_templates/incident-template/","title":"Incident:  (YYYY-MM-DD) <p>Status: draft | closed</p>","text":""},{"location":"_templates/incident-template/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): YYYY-MM-DD</li> <li>Severity (see <code>operations/incidents/severity.md</code>): sev0 | sev1 | sev2 | sev3</li> <li>Environment: production | staging | dev</li> <li>Primary area: crawl | indexing | storage | api | replay | search | infra</li> <li>Owner:  <li>Start (UTC): YYYY-MM-DDTHH:MM:SSZ</li> <li>End (UTC): YYYY-MM-DDTHH:MM:SSZ (or ongoing)</li>"},{"location":"_templates/incident-template/#summary","title":"Summary","text":"<p>What happened in 2\u20135 sentences, written for someone who wasn\u2019t online during the incident.</p>"},{"location":"_templates/incident-template/#impact","title":"Impact","text":"<ul> <li>User-facing impact:</li> <li>Internal impact (ops burden, automation failures, etc):</li> <li>Data impact:</li> <li>Data loss: yes/no/unknown</li> <li>Data integrity risk: yes/no/unknown</li> <li>Recovery completeness: complete/partial/unknown</li> <li>Duration:</li> </ul>"},{"location":"_templates/incident-template/#detection","title":"Detection","text":"<ul> <li>How was it detected (alert, operator check, user report)?</li> <li>What signals were most useful (commands/metrics/logs)?</li> </ul>"},{"location":"_templates/incident-template/#decision-log-optional-but-recommended-for-sev0sev1","title":"Decision log (optional but recommended for sev0/sev1)","text":"<p>Record key decisions and why they were made (especially if they trade off data integrity vs speed).</p> <ul> <li>YYYY-MM-DDTHH:MM:SSZ \u2014 Decision:  (why: , risks: )"},{"location":"_templates/incident-template/#timeline-utc","title":"Timeline (UTC)","text":"<p>Keep this as a chronological log. Prefer timestamps.</p> <ul> <li>YYYY-MM-DDTHH:MM:SSZ \u2014  <li>YYYY-MM-DDTHH:MM:SSZ \u2014"},{"location":"_templates/incident-template/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger:</li> <li>Underlying cause(s):</li> </ul>"},{"location":"_templates/incident-template/#contributing-factors","title":"Contributing factors","text":"<ul> <li>What made this worse or harder to debug?</li> </ul>"},{"location":"_templates/incident-template/#resolution-recovery","title":"Resolution / Recovery","text":"<p>Describe the recovery steps taken, in the order performed, with commands if helpful.</p>"},{"location":"_templates/incident-template/#post-incident-verification","title":"Post-incident verification","text":"<p>What you did to confirm we\u2019re actually healthy (and not just \u201crunning\u201d).</p> <ul> <li>Public surface checks:</li> <li>Worker/job health checks:</li> <li>Storage/mount checks (if relevant):</li> <li>Integrity checks (if relevant):</li> </ul>"},{"location":"_templates/incident-template/#public-communication-optional-do-this-when-it-changes-user-expectations","title":"Public communication (optional; do this when it changes user expectations)","text":"<p>Keep this public-safe (no sensitive incident details).</p> <ul> <li>Public status update (where/when):</li> <li>Changelog entry (date/link):</li> <li>Public summary (2\u20135 sentences):</li> </ul>"},{"location":"_templates/incident-template/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li> <li>"},{"location":"_templates/incident-template/#action-items-todos","title":"Action items (TODOs)","text":"<p>Make these specific, small, and verifiable. Link to issues/PRs/roadmaps if they exist.</p> <ul> <li>  (owner=, priority=, due=) <li>  (owner=, priority=, due=)"},{"location":"_templates/incident-template/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>What can be automated safely?</li> <li>What should stay manual (risk/false positives)?</li> </ul>"},{"location":"_templates/incident-template/#references-artifacts","title":"References / Artifacts","text":"<ul> <li><code>./scripts/vps-crawl-status.sh</code> snapshot(s):</li> <li>Relevant log path(s):</li> <li>Dashboard link(s) / metric names:</li> <li>Related playbooks/runbooks:</li> </ul>"},{"location":"_templates/mentions-log-template/","title":"Mentions Log (template)","text":"<p>Purpose: A lightweight, public-safe log of mentions, links, or citations. Populate this only with information that is already public and permitted.</p> <p>Do NOT add private emails or outreach notes.</p> <p>Canonical log (real entries live here):</p> <ul> <li><code>operations/mentions-log.md</code></li> </ul>"},{"location":"_templates/mentions-log-template/#mentions-log","title":"Mentions log","text":"Date (UTC) Organization / Outlet Link Context Permission to name? YYYY-MM-DD Example University Library https://example.org/resource Added to digital scholarship resources Yes"},{"location":"_templates/mentions-log-template/#notes","title":"Notes","text":"<ul> <li>Only include public links or citations.</li> <li>Keep descriptions factual and short.</li> <li>If permission is unclear, use \"Pending\" and do not name the organization in   public-facing copy until confirmed.</li> </ul>"},{"location":"_templates/ops-ui-friction-log-template/","title":"Ops UI friction log (template)","text":"<p>Goal: decide whether a bespoke admin/ops UI is worth building by capturing real operator pain over time.</p> <p>Guideline:</p> <ul> <li>Use Grafana dashboards + existing JSON admin endpoints first.</li> <li>Only build a bespoke UI if it clearly reduces recurring toil.</li> </ul>"},{"location":"_templates/ops-ui-friction-log-template/#entry","title":"Entry","text":"<p>Date (UTC):</p> <p>Operator:</p> <p>Context:</p> <ul> <li>What were you trying to do? (triage, investigate outage, verify deploy, investigate search quality, etc.)</li> <li>What triggered it? (alert, user report, scheduled check, curiosity)</li> </ul> <p>What worked well:</p> <ul> <li>What was quick/easy? (dashboard answered it, existing playbook, one command)</li> </ul> <p>Friction / pain:</p> <ul> <li>What took longer than it should?</li> <li>Did you need SSH for more than port-forwarding?</li> <li>Did you need to manually handle tokens/headers?</li> <li>Did you need to \u201chunt\u201d for the right endpoint/query?</li> </ul> <p>Impact:</p> <ul> <li>Time spent (rough): <code>X minutes</code></li> <li>Frequency: one-off / weekly / daily / during incidents</li> <li>Risk: low / medium / high (chance of operator mistake)</li> </ul> <p>Workaround used (today):</p> <ul> <li>Command(s) / link(s) / dashboard(s):</li> </ul> <p>Proposed improvement:</p> <ul> <li>Dashboard improvement? (new panel/table/link)</li> <li>Script improvement? (new helper, safer defaults)</li> <li>Doc/playbook improvement?</li> <li>Is a bespoke UI actually required? Why?</li> </ul> <p>Decision signal:</p> <ul> <li>If this happened again, would a bespoke UI save meaningful time?</li> </ul>"},{"location":"_templates/playbook-template/","title":"Playbook:  (operators) <p>Purpose: one sentence on what this playbook achieves.</p> <p>This is a short, task-oriented checklist. Keep it procedural, public-safe, and low-toil.</p>","text":""},{"location":"_templates/playbook-template/#when-to-use","title":"When to use","text":"<ul> <li> <li>"},{"location":"_templates/playbook-template/#preconditions-access","title":"Preconditions / access","text":"<ul> <li>Environment: production | staging | local</li> <li>Required access: haadmin; <code>sudo</code> required?&gt; <li>Required inputs:"},{"location":"_templates/playbook-template/#safety-guardrails","title":"Safety / guardrails","text":"<ul> <li>What could go wrong?</li> <li>What should you not do during this procedure?</li> <li>Any caps/cooldowns/sentinels that must be in place?</li> </ul>"},{"location":"_templates/playbook-template/#steps","title":"Steps","text":"<p>1)  (what it changes) 2)  (what it changes) 3) &lt;\u2026&gt;"},{"location":"_templates/playbook-template/#verification-done-criteria","title":"Verification (\u201cdone\u201d criteria)","text":"<ul> <li> <li>"},{"location":"_templates/playbook-template/#rollback-recovery-if-needed","title":"Rollback / recovery (if needed)","text":"<ul> <li>"},{"location":"_templates/playbook-template/#references","title":"References","text":"<ul> <li>Canonical runbook/checklist: </li> <li>Related playbooks: </li> <li>Relevant incident note(s): </li> </ul>"},{"location":"_templates/restore-test-log-template/","title":"Restore Test Log (template)","text":"<p>Use this template to record quarterly restore tests. Keep it public-safe: no secrets, credentials, or internal IPs.</p>"},{"location":"_templates/restore-test-log-template/#restore-test-record","title":"Restore test record","text":"<ul> <li>Date (UTC):</li> <li>Operator:</li> <li>Backup source used: (e.g., latest nightly dump, date, location)</li> <li>Restore target: (local temp DB / staging host)</li> <li>Restore method: (command summary)</li> <li>Schema check: (<code>alembic current</code> output)</li> <li>API checks: (<code>/api/health</code>, <code>/api/stats</code>, <code>/api/sources</code>)</li> <li>Result: Pass / Fail</li> <li>Notes / anomalies:</li> <li>Follow-up actions:</li> </ul>"},{"location":"_templates/runbook-template/","title":"Runbook:  (operators) <p>Purpose: one paragraph describing what this runbook covers and when it is the canonical reference.</p>","text":""},{"location":"_templates/runbook-template/#scope","title":"Scope","text":"<ul> <li>Environment(s): production | staging | dev</li> <li>Audience: operator | developer | both</li> <li>Non-goals: what this runbook explicitly does not cover</li> </ul>"},{"location":"_templates/runbook-template/#preconditions","title":"Preconditions","text":"<ul> <li>Required access (Tailscale, SSH user, <code>sudo</code>, secrets location)</li> <li>Required inputs (env vars, paths, hostnames, domains)</li> <li>Required dependencies (packages, services)</li> </ul>"},{"location":"_templates/runbook-template/#architecture-topology-short","title":"Architecture / topology (short)","text":"<ul> <li>Components involved (API, worker, DB, reverse proxy, storage)</li> <li>Network posture (public ports vs loopback-only vs tailnet-only)</li> <li>Data paths (where state and artifacts live)</li> </ul>"},{"location":"_templates/runbook-template/#procedure","title":"Procedure","text":""},{"location":"_templates/runbook-template/#1","title":"1)  <pre><code>&lt;command&gt;\n</code></pre> <p>What this changes:</p> <ul> <li>","text":""},{"location":"_templates/runbook-template/#2","title":"2)  <pre><code>&lt;command&gt;\n</code></pre> <p>What this changes:</p> <ul> <li>","text":""},{"location":"_templates/runbook-template/#verification-done-criteria","title":"Verification (\u201cdone\u201d criteria)","text":"<ul> <li>Public surface:  <li>Internal health:  <li>Drift / policy checks:"},{"location":"_templates/runbook-template/#rollback-recovery","title":"Rollback / recovery","text":"<ul> <li>Safe rollback strategy (fast path)</li> <li>What to avoid (data integrity risks)</li> </ul>"},{"location":"_templates/runbook-template/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Common failures and the first 1\u20133 commands to triage</li> <li>Pointers to deeper playbooks and incident response</li> </ul>"},{"location":"_templates/runbook-template/#references","title":"References","text":"<ul> <li>Related playbooks: </li> <li>Related checklists: </li> <li>Related incident notes: </li> </ul>"},{"location":"datasets-external/","title":"Datasets","text":"<p>The HealthArchive datasets live in a separate repository:</p> <ul> <li>Repo: https://github.com/jerdaw/healtharchive-datasets</li> <li>Releases: https://github.com/jerdaw/healtharchive-datasets/releases</li> </ul> <p>This backend docs portal links out to the canonical datasets docs; it does not mirror them.</p> <p>Related backend docs:</p> <ul> <li>Dataset release runbook (ops): <code>../operations/dataset-release-runbook.md</code></li> <li>Export integrity contract: <code>../operations/export-integrity-contract.md</code></li> </ul>"},{"location":"decisions/","title":"Decision records (ADR-lite)","text":"<p>This folder contains decision records for high-stakes choices that affect:</p> <ul> <li>security posture,</li> <li>privacy / data handling,</li> <li>public vs private surfaces,</li> <li>operational invariants (what must remain true over time).</li> </ul> <p>Goal: make important choices legible and durable so they don\u2019t get lost in chat history, PR threads, or implicit \u201ctribal knowledge\u201d.</p> <p>Related:</p> <ul> <li>Documentation policy and doc taxonomy: <code>../documentation-guidelines.md</code></li> <li>Public/private boundaries: <code>../operations/observability-and-private-stats.md</code></li> <li>Data handling and retention: <code>../operations/data-handling-retention.md</code></li> <li>Production invariants (drift policy): <code>../operations/baseline-drift.md</code></li> </ul>"},{"location":"decisions/#what-goes-here-examples","title":"What goes here (examples)","text":"<ul> <li>Decisions that change public attack surface (e.g., making an endpoint public/private).</li> <li>Decisions that change what data is collected or retained (especially anything user-related).</li> <li>Decisions that change operational safety rails (automation posture, caps, sentinels).</li> <li>Decisions that change reproducibility guarantees (exports, dataset release immutability).</li> </ul>"},{"location":"decisions/#what-does-not-go-here","title":"What does not go here","text":"<ul> <li>Backlog items and implementation steps (use <code>../planning/</code>).</li> <li>Incident timelines and recovery notes (use <code>../operations/incidents/</code>).</li> <li>Routine ops logs (restore tests, adoption signals; use <code>/srv/healtharchive/ops/...</code>).</li> </ul>"},{"location":"decisions/#naming","title":"Naming","text":"<p>One file per decision:</p> <ul> <li><code>YYYY-MM-DD-short-title.md</code> (UTC date the decision is made/accepted)</li> </ul> <p>If multiple decisions occur on one day, add a suffix:</p> <ul> <li><code>YYYY-MM-DD-short-title-a.md</code>, <code>...-b.md</code></li> </ul>"},{"location":"decisions/#how-to-create-a-new-decision-record","title":"How to create a new decision record","text":"<p>1) Copy the template: <code>../_templates/decision-template.md</code> 2) Fill Context + Decision first. 3) Record alternatives briefly (what you didn\u2019t do, and why). 4) Link to supporting artifacts (PRs, incident notes, runbooks, issues). 5) Mark status as <code>accepted</code> once you commit to it.</p> <p>If a decision changes later, create a new decision record and mark the old one as <code>superseded</code> (link both directions).</p>"},{"location":"decisions/#decision-records","title":"Decision records","text":"<ul> <li><code>2026-02-07-git-first-vps-changes.md</code></li> <li><code>2026-02-06-per-source-crawl-profiles-and-annual-reconciliation.md</code></li> <li><code>2026-02-03-crawl-auto-recover-queue-fill.md</code></li> <li><code>2026-02-03-crawl-job-db-state-reconciliation.md</code></li> <li><code>2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage.md</code></li> <li><code>2026-01-23-annual-crawl-throughput-and-artifacts.md</code></li> <li><code>2026-01-19-annual-crawl-resiliency-and-queue-order.md</code></li> <li><code>2026-01-19-ops-first-monitoring.md</code></li> <li><code>2026-01-18-search-ranking-v3.md</code></li> <li><code>2026-01-09-public-incident-disclosure-posture.md</code></li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/","title":"Decision: Public incident disclosure posture (Option B for now) (2026-01-09)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#context","title":"Context","text":"<ul> <li>HealthArchive is early in its operational lifecycle and will likely see a higher rate of sev0/sev1 incidents while reliability work is still being built out.</li> <li>We want to maintain transparency without creating constant public \u201cincident noise\u201d that trains users to ignore updates.</li> <li>We already capture internal, public-safe incident notes for operational learning and repeatability under <code>docs/operations/incidents/</code>.</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#decision","title":"Decision","text":"<ul> <li>We will use Option B by default:</li> <li>publish a public-safe note (in <code>/changelog</code> and/or <code>/status</code>) only when an incident changes user expectations:<ul> <li>user-visible outage or major degradation,</li> <li>credible integrity risk,</li> <li>security posture change,</li> <li>public policy/governance change.</li> </ul> </li> <li>We will still write internal incident notes when appropriate (per <code>docs/operations/incidents/README.md</code>).</li> <li>We will revisit moving to Option A (always publish a public-safe note for sev0/sev1) once operations are demonstrably stable over multiple full campaign cycles.</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#rationale","title":"Rationale","text":"<ul> <li>Option B preserves transparency for incidents that matter to user trust and interpretation of the archive.</li> <li>It avoids turning the public changelog/status into a high-volume incident feed during an early, fast-changing period.</li> <li>Internal incident notes remain the \u201cfull fidelity\u201d learning system and can still drive follow-ups and hardening work.</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Option A (always publish public-safe notes for sev0/sev1)</li> <li>Rejected for now due to likely high volume during stabilization; risk of public update fatigue.</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#positive","title":"Positive","text":"<ul> <li>Public reporting remains high-signal and user-relevant.</li> <li>Internal learning remains intact (incident notes + follow-ups).</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#negative-risks","title":"Negative / risks","text":"<ul> <li>Some operator-relevant incidents may not be visible publicly, even if they were sev0/sev1 internally.</li> <li>Mitigation: treat \u201cchanges user expectations\u201d as the trigger, not severity alone, and err on the side of communicating when unsure.</li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Incident templates and severity rubric should reflect the \u201cOption B\u201d trigger:</li> <li><code>docs/_templates/incident-template.md</code></li> <li><code>docs/operations/incidents/severity.md</code></li> <li>Ops cadence includes routine doc maintenance so these rules don\u2019t drift:</li> <li><code>docs/operations/ops-cadence-checklist.md</code></li> </ul>"},{"location":"decisions/2026-01-09-public-incident-disclosure-posture/#references","title":"References","text":"<ul> <li>Incident notes process: <code>../operations/incidents/README.md</code></li> <li>Changelog process: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md</li> <li>Future roadmap note: <code>../planning/roadmap.md</code></li> </ul>"},{"location":"decisions/2026-01-18-search-ranking-v3/","title":"ADR: Search ranking v3 with is_archived column and enhanced signals","text":"<p>Date: 2026-01-18</p> <p>Status: Accepted</p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#context","title":"Context","text":"<p>Search quality is central to HealthArchive's value proposition - researchers need to discover relevant captures efficiently. The v2 ranking system (introduced in 2025) provided query-mode sensitive blending but had two key limitations:</p> <ol> <li> <p>Archived content detection was heuristic-only - We detected archived pages via title/snippet text patterns, causing false positives/negatives and making the signal unstable across content updates.</p> </li> <li> <p>Missing modern ranking signals - No recency preference for broad queries, no exact title matching boost, and fixed BM25 weights limited relevance tuning.</p> </li> </ol> <p>The roadmap <code>docs/planning/implemented/2026-01-03-search-ranking-and-snippets-v3.md</code> defined a phased implementation plan to address these gaps while maintaining single-VPS constraints.</p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#decision","title":"Decision","text":"<p>We implemented search ranking v3 with three major enhancements:</p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#1-stable-archived-detection-via-database-column","title":"1. Stable Archived Detection via Database Column","text":"<p>Change: Added <code>Snapshot.is_archived</code> (nullable boolean) populated at index-time.</p> <p>Rationale: - Moves archived detection from query-time heuristics to index-time computation - Database column is stable across snippet updates and query variations - Nullable design allows graceful handling during migration (fallback to heuristics for NULL values)</p> <p>Implementation: - Alembic migration: <code>alembic/versions/0013_snapshot_is_archived.py</code> - Detection logic: <code>src/ha_backend/indexing/text_extraction.py::detect_is_archived()</code> - Ranking integration: <code>src/ha_backend/api/routes_public.py::build_archived_penalty()</code></p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#2-enhanced-text-extraction-for-better-fts-and-snippets","title":"2. Enhanced Text Extraction for Better FTS and Snippets","text":"<p>Changes: - ARIA role pruning (<code>role=navigation</code>, <code>role=banner</code>, etc.) - Content root scoring (prefer <code>&lt;main&gt;</code>, <code>&lt;article&gt;</code> over generic <code>&lt;div&gt;</code>) - Boilerplate phrase filtering (\"Skip to content\", cookies banners) - Extended FTS content to ~4KB (up from ~280 char snippets)</p> <p>Rationale: - Removes navigation boilerplate that pollutes snippets - Improves FTS match quality by indexing actual page content - 4KB limit balances index size vs. content coverage</p> <p>Implementation: - Text extraction: <code>src/ha_backend/indexing/text_extraction.py</code> - FTS integration: <code>src/ha_backend/search.py::build_search_vector()</code> - Pipeline: <code>src/ha_backend/indexing/pipeline.py</code></p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#3-additional-ranking-signals","title":"3. Additional Ranking Signals","text":"<p>New signals in v3:</p> <ol> <li>Recency boost (broad/mixed queries only):</li> <li>Formula: <code>coef * ln(1 + 365 / days_ago)</code></li> <li>Broad: 0.15, Mixed: 0.08, Specific: 0.0</li> <li> <p>Rationale: Recent content is more valuable for broad exploratory queries</p> </li> <li> <p>Title exact-match boost:</p> </li> <li>Bonus when query appears as substring in title</li> <li>Broad: +0.35, Mixed: +0.30, Specific: +0.25</li> <li> <p>Rationale: Stronger signal than token matching; indicates highly relevant pages</p> </li> <li> <p>BM25 weight tuning (ts_rank weights):</p> </li> <li>Increased title weight (A): 1.0 \u2192 1.2 for broad queries</li> <li>Reduced URL weight (D): 0.1 \u2192 0.05</li> <li>Rationale: Titles are more reliable than URL tokens for relevance</li> </ol> <p>Implementation: - Config: <code>src/ha_backend/search_ranking.py</code> - API wiring: <code>src/ha_backend/api/routes_public.py</code></p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#consequences","title":"Consequences","text":"<p>Positive: - \u2705 Stable archived detection reduces query-time variability - \u2705 Cleaner snippets improve user experience - \u2705 Recency boost helps with broad queries (\"covid\" prefers recent guidance) - \u2705 Title exact-match strongly signals relevance - \u2705 Database-backed <code>is_archived</code> enables analytics and filters</p> <p>Neutral: - \u26a0\ufe0f Nullable <code>is_archived</code> requires migration planning (run Alembic + backfill on production) - \u26a0\ufe0f Evaluation required before making v3 default (<code>HA_SEARCH_RANKING_VERSION=v3</code>)</p> <p>Negative: - \u274c Slight index storage increase due to 4KB FTS content (acceptable given single-VPS storage capacity)</p>"},{"location":"decisions/2026-01-18-search-ranking-v3/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>Opt-in via <code>ranking=v3</code> query parameter or <code>HA_SEARCH_RANKING_VERSION=v3</code> environment variable</li> <li>V2 remains default until evaluation completes</li> <li>All 234 tests pass (28 new tests for v3 functionality)</li> <li>Evaluation tooling updated: <code>scripts/search-eval-capture.sh</code> supports <code>--ranking v3</code></li> </ul>"},{"location":"decisions/2026-01-18-search-ranking-v3/#references","title":"References","text":"<ul> <li>Roadmap: <code>docs/planning/implemented/2026-01-03-search-ranking-and-snippets-v3.md</code></li> <li>Migration: <code>alembic/versions/0013_snapshot_is_archived.py</code></li> <li>Tests: <code>tests/test_text_extraction_v3.py</code>, <code>tests/test_ranking_v3_signals.py</code></li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/","title":"Decision: Annual crawl resiliency defaults and deterministic queue order (2026-01-19)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#context","title":"Context","text":"<ul> <li>HealthArchive\u2019s annual campaign crawls are completeness-first (full-fidelity backups), not \u201cbest effort within a page cap\u201d.</li> <li>A 2026 annual crawl incident showed multiple failure modes that increased operator load and reduced progress:</li> <li>\u201cNoisy but progressing\u201d sites (notably <code>canada.ca</code>) hit low adaptive timeout thresholds, causing repeated restarts and long backoff delays.</li> <li>The single-worker queue pick order was ambiguous when multiple jobs were enqueued with identical <code>queued_at</code>, leading to non-deterministic job selection and starvation.</li> <li>Invalid crawler CLI args (e.g., unsupported Zimit flags) caused immediate job failure and retry churn.</li> <li>Constraints:</li> <li>Production currently uses a single worker loop and a small VPS.</li> <li>We want safe-by-default automation and predictable operations.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#decision","title":"Decision","text":"<ul> <li>For annual campaign jobs, we will use resiliency-oriented defaults that tolerate \u201cnoisy but progressing\u201d crawls:</li> <li><code>max_container_restarts &gt;= 20</code></li> <li><code>error_threshold_timeout = 50</code></li> <li><code>error_threshold_http = 50</code></li> <li><code>backoff_delay_minutes = 2</code></li> <li><code>ha-backend schedule-annual</code> will stagger per-source <code>queued_at</code> timestamps so the single-worker pick order is deterministic (hc \u2192 phac \u2192 cihr).</li> <li>We will treat invalid CLI / config failures (e.g., \u201cunrecognized arguments\u201d) as <code>infra_error_config</code> so the worker does not churn retry budgets.</li> <li>Annual campaign policy remains: no page/depth caps. Use scope rules to bound crawls, not limits that risk incompleteness.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#rationale","title":"Rationale","text":"<ul> <li>Long annual crawls inevitably hit timeouts and transient network/protocol issues; restart thrash and long backoffs reduce throughput and increase operator intervention.</li> <li>Deterministic <code>queued_at</code> ordering makes operations predictable and prevents \u201cqueue tie\u201d ambiguity in a single-worker environment.</li> <li>Classifying invalid CLI args as configuration errors surfaces the real problem quickly and avoids burning retries on a doomed job.</li> <li>Scope rules preserve the completeness-first mission without relying on early-termination caps.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Keep low default thresholds + long backoff \u2014 rejected: causes restart thrash and slows progress on long crawls.</li> <li>Add page/depth caps to guarantee completion \u2014 rejected: conflicts with completeness-first archival goals and can silently truncate captures.</li> <li>Add more workers/parallelism immediately \u2014 deferred: increases ops surface and resource needs; can be revisited once baseline stability is proven.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#positive","title":"Positive","text":"<ul> <li>Fewer \u201cthrash loops\u201d and shorter recovery delays for long annual crawls.</li> <li>Predictable queue order reduces starvation and makes on-call behavior easier to reason about.</li> <li>Faster identification of invalid crawler argument/config mistakes.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#negative-risks","title":"Negative / risks","text":"<ul> <li>Higher thresholds may delay intervention for truly stuck crawls; mitigation is improved metrics (state file + restart counters) and stalled/progress-age alerts.</li> <li>More container restarts increase resource usage; mitigated by restart caps and monitoring.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Code defaults:</li> <li>Annual job defaults include the new tool options (<code>src/ha_backend/job_registry.py</code>).</li> <li><code>schedule-annual</code> sets distinct <code>queued_at</code> per source (<code>src/ha_backend/cli.py</code>).</li> <li>Automation:</li> <li>Auto-recover enforces annual minima when recovering jobs (<code>scripts/vps-crawl-auto-recover.py</code>).</li> <li>Observability:</li> <li>Metrics exporter emits <code>.archive_state.json</code> health + restart counters (<code>scripts/vps-crawl-metrics-textfile.py</code>).</li> <li>Tests:</li> <li><code>tests/test_cli_schedule_annual.py</code></li> <li><code>tests/test_ops_crawl_auto_recover_tool_options.py</code></li> <li><code>tests/test_jobs_persistent.py</code></li> <li><code>tests/test_ops_crawl_metrics_textfile_state.py</code></li> </ul> <p>Rollback:</p> <ul> <li>Revert the defaults and ordering logic in the above files; redeploy and restart the worker.</li> </ul>"},{"location":"decisions/2026-01-19-annual-crawl-resiliency-and-queue-order/#references","title":"References","text":"<ul> <li>Related canonical docs:</li> <li><code>docs/operations/annual-campaign.md</code></li> <li><code>docs/reference/archive-tool.md</code></li> <li><code>docs/architecture.md</code></li> <li>Related scripts:</li> <li><code>scripts/vps-crawl-status.sh</code></li> <li><code>scripts/vps-crawl-auto-recover.py</code></li> <li><code>scripts/vps-crawl-metrics-textfile.py</code></li> </ul>"},{"location":"decisions/2026-01-19-ops-first-monitoring/","title":"2026-01-19: Ops-First Monitoring Strategy (Textfile Collectors)","text":""},{"location":"decisions/2026-01-19-ops-first-monitoring/#context","title":"Context","text":"<p>The annual crawl campaign (2026) required deep visibility into the crawling process (job 6, 7, etc.), which runs as a Docker container managed by <code>healtharchive-worker</code>. We needed to know:</p> <ol> <li>Is the crawl actually writing pages? (Progress monitoring)</li> <li>Is the SSHFS mount stable? (Infrastructure health)</li> <li>Is independent indexing starting after the crawl finishes? (Pipeline integrity)</li> </ol> <p>Existing Prometheus exporters (<code>node_exporter</code>) give system-level metrics but lack application-specific context for these batch jobs.</p>"},{"location":"decisions/2026-01-19-ops-first-monitoring/#decision","title":"Decision","text":"<p>We decided to implement an \"Ops-First\" monitoring strategy using the Prometheus Node Exporter Textfile Collector pattern, driven by simple Systemd timers and Python scripts.</p>"},{"location":"decisions/2026-01-19-ops-first-monitoring/#key-components","title":"Key Components","text":"<ol> <li>Script-Driven Metrics: A dedicated script (<code>scripts/vps-crawl-metrics-textfile.py</code>) that queries the DB and probes filesystem state (logs, mounts) to generate <code>.prom</code> files.</li> <li>Systemd Timers: Instead of a long-running daemon, we use <code>healtharchive-crawl-metrics-textfile.timer</code> to run the script every minute. This avoids memory leaks and makes the monitoring itself robust and stateless.</li> <li>State-File Coupling: The crawler (<code>archive_tool</code>) writes a <code>.archive_state.json</code> file. The monitoring script consumes this. This decoupling means the monitor doesn't need to query the Docker container directly.</li> </ol>"},{"location":"decisions/2026-01-19-ops-first-monitoring/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-01-19-ops-first-monitoring/#positive","title":"Positive","text":"<ul> <li>Simplicity: No new long-running services to manage.</li> <li>Robustness: If the monitor crashes, systemd restarts it next minute.</li> <li>Decoupling: Monitoring logic is separate from core crawler logic.</li> </ul>"},{"location":"decisions/2026-01-19-ops-first-monitoring/#negative","title":"Negative","text":"<ul> <li>Latency: Metrics are updated minutely, not real-time (acceptable for long-running crawls).</li> <li>Disk I/O: Constant reading of logs/state files (mitigated by <code>tail</code> logic).</li> </ul>"},{"location":"decisions/2026-01-19-ops-first-monitoring/#status","title":"Status","text":"<p>Accepted and Implemented (Phase 4 &amp; 6 of Hardening Mission).</p>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/","title":"Decision: Annual crawl throughput and WARC-first artifacts (2026-01-23)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#context","title":"Context","text":"<ul> <li>HealthArchive\u2019s annual campaign is completeness-first within explicit scope boundaries and search-first for readiness.</li> <li>Production runs on a single VPS (Hetzner <code>cx33</code>: 4 vCPU / 8GB RAM / 80GB SSD) with optional StorageBox for cold storage.</li> <li>The backend indexes WARCs into <code>Snapshot</code> rows; it does not read <code>.zim</code> files. Building ZIMs during the campaign adds wall-clock time and failure surface without improving \u201csearch readiness\u201d.</li> <li>Large, browser-driven crawls (notably <code>canada.ca</code>) benefit from modest parallelism and a larger container <code>/dev/shm</code> to reduce timeouts/stalls and avoid restart churn.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#decision","title":"Decision","text":"<ul> <li>Annual campaign jobs will default to modest crawl parallelism on the single VPS:</li> <li><code>tool_options.initial_workers = 2</code></li> <li><code>tool_options.docker_shm_size = \"1g\"</code></li> <li><code>tool_options.stall_timeout_minutes = 60</code> for <code>canada.ca</code> sources</li> <li>Annual campaign jobs will default to WARC-first artifacts:</li> <li><code>tool_options.skip_final_build = True</code> to skip optional <code>.zim</code> generation during the campaign.</li> <li>Shared-host <code>canada.ca</code> sources will default to querystring-averse scope rules for content paths to reduce duplicate/trap-like expansions while preserving completeness within intended boundaries.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#rationale","title":"Rationale","text":"<ul> <li>Using <code>2</code> workers on a 4 vCPU host is a conservative way to improve throughput without introducing multi-job concurrency complexity.</li> <li>Increasing container <code>/dev/shm</code> is a low-risk stability improvement for browser-driven crawls.</li> <li>Skipping ZIM generation keeps the critical path focused on: crawl WARCs \u2192 index \u2192 searchable. ZIMs can be generated later as a secondary artifact if desired.</li> <li>Excluding querystring/fragment variants for <code>canada.ca</code> content paths reduces duplicate work and the risk of trap-like URL expansions without relying on page/depth caps.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Keep <code>initial_workers=1</code> \u2014 rejected: underutilizes the host and increases the likelihood a single slow URL dominates wall-clock progress.</li> <li>Build <code>.zim</code> during the annual campaign \u2014 rejected: increases time-to-search-readiness and adds an extra failure surface; backend does not require ZIMs.</li> <li>Add page/depth caps \u2014 rejected: conflicts with completeness-first goals and risks silent truncation.</li> <li>Run multiple sources concurrently \u2014 deferred: increases ops surface area and complicates resource contention on a small VPS.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#positive","title":"Positive","text":"<ul> <li>Faster wall-clock progress on the annual campaign without changing \u201cwhat is in scope\u201d.</li> <li>Reduced time lost to stalls/restarts on browser-driven crawls.</li> <li>Clear separation between \u201csearch readiness\u201d (WARCs indexed) and optional offline artifacts (ZIMs).</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#negative-risks","title":"Negative / risks","text":"<ul> <li>Slightly higher load on target sites due to parallelism; mitigation is modest concurrency and monitoring.</li> <li>Excluding querystring variants can omit some non-canonical pages; mitigation is explicit scope review if a source relies on query-driven content.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Rollout is via:</li> <li><code>ha_backend.job_registry</code> defaults for annual sources.</li> <li><code>archive_tool</code> support for <code>--skip-final-build</code> and <code>--docker-shm-size</code>.</li> <li>Verify with:</li> <li>crawl metrics (progress age, stalled flag, restart rate),</li> <li>successful indexing immediately after crawl completion,</li> <li>spot-check that <code>canada.ca</code> scope still matches canonical content URLs and continues to capture referenced assets.</li> </ul> <p>Rollback:</p> <ul> <li>Set annual defaults back to <code>initial_workers=1</code>, remove <code>docker_shm_size</code>, and set <code>skip_final_build=false</code>.</li> <li>Redeploy backend and restart the worker.</li> </ul>"},{"location":"decisions/2026-01-23-annual-crawl-throughput-and-artifacts/#references","title":"References","text":"<ul> <li>Related canonical docs:</li> <li><code>docs/operations/annual-campaign.md</code></li> <li><code>docs/reference/archive-tool.md</code></li> <li><code>docs/architecture.md</code></li> <li>Related monitoring:</li> <li><code>docs/operations/monitoring-and-alerting.md</code></li> <li>Related prior decision:</li> <li><code>docs/decisions/2026-01-19-annual-crawl-resiliency-and-queue-order.md</code></li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/","title":"Decision: Single-VPS ops automation guardrails for crawl + storage recovery (2026-01-24)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#context","title":"Context","text":"<ul> <li>The single-VPS production deployment experienced a failure mode where Storage Box / FUSE-backed paths became stale and raised <code>OSError: [Errno 107] Transport endpoint is not connected</code>, which led to:</li> <li>a retry storm (tight re-pick loop on a fast-failing <code>retryable</code> job), and</li> <li>periods where crawl progress stopped until manual operator intervention.</li> <li>We need automation that improves resilience without increasing the chance of data loss or corrupting in-flight crawl outputs.</li> <li>Constraints:</li> <li>single host, limited ops capacity, completeness-first crawl policy</li> <li>automation must be safe-by-default and easy to disable instantly</li> <li>observability must be strong enough to debug incidents without log-diving</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#decision","title":"Decision","text":"<ul> <li>We will implement conservative, sentinel-gated watchdog automation for crawl/storage recovery on the single VPS:</li> <li>bounded and rate-limited,</li> <li>idempotent where possible,</li> <li>heavily biased toward \u201cskip\u201d instead of risky actions,</li> <li>and emitting Prometheus textfile metrics for alerting and forensics.</li> <li>We will provide an operator-friendly, detached job execution path for re-running a specific crawl job without keeping SSH sessions open.</li> <li>We will keep documentation English-only (docs portal policy) and capture operational facts and follow-ups in incident notes + implemented roadmaps.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#rationale","title":"Rationale","text":"<ul> <li>Sentinel-gated timers and strong rate limits reduce the risk of automation doing harm during partial outages or deploys.</li> <li>Textfile metrics (node_exporter) make it possible to alert and to diagnose \u201cstuck but not down\u201d failure modes quickly.</li> <li>Detached job execution via transient systemd units prevents operator error and reduces the need for long-lived interactive sessions.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Add more aggressive auto-recovery (always unmount/remount immediately).</li> <li>Rejected: too risky when a crawl container may be writing; potential for partial writes or frontier loss.</li> <li>Add new infrastructure (multi-host, queue workers, object storage).</li> <li>Rejected for now: out of scope for immediate single-VPS stability improvements.</li> <li>Leave recovery fully manual.</li> <li>Rejected: does not meet operational goals; increases toil and time-to-recovery.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#positive","title":"Positive","text":"<ul> <li>Retry storms are mitigated and stale hot paths can be repaired earlier (including for queued/retryable jobs).</li> <li>Operators can re-run jobs without \u201ckeeping a terminal open\u201d.</li> <li>Alerts can be based on stable metrics instead of ad-hoc log greps.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#negative-risks","title":"Negative / risks","text":"<ul> <li>Automation adds moving parts; misconfiguration could cause unnecessary churn or flapping if safety caps are removed.</li> <li>Some recovery steps remain intentionally manual when they intersect with in-flight writes; this trades faster auto-recovery for safety.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Verify watchdog scripts in dry-run modes and via unit tests.</li> <li>On the VPS, enable the timers only after verifying sentinel files exist and <code>vps-crawl-status.sh</code> shows healthy metrics.</li> <li>Rollback: remove sentinel files and disable timers; revert to manual playbooks.</li> </ul>"},{"location":"decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage/#references","title":"References","text":"<ul> <li>Related canonical docs:</li> <li><code>docs/operations/monitoring-and-alerting.md</code></li> <li><code>docs/deployment/systemd/README.md</code></li> <li>Related incident notes:</li> <li><code>docs/operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop.md</code></li> <li>Related implemented roadmap:</li> <li><code>docs/planning/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening.md</code></li> </ul>"},{"location":"decisions/2026-02-03-crawl-auto-recover-queue-fill/","title":"Decision: Crawl auto-recover also fills underfilled annual crawls (2026-02-03)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-02-03-crawl-auto-recover-queue-fill/#context","title":"Context","text":"<ul> <li>Annual campaigns can have multiple crawl jobs running concurrently (worker + detached <code>systemd-run</code> units).</li> <li>The crawl auto-recover watchdog can correctly detect stalled running jobs and mark them <code>retryable</code>.</li> <li>A recovered job can remain <code>retryable</code> indefinitely when the worker is already busy running another crawl, leaving the annual campaign underfilled (reduced throughput, slow completion).</li> <li>We want a durable, automated way to restore the intended annual crawl concurrency without requiring manual operator intervention.</li> </ul>"},{"location":"decisions/2026-02-03-crawl-auto-recover-queue-fill/#decision","title":"Decision","text":"<ul> <li>Extend <code>scripts/vps-crawl-auto-recover.py</code> to optionally auto-start queued/retryable annual jobs when the campaign is underfilled and no stalled jobs are detected.</li> <li>Keep the behavior opt-in via a systemd flag (<code>--ensure-min-running-jobs</code>) and protected by safety rails (deploy lock, disk threshold, per-job daily cap).</li> <li>Treat legacy annual jobs (created before <code>campaign_kind</code> / <code>campaign_year</code> existed) as eligible for queue fill when their name/output dir matches the canonical annual suffix <code>-YYYY0101</code> (e.g., <code>phac-20260101</code>), and backfill missing campaign metadata before auto-starting.</li> </ul>"},{"location":"decisions/2026-02-03-crawl-auto-recover-queue-fill/#rationale","title":"Rationale","text":"<ul> <li>The crawl auto-recover watchdog already has the correct operational guardrails (sentinel gating, deploy lock avoidance, Prometheus textfile metrics, and a timer cadence).</li> <li>Queue fill solves a real operational gap: the system can \u201crecover\u201d a stalled job but still fail to return to the desired running set.</li> <li>Starting a job via <code>systemd-run</code> is a proven pattern on the VPS for running multiple crawls concurrently without changing the worker architecture.</li> </ul>"},{"location":"decisions/2026-02-03-crawl-auto-recover-queue-fill/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Modify the worker to run multiple jobs concurrently.</li> <li>Rejected: changes core runtime model and increases complexity/risk during active annual crawls.</li> <li>Add a separate \u201cqueue fill\u201d watchdog/service.</li> <li>Rejected: duplicates guardrails/metrics and increases the ops surface area.</li> <li>Keep manual operator intervention as the only path.</li> <li>Rejected: not sustainable; increases toil and increases time-to-recovery.</li> </ul>"},{"location":"decisions/2026-02-03-crawl-auto-recover-queue-fill/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-02-03-crawl-auto-recover-queue-fill/#positive","title":"Positive","text":"<ul> <li>Annual crawls can return to the intended concurrency automatically after a stall recovery.</li> <li>Operators can validate behavior safely with dry-run drills.</li> <li>Adds explicit observability (<code>starts_total</code>) and state tracking (<code>starts</code> history) for auto-start actions.</li> </ul>"},{"location":"decisions/2026-02-03-crawl-auto-recover-queue-fill/#negative-risks","title":"Negative / risks","text":"<ul> <li>A misconfigured minimum running target could start additional crawls when disk/headroom is insufficient.</li> <li>If a source is fundamentally broken, auto-start could create repeated start attempts (mitigated by per-job daily caps).</li> </ul>"},{"location":"decisions/2026-02-03-crawl-auto-recover-queue-fill/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Unit tests cover the auto-start decision logic, state recording, and metrics output.</li> <li>Systemd template enables queue fill by default on the VPS:</li> <li><code>docs/deployment/systemd/healtharchive-crawl-auto-recover.service</code></li> <li>Operational drill:</li> <li><code>docs/operations/playbooks/crawl/crawl-auto-recover-drills.md</code> (queue fill / auto-start section)</li> <li>Rollback:</li> <li>Remove <code>--ensure-min-running-jobs</code> from the systemd unit (or set it to <code>0</code>) and redeploy.</li> </ul>"},{"location":"decisions/2026-02-03-crawl-auto-recover-queue-fill/#references","title":"References","text":"<ul> <li>Related canonical docs:</li> <li><code>docs/operations/thresholds-and-tuning.md</code></li> <li>Related playbooks/runbooks:</li> <li><code>docs/operations/playbooks/crawl/crawl-auto-recover-drills.md</code></li> <li><code>docs/operations/playbooks/crawl/crawl-stalls.md</code></li> <li>Related decisions:</li> <li><code>docs/decisions/2026-02-03-crawl-job-db-state-reconciliation.md</code></li> </ul>"},{"location":"decisions/2026-02-03-crawl-job-db-state-reconciliation/","title":"Decision: Crawl job DB state reconciliation (2026-02-03)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-02-03-crawl-job-db-state-reconciliation/#context","title":"Context","text":"<p>HealthArchive crawl jobs have two overlapping \u201csources of truth\u201d during execution:</p> <ul> <li>DB state (<code>archive_jobs.status</code>) used by the worker, monitoring, and operator tooling.</li> <li>Runtime state (active processes / held job locks) that proves a crawl is currently running.</li> </ul> <p>During the 2026 annual campaign we observed cases where a crawl was actively running but the DB reported the job as not running (e.g., <code>retryable</code>). This drift can happen after manual operator interventions (e.g., marking a stuck job retryable), restarts, or older runners that did not reliably update DB status.</p> <p>When DB state drifts from runtime reality:</p> <ul> <li>monitoring and stall detection becomes misleading,</li> <li>automation can make incorrect decisions (e.g., \u201crecover\u201d a job that is already running),</li> <li>operators lose the ability to reason about campaign progress from <code>ha-backend</code> job listings.</li> </ul>"},{"location":"decisions/2026-02-03-crawl-job-db-state-reconciliation/#decision","title":"Decision","text":"<ul> <li>When crawl auto-recovery runs in apply mode (and automation is enabled), we will   reconcile DB job status back to <code>running</code> when a job is clearly running according to:</li> <li>a held per-job lock (strong signal), or</li> <li>an active crawl process that is attributable to the job\u2019s <code>output_dir</code> (fallback for older runners).</li> <li>Reconciliation will be:</li> <li>sentinel-gated (only when crawl auto-recover automation is enabled),</li> <li>bounded and conservative (strong signals only),</li> <li>idempotent (safe to re-run),</li> <li>dry-run visible (prints what would change when not applying).</li> </ul>"},{"location":"decisions/2026-02-03-crawl-job-db-state-reconciliation/#rationale","title":"Rationale","text":"<p>This is the smallest durable fix that keeps DB state usable as the operator interface without introducing new manual steps.</p> <p>The crawl auto-recover watchdog is already:</p> <ul> <li>periodic (timer-based),</li> <li>sentinel-gated,</li> <li>and coupled to crawl-health monitoring.</li> </ul> <p>Using it as the place to repair DB drift keeps the system consistent and avoids a second \u201coperator must remember to run X\u201d workflow.</p>"},{"location":"decisions/2026-02-03-crawl-job-db-state-reconciliation/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Do nothing; require manual DB repair \u2014 rejected because it is fragile, slow, and leads to   repeated operator confusion during long-running campaigns.</li> <li>Create a separate \u201creconcile-job-state\u201d command \u2014 rejected because it adds another tool that   operators must remember to run, and it would still need the same safety rules/signals.</li> <li>Treat runtime state as truth for monitoring only (no DB writes) \u2014 rejected because drift   persists and downstream tooling continues to show incorrect job state.</li> </ul>"},{"location":"decisions/2026-02-03-crawl-job-db-state-reconciliation/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-02-03-crawl-job-db-state-reconciliation/#positive","title":"Positive","text":"<ul> <li><code>ha-backend list-jobs --status running</code> reflects reality during campaigns.</li> <li>Stall detection and recovery logic operates on accurate <code>status=running</code> jobs.</li> <li>Fewer unsafe/incorrect recoveries when jobs were manually marked <code>retryable</code> but still running.</li> </ul>"},{"location":"decisions/2026-02-03-crawl-job-db-state-reconciliation/#negative-risks","title":"Negative / risks","text":"<ul> <li>Risk of incorrectly marking a job <code>running</code> if the signal is misattributed.</li> <li>Mitigation: prefer held job locks; fallback process matching is conservative and requires a     job to look \u201cin-flight\u201d (<code>started_at</code> set, <code>finished_at</code> unset) and to match output-dir related     processes.</li> </ul>"},{"location":"decisions/2026-02-03-crawl-job-db-state-reconciliation/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Verify in dry-run/drill mode using <code>scripts/vps-crawl-auto-recover.py</code> with simulated jobs.</li> <li>In production apply mode, watchdog logs will show reconciliation actions, e.g.:</li> <li>\u201csynced \u2026 job(s) to status=running based on held job locks\u201d</li> <li>\u201csynced \u2026 job(s) to status=running based on active crawl processes\u201d</li> <li>Rollback: disable crawl auto-recover automation by removing the sentinel file   <code>/etc/healtharchive/crawl-auto-recover-enabled</code> (and/or revert the change).</li> </ul>"},{"location":"decisions/2026-02-03-crawl-job-db-state-reconciliation/#references","title":"References","text":"<ul> <li>Implementation: <code>scripts/vps-crawl-auto-recover.py</code> (DB drift reconciliation block)</li> <li>Operator docs: <code>docs/operations/playbooks/crawl/crawl-stalls.md</code>, <code>docs/operations/playbooks/crawl/crawl-auto-recover-drills.md</code></li> <li>Related thresholds: <code>docs/operations/thresholds-and-tuning.md</code></li> </ul>"},{"location":"decisions/2026-02-06-per-source-crawl-profiles-and-annual-reconciliation/","title":"Decision: Per-source crawl profiles + annual reconciliation (2026-02-06)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-02-06-per-source-crawl-profiles-and-annual-reconciliation/#context","title":"Context","text":"<ul> <li>We run long-lived annual crawls for multiple sources with meaningfully different behavior (crawl rate, blocking/noise, restart tolerance).</li> <li>A single global set of crawl tool options forces bad tradeoffs:</li> <li>too strict for \u201cnoisy\u201d sources (unnecessary restarts),</li> <li>too lax for \u201cclean\u201d sources (slower detection of real problems),</li> <li>and makes tuning changes hard to apply safely to already-created annual jobs.</li> <li>Constraints:</li> <li>Do not compromise crawl completeness or accuracy.</li> <li>Prefer changes that are safe under repeated restarts and partial failures.</li> <li>Keep operator workflows simple (one command to reconcile; visibility via metrics/alerts).</li> </ul>"},{"location":"decisions/2026-02-06-per-source-crawl-profiles-and-annual-reconciliation/#decision","title":"Decision","text":"<ul> <li>We will maintain per-source crawl tuning profiles in code and use them when creating annual jobs.</li> <li>We will provide an operator-safe reconciliation mechanism to update tool options on already-created annual jobs without editing the DB manually.</li> <li>We will treat crawl churn (restarts/new crawl phases) as an operational signal and alert on sustained churn.</li> </ul>"},{"location":"decisions/2026-02-06-per-source-crawl-profiles-and-annual-reconciliation/#rationale","title":"Rationale","text":"<p>Per-source profiles keep the default configuration aligned with reality: different sites have different noise and block patterns. A reconciliation command gives us a controlled way to apply improved defaults to existing annual jobs (so retries/restarts adopt the new settings) without sacrificing reproducibility or integrity.</p>"},{"location":"decisions/2026-02-06-per-source-crawl-profiles-and-annual-reconciliation/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Keep one global profile \u2014 rejected: forces poor compromises and increases churn.</li> <li>Tune ad-hoc in production DB for each annual job \u2014 rejected: error-prone, hard to audit, and drifts from code defaults.</li> <li>Recreate annual jobs to apply new defaults \u2014 rejected: operationally risky and makes continuity/completeness harder to reason about.</li> </ul>"},{"location":"decisions/2026-02-06-per-source-crawl-profiles-and-annual-reconciliation/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-02-06-per-source-crawl-profiles-and-annual-reconciliation/#positive","title":"Positive","text":"<ul> <li>Tuning is explicit, per-source, and versioned with code.</li> <li>Operators have a single reconciliation workflow for already-created annual jobs.</li> <li>Monitoring can distinguish \u201cslow crawl\u201d vs \u201cchurn\u201d problems per source.</li> </ul>"},{"location":"decisions/2026-02-06-per-source-crawl-profiles-and-annual-reconciliation/#negative-risks","title":"Negative / risks","text":"<ul> <li>Profiles can become stale; requires periodic review based on observed behavior.</li> <li>Reconciliation updates don\u2019t change a currently running container mid-flight; improvements apply on the next restart/retry cycle.</li> </ul>"},{"location":"decisions/2026-02-06-per-source-crawl-profiles-and-annual-reconciliation/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Verify the reconciliation command reports intended deltas before applying:</li> <li><code>ha-backend reconcile-annual-tool-options --year &lt;YEAR&gt;</code></li> <li><code>ha-backend reconcile-annual-tool-options --year &lt;YEAR&gt; --apply</code></li> <li>Verify metrics reflect:</li> <li>per-source crawl rate alerts (<code>HealthArchiveCrawlRateSlow*</code>)</li> <li>churn alert (<code>HealthArchiveCrawlNewPhaseChurn</code>)</li> <li>Rollback: revert profile changes in <code>ha_backend/job_registry.py</code> and re-run reconciliation (apply) to restore prior options for annual jobs.</li> </ul>"},{"location":"decisions/2026-02-06-per-source-crawl-profiles-and-annual-reconciliation/#references","title":"References","text":"<ul> <li>Related canonical docs:</li> <li><code>docs/operations/thresholds-and-tuning.md</code></li> <li><code>docs/operations/monitoring-and-alerting.md</code></li> <li>Related playbooks/runbooks:</li> <li><code>docs/tutorials/debug-crawl.md</code></li> <li><code>docs/operations/playbooks/annual-campaign.md</code></li> </ul>"},{"location":"decisions/2026-02-07-git-first-vps-changes/","title":"Decision: Git-first VPS changes; keep /opt checkout clean (2026-02-07)","text":"<p>Status: accepted</p>"},{"location":"decisions/2026-02-07-git-first-vps-changes/#context","title":"Context","text":"<ul> <li>The production VPS uses a git checkout at <code>/opt/healtharchive-backend</code> as the deploy source of truth.</li> <li><code>scripts/vps-deploy.sh</code> (and related automation) intentionally refuses a dirty working tree because it makes deploys non-reproducible and breaks rollback semantics.</li> <li>Operator friction was observed when ad-hoc <code>scp</code>-copied scripts (or local edits) landed in <code>/opt/healtharchive-backend</code>, causing:</li> <li>dirty-tree deploy failures,</li> <li>configuration drift and \u201cit works on this machine\u201d behavior,</li> <li>unclear provenance of production changes.</li> <li>This project is currently solo-operated; the lowest-cost way to stay sane is to make the \u201cright path\u201d the easiest path.</li> </ul>"},{"location":"decisions/2026-02-07-git-first-vps-changes/#decision","title":"Decision","text":"<ul> <li>We will treat <code>/opt/healtharchive-backend</code> as a git-managed deploy artifact only:</li> <li>no ad-hoc <code>scp</code> into that working tree,</li> <li>no uncommitted production edits in that tree.</li> <li>We will distribute operator helpers (deploy wrappers, diagnostics scripts) via git:</li> <li>land in repo,</li> <li>deploy/pull to VPS,</li> <li>optionally install to <code>/usr/local/bin</code> from the pulled checkout.</li> <li>We will use a clearly labeled backend-only deploy mode only when the public frontend is externally broken (example: Vercel <code>402</code>), and keep the default deploy gate strict.</li> </ul>"},{"location":"decisions/2026-02-07-git-first-vps-changes/#rationale","title":"Rationale","text":"<ul> <li>Git-based deploys preserve provenance, make rollbacks deterministic, and keep \u201cwhat is running\u201d legible.</li> <li>A clean <code>/opt</code> checkout is a hard precondition for reliable automation and incident response.</li> <li>Avoiding <code>scp</code> eliminates a common source of silent drift and broken deploy gates.</li> </ul>"},{"location":"decisions/2026-02-07-git-first-vps-changes/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Continue allowing ad-hoc <code>scp</code> into <code>/opt/healtharchive-backend</code> \u2014 rejected: leads to dirty-tree deploy failures and untracked production drift.</li> <li>Maintain a separate \u201cops scripts\u201d copy outside git (manual sync) \u2014 rejected: additional moving parts, worse provenance, easy to forget.</li> </ul>"},{"location":"decisions/2026-02-07-git-first-vps-changes/#consequences","title":"Consequences","text":""},{"location":"decisions/2026-02-07-git-first-vps-changes/#positive","title":"Positive","text":"<ul> <li>Deploys remain reproducible and rollbackable.</li> <li>Automation that depends on \u201crepo state\u201d behaves predictably.</li> <li>Operator workflow becomes simpler: <code>git pull</code>, then run the standard command.</li> </ul>"},{"location":"decisions/2026-02-07-git-first-vps-changes/#negative-risks","title":"Negative / risks","text":"<ul> <li>Urgent hotfixes require a git change (or temporary workaround outside <code>/opt</code>), which can feel slower in the moment.</li> <li>Operators must resist the temptation to \u201cjust scp a script in\u201d.</li> </ul>"},{"location":"decisions/2026-02-07-git-first-vps-changes/#verification-rollout","title":"Verification / rollout","text":"<ul> <li>Verification:</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; git status --porcelain</code> is empty before deploys.</li> <li>Use <code>./scripts/vps-hetzdeploy.sh</code> (or installed <code>/usr/local/bin/hetzdeploy</code>) for routine deploys.</li> <li>Rollback:</li> <li><code>git log --oneline</code> to identify the previous known-good SHA.</li> <li><code>git checkout &lt;sha&gt;</code> (or <code>git revert</code> in repo + redeploy) depending on the change type.</li> </ul>"},{"location":"decisions/2026-02-07-git-first-vps-changes/#references","title":"References","text":"<ul> <li>Related canonical docs:</li> <li><code>docs/operations/playbooks/core/deploy-and-verify.md</code></li> <li><code>docs/deployment/production-single-vps.md</code></li> <li>Related implementation plans:</li> <li><code>docs/planning/implemented/2026-02-07-deploy-workflow-hardening.md</code></li> <li>Related scripts:</li> <li><code>scripts/vps-deploy.sh</code></li> <li><code>scripts/vps-hetzdeploy.sh</code></li> </ul>"},{"location":"deployment/","title":"Deployment docs","text":""},{"location":"deployment/#start-here","title":"Start Here","text":"<p>Deploying to production? - Main: Production Runbook \u2014 Current production setup (Hetzner VPS) - Config: Configuration \u2014 Cross-repo env vars - Checklist: Hosting Checklist \u2014 DNS, CORS, Vercel</p> <p>Quick reference: | Task | Documentation | |------|---------------| | Deploy to VPS | Production Runbook | | Configure environment | Configuration | | Setup systemd services | Systemd Units | | Rollback search changes | Search Rollout |</p>"},{"location":"deployment/#all-deployment-documentation","title":"All Deployment Documentation","text":"<ul> <li>Current production runbook: <code>production-single-vps.md</code></li> <li>Includes the recommended deploy helper: <code>scripts/vps-deploy.sh</code></li> <li>Runbook template (for new runbooks): <code>../_templates/runbook-template.md</code></li> <li>Systemd unit templates (annual scheduling, worker priority, replay reconcile): <code>systemd/README.md</code></li> <li>Search ranking rollout: <code>search-rollout.md</code></li> <li>Deployment checklist / Vercel wiring: <code>hosting-and-live-server-to-dos.md</code></li> <li>Cross\u2011repo env vars + host matrix: <code>environments-and-configuration.md</code></li> <li>Generic checklists:</li> <li><code>production-rollout-checklist.md</code></li> <li><code>staging-rollout-checklist.md</code> (optional future)</li> </ul>"},{"location":"deployment/disaster-recovery/","title":"Disaster Recovery Runbook","text":"<p>Last Updated: 2026-01-18 Status: Active</p>"},{"location":"deployment/disaster-recovery/#recovery-objectives","title":"Recovery Objectives","text":"<p>In the context of HealthArchive, these objectives define our boundaries for data loss and downtime during a major failure.</p> <ul> <li>RPO (Recovery Point Objective): The maximum age of files that must be recovered from backup storage for operations to resume. It defines our \"data loss tolerance.\"</li> <li>RTO (Recovery Time Objective): The maximum duration of time within which service must be restored after a disaster. It defines our \"downtime tolerance.\"</li> <li>MTTR (Mean Time To Recovery): The average time taken to repair a failed component and return it to service.</li> </ul>"},{"location":"deployment/disaster-recovery/#rpo-recovery-point-objective","title":"RPO (Recovery Point Objective)","text":"<p>Target: 24 hours</p> <p>Rationale: - We perform nightly backups of the database and configuration. - Crawl data (WARCs) is tiered to storage regularly. - Up to 24 hours of data loss (recent crawls, user actions) is considered acceptable for the current service criticality level (research access, no real-time critical operational dependencies). Data can often be re-crawled.</p>"},{"location":"deployment/disaster-recovery/#rto-recovery-time-objective","title":"RTO (Recovery Time Objective)","text":"<p>Target: 8 hours</p> <p>Rationale: - Recovery involves manual provisioning of a new VPS, installing dependencies, and restoring from backup. - This timeframe allows a single operator to perform these steps during a standard workday.</p>"},{"location":"deployment/disaster-recovery/#mttr-mean-time-to-recovery","title":"MTTR (Mean Time To Recovery)","text":"<p>Target: 4 hours</p> <p>Rationale: - For partial failures (e.g., service restart, database recovery without full VPS loss), we aim to restore service within 4 hours.</p>"},{"location":"deployment/disaster-recovery/#when-to-revisit","title":"When to Revisit","text":"<p>These targets should be reviewed: - Annually: During the full DR drill. - Service Changes: If the service criticality increases (e.g., adding real-time users). - Architecture Changes: If moving from a single VPS to a multi-node/HA setup. - Scale Changes: If the dataset size grows significantly enough to impact restoration times.</p>"},{"location":"deployment/disaster-recovery/#scenarios","title":"Scenarios","text":""},{"location":"deployment/disaster-recovery/#scenario-a-complete-vps-loss-nas-backup-available","title":"Scenario A: Complete VPS loss (NAS backup available)","text":"<p>Most likely DR scenario. Requires provisioning new VPS and restoring from offsite NAS backup.</p>"},{"location":"deployment/disaster-recovery/#scenario-b-database-corruption","title":"Scenario B: Database corruption","text":"<p>Restoration from local or NAS <code>pg_dump</code>.</p>"},{"location":"deployment/disaster-recovery/#scenario-c-storage-failure","title":"Scenario C: Storage failure","text":"<p>Recovery of WARC files from tiered storage or accepted data loss.</p>"},{"location":"deployment/disaster-recovery/#procedures","title":"Procedures","text":""},{"location":"deployment/disaster-recovery/#1-vps-complete-restoration-scenario-a","title":"1. VPS Complete Restoration (Scenario A)","text":"<p>Prerequisites: - Access to Hetzner Cloud Console. - Access to Synology NAS (via physical access or alternative network if Tailscale is down). - SSH key for <code>haadmin</code> available locally.</p>"},{"location":"deployment/disaster-recovery/#step-1-provision-new-vps","title":"Step 1: Provision New VPS","text":"<ol> <li> <p>Create Server: Follow the standard provisioning steps in Production Single VPS.</p> <ul> <li>Image: Ubuntu 24.04 LTS.</li> <li>Updates: <code>sudo apt update &amp;&amp; sudo apt upgrade -y</code>.</li> <li>User: Create <code>haadmin</code> user and harden SSH.</li> </ul> </li> <li> <p>Configure Networking:</p> <ul> <li>Set up Firewall rules (Allow 80/443, block 22 public, allow Tailscale UDP).</li> </ul> </li> </ol>"},{"location":"deployment/disaster-recovery/#step-2-install-base-dependencies","title":"Step 2: Install Base Dependencies","text":"<p>Run as <code>haadmin</code>: <pre><code>sudo apt install -y docker.io postgresql postgresql-contrib python3-venv python3-pip git curl build-essential pkg-config unzip\nsudo systemctl enable --now docker postgresql\n</code></pre></p>"},{"location":"deployment/disaster-recovery/#step-3-re-join-tailscale","title":"Step 3: Re-join Tailscale","text":"<ol> <li>Install Tailscale: <code>curl -fsSL https://tailscale.com/install.sh | sh</code>.</li> <li>Authenticate: <code>sudo tailscale up --ssh</code>.<ul> <li>Note: If possible, reuse the old IP/hostname from the admin console to simplify ACLs, or update ACLs to trust the new node.</li> </ul> </li> </ol>"},{"location":"deployment/disaster-recovery/#step-4-prepare-directories","title":"Step 4: Prepare Directories","text":"<pre><code>sudo groupadd --system healtharchive\nsudo mkdir -p /srv/healtharchive/{jobs,backups,ops}\nsudo chown -R haadmin:haadmin /srv/healtharchive/jobs\nsudo chown root:healtharchive /srv/healtharchive/backups /srv/healtharchive/ops\nsudo chmod 2770 /srv/healtharchive/backups /srv/healtharchive/ops\n</code></pre>"},{"location":"deployment/disaster-recovery/#step-5-retrieve-backup-from-nas","title":"Step 5: Retrieve Backup from NAS","text":"<p>If Tailscale is up on both ends: 1.  SSH to NAS: <code>ssh user@nas-ip</code>. 2.  Rsync backup to new VPS:     <pre><code>rsync -av /volume1/nobak/healtharchive/backups/db/latest.dump haadmin@new-vps-ip:/srv/healtharchive/backups/\n</code></pre> Alternatively, pull from VPS: <pre><code>scp user@nas-ip:/path/to/backup.dump /srv/healtharchive/backups/latest.dump\n</code></pre></p>"},{"location":"deployment/disaster-recovery/#step-6-restore-database","title":"Step 6: Restore Database","text":"<ol> <li>Create DB and User:     <pre><code>sudo -u postgres psql -c \"CREATE USER healtharchive WITH PASSWORD '&lt;password_from_backup_env&gt;';\"\nsudo -u postgres psql -c \"CREATE DATABASE healtharchive OWNER healtharchive;\"\n</code></pre></li> <li>Restore Schema and Data:     <pre><code>sudo -u postgres pg_restore -d healtharchive /srv/healtharchive/backups/latest.dump\n</code></pre></li> </ol>"},{"location":"deployment/disaster-recovery/#step-7-restore-application","title":"Step 7: Restore Application","text":"<ol> <li>Clone Repository:     <pre><code>git clone https://github.com/jerdaw/healtharchive-backend.git /opt/healtharchive-backend\ncd /opt/healtharchive-backend\npython3 -m venv .venv\n./.venv/bin/pip install -e \".[dev]\" \"psycopg[binary]\"\n</code></pre></li> <li>Restore Configuration:<ul> <li>Restore <code>/etc/healtharchive/backend.env</code> from your distinct secure offsite storage (e.g., password manager notes). Do not lose this file.</li> <li>If needed, regenerate the <code>ADMIN_TOKEN</code>.</li> </ul> </li> </ol>"},{"location":"deployment/disaster-recovery/#step-8-re-mount-storage-restore-warcs","title":"Step 8: Re-mount Storage / Restore WARCs","text":"<ul> <li>Mount the Storage Box (tiered storage) to <code>/srv/healtharchive/storagebox</code> using <code>sshfs</code> (see <code>production-single-vps.md</code>).</li> <li>If local WARCs were lost (<code>/srv/healtharchive/jobs</code>), you have two options:<ol> <li>Rescan: If files exist on Storage Box, re-import headers (slow).</li> <li>Empty Start: Start with empty local jobs; historical data remains on Storage Box/index.</li> </ol> </li> </ul>"},{"location":"deployment/disaster-recovery/#2-database-intact-restoration-scenario-b","title":"2. Database Intact Restoration (Scenario B)","text":"<p>Use this procedure when the VPS is running but the database is corrupted or dropped.</p> <p>Prerequisites: - Backup file available (local or NAS). - PostgreSQL service is running.</p>"},{"location":"deployment/disaster-recovery/#step-1-locate-backup","title":"Step 1: Locate Backup","text":"<ul> <li>Format: <code>pg_dump -Fc</code> (custom format, compressed).</li> <li>Local: <code>/srv/healtharchive/backups/</code><ul> <li>Naming: <code>healtharchive_&lt;timestamp&gt;.dump</code></li> <li>Retention: 14 days.</li> </ul> </li> <li>NAS: <code>/volume1/nobak/healtharchive/backups/db/</code> (needs retrieval)<ul> <li>Retention: Long-term/Permanent.</li> </ul> </li> </ul>"},{"location":"deployment/disaster-recovery/#step-2-restore-database","title":"Step 2: Restore Database","text":"<p>Warning: This will overwrite the current database state.</p> <ol> <li> <p>Drop and Recreate: <pre><code>sudo -u postgres dropdb --if-exists healtharchive_restored\nsudo -u postgres createdb healtharchive_restored\n</code></pre></p> </li> <li> <p>Restore from Dump: <pre><code># Replace &lt;backup_file&gt; with actual filename\nsudo -u postgres pg_restore -d healtharchive_restored -Fc /path/to/backup.dump\n</code></pre></p> </li> <li> <p>Verify Restoration:     Check that tables are populated:     <pre><code>sudo -u postgres psql -d healtharchive_restored -c \"SELECT count(*) FROM snapshots;\"\n</code></pre></p> </li> <li> <p>Swap Databases:     Stop services to preventing locking:     <pre><code>sudo systemctl stop healtharchive-api healtharchive-worker\n</code></pre></p> <p>Swap: <pre><code>sudo -u postgres psql -c \"ALTER DATABASE healtharchive RENAME TO healtharchive_old;\"\nsudo -u postgres psql -c \"ALTER DATABASE healtharchive_restored RENAME TO healtharchive;\"\n</code></pre></p> </li> <li> <p>Restart Services: <pre><code>sudo systemctl start healtharchive-api healtharchive-worker\n</code></pre></p> </li> </ol>"},{"location":"deployment/disaster-recovery/#step-3-integrity-verification","title":"Step 3: Integrity Verification","text":"<ul> <li>Row Counts: Compare <code>SELECT count(*) FROM snapshots</code> with expected values.</li> <li>Recent Data: Check for the most recent captures <code>SELECT * FROM snapshots ORDER BY id DESC LIMIT 5;</code>.</li> <li>Foreign Keys: <code>pg_restore</code> would have failed on constraint violations, but check application logs for ORM errors.</li> <li>Orphaned Records: Ensure core relations are intact:   <pre><code>-- Check for snapshots without sources\nSELECT count(*) FROM snapshots WHERE source_id NOT IN (SELECT id FROM sources);\n</code></pre></li> </ul>"},{"location":"deployment/disaster-recovery/#step-4-partial-restoration-advanced","title":"Step 4: Partial Restoration (Advanced)","text":"<ul> <li>Specific Table: Use <code>pg_restore -t &lt;tablename&gt;</code> to restore only one table to a temp DB, then copy data.</li> <li>Verify on Separate Server: For high-stakes restorations, perform the restoration on a development or temporary VPS first to verify integrity before swapping production.</li> <li>Point-in-Time: Requires WAL archiving (currently not enabled; rely on nightly dumps).</li> </ul>"},{"location":"deployment/disaster-recovery/#3-archive-root-recovery-scenario-c","title":"3. Archive Root Recovery (Scenario C)","text":"<p>Use this procedure when WARC files or the archive storage structure is compromised.</p> <p>Archive Root Structure: <pre><code>/srv/healtharchive/jobs/\n\u251c\u2500\u2500 &lt;source_slug&gt;-&lt;year&gt;-&lt;month&gt;/  # Job Output Directories\n\u2502   \u251c\u2500\u2500 warcs/                     # Stable WARC files\n\u2502   \u2502   \u251c\u2500\u2500 manifest.json          # Mapping of source -&gt; stable filenames\n\u2502   \u2502   \u2514\u2500\u2500 warc-000001.warc.gz\n\u2502   \u251c\u2500\u2500 provenance/                # Metadata preservation\n\u2502   \u2502   \u2514\u2500\u2500 archive_state.json\n\u2502   \u2514\u2500\u2500 logs/\n\u2514\u2500\u2500 tiered/                        # Mount point for cold storage (Storage Box)\n</code></pre></p>"},{"location":"deployment/disaster-recovery/#recovery-scenarios","title":"Recovery Scenarios","text":"<p>Case 1: Local WARCs lost (e.g., accidental deletion), Tiered storage intact This is the most common recovery case. 1.  Check Tiered Storage: Verify header-only WARCs or full files exist in <code>/srv/healtharchive/storagebox</code>.     2.  Re-import Headers/WARCs (Slow but safe):         If the database is intact, you don't need the local WARCs effectively immediately for the site to work, but the Replay service will fail for those snapshots.         To restore replayability, copy the WARCs back from tiered storage:         <pre><code># Example: Restore specific job\nrsync -av /srv/healtharchive/storagebox/jobs/hc-2026-01/ /srv/healtharchive/jobs/hc-2026-01/\n</code></pre>     3.  Verify against Manifest: <pre><code># Check that all files in manifest exist and have correct sizes\ncat /srv/healtharchive/jobs/hc-2026-01/warcs/manifest.json | jq .records\n</code></pre></p> <p>Case 2: Tiered storage unavailable, Local intact 1.  Run in Degraded Mode: Operations can continue using local WARCs. 2.  Disable Tiering: Stop the tiering cron job/timer to prevent errors. 3.  Restore Connection: Troubleshoot <code>sshfs</code> mount or Storage Box availability. 4.  Re-enable Tiering: Once fixed, the system will resume tiering new WARCs.</p> <p>Case 3: All copies lost (Catastrophic) 1.  Accept Data Loss: Crawl data is gone. 2.  Clean Database: You may need to truncate <code>snapshots</code> table if it references missing files, or mark them as lost. 3.  Re-crawl: Trigger new manual crawls for critical sources.</p>"},{"location":"deployment/disaster-recovery/#integrity-verification","title":"Integrity Verification","text":"<ol> <li>WARC Validation: <pre><code># Validate a single WARC file\nwarcio validate /path/to/file.warc.gz\n</code></pre></li> <li>Database Consistency:     Ensure database records point to existing files (custom script required).</li> </ol>"},{"location":"deployment/disaster-recovery/#re-tiering-and-consolidation-procedure","title":"Re-tiering and Consolidation Procedure","text":"<p>If tiered storage was wiped and replaced, or if you need to stabilize newly crawled data:</p> <ol> <li>Consolidate WARCs: Ensure files are moved from <code>.tmp*</code> to stable <code>warcs/</code> folders and manifests are updated.     <pre><code># Run as haadmin in .venv\nha-backend consolidate-warcs --id &lt;JOB_ID&gt;\n</code></pre></li> <li>Verify Local Integrity: Ensure local WARCs match their manifest and are valid.</li> <li>Force Tiering: Run the tiering command manually to re-upload everything:     <pre><code># Run as haadmin in .venv\nha-backend tier-warcs --force --dry-run  # Check first\nha-backend tier-warcs --force            # Execute\n</code></pre></li> <li>Verify Tiered Copies: Check that the files on the Storage Box match the local stable WARCs.</li> </ol>"},{"location":"deployment/disaster-recovery/#4-service-startup-sequence","title":"4. Service Startup Sequence","text":"<p>Order is critical:</p> <ol> <li>Database: <code>sudo systemctl start postgresql</code><ul> <li>Health Check: <code>sudo systemctl status postgresql</code> or <code>pg_isready</code></li> <li>Failure: Check disk space (<code>df -h</code>) and logs (<code>journalctl -u postgresql</code>).</li> </ul> </li> <li>API: <code>sudo systemctl start healtharchive-api</code><ul> <li>Health Check: <code>curl http://localhost:8001/api/health</code></li> <li>Failure: Check <code>/etc/healtharchive/backend.env</code> and <code>journalctl -u healtharchive-api -n 100</code>.</li> </ul> </li> <li>Worker: <code>sudo systemctl start healtharchive-worker</code><ul> <li>Health Check: <code>sudo systemctl status healtharchive-worker</code> (Check logs for \"Worker started\").</li> <li>Failure: Check database connectivity and logs.</li> </ul> </li> <li>Replay (Optional): Start pywb if configured.<ul> <li>Health Check: <code>curl http://localhost:8080</code> (or configured port).</li> </ul> </li> <li>Reverse Proxy: <code>sudo systemctl start caddy</code><ul> <li>Health Check: <code>sudo systemctl status caddy</code></li> <li>Failure: <code>sudo caddy validate --config /etc/caddy/Caddyfile</code>.</li> </ul> </li> </ol>"},{"location":"deployment/disaster-recovery/#5-verification-checklist","title":"5. Verification Checklist","text":"<p>Run these checks immediately after startup:</p> <ul> <li> Database Connectivity: <code>sudo -u postgres psql -d healtharchive -c 'SELECT count(*) FROM sources;'</code> (Should &gt; 0)</li> <li> API Health: <code>curl http://localhost:8001/api/health</code> -&gt; <code>{\"status\":\"ok\"}</code></li> <li> Public Endpoint (HTTPS): <code>curl -I https://api.healtharchive.ca/api/health</code> (Verify TLS works)</li> <li> Search Index: Query a known term via the frontend or API.</li> <li> Worker Health: Check logs for \"Worker started\" and no immediate crashes.</li> <li> Snapshot Viewing: Visit a known snapshot URL (e.g., the smoke test snapshot ID 1).</li> <li> Monitoring Reconnected: Confirm that Healthchecks.io, Prometheus, and external uptime monitors are receiving signals from the new VPS.</li> </ul>"},{"location":"deployment/disaster-recovery/#dr-drills","title":"DR Drills","text":"<p>Regular testing ensures that these procedures remain effective and that operators are familiar with the recovery process.</p>"},{"location":"deployment/disaster-recovery/#schedule","title":"Schedule","text":"Drill Type Frequency Next Due Owner Scope Tabletop Quarterly Q1 2026 Operator Review procedure, check credentials, identify gaps. Partial Restore Quarterly Q1 2026 Operator Restore database summary/integrity check on local dev machine. Full DR Annual 2026-06 Operator Full recovery from backup to a fresh VPS."},{"location":"deployment/disaster-recovery/#procedures_1","title":"Procedures","text":""},{"location":"deployment/disaster-recovery/#1-tabletop-drill","title":"1. Tabletop Drill","text":"<p>Objective: Verify documentation accuracy and credential availability without interacting with production.</p> <ol> <li>Read-Through: Walk through the \"Complete VPS Restoration (Scenario A)\" procedure step-by-step.</li> <li>Credential Check: Verify you can locate/access:<ul> <li>Hetzner Cloud Console password/2FA.</li> <li>Synology NAS SSH keys.</li> <li>Encrypted backup of <code>/etc/healtharchive/backend.env</code>.</li> <li>Domain DNS controls (Namecheap).</li> </ul> </li> <li>Success Criteria:<ul> <li>All restoration steps are understood and commands are valid.</li> <li>All required credentials are confirmed as accessible and current.</li> </ul> </li> <li>Documentation &amp; Follow-up:<ul> <li>Fix any broken links, outdated commands, or unclear instructions found during the read-through.</li> <li>Record findings in the Results Log (see below).</li> </ul> </li> </ol>"},{"location":"deployment/disaster-recovery/#2-partial-restoration-drill","title":"2. Partial Restoration Drill","text":"<p>Objective: Verify backup integrity and database restorability.</p> <ol> <li>Retrieve Backup: Download the latest actual <code>healtharchive_&lt;ts&gt;.dump</code> from the NAS or VPS.</li> <li>Local Restore:<ul> <li>Spin up a local Docker Postgres container or use a local dev DB.</li> <li>Run the Scenario B (Database Corruption) restoration steps against this local instance.</li> </ul> </li> <li>Success Criteria:<ul> <li><code>pg_restore</code> completes without fatal errors.</li> <li>Row counts for <code>snapshots</code> match or are within expected growth margins.</li> <li>Recent captures are present and readable.</li> </ul> </li> <li>Documentation &amp; Follow-up:<ul> <li>Record the size of the backup and restoration time in the Results Log.</li> <li>If corruption is found, investigate backup job logs and schedule an immediate re-run.</li> </ul> </li> <li>Cleanup: Delete the local test database and backup file.</li> </ol>"},{"location":"deployment/disaster-recovery/#3-full-dr-drill-annual","title":"3. Full DR Drill (Annual)","text":"<p>Objective: Prove total system recovery capability.</p> <p>Prerequisites: - Perform during low-traffic window (e.g., weekend). - Budget ~$5 for temporary VPS costs.</p> <p>Procedure: 1.  Provision: Create a new VPS (e.g., <code>dr-test-2026</code>) in Hetzner. DO NOT DELETE THE EXISTING PRODUCTION VPS. 2.  Execute Scenario A: Follow \"VPS Complete Restoration\" strictly.     - Modification: When restoring <code>backend.env</code>, change <code>HEALTHARCHIVE_PUBLIC_SITE_URL</code> to the temporary IP or a test subdomain to avoid DNS conflicts.     - Modification: Do not switch the main DNS (A record) unless you are intentionally testing failover (requires downtime). 3.  Verify &amp; Success Criteria:     - Run the complete \"Verification Checklist\" on the new host; all checks must pass.     - Verify you can pull a WARC file from tiered storage.     - Total restoration time is within the 8-hour RTO. 4.  Documentation &amp; Follow-up:     - Record total time to recovery (RTO metric) and any blockers in the Results Log.     - Update the MTTR/RTO targets if they are consistently missed or easily exceeded. 5.  Teardown:     - Destroy the temporary VPS.     - Remove the temporary node from Tailscale.</p>"},{"location":"deployment/disaster-recovery/#results-log","title":"Results Log","text":"<p>Copy and paste this template to <code>docs/operations/dr-logs/&lt;YYYY-MM-DD&gt;-drill-report.md</code>:</p> <pre><code># DR Drill Report: &lt;Date&gt;\n\n**Drill Type:** (Tabletop / Partial / Full)\n**Operator:** &lt;Name&gt;\n**Time Started:** &lt;HH:MM UTC&gt;\n**Time Finished:** &lt;HH:MM UTC&gt;\n**Total Duration:** &lt;Minutes&gt;\n\n## Outcome\n- [ ] Success (All objectives met)\n- [ ] Partial Success (Objectives met with issues)\n- [ ] Failure (Could not complete recovery)\n\n## Metric\n- **RTO Achieved:** N/A (or actual time if Full Drill)\n- **Backup Age:** &lt;Hours since last backup&gt; (RPO check)\n\n## Issues Encountered\n1. Issue description...\n\n## Documentation Updates Required\n- [ ] Update section X.Y...\n</code></pre>"},{"location":"deployment/environments-and-configuration/","title":"Environments and configuration (frontend + backend)","text":"<p>This document is the canonical cross-repo reference for how the backend (<code>healtharchive-backend</code>) and frontend (<code>healtharchive-frontend</code>) are wired together across environments.</p> <p>The root <code>ENVIRONMENTS.md</code> is a pointer to this file to avoid duplication.</p> <p>It is useful when:</p> <ul> <li>Setting or auditing environment variables (Vercel + backend host).</li> <li>Double\u2011checking that frontend hosts, backend hosts, and backend CORS settings   line up.</li> </ul> <p>For deeper operational details, see:</p> <ul> <li><code>production-single-vps.md</code> (current production runbook)</li> <li><code>hosting-and-live-server-to-dos.md</code> (high-level deployment checklist)</li> <li><code>../operations/monitoring-and-ci-checklist.md</code> (uptime/monitoring guidance)</li> <li><code>../operations/baseline-drift.md</code> (production drift checks: policy vs observed)</li> <li>Frontend docs: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/implementation-guide.md</li> <li>Frontend verification: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md</li> </ul>"},{"location":"deployment/environments-and-configuration/#1-environments-at-a-glance","title":"1) Environments at a glance","text":""},{"location":"deployment/environments-and-configuration/#what-exists-today","title":"What exists today","text":"<ul> <li>Single backend API: <code>https://api.healtharchive.ca</code></li> <li>No separate staging backend (by design)</li> <li>Backend CORS allowlist is intentionally strict:</li> <li><code>https://healtharchive.ca</code></li> <li><code>https://www.healtharchive.ca</code></li> <li><code>https://healtharchive.vercel.app</code></li> <li><code>https://replay.healtharchive.ca</code> (for the optional replay banner and direct replay UX)</li> </ul> <p>Expected limitation (by design):</p> <ul> <li>Branch preview URLs like <code>https://healtharchive-git-...vercel.app</code> may fall   back to demo mode until we explicitly allow those origins (CORS).</li> </ul>"},{"location":"deployment/environments-and-configuration/#11-validate-production-wiring-recommended","title":"1.1 Validate production wiring (recommended)","text":"<p>On the production VPS, run the baseline drift check in live mode:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/check_baseline_drift.py --mode live\n</code></pre> <p>This validates:</p> <ul> <li>env vars are set as expected (including CORS allowlist),</li> <li>HSTS is configured and observed,</li> <li>admin endpoints are protected,</li> <li>CORS headers are actually returned for the allowed origins.</li> </ul>"},{"location":"deployment/environments-and-configuration/#matrix","title":"Matrix","text":"Environment Frontend (browser origin) Backend API base Notes Local dev <code>http://localhost:3000</code> <code>http://127.0.0.1:8001</code> Local dev flow. Vercel project domain <code>https://healtharchive.vercel.app</code> <code>https://api.healtharchive.ca</code> Allowed by CORS; useful as a stable \u201cnon-custom-domain\u201d URL. Production <code>https://healtharchive.ca</code> / <code>https://www.healtharchive.ca</code> <code>https://api.healtharchive.ca</code> Primary public site. Branch previews (Vercel) <code>https://healtharchive-git-...vercel.app</code> <code>https://api.healtharchive.ca</code> May fall back to demo mode due to strict CORS. <p>Optional future:</p> Environment Frontend (browser origin) Backend API base Notes Staging API (optional) Preview URLs or a dedicated staging frontend <code>https://api-staging.healtharchive.ca</code> Only if you decide you want a separate staging backend later."},{"location":"deployment/environments-and-configuration/#2-backend-configuration-healtharchive-backend","title":"2) Backend configuration (healtharchive-backend)","text":"<p>All backend env vars are read by:</p> <ul> <li><code>src/ha_backend/config.py</code></li> <li><code>src/ha_backend/api/deps.py</code></li> <li>Search ranking selection is controlled by <code>HA_SEARCH_RANKING_VERSION</code> (and can be overridden per-request with <code>ranking=v1|v2</code> on <code>/api/search</code>).</li> </ul>"},{"location":"deployment/environments-and-configuration/#21-local-development-typical","title":"2.1 Local development (typical)","text":"<p>Example shell setup (or via <code>.env.example</code> \u2192 <code>.env</code>, git-ignored):</p> <pre><code>export HEALTHARCHIVE_ENV=development\nexport HEALTHARCHIVE_DATABASE_URL=sqlite:///$(pwd)/.dev-healtharchive.db\nexport HEALTHARCHIVE_ARCHIVE_ROOT=$(pwd)/.dev-archive-root\nexport HEALTHARCHIVE_ZIMIT_DOCKER_IMAGE=ghcr.io/openzim/zimit  # optional override (pin by tag or digest)\nexport HEALTHARCHIVE_ADMIN_TOKEN=localdev-admin\nexport HEALTHARCHIVE_LOG_LEVEL=DEBUG\nexport HA_SEARCH_RANKING_VERSION=v2\nexport HA_PAGES_FASTPATH=1\nexport HEALTHARCHIVE_REPLAY_BASE_URL=http://127.0.0.1:8090\nexport HEALTHARCHIVE_REPLAY_PREVIEW_DIR=$(pwd)/.dev-replay-previews\nexport HEALTHARCHIVE_EXPORTS_ENABLED=1\nexport HEALTHARCHIVE_EXPORTS_DEFAULT_LIMIT=1000\nexport HEALTHARCHIVE_EXPORTS_MAX_LIMIT=10000\n</code></pre>"},{"location":"deployment/environments-and-configuration/#22-production-current","title":"2.2 Production (current)","text":"<p>On the production backend host (systemd env file / Docker env / PaaS env):</p> <pre><code>export HEALTHARCHIVE_ENV=production\nexport HEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://healtharchive:&lt;DB_PASSWORD&gt;@127.0.0.1:5432/healtharchive\nexport HEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs\nexport HEALTHARCHIVE_ZIMIT_DOCKER_IMAGE=ghcr.io/openzim/zimit@sha256:&lt;PINNED_DIGEST&gt;\nexport HEALTHARCHIVE_ADMIN_TOKEN=&lt;LONG_RANDOM_SECRET&gt;\nexport HEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.ca,https://www.healtharchive.ca,https://healtharchive.vercel.app,https://replay.healtharchive.ca\nexport HEALTHARCHIVE_LOG_LEVEL=INFO\nexport HA_SEARCH_RANKING_VERSION=v2\nexport HA_PAGES_FASTPATH=1\nexport HEALTHARCHIVE_USAGE_METRICS_ENABLED=1\nexport HEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS=30\nexport HEALTHARCHIVE_CHANGE_TRACKING_ENABLED=1\nexport HEALTHARCHIVE_EXPORTS_ENABLED=1\nexport HEALTHARCHIVE_EXPORTS_DEFAULT_LIMIT=1000\nexport HEALTHARCHIVE_EXPORTS_MAX_LIMIT=10000\nexport HEALTHARCHIVE_PUBLIC_SITE_URL=https://healtharchive.ca\nexport HEALTHARCHIVE_REPLAY_BASE_URL=https://replay.healtharchive.ca\nexport HEALTHARCHIVE_REPLAY_PREVIEW_DIR=/srv/healtharchive/replay/previews\n</code></pre> <p>Notes:</p> <ul> <li><code>HEALTHARCHIVE_ADMIN_TOKEN</code> should be a long random secret stored in a secret   manager (e.g., Bitwarden + server env), never committed.</li> <li><code>HEALTHARCHIVE_ZIMIT_DOCKER_IMAGE</code> pins the crawler container image used by <code>archive_tool</code>.   Use a digest (<code>...@sha256:...</code>) in production to avoid upstream <code>latest</code> changes breaking crawls.</li> <li><code>HEALTHARCHIVE_REPLAY_BASE_URL</code> enables <code>browseUrl</code> fields in <code>/api/search</code>   and <code>/api/snapshot/{id}</code> so the frontend can embed the replay service.</li> <li><code>HEALTHARCHIVE_USAGE_METRICS_ENABLED</code> controls whether aggregated daily usage   counts are recorded; disable it for a metrics-free deployment.</li> <li><code>HEALTHARCHIVE_CHANGE_TRACKING_ENABLED</code> controls whether change tracking   endpoints/diff feeds are active (disable if you are not running the pipeline).</li> <li>Compare-live controls (public snapshot vs live diffs):</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_ENABLED</code> (default <code>1</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_TIMEOUT_SECONDS</code> (default <code>8</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_REDIRECTS</code> (default <code>4</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_BYTES</code> (default <code>2000000</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_ARCHIVE_BYTES</code> (default <code>2000000</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_RENDER_LINES</code> (default <code>5000</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_CONCURRENCY</code> (default <code>4</code>).</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_USER_AGENT</code> (default identifies HealthArchive).</li> <li>Indexing integrity (optional, Phase 4 safety rail):</li> <li><code>HEALTHARCHIVE_INDEX_WARC_VERIFY_LEVEL</code> (default <code>0</code>; allowed: <code>0|1|2</code>).</li> <li><code>HEALTHARCHIVE_INDEX_WARC_VERIFY_MAX_DECOMPRESSED_BYTES</code> (default unset; bounds Level 1 gzip checks per file).</li> <li><code>HEALTHARCHIVE_INDEX_WARC_VERIFY_MAX_RECORDS</code> (default unset; bounds Level 2 WARC iteration per file).</li> <li><code>HEALTHARCHIVE_PUBLIC_SITE_URL</code> sets the public base URL used in RSS links.</li> <li>In <code>production</code> (and <code>staging</code>), if the admin token is missing, admin/metrics   endpoints fail closed (HTTP 500) instead of being left open.</li> <li><code>HEALTHARCHIVE_CORS_ORIGINS</code> should be kept as narrow as possible; it controls   which browser origins can call public API routes.</li> <li>If you use the optional replay banner / direct replay UX, the replay origin   must also be allowed by CORS so the banner can call <code>/api/replay/resolve</code>.</li> </ul>"},{"location":"deployment/environments-and-configuration/#23-optional-staging-backend-future","title":"2.3 Optional: staging backend (future)","text":"<p>If you later add a separate staging backend, it should generally mirror production except for DB/archive root and CORS origins:</p> <pre><code>export HEALTHARCHIVE_ENV=staging\nexport HEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://healtharchive:&lt;DB_PASSWORD&gt;@127.0.0.1:5432/healtharchive_staging\nexport HEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs-staging\nexport HEALTHARCHIVE_ADMIN_TOKEN=&lt;LONG_RANDOM_SECRET&gt;\nexport HEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.vercel.app\nexport HEALTHARCHIVE_LOG_LEVEL=INFO\nexport HEALTHARCHIVE_USAGE_METRICS_ENABLED=1\nexport HEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS=30\nexport HEALTHARCHIVE_CHANGE_TRACKING_ENABLED=1\nexport HEALTHARCHIVE_EXPORTS_ENABLED=1\nexport HEALTHARCHIVE_EXPORTS_DEFAULT_LIMIT=1000\nexport HEALTHARCHIVE_EXPORTS_MAX_LIMIT=10000\nexport HEALTHARCHIVE_PUBLIC_SITE_URL=https://healtharchive.ca\n</code></pre>"},{"location":"deployment/environments-and-configuration/#3-frontend-configuration-healtharchive-frontend","title":"3) Frontend configuration (healtharchive-frontend)","text":"<p>The frontend reads env vars at build time.</p>"},{"location":"deployment/environments-and-configuration/#31-local-development","title":"3.1 Local development","text":"<p>Frontend repo <code>.env.local</code> (git-ignored):</p> <pre><code>NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8001\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=true\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=true\nNEXT_PUBLIC_SHOW_API_BASE_HINT=true\n</code></pre>"},{"location":"deployment/environments-and-configuration/#32-vercel-production-env","title":"3.2 Vercel Production env","text":"<p>In Vercel \u2192 Settings \u2192 Environment Variables \u2192 Production:</p> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api.healtharchive.ca\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=false\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=false\nNEXT_PUBLIC_SHOW_API_BASE_HINT=false\n</code></pre>"},{"location":"deployment/environments-and-configuration/#33-vercel-preview-env","title":"3.3 Vercel Preview env","text":"<p>In Vercel \u2192 Settings \u2192 Environment Variables \u2192 Preview:</p> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api.healtharchive.ca\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=true\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=true\nNEXT_PUBLIC_SHOW_API_BASE_HINT=true\n</code></pre> <p>Note:</p> <ul> <li>Even with the Preview env var set, branch preview URLs may still fall back to   demo mode unless the backend CORS allowlist includes those preview origins.</li> </ul>"},{"location":"deployment/environments-and-configuration/#4-security-notes-secrets-cors","title":"4) Security notes (secrets + CORS)","text":"<ul> <li>Never commit secrets:</li> <li>No real <code>HEALTHARCHIVE_ADMIN_TOKEN</code>, DB passwords, Healthchecks URLs, etc.</li> <li>Use placeholders in docs and store real values in Bitwarden + server/Vercel     env settings.</li> <li>CORS is a security control:</li> <li>Tight allowlists reduce accidental exposure of browser-accessible APIs.</li> <li>If you loosen CORS to include branch previews, do it deliberately and     document the tradeoff.</li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/","title":"Hosting &amp; Live Server TODOs (Backend + Frontend)","text":"<p>This document tracks the remaining infrastructure / hosting steps needed to run HealthArchive.ca with a fully wired frontend + backend in production (and optionally add a staging environment later).</p> <p>Nothing in here requires code changes \u2013 it is all environment configuration, DNS, and manual verification.</p> <p>Note: The current production deployment is a single Hetzner VPS using Tailscale-only SSH, Caddy TLS, and nightly DB backups with an NAS pull. See <code>production-single-vps.md</code> for the exact runbook that was implemented.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#0-quick-index-of-remoteonly-tasks","title":"0. Quick index of \u201cremote\u2011only\u201d tasks","text":"<p>Use this as a map of everything that must be done outside your local dev environment (i.e., on live servers, in Vercel, or in the GitHub UI). Each item links to the detailed checklist later in this file.</p> <ul> <li>On the backend server (production) \u2013 see \u00a72 and \u00a74:</li> <li> Provision a Postgres DB and set <code>HEALTHARCHIVE_DATABASE_URL</code>.</li> <li> Choose and provision storage for <code>HEALTHARCHIVE_ARCHIVE_ROOT</code>.</li> <li> Configure <code>HEALTHARCHIVE_ADMIN_TOKEN</code> and <code>HEALTHARCHIVE_CORS_ORIGINS</code>.</li> <li> Reload/restart the backend service with the new env vars.</li> <li> Verify <code>/api/health</code>, <code>/api/sources</code>, <code>/api/search</code>, and CORS headers         over HTTPS.</li> <li> Ensure HTTPS is enforced (HTTP\u2192HTTPS redirect) and HSTS is enabled for         <code>api.healtharchive.ca</code> (and <code>api-staging.healtharchive.ca</code> only if you         later create a staging API).</li> <li> <p> Configure DNS for <code>api.healtharchive.ca</code> (and optionally         <code>api-staging.healtharchive.ca</code> if you later create a staging API)         pointing at the backend.</p> </li> <li> <p>In Vercel for the frontend \u2013 see \u00a73 and \u00a75:</p> </li> <li> Ensure the <code>healtharchive-frontend</code> GitHub repo is connected to a         Vercel project.</li> <li> Set <code>NEXT_PUBLIC_API_BASE_URL</code> for Production and Preview         environments.</li> <li> Configure diagnostics flags         (<code>NEXT_PUBLIC_SHOW_API_HEALTH_BANNER</code>,         <code>NEXT_PUBLIC_LOG_API_HEALTH_FAILURE</code>,         <code>NEXT_PUBLIC_SHOW_API_BASE_HINT</code>) per environment.</li> <li> <p> Trigger deployments and run the browser\u2011side smoke checks on         <code>/archive</code>, <code>/archive/browse-by-source</code>, and <code>/snapshot/[id]</code>.</p> </li> <li> <p>In GitHub for both repos \u2013 see \u00a77:</p> </li> <li> Commit and push CI workflows:         <code>.github/workflows/backend-ci.yml</code> and         https://github.com/jerdaw/healtharchive-frontend/blob/main/.github/workflows/frontend-ci.yml</li> <li> Enable Actions in the GitHub UI if prompted.</li> <li> Configure branch protection on <code>main</code> to require the CI checks before         merging.</li> </ul> <p>You can tick off these high\u2011level items as you go, using the later sections for the exact commands and UI steps.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#1-decide-canonical-urls-onetime-decision","title":"1. Decide canonical URLs (one\u2011time decision)","text":"<p>Before configuring env vars, confirm the URLs you want to use:</p> <ul> <li>Frontend \u2013 production</li> <li><code>https://healtharchive.ca</code></li> <li> <p><code>https://www.healtharchive.ca</code></p> </li> <li> <p>Frontend \u2013 preview</p> </li> <li><code>https://healtharchive.vercel.app</code> (Vercel default)</li> <li> <p>plus any branch\u2011preview URLs Vercel creates</p> </li> <li> <p>Backend \u2013 production API (current choice: single API for everything)</p> </li> <li> <p><code>https://api.healtharchive.ca</code> (used by both Preview and Production frontends)</p> </li> <li> <p>Backend \u2013 staging API (optional future)</p> </li> <li><code>https://api-staging.healtharchive.ca</code> (only if you later decide you want one)</li> </ul> <p>Once you\u2019re happy with those hostnames, the remaining steps in this document assume that naming. Substitute your actual choices as needed.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#2-backend-configuration-cors-env","title":"2. Backend configuration (CORS + env)","text":"<p>The backend already supports CORS and uses environment variables for its DB and archive root. Production/staging configuration is about setting the right env vars in the host environment and restarting the service.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#21-environment-variables-to-set","title":"2.1. Environment variables to set","text":"<p>On each backend deployment (systemd unit, Docker container, or PaaS app), configure the following environment variables on the remote host (not just in your local shell). Typical flow:</p> <ol> <li>SSH into the server or open your cloud provider\u2019s \u201cenvironment variables\u201d    UI for the backend app.</li> <li>Add/update the variables below.</li> <li> <p>Restart the backend service (see \u00a72.2).</p> </li> <li> <p><code>HEALTHARCHIVE_DATABASE_URL</code></p> </li> <li>Points at your production/staging DB (Postgres recommended).</li> <li> <p>Example:</p> <pre><code>export HEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://user:pass@db-host:5432/healtharchive\n</code></pre> </li> <li> <p><code>HEALTHARCHIVE_ENV</code></p> </li> <li>High\u2011level environment hint used by admin auth.</li> <li>Recommended values:<ul> <li><code>development</code> (or unset) for local dev.</li> <li><code>staging</code> for staging hosts.</li> <li><code>production</code> for production hosts.</li> </ul> </li> <li> <p>When <code>HEALTHARCHIVE_ENV</code> is <code>staging</code> or <code>production</code> and     <code>HEALTHARCHIVE_ADMIN_TOKEN</code> is unset, admin and metrics endpoints fail     closed with HTTP 500 instead of being left open.</p> </li> <li> <p><code>HEALTHARCHIVE_ARCHIVE_ROOT</code></p> </li> <li>Root directory where crawl jobs and WARCs will be written.</li> <li> <p>Must be on a filesystem with enough space and backups appropriate for     your risk tolerance.</p> <pre><code>export HEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs\n</code></pre> </li> <li> <p><code>HEALTHARCHIVE_ADMIN_TOKEN</code></p> </li> <li>Token required for <code>/api/admin/*</code> and <code>/metrics</code> when set.</li> <li> <p>Should be a strong random string, stored only in secure places (not     committed to git).</p> <pre><code>export HEALTHARCHIVE_ADMIN_TOKEN=\"some-long-random-string\"\n</code></pre> </li> <li> <p><code>HEALTHARCHIVE_CORS_ORIGINS</code></p> </li> <li>Critical for frontend integration.</li> <li>Comma\u2011separated list of frontend origins allowed to call the public API.</li> <li>When set, overrides the built\u2011in defaults.</li> </ol> <p>Production example (frontend at <code>healtharchive.ca</code>):</p> <pre><code>export HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.ca,https://www.healtharchive.ca\"\n</code></pre> <p>Staging example (frontend at <code>healtharchive.vercel.app</code>):</p> <pre><code>export HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.vercel.app\"\n</code></pre> <p>Optional local dev access to prod/staging API:</p> <pre><code>export HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.ca,https://www.healtharchive.ca,http://localhost:3000\"\n# or with staging:\nexport HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.vercel.app,http://localhost:3000\"\n</code></pre>"},{"location":"deployment/hosting-and-live-server-to-dos/#22-apply-config-and-restart-services","title":"2.2. Apply config and restart services","text":"<p>How you do this depends on your hosting stack:</p> <ul> <li>systemd unit:</li> <li>Add env vars to the unit file (<code>Environment=</code> lines) or a drop\u2011in     <code>EnvironmentFile=/etc/default/healtharchive-backend</code>.</li> <li> <p>Reload + restart:     <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart healtharchive-backend.service\n</code></pre></p> </li> <li> <p>Docker / Docker Compose:</p> </li> <li>Add env vars under <code>environment:</code> in your compose file or <code>docker run</code>     command.</li> <li> <p>Recreate containers:     <pre><code>docker compose up -d --force-recreate backend\n</code></pre></p> </li> <li> <p>PaaS (Render, Fly.io, Heroku, etc.):</p> </li> <li>Use the provider\u2019s UI/CLI to set env vars.</li> <li>Trigger a deployment or restart.</li> </ul> <p>In staging and production you will typically run two backend processes:</p> <ul> <li>An API process (FastAPI + uvicorn) that serves <code>/api/**</code> and <code>/metrics</code>.</li> <li>A worker process (<code>ha-backend start-worker --poll-interval 30</code>) that   continuously processes queued jobs.</li> </ul> <p>Both processes must see the same <code>HEALTHARCHIVE_DATABASE_URL</code>, <code>HEALTHARCHIVE_ARCHIVE_ROOT</code>, and related env vars from \u00a72.1 so they share jobs and archive output consistently.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#23-backend-smoke-checks-stagingprod","title":"2.3. Backend smoke checks (staging/prod)","text":"<p>From a machine that can reach the backend host:</p> <ol> <li>API health</li> </ol> <pre><code>curl -i \"https://api.healtharchive.ca/api/health\"\n</code></pre> <p>Check:    - HTTP 200.    - JSON body like:      <pre><code>{\"status\":\"ok\",\"checks\":{\"db\":\"ok\",\"jobs\":{...},\"snapshots\":{\"total\":...}}}\n</code></pre></p> <ol> <li> <p>CORS headers</p> </li> <li> <p>Call the API with a fake <code>Origin</code> header matching your frontend:      <pre><code>curl -i \\\n  -H \"Origin: https://healtharchive.ca\" \\\n  \"https://api.healtharchive.ca/api/health\"\n</code></pre></p> </li> <li> <p>Response should include:      <pre><code>Access-Control-Allow-Origin: https://healtharchive.ca\nVary: Origin\n</code></pre></p> </li> <li> <p>Basic public routes</p> </li> <li> <p>Verify:      <pre><code>curl -i \"https://api.healtharchive.ca/api/sources\"\ncurl -i \"https://api.healtharchive.ca/api/search?page=1&amp;pageSize=10\"\n</code></pre></p> </li> <li> <p>Expect HTTP 200, JSON bodies, and CORS headers.</p> </li> <li> <p>Security headers</p> </li> <li> <p>Confirm that security-related headers are present on responses:</p> <pre><code>curl -i \"https://api.healtharchive.ca/api/health\" | sed -n '1,20p'\n</code></pre> </li> <li> <p>Look for:</p> <ul> <li><code>X-Content-Type-Options: nosniff</code></li> <li><code>Referrer-Policy: strict-origin-when-cross-origin</code></li> <li><code>X-Frame-Options: SAMEORIGIN</code></li> <li><code>Permissions-Policy: geolocation=(), microphone=(), camera=()</code></li> </ul> </li> </ol>"},{"location":"deployment/hosting-and-live-server-to-dos/#24-archive-storage-retention","title":"2.4. Archive storage &amp; retention","text":"<p>The <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> directory is where crawl jobs and WARCs live. In staging and production you should treat it as persistent, non\u2011ephemeral storage and have a basic retention plan.</p> <p>Checklist for each non\u2011dev environment:</p> <ul> <li> Place <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> on a filesystem that:</li> <li>Is not ephemeral (survives VM/container restarts).</li> <li>Has enough capacity for expected WARCs and logs.</li> <li>Has a backup or snapshot policy appropriate for your risk tolerance.</li> <li> Decide whether this path is:</li> <li>Backed up regularly (if you want WARCs as part of a disaster\u2011recovery     story), or</li> <li>Treated as \u201cbest\u2011effort cache\u201d (if you rely on ZIMs/exports or other     secondary storage).</li> <li> Decide when it is safe to delete temporary crawl artifacts:</li> <li>Only once jobs are <code>indexed</code> or <code>index_failed</code> and you have verified any     desired ZIMs/exports.</li> <li>Use the <code>ha-backend cleanup-job --id JOB_ID --mode temp</code> command for this     cleanup; it removes <code>.tmp*</code> directories and <code>.archive_state.json</code> but     leaves the main job directory and any final ZIMs.</li> <li>If you are using replay (pywb) for a job, do not run <code>cleanup-job --mode temp</code>     for that job \u2014 replay depends on the WARCs remaining on disk.</li> <li>If replay is enabled globally (<code>HEALTHARCHIVE_REPLAY_BASE_URL</code> is set),     <code>cleanup-job --mode temp</code> will refuse unless you pass <code>--force</code>. Treat     <code>--force</code> as an emergency override (it can break replay by deleting WARCs).</li> <li> For larger deployments, consider:</li> <li>Keeping a simple inventory of jobs (via <code>/api/admin/jobs</code> and metrics) so     you know roughly how many indexed jobs you have and how big <code>jobs/</code> is.</li> <li>Periodically reviewing <code>cleanup_status</code> via <code>/metrics</code>     (<code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"temp_cleaned\"}</code>)     to ensure temp artifacts are being pruned over time.</li> </ul> <p>For local development it is sufficient to keep <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> inside the repo (e.g. <code>&lt;./.dev-archive-root&gt;</code>) and delete it manually when you want a clean slate.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#3-frontend-configuration-vercel-env-vars","title":"3. Frontend configuration (Vercel env vars)","text":"<p>The Next.js app reads <code>NEXT_PUBLIC_API_BASE_URL</code> at build time and uses it for all backend requests. It must be set separately for each environment in Vercel.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#31-production-env-vars-vercel","title":"3.1. Production env vars (Vercel)","text":"<p>In the Vercel dashboard for the <code>healtharchive-frontend</code> project:</p> <ol> <li>Log in to https://vercel.com with the GitHub account that owns    <code>healtharchive-frontend</code>.</li> <li>From the Vercel dashboard, click the healtharchive-frontend project.</li> <li>Go to Settings \u2192 Environment Variables.</li> <li>Under Production, add:</li> </ol> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api.healtharchive.ca\n</code></pre> <ol> <li>(Optional, but recommended) keep diagnostics off in production:</li> </ol> <pre><code>NEXT_PUBLIC_SHOW_API_HEALTH_BANNER=false\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=false\nNEXT_PUBLIC_SHOW_API_BASE_HINT=false\n</code></pre> <ol> <li>Trigger a new deployment of the <code>main</code> branch:</li> <li>Either click Deploy for the latest <code>main</code> commit in Vercel, or push a      new commit to <code>main</code> so Vercel automatically builds and deploys.</li> </ol>"},{"location":"deployment/hosting-and-live-server-to-dos/#32-preview-env-vars-vercel","title":"3.2. Preview env vars (Vercel)","text":"<p>Still in Vercel:</p> <ol> <li>In the same Settings \u2192 Environment Variables screen, switch to the    Preview tab.</li> <li>Under Preview environment variables, add:</li> </ol> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api.healtharchive.ca\n</code></pre> <ol> <li>Enable diagnostics to make issues more obvious:</li> </ol> <pre><code>NEXT_PUBLIC_SHOW_API_HEALTH_BANNER=true\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=true\nNEXT_PUBLIC_SHOW_API_BASE_HINT=true\n</code></pre> <ol> <li>Deploy a preview build (push a commit to a non-<code>main</code> branch) and note the    preview URL Vercel creates.</li> </ol> <p>Expected limitation (by design): because the backend uses a strict CORS    allowlist, branch preview URLs like <code>https://healtharchive-git-...vercel.app</code>    may fall back to demo mode until you explicitly allow those origins.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#33-local-development-env-already-mostly-done","title":"3.3. Local development env (already mostly done)","text":"<p>In the frontend repo <code>.env.local</code> (not committed):</p> <pre><code>NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:8001\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=true\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=true\nNEXT_PUBLIC_SHOW_API_BASE_HINT=true\n</code></pre> <p>This is the template for your local dev; Vercel envs for Preview/Production should mirror the same shape but with different API URLs and diagnostics typically disabled in production.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#4-dns-todos","title":"4. DNS TODOs","text":"<p>Ensure DNS records are in place for the backend hosts.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#41-production-api-dns","title":"4.1. Production API DNS","text":"<ul> <li>In your DNS provider\u2019s UI (e.g., Namecheap, Cloudflare, Route 53), locate the   zone for <code>healtharchive.ca</code>.</li> <li>Create a record for <code>api.healtharchive.ca</code>:</li> <li>If the backend is on a VM with a fixed IP:<ul> <li>Add an <code>A</code> record (and <code>AAAA</code> for IPv6 if applicable) pointing to the   backend server IP.</li> </ul> </li> <li>If the backend is behind a load balancer or PaaS:<ul> <li>Add a <code>CNAME</code> pointing at the provider hostname (e.g.,   <code>your-app.region.cloudprovider.com</code>).</li> </ul> </li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/#42-staging-api-dns-optional","title":"4.2. Staging API DNS (optional)","text":"<ul> <li>If you want a separate staging backend, create <code>api-staging.healtharchive.ca</code>   in the same DNS zone:</li> <li>Use an <code>A</code>/<code>AAAA</code> record (for a separate staging VM) or a <code>CNAME</code> (for a     staging app/load balancer) pointing at the staging backend host.</li> </ul> <p>After DNS is configured:</p> <ul> <li>Verify with:   <pre><code>dig +short api.healtharchive.ca\ndig +short api-staging.healtharchive.ca\n</code></pre></li> <li>Then run the API health curl commands in \u00a72.3 against the HTTPS URLs.</li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/#43-tls-https-and-hsts","title":"4.3. TLS / HTTPS and HSTS","text":"<ul> <li>Terminate TLS (HTTPS) for <code>api.healtharchive.ca</code> (and   <code>api-staging.healtharchive.ca</code> if applicable) at your reverse proxy or load   balancer:</li> <li>Use Let's Encrypt or a managed certificate.</li> <li>Configure HTTP\u2192HTTPS redirects for all HTTP traffic.</li> <li>Add an <code>Strict-Transport-Security</code> header on HTTPS responses to enforce   long-lived HTTPS in browsers. For example, in Nginx:</li> </ul> <pre><code>add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n</code></pre> <ul> <li>After enabling HSTS, verify with:</li> </ul> <pre><code>curl -i \"https://api.healtharchive.ca/api/health\" | grep -i strict-transport-security\n</code></pre>"},{"location":"deployment/hosting-and-live-server-to-dos/#5-endtoend-smoke-checklist-stagingprod","title":"5. End\u2011to\u2011end smoke checklist (staging/prod)","text":"<p>Once backend env vars, Vercel env vars, and DNS are in place:</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#51-from-the-frontend-domain","title":"5.1. From the frontend domain","text":"<p>On production (<code>https://healtharchive.ca</code>) and the Vercel domain (<code>https://healtharchive.vercel.app</code>):</p> <ol> <li>Visit <code>/archive</code>:</li> <li>With backend up:<ul> <li>Filters header should show <code>Filters (live API)</code>.</li> <li>If the DB has snapshots, you\u2019ll see real data (no demo fallback notice).</li> </ul> </li> <li> <p>If the backend is unreachable:</p> <ul> <li>Filters header changes to <code>Filters (demo dataset fallback)</code>.</li> <li>Demo records appear instead of live data.</li> <li>A small \u201cBackend unreachable\u201d banner may appear when diagnostics are enabled.</li> </ul> </li> <li> <p>Try filtering:</p> </li> <li>Choose a source, e.g. <code>source=hc</code>.</li> <li>URL updates with <code>?source=hc</code>.</li> <li> <p>Results list changes accordingly (when live snapshots exist).</p> </li> <li> <p>Navigate to <code>/archive/browse-by-source</code>:</p> </li> <li>With backend up:<ul> <li>Cards should show real record counts from <code>/api/sources</code>.</li> </ul> </li> <li> <p>If the backend is unreachable:</p> <ul> <li>\u201cBackend unavailable\u201d callout appears and demo summaries are shown.</li> </ul> </li> <li> <p>Open a snapshot detail page <code>/snapshot/[id]</code>:</p> </li> <li>For a real backend snapshot ID:<ul> <li>Metadata (title, source, date, language, URL) is from <code>/api/snapshot/{id}</code>.</li> <li>\u201cOpen raw snapshot\u201d ultimately points at <code>https://api\u2026/api/snapshots/raw/{id}</code>    on the API host (the frontend prefixes the <code>rawSnapshotUrl</code> path from    the API with <code>NEXT_PUBLIC_API_BASE_URL</code>).</li> </ul> </li> <li>For a demo snapshot ID:<ul> <li>Metadata comes from the bundled demo dataset, and the iframe points    into <code>/demo-archive/**</code>.</li> </ul> </li> </ol>"},{"location":"deployment/hosting-and-live-server-to-dos/#52-console-diagnostics-preview","title":"5.2. Console diagnostics (Preview)","text":"<p>On a Preview deployment, with diagnostics enabled:</p> <ul> <li>Open <code>/archive</code> and check the browser console:</li> <li>You should see something like:     <pre><code>[healtharchive] API base URL (from NEXT_PUBLIC_API_BASE_URL or default): https://api.healtharchive.ca\n</code></pre></li> <li>If the base URL is wrong or the API is unreachable, the health banner and     warning logs will make it obvious.</li> </ul> <p>Production deployments typically keep diagnostics turned off, so you may not see these console logs even when everything is wired correctly.</p> <p>This document should be revisited and checked off as each environment (local, production, and optional future staging) is brought fully online.</p> <p>For a more detailed staging rollout, see:</p> <ul> <li><code>staging-rollout-checklist.md</code></li> </ul> <p>For a more detailed production rollout, see:</p> <ul> <li><code>production-rollout-checklist.md</code></li> </ul> <p>For a more detailed Preview/Production verification of CSP, headers, CORS, and the snapshot viewer iframe behavior, see:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md</li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/#53-monitoring-uptime-checks-optional-but-recommended","title":"5.3. Monitoring &amp; uptime checks (optional but recommended)","text":"<ul> <li>Configure an external uptime monitor (e.g., UptimeRobot, healthchecks.io, or   your cloud provider) to poll:</li> <li><code>https://api.healtharchive.ca/api/health</code> (backend health).</li> <li><code>https://healtharchive.ca/archive</code> (frontend &amp; backend integration).</li> <li>Configure alerts (email/Slack/etc.) for repeated failures or slow responses.</li> <li>If you deploy Prometheus or a similar system, scrape   <code>https://api.healtharchive.ca/metrics</code> and build dashboards/alerts for:</li> <li><code>healtharchive_jobs_total{status=\"failed\"}</code> \u2013 job failures.</li> <li><code>healtharchive_snapshots_total</code> \u2013 sudden jumps in snapshot count.</li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/#6-admin-operator-access-todos","title":"6. Admin / operator access TODOs","text":"<ul> <li> Configure <code>HEALTHARCHIVE_ADMIN_TOKEN</code> in every non\u2011dev environment:</li> <li>Set a long, random value via your hosting platform\u2019s secret manager.</li> <li>Do not commit the token to the repo or to any checked\u2011in <code>.env</code> file.</li> <li> Verify that <code>/api/admin/*</code> and <code>/metrics</code> require the token:</li> <li>Without headers:     <pre><code>curl -i \"https://api.healtharchive.ca/api/admin/jobs\"\ncurl -i \"https://api.healtharchive.ca/metrics\"\n</code></pre>     Expect <code>403 Forbidden</code> when the token is configured.</li> <li>With token:     <pre><code>curl -i \\\n  -H \"Authorization: Bearer $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  \"https://api.healtharchive.ca/api/admin/jobs\"\ncurl -i \\\n  -H \"Authorization: Bearer $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  \"https://api.healtharchive.ca/metrics\"\n</code></pre>     Expect <code>200 OK</code>.</li> <li> Decide how operators will call admin APIs:</li> <li>Short\u2011term: direct <code>curl</code>/CLI usage with the token exported in the shell.</li> <li>Longer\u2011term (optional): a separate admin console (e.g.,     <code>https://admin.healtharchive.ca</code>) that runs in a trusted environment and     never exposes <code>HEALTHARCHIVE_ADMIN_TOKEN</code> to browser JavaScript.</li> <li> If you later add an admin console:</li> <li>Protect it behind SSO, VPN, or other strong authentication.</li> <li>Avoid linking it from the public site navigation.</li> <li>Exclude admin URLs from search indexing (robots.txt and/or <code>&lt;meta&gt;</code> tags).</li> </ul>"},{"location":"deployment/hosting-and-live-server-to-dos/#7-github-actions-branch-protection-todos","title":"7. GitHub Actions &amp; branch protection TODOs","text":"<p>Continuous integration is wired via workflow files in each repo, but it only becomes effective once you commit/push them and (optionally) protect branches.</p>"},{"location":"deployment/hosting-and-live-server-to-dos/#61-enable-and-verify-github-actions","title":"6.1. Enable and verify GitHub Actions","text":"<p>For each repo (<code>healtharchive-backend</code> and <code>healtharchive-frontend</code>):</p> <ol> <li> <p>Ensure the workflow files are present (already true in this repo) and    enabled in the GitHub UI:</p> </li> <li> <p>Navigate to the repository on https://github.com.</p> </li> <li>Click the Actions tab.</li> <li> <p>If GitHub shows a banner like \u201cWorkflows are disabled for this fork,\u201d      click Enable workflows.</p> </li> <li> <p>Push a test commit or re\u2011run the latest workflow to verify that a run is    triggered for branch <code>main</code> and for pull requests:</p> </li> <li> <p>Backend CI should:</p> <ul> <li>Run <code>make check</code>.</li> </ul> </li> <li>Frontend CI should:<ul> <li>Install deps via <code>npm ci</code>.</li> <li>Run <code>npm run check</code>.</li> </ul> </li> </ol>"},{"location":"deployment/hosting-and-live-server-to-dos/#62-configure-branch-protection-optional-but-recommended","title":"6.2. Configure branch protection (optional but recommended)","text":"<p>To prevent merging changes that break tests or linting:</p> <ol> <li>For each GitHub repo, open the repository page and go to    Settings \u2192 Branches.</li> <li> <p>Under Branch protection rules, click Add rule (or edit an existing    rule) and set:</p> </li> <li> <p>Branch name pattern: <code>main</code></p> </li> <li>Enable Require a pull request before merging (tune review settings as      you prefer).</li> <li>Enable Require status checks to pass before merging and select the CI      workflows:<ul> <li>In the backend repo, select the check corresponding to    <code>.github/workflows/backend-ci.yml</code> (e.g., <code>Backend CI</code>).</li> <li>In the frontend repo, select the check corresponding to    https://github.com/jerdaw/healtharchive-frontend/blob/main/.github/workflows/frontend-ci.yml (e.g., <code>Frontend CI</code>).</li> </ul> </li> <li> <p>Optionally enable Include administrators so even admin users must      wait for green CI.</p> </li> <li> <p>Click Create or Save changes to persist the rule.</p> </li> </ol> <p>After this, any PR targeting <code>main</code> will need green CI checks before it can be merged, ensuring that:</p> <ul> <li>Backend changes don\u2019t break the pytest suite.</li> <li>Frontend changes don\u2019t break linting or Vitest tests.</li> </ul>"},{"location":"deployment/pages-table-rollout/","title":"Pages table rollout (browse performance + capture counts)","text":"<p>The backend can optionally maintain a <code>pages</code> table that materializes a per\u2011source \u201cpage\u201d concept (grouped by <code>normalized_url_group</code>) from the raw <code>snapshots</code> table.</p> <p>Important: this is metadata only. It does not modify WARCs, does not delete snapshots, and does not affect replay fidelity.</p>"},{"location":"deployment/pages-table-rollout/#what-it-improves","title":"What it improves","text":"<ul> <li>Browse performance for <code>GET /api/search?view=pages</code> when there is no   search query (and no date range). This avoids expensive window functions over   the entire <code>snapshots</code> table.</li> <li>Adds <code>pageSnapshotsCount</code> to page-browse results so the frontend can show   \u201cCaptures N\u201d.</li> </ul> <p>Keyword searches (<code>q=...</code>) and date-range filters still run directly against <code>snapshots</code> to keep correctness predictable.</p>"},{"location":"deployment/pages-table-rollout/#rollout-steps-production","title":"Rollout steps (production)","text":"<p>1) Apply migrations:</p> <pre><code>./.venv/bin/alembic upgrade head\n</code></pre> <p>2) Backfill the table once:</p> <pre><code>./.venv/bin/ha-backend rebuild-pages --truncate\n</code></pre> <p>Notes:</p> <ul> <li>On Postgres, the CLI may print <code>upserted unknown</code> because the DB driver often   does not report an accurate <code>rowcount</code> for large <code>INSERT ... SELECT</code> statements.   Use the verification steps below (or <code>SELECT count(*) FROM pages;</code>) to confirm   it worked.</li> <li>For large datasets this can take a while; run it in <code>tmux</code> or off-peak.</li> <li>The worker will keep the table updated for newly indexed jobs (incremental   rebuilds happen after indexing).</li> </ul>"},{"location":"deployment/pages-table-rollout/#verification","title":"Verification","text":"<p>1) Confirm pages browse includes capture counts:</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?view=pages&amp;pageSize=1\" | python3 -m json.tool | head\n</code></pre> <p>You should see <code>pageSnapshotsCount</code> as an integer (not <code>null</code>) on results.</p> <p>2) Confirm metrics (admin token required):</p> <pre><code>curl -s https://api.healtharchive.ca/metrics \\\n  -H \"Authorization: Bearer $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  | grep -E \"healtharchive_pages_(table_present|total|fastpath_enabled)|healtharchive_search_mode_total\\\\{mode=\\\\\\\"pages_fastpath\\\\\\\"\\\\}\"\n</code></pre>"},{"location":"deployment/pages-table-rollout/#rollback-safety-valve","title":"Rollback / safety valve","text":"<p>If anything looks suspicious in production (for example: browse ordering or unexpected results), you can disable the fast path without touching data:</p> <p>1) Set <code>HA_PAGES_FASTPATH=0</code> in <code>/etc/healtharchive/backend.env</code> 2) Restart only the API process:</p> <pre><code>sudo systemctl restart healtharchive-api\n</code></pre> <p>This forces <code>view=pages</code> browsing to fall back to snapshot-based grouping.</p>"},{"location":"deployment/production-rollout-checklist/","title":"Production rollout checklist \u2013 backend + frontend","text":"<p>This file is a step\u2011by\u2011step checklist for bringing the production environment online, based on the same patterns used for staging.</p> <p>Assumptions:</p> <ul> <li>Production API host: <code>https://api.healtharchive.ca</code></li> <li>Production frontend: <code>https://healtharchive.ca</code> and <code>https://www.healtharchive.ca</code></li> <li>Code from <code>main</code> in both repos is what you intend to deploy.</li> </ul> <p>Everything here happens on:</p> <ul> <li>The production backend host (VM/container/PaaS).</li> <li>Vercel (for the frontend).</li> <li>GitHub (for CI/branch protection).</li> </ul> <p>Nothing in this file requires changes to your local dev environment.</p> <p>For background, see:</p> <ul> <li><code>hosting-and-live-server-to-dos.md</code></li> <li><code>environments-and-configuration.md</code></li> <li><code>production-single-vps.md</code> (current production runbook)</li> <li><code>staging-rollout-checklist.md</code> (optional future)</li> </ul>"},{"location":"deployment/production-rollout-checklist/#1-backend-production-environment","title":"1. Backend production environment","text":""},{"location":"deployment/production-rollout-checklist/#11-set-env-vars-on-the-production-backend-host","title":"1.1 Set env vars on the production backend host","text":"<p>On the production host (VM/container/PaaS):</p> <ol> <li>Decide where you want production jobs and WARCs to live, e.g.:</li> </ol> <pre><code>/srv/healtharchive/jobs\n</code></pre> <ol> <li>Configure env vars for the backend app (via systemd env file, Docker env,    or PaaS UI). Typical values:</li> </ol> <pre><code>export HEALTHARCHIVE_ENV=production\nexport HEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://user:pass@db-host:5432/healtharchive\nexport HEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs\nexport HEALTHARCHIVE_ADMIN_TOKEN=\"&lt;prod-long-random-secret&gt;\"\nexport HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.ca,https://www.healtharchive.ca\"\nexport HEALTHARCHIVE_LOG_LEVEL=INFO\n</code></pre> <p>Adjust the DB URL and archive root to match your actual production    infrastructure. <code>HEALTHARCHIVE_CORS_ORIGINS</code> should be as narrow as    possible in production: usually just the public frontend origins.</p>"},{"location":"deployment/production-rollout-checklist/#12-run-migrations-and-seed-sources","title":"1.2 Run migrations and seed sources","text":"<p>From a checkout of <code>healtharchive-backend</code> at the deployed revision on the production host:</p> <pre><code>cd /path/to/healtharchive-backend\n\n# Activate venv or ensure dependencies are installed\nalembic upgrade head\nha-backend seed-sources\n</code></pre> <p>This:</p> <ul> <li>Applies all Alembic migrations to the production DB.</li> <li>Ensures baseline <code>Source</code> rows exist (idempotent).</li> </ul> <p>If you have deployed the link-signal schema (tables <code>snapshot_outlinks</code> and <code>page_signals</code>), recompute page signals (includes <code>pagerank</code> when present):</p> <pre><code>ha-backend recompute-page-signals\n</code></pre>"},{"location":"deployment/production-rollout-checklist/#13-start-api-worker-processes","title":"1.3 Start API + worker processes","text":"<p>Configure your process manager to run:</p> <ul> <li>API:</li> </ul> <pre><code>uvicorn ha_backend.api:app --host 0.0.0.0 --port 8001\n</code></pre> <ul> <li>Worker:</li> </ul> <pre><code>ha-backend start-worker --poll-interval 30\n</code></pre> <p>Both processes must see the same <code>HEALTHARCHIVE_*</code> env vars from 1.1.</p> <p>Optional (recommended): enable the blended search ranking by default:</p> <pre><code>export HA_SEARCH_RANKING_VERSION=v2\n</code></pre> <p>Rollback is instant: set <code>HA_SEARCH_RANKING_VERSION=v1</code> and restart the API process.</p>"},{"location":"deployment/production-rollout-checklist/#14-dns-and-tls-for-the-api","title":"1.4 DNS and TLS for the API","text":"<p>In your DNS provider (e.g. Namecheap, Cloudflare, Route 53):</p> <ol> <li>Create/verify records for <code>api.healtharchive.ca</code>:</li> <li><code>A</code> / <code>AAAA</code> pointing at the backend host IP, or</li> <li> <p><code>CNAME</code> pointing at a load balancer / PaaS hostname.</p> </li> <li> <p>Ensure TLS is terminated correctly:</p> </li> <li>Use Let\u2019s Encrypt or a managed certificate for <code>api.healtharchive.ca</code>.</li> <li>Configure HTTP\u2192HTTPS redirects.</li> <li> <p>Add an HSTS header at the reverse proxy/load balancer layer, e.g.:</p> <pre><code>add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n</code></pre> </li> <li> <p>Quick checks from your own machine:</p> </li> </ol> <pre><code>curl -i \"https://api.healtharchive.ca/api/health\"\n\ncurl -i \\\n  -H \"Origin: https://healtharchive.ca\" \\\n  \"https://api.healtharchive.ca/api/health\"\n</code></pre> <p>Verify:</p> <ul> <li>HTTP 200 and <code>\"status\":\"ok\"</code> in the JSON body.</li> <li><code>Access-Control-Allow-Origin: https://healtharchive.ca</code> and <code>Vary: Origin</code>.</li> </ul>"},{"location":"deployment/production-rollout-checklist/#15-seed-initial-production-snapshots","title":"1.5 Seed initial production snapshots","text":"<p>How you seed production is a policy choice; some options:</p> <ul> <li>Use a few small, controlled crawls driven by the worker:</li> <li><code>ha-backend create-job --source hc</code></li> <li><code>ha-backend create-job --source phac</code></li> <li>Let the worker process these jobs and attempt indexing.</li> <li>Use a synthetic WARC snapshot (same pattern as staging) for a minimal   initial smoke test.</li> </ul> <p>At minimum, create one snapshot and note its ID <code>N_prod</code> so you can test the viewer end\u2011to\u2011end:</p> <pre><code>curl -i \"https://api.healtharchive.ca/api/snapshot/&lt;N_prod&gt;\"\ncurl -i \"https://api.healtharchive.ca/api/snapshots/raw/&lt;N_prod&gt;\"\n</code></pre>"},{"location":"deployment/production-rollout-checklist/#2-frontend-production-configuration-vercel","title":"2. Frontend production configuration (Vercel)","text":""},{"location":"deployment/production-rollout-checklist/#21-production-env-vars-vercel","title":"2.1 Production env vars (Vercel)","text":"<p>In the Vercel project for <code>healtharchive-frontend</code>:</p> <ol> <li>Go to Settings \u2192 Environment Variables \u2192 Production.</li> <li>Set:</li> </ol> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api.healtharchive.ca\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=false\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=false\nNEXT_PUBLIC_SHOW_API_BASE_HINT=false\n</code></pre> <ol> <li>Save and trigger a new Production deployment (by pushing to <code>main</code> or    clicking Redeploy for the latest <code>main</code> commit).</li> </ol>"},{"location":"deployment/production-rollout-checklist/#22-frontend-domains","title":"2.2 Frontend domains","text":"<p>In Vercel + DNS:</p> <ul> <li>Ensure:</li> <li><code>healtharchive.ca</code> and <code>www.healtharchive.ca</code> are pointed at Vercel.</li> <li>Any old records (e.g., GitHub Pages IPs) have been removed.</li> </ul> <p>Once the production deployment completes, visiting <code>https://healtharchive.ca</code> should show the live frontend.</p>"},{"location":"deployment/production-rollout-checklist/#3-production-verification-browser","title":"3. Production verification (browser)","text":"<p>With the production backend and frontend deployed:</p>"},{"location":"deployment/production-rollout-checklist/#31-archive-pages","title":"3.1 Archive pages","text":"<ol> <li>Visit:</li> </ol> <pre><code>https://healtharchive.ca/archive\n</code></pre> <ol> <li>Verify:</li> <li>The filters header shows <code>Filters (live API)</code> when the backend is up.</li> <li> <p>If the DB has snapshots, results reflect real data (no demo fallback      notice).</p> </li> <li> <p>Visit:</p> </li> </ol> <pre><code>https://healtharchive.ca/archive/browse-by-source\n</code></pre> <ul> <li>Cards should show real counts from <code>/api/sources</code>.</li> </ul>"},{"location":"deployment/production-rollout-checklist/#32-snapshot-viewer","title":"3.2 Snapshot viewer","text":"<ol> <li>Visit the production snapshot using <code>N_prod</code> from \u00a71.5:</li> </ol> <pre><code>https://healtharchive.ca/snapshot/&lt;N_prod&gt;\n</code></pre> <ol> <li>Confirm:</li> <li>Metadata (title, source, date, language, URL) matches      <code>/api/snapshot/&lt;N_prod&gt;</code>.</li> <li>\u201cOpen raw snapshot\u201d opens <code>https://api.healtharchive.ca/api/snapshots/raw/&lt;N_prod&gt;</code>.</li> <li> <p>The embedded iframe loads the same URL and renders the HTML.</p> </li> <li> <p>In DevTools \u2192 Network:</p> </li> <li>Confirm the iframe request goes to <code>api.healtharchive.ca</code>.</li> <li>Confirm security headers match staging expectations (no      <code>X-Frame-Options</code> on the raw snapshot route; other headers present).</li> </ol>"},{"location":"deployment/production-rollout-checklist/#4-monitoring-ci-signoff","title":"4. Monitoring &amp; CI sign\u2011off","text":"<p>Once production is healthy, tie this back to:</p> <ul> <li>Monitoring &amp; uptime (see <code>hosting-and-live-server-to-dos.md</code> \u00a75.3):</li> <li>Configure uptime checks for:<ul> <li><code>https://api.healtharchive.ca/api/health</code></li> <li><code>https://healtharchive.ca/archive</code></li> </ul> </li> <li> <p>If you have Prometheus or similar, scrape:</p> <ul> <li><code>https://api.healtharchive.ca/metrics</code></li> <li>Build alerts on:</li> <li><code>healtharchive_jobs_total{status=\"failed\"}</code></li> <li><code>healtharchive_snapshots_total</code></li> <li><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"temp_cleaned\"}</code></li> </ul> </li> <li> <p>CI &amp; branch protection (see <code>hosting-and-live-server-to-dos.md</code> \u00a77):</p> </li> <li>Ensure GitHub Actions workflows are enabled and passing.</li> <li>Verify backend <code>main</code> protection still matches the solo-dev ruleset profile in     <code>../operations/monitoring-and-ci-checklist.md</code> \u00a73.2:<ul> <li>required check: <code>Backend CI / test</code></li> <li>keep <code>Backend CI / e2e-smoke</code> and <code>Backend CI (Full) / test-full</code> non-required</li> <li>keep <code>Restrict deletions</code> and <code>Block force pushes</code> enabled</li> </ul> </li> </ul> <p>With this in place, <code>main</code> deploys cleanly to production, and you have health and metrics coverage for both the API and the frontend.</p>"},{"location":"deployment/production-single-vps/","title":"HealthArchive.ca \u2013 Production on a Single VPS (Hetzner + Tailscale)","text":"<p>This is the record of the current production deployment. It is a single VPS that runs Postgres, the API, the worker, Caddy (TLS), and all archive storage. SSH is private-only via Tailscale; the public internet only sees ports <code>80/443</code>.</p> <p>Use this as the canonical runbook for rebuilding the stack, auditing it, or explaining it to new operators.</p> <p>For recovery from total failure, see the Disaster Recovery Runbook.</p>"},{"location":"deployment/production-single-vps/#1-hosting-topology","title":"1) Hosting / topology","text":"<ul> <li>Provider / size: Hetzner Cloud, <code>cx33</code> (Cost-Optimized, 4 vCPU / 8GB RAM / 80GB SSD)</li> <li>Region: Nuremberg (cost-optimized not available in US-East at the time)</li> <li>Public services: <code>api.healtharchive.ca</code> on 80/443 via Caddy</li> <li>Replay (optional): <code>replay.healtharchive.ca</code> via Caddy \u2192 pywb (see <code>deployment/replay-service-pywb.md</code>)</li> <li>Private-only: SSH on Tailscale (<code>tailscale0</code>), no public port 22</li> <li>Storage:</li> <li><code>/srv/healtharchive/jobs</code> \u2013 archive root (WARCs / job outputs)</li> <li><code>/srv/healtharchive/backups</code> \u2013 DB dumps</li> <li>(Optional) StorageBox mount (cold storage / tiering; not a crawl hot-path)</li> <li>Database: Local Postgres on the VPS</li> <li>Monitoring/alerts:</li> <li>Healthchecks.io pings for DB backup success/failure</li> <li>Healthchecks.io pings for disk-usage threshold</li> <li>(External uptime checks recommended: <code>/api/health</code> and <code>/archive</code>)</li> <li>Backups: Nightly <code>pg_dump -Fc</code> \u2192 <code>/srv/healtharchive/backups</code>, retained 14 days</li> <li>Offsite copy: Synology NAS pulls backups over Tailscale via rsync/SSH</li> </ul>"},{"location":"deployment/production-single-vps/#2-provision-os-hardening-hetzner","title":"2) Provision &amp; OS hardening (Hetzner)","text":"<p>1) Create server:    - Type: Cost-Optimized, x86, <code>cx33</code>    - Region: Nuremberg    - OS: Ubuntu 24.04 LTS    - Attach SSH public key; no password login 2) Hetzner Cloud Firewall (final state):    - Allow TCP 80, 443 (anywhere)    - Allow UDP 41641 (anywhere) for Tailscale    - No public TCP 22 3) OS setup:    - Create <code>haadmin</code> (sudo), disable root SSH login, disable SSH passwords    - Enable <code>unattended-upgrades</code>    - UFW: allow 80/443, allow 22 only on <code>tailscale0</code>, allow 41641/udp</p>"},{"location":"deployment/production-single-vps/#3-runtime-dependencies","title":"3) Runtime dependencies","text":"<p>On the VPS (as <code>haadmin</code>):</p> <pre><code>sudo apt update\nsudo apt -y install docker.io \\\n  postgresql postgresql-contrib \\\n  python3 python3-venv python3-pip \\\n  git curl build-essential pkg-config unzip\nsudo systemctl enable --now docker postgresql\n</code></pre>"},{"location":"deployment/production-single-vps/#swap-recommended-on-cx33","title":"Swap (recommended on cx33)","text":"<p>Annual crawls are long-running and browser-driven; having a small swap file helps avoid OOM-driven churn and reduces time lost to restarts.</p> <p>Recommended on <code>cx33</code> (8GB RAM): add a <code>4G</code> swapfile on the local SSD:</p> <pre><code>sudo fallocate -l 4G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\necho '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab\nswapon --show\n</code></pre> <p>Notes:</p> <ul> <li>Docker Compose is optional for this stack. On Ubuntu 24.04, the packaged   Compose plugin is often <code>docker-compose-v2</code> (not <code>docker-compose-plugin</code>):</li> </ul> <pre><code>sudo apt -y install docker-compose-v2\ndocker compose version\n</code></pre> <p>Directories:</p> <pre><code>sudo groupadd --system healtharchive 2&gt;/dev/null || true\nsudo mkdir -p /srv/healtharchive/jobs /srv/healtharchive/backups /srv/healtharchive/ops\nsudo chown -R haadmin:haadmin /srv/healtharchive/jobs\nsudo chown root:healtharchive /srv/healtharchive/backups\nsudo chmod 2770 /srv/healtharchive/backups\nsudo chown root:healtharchive /srv/healtharchive/ops\nsudo chmod 2770 /srv/healtharchive/ops\n</code></pre> <p>Ops directories (public-safe logs + artifacts):</p> <ul> <li><code>root:healtharchive</code> ownership + <code>2770</code> perms is intentional:</li> <li><code>root</code> owns the directory tree</li> <li>operators (e.g., <code>haadmin</code>) write via the <code>healtharchive</code> group</li> <li>the setgid bit keeps group ownership consistent on new files/dirs</li> </ul> <p>Create the standard subdirectories:</p> <pre><code>sudo mkdir -p \\\n  /srv/healtharchive/ops/baseline \\\n  /srv/healtharchive/ops/restore-tests \\\n  /srv/healtharchive/ops/adoption \\\n  /srv/healtharchive/ops/search-eval\nsudo chown -R root:healtharchive /srv/healtharchive/ops\nsudo chmod 2770 /srv/healtharchive/ops /srv/healtharchive/ops/*\n</code></pre> <p>Postgres:</p> <pre><code>sudo -u postgres psql -c \"CREATE USER healtharchive WITH PASSWORD '&lt;DB_PASSWORD&gt;';\"\nsudo -u postgres psql -c \"CREATE DATABASE healtharchive OWNER healtharchive;\"\n</code></pre>"},{"location":"deployment/production-single-vps/#4-backend-deploy-api-worker-systemd","title":"4) Backend deploy (API + worker, systemd)","text":"<p>Clone + venv:</p> <pre><code>sudo mkdir -p /opt &amp;&amp; sudo chown haadmin:haadmin /opt\ngit clone https://github.com/jerdaw/healtharchive-backend.git /opt/healtharchive-backend\ncd /opt/healtharchive-backend\npython3 -m venv .venv\n./.venv/bin/pip install --upgrade pip\n./.venv/bin/pip install -e \".[dev]\" \"psycopg[binary]\"\n</code></pre> <p>Env file (root-owned, group-readable):</p> <pre><code>sudo groupadd --system healtharchive 2&gt;/dev/null || true\nsudo usermod -aG healtharchive haadmin\nsudo install -d -m 750 -o root -g healtharchive /etc/healtharchive\nsudo tee /etc/healtharchive/backend.env &gt;/dev/null &lt;&lt;'EOF'\nHEALTHARCHIVE_ENV=production\nHEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://healtharchive:&lt;DB_PASSWORD&gt;@127.0.0.1:5432/healtharchive\n# Keep the crawl hot-path on the local SSD for throughput; use the StorageBox only for cold storage/tiering.\nHEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs\nHEALTHARCHIVE_ADMIN_TOKEN=&lt;LONG_RANDOM_TOKEN&gt;\nHEALTHARCHIVE_CORS_ORIGINS=https://healtharchive.ca,https://www.healtharchive.ca,https://healtharchive.vercel.app,https://replay.healtharchive.ca\nHEALTHARCHIVE_LOG_LEVEL=INFO\nHA_SEARCH_RANKING_VERSION=v2\nHA_PAGES_FASTPATH=1\n\n# Optional: aggregated, privacy-preserving usage metrics (daily counts only).\n# Drives the public reporting pages (`/status` and `/impact`) via `GET /api/usage`.\nHEALTHARCHIVE_USAGE_METRICS_ENABLED=1\nHEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS=30\n\n# Optional: change tracking + diff feeds.\nHEALTHARCHIVE_CHANGE_TRACKING_ENABLED=1\n\n# Optional: compare-live (snapshot vs current live page).\n# Defaults are safe, but you can tune these if needed.\nHEALTHARCHIVE_COMPARE_LIVE_ENABLED=1\nHEALTHARCHIVE_COMPARE_LIVE_TIMEOUT_SECONDS=8\nHEALTHARCHIVE_COMPARE_LIVE_MAX_REDIRECTS=4\nHEALTHARCHIVE_COMPARE_LIVE_MAX_BYTES=2000000\nHEALTHARCHIVE_COMPARE_LIVE_MAX_ARCHIVE_BYTES=2000000\nHEALTHARCHIVE_COMPARE_LIVE_MAX_RENDER_LINES=5000\nHEALTHARCHIVE_COMPARE_LIVE_MAX_CONCURRENCY=4\n# HEALTHARCHIVE_COMPARE_LIVE_USER_AGENT=HealthArchiveCompareLive/1.0 (+https://healtharchive.ca)\n\n# Optional: research exports.\n# Controls the public metadata export endpoints under `/api/exports`.\nHEALTHARCHIVE_EXPORTS_ENABLED=1\nHEALTHARCHIVE_EXPORTS_DEFAULT_LIMIT=1000\nHEALTHARCHIVE_EXPORTS_MAX_LIMIT=10000\n\n# Public site base URL for RSS feed links and public compare URLs.\nHEALTHARCHIVE_PUBLIC_SITE_URL=https://healtharchive.ca\n\n# Optional: replay integration (pywb). Enables `browseUrl` fields in the public API.\n# HEALTHARCHIVE_REPLAY_BASE_URL=https://replay.healtharchive.ca\n\n# Optional: cached replay preview images (homepage thumbnails for /archive cards).\n# HEALTHARCHIVE_REPLAY_PREVIEW_DIR=/srv/healtharchive/replay/previews\nEOF\nsudo chown root:healtharchive /etc/healtharchive/backend.env\nsudo chmod 640 /etc/healtharchive/backend.env\n</code></pre> <p>Migrate + seed:</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n./.venv/bin/alembic upgrade head\n./.venv/bin/ha-backend seed-sources\n./.venv/bin/ha-backend recompute-page-signals\n./.venv/bin/ha-backend rebuild-pages --truncate\n</code></pre> <p>Systemd services:</p> <ul> <li>API: <code>/etc/systemd/system/healtharchive-api.service</code></li> <li><code>ExecStart=/opt/healtharchive-backend/.venv/bin/uvicorn ha_backend.api:app --host 127.0.0.1 --port 8001</code></li> <li><code>EnvironmentFile=/etc/healtharchive/backend.env</code></li> <li>Worker: <code>/etc/systemd/system/healtharchive-worker.service</code></li> <li><code>ExecStart=/opt/healtharchive-backend/.venv/bin/ha-backend start-worker --poll-interval 30</code></li> </ul> <p>Optional systemd automation (recommended):</p> <ul> <li>Install/update systemd unit templates from this repo:</li> <li><code>./scripts/vps-install-systemd-units.sh --apply</code></li> <li>Templates + enablement steps: <code>deployment/systemd/README.md</code></li> <li>Baseline drift check timer (weekly; low-risk, recommended):</li> <li>Templates + enablement steps: <code>deployment/systemd/README.md</code></li> <li>Annual scheduling timer (Jan 01 UTC) + worker priority drop-in:</li> <li>Templates + install steps: <code>deployment/systemd/README.md</code></li> <li>Replay reconciliation timer (pywb indexing; capped, optional):</li> <li>Templates + install steps: <code>deployment/systemd/README.md</code></li> <li>Change tracking timer (edition-aware diffs; capped):</li> <li>Templates + install steps: <code>deployment/systemd/README.md</code></li> <li>Annual search verification capture (optional; safe):</li> <li>Templates + enablement steps: <code>deployment/systemd/README.md</code></li> </ul> <p>Enable + start:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable --now healtharchive-api healtharchive-worker\ncurl -i http://127.0.0.1:8001/api/health\n</code></pre> <p>Routine deploys (after initial install):</p> <pre><code>cd /opt/healtharchive-backend\n\n# Dry-run (prints actions):\n./scripts/vps-deploy.sh\n\n# Deploy latest main (fast-forward only):\n./scripts/vps-deploy.sh --apply\n\n# Deploy pinned commit:\n./scripts/vps-deploy.sh --apply --ref &lt;GIT_SHA&gt;\n</code></pre> <p>Recommended wrapper (routine use):</p> <pre><code>./scripts/vps-hetzdeploy.sh\n</code></pre> <p>Recommended: install <code>hetzdeploy</code> as a real command (avoid fragile aliases):</p> <pre><code>sudo ./scripts/vps-install-hetzdeploy.sh --apply\n\n# Then you can run it from anywhere:\nhetzdeploy\n</code></pre> <p>Notes:</p> <ul> <li>The deploy script runs a baseline drift check by default to catch   misconfiguration (filesystem perms, systemd enablement, env allowlists, etc.).</li> <li>Artifacts are written to: <code>/srv/healtharchive/ops/baseline/</code></li> <li>You can skip in emergencies: <code>./scripts/vps-deploy.sh --apply --skip-baseline-drift</code></li> <li>To include live HTTPS checks (HSTS, CORS headers, admin/metrics auth): <code>./scripts/vps-deploy.sh --apply --baseline-mode live</code></li> <li>If you update systemd unit templates in the repo, you can apply them during deploy:</li> <li><code>./scripts/vps-deploy.sh --apply --install-systemd-units</code></li> <li>If you update Prometheus alert rules, you can apply them during deploy:</li> <li><code>./scripts/vps-deploy.sh --apply --apply-alerting</code></li> <li>Requires alerting to be configured (webhook secret present at <code>/etc/healtharchive/observability/alertmanager_webhook_url</code>).</li> <li>The baseline policy (desired state) is versioned in git at:   <code>docs/operations/production-baseline-policy.toml</code></li> <li>The deploy script runs a public-surface smoke verify by default (public API + frontend + replay + usage):</li> <li><code>./scripts/verify_public_surface.py</code> (defaults to <code>https://api.healtharchive.ca</code> and <code>https://www.healtharchive.ca</code>)</li> <li>You can skip in emergencies: <code>./scripts/vps-deploy.sh --apply --skip-public-surface-verify</code></li> <li>If the public frontend is externally down (e.g., Vercel <code>402 Payment required</code>), use:<ul> <li><code>./scripts/vps-hetzdeploy.sh --mode backend-only</code></li> <li>Or (if installed): <code>hetzdeploy --mode backend-only</code></li> </ul> </li> <li>Crawl-safety: if any jobs are <code>status=running</code>, the deploy helper will restart <code>healtharchive-api</code> but will   skip restarting <code>healtharchive-worker</code> by default (to avoid SIGTERMing an active crawl).</li> <li>To force a worker restart (only when you are OK interrupting crawls): <code>./scripts/vps-deploy.sh --apply --force-worker-restart</code></li> <li>To explicitly skip the worker restart regardless of job status: <code>./scripts/vps-deploy.sh --apply --skip-worker-restart</code></li> </ul>"},{"location":"deployment/production-single-vps/#41-observability-prometheus-grafana-operator-only","title":"4.1) Observability (Prometheus + Grafana; operator-only)","text":"<p>This is the private ops stack:</p> <ul> <li>Prometheus collects metrics (backend + host + Postgres exporters).</li> <li>Grafana shows dashboards (\u201cprivate stats\u201d).</li> <li>Alertmanager sends alerts to one operator channel (via the webhook relay).</li> </ul> <p>The important safety rule:</p> <ul> <li>These services bind to <code>127.0.0.1</code> on the VPS (loopback-only) and are accessed over the tailnet (Tailscale) using an SSH port-forward.</li> <li>Do not add Caddy vhosts for Prometheus/Grafana (keep them off the public internet).</li> </ul> <p>Install flow (VPS):</p> <ul> <li>Follow the observability playbooks under <code>docs/operations/playbooks/</code>:</li> <li><code>docs/operations/playbooks/observability/observability-guide.md</code></li> <li><code>docs/operations/playbooks/observability/monitoring-and-alerting.md</code></li> </ul> <p>Where things live (VPS):</p> <ul> <li>Secrets (never commit): <code>/etc/healtharchive/observability/</code></li> <li>Prometheus config: <code>/etc/prometheus/prometheus.yml</code> and <code>/etc/prometheus/rules/</code></li> <li>Alertmanager config: <code>/etc/prometheus/alertmanager.yml</code></li> <li>Grafana dashboards provisioning: <code>/etc/grafana/provisioning/dashboards/healtharchive.yaml</code></li> <li>Public-safe dashboard JSON + ops artifacts: <code>/srv/healtharchive/ops/observability/</code></li> </ul> <p>Access from your laptop (via tailnet-only SSH):</p> <pre><code># Tunnel Grafana + Prometheus + admin proxy to your local machine.\n# Keep this terminal open.\nssh -N \\\n  -L 3000:127.0.0.1:3000 \\\n  -L 9090:127.0.0.1:9090 \\\n  -L 8002:127.0.0.1:8002 \\\n  haadmin@&lt;vps-tailscale-ip&gt;\n</code></pre> <p>Then open on your laptop:</p> <ul> <li>Grafana: <code>http://127.0.0.1:3000/</code></li> <li>Prometheus UI (optional): <code>http://127.0.0.1:9090/</code></li> <li>Admin proxy (operator triage; browser-friendly): <code>http://127.0.0.1:8002/</code></li> </ul> <p>Restart services (VPS):</p> <pre><code>sudo systemctl restart \\\n  prometheus \\\n  prometheus-alertmanager \\\n  prometheus-node-exporter \\\n  prometheus-postgres-exporter \\\n  grafana-server \\\n  healtharchive-pushover-relay \\\n  healtharchive-admin-proxy\n</code></pre>"},{"location":"deployment/production-single-vps/#42-storage-box-sshfs-stale-mount-failures-errno-107","title":"4.2) Storage Box / <code>sshfs</code> stale mount failures (Errno 107)","text":"<p>HealthArchive uses a Storage Box (via <code>sshfs</code>) as a cold tier in production (WARC tiering).</p> <p>Important failure mode:</p> <ul> <li>A mount can appear \u201cpresent\u201d but be stale/unreadable, causing:</li> <li><code>OSError: [Errno 107] Transport endpoint is not connected</code></li> </ul> <p>This can break:</p> <ul> <li>crawl progress metrics,</li> <li>archive job output dirs under <code>/srv/healtharchive/jobs/**</code>,</li> <li>and the worker/job lifecycle.</li> </ul> <p>Fast triage:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-crawl-status.sh --year \"$(date -u +%Y)\"\nls -la /srv/healtharchive/storagebox &gt;/dev/null &amp;&amp; echo \"OK: storagebox readable\" || echo \"BAD: storagebox unreadable\"\nmount | rg '/srv/healtharchive/jobs/|/srv/healtharchive/storagebox'\n</code></pre> <p>Recovery playbook:</p> <ul> <li><code>../operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"deployment/production-single-vps/#5-https-dns-caddy","title":"5) HTTPS + DNS (Caddy)","text":"<p>1) DNS (Namecheap): <code>A api.healtharchive.ca -&gt; &lt;VPS_PUBLIC_IP&gt;</code> 2) Install Caddy: <code>sudo apt -y install caddy</code> 3) Caddyfile: <code>/etc/caddy/Caddyfile</code></p> <pre><code>api.healtharchive.ca {\n  header Strict-Transport-Security \"max-age=31536000\"\n  reverse_proxy 127.0.0.1:8001\n}\n</code></pre> <p>4) Validate + reload:</p> <pre><code>sudo caddy fmt --overwrite /etc/caddy/Caddyfile\nsudo caddy validate --config /etc/caddy/Caddyfile\nsudo systemctl reload caddy\n</code></pre> <p>Verify:</p> <pre><code>curl -i https://api.healtharchive.ca/api/health\n</code></pre>"},{"location":"deployment/production-single-vps/#51-optional-replay-service-pywb","title":"5.1) Optional: replay service (pywb)","text":"<p>Full-fidelity browsing (CSS/JS/images) requires a replay engine. If you want \u201cclick links and stay inside the archived backup\u201d, deploy pywb behind Caddy:</p> <ul> <li>Runbook: <code>deployment/replay-service-pywb.md</code></li> </ul> <p>Operational warning:</p> <ul> <li><code>ha-backend cleanup-job --mode temp</code> removes temp dirs including WARCs.   Replay depends on WARCs staying on disk, so do not run cleanup for any job   you intend to keep replayable.   If replay is enabled globally (<code>HEALTHARCHIVE_REPLAY_BASE_URL</code> is set),   <code>cleanup-job --mode temp</code> will refuse unless you pass <code>--force</code>.</li> </ul> <p>Optional UX improvement:</p> <ul> <li>If <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code> is configured, the API can serve cached   PNG \u201chomepage previews\u201d used by the frontend on <code>/archive</code>.   See <code>deployment/replay-service-pywb.md</code> (\u201cCached source preview images\u201d) for   the generation command.</li> </ul>"},{"location":"deployment/production-single-vps/#6-tailscale-sshprivate-access-only","title":"6) Tailscale (SSH/private access only)","text":"<ul> <li>Installed on VPS, NAS, and admin workstation.</li> <li>VPS Tailscale IP: <code>100.x.y.z</code> (example)</li> <li>SSH only allowed on <code>tailscale0</code> in UFW; public port 22 blocked at Hetzner.</li> <li>Hetzner firewall adds UDP 41641 for better Tailscale connectivity.</li> <li>Recommended: disable Tailscale key expiry for the VPS and NAS devices in the   Tailscale admin UI so access does not silently expire.</li> </ul> <p>Usage:</p> <pre><code>ssh -i ~/.ssh/healtharchive_hetzner haadmin@100.x.y.z\n</code></pre> <p>Public SSH: - Expected to fail: <code>ssh haadmin@api.healtharchive.ca</code> (closed).</p>"},{"location":"deployment/production-single-vps/#7-backups-nas-pull-rsync-over-tailscale","title":"7) Backups + NAS pull (rsync over Tailscale)","text":"<p>VPS backup user: - <code>habackup</code> user with NAS public key in <code>/home/habackup/.ssh/authorized_keys</code></p> <p>Backup script: <code>/usr/local/bin/healtharchive-db-backup</code> - <code>pg_dump -Fc</code> to <code>/srv/healtharchive/backups/healtharchive_&lt;ts&gt;.dump</code> - 14-day retention - Healthchecks <code>/start</code>/<code>/fail</code>/success pings (see \u00a78)</p> <p>Systemd: - <code>/etc/systemd/system/healtharchive-db-backup.service</code> - <code>/etc/systemd/system/healtharchive-db-backup.timer</code> (daily ~03:30 UTC, randomized delay)</p> <p>NAS pull: - NAS key: <code>~/.ssh/ha_backup_nas</code> (no passphrase) - SSH config alias on NAS:</p> <pre><code>Host ha-vps\n  HostName 100.x.y.z\n  User habackup\n  IdentityFile ~/.ssh/ha_backup_nas\n  IdentitiesOnly yes\n  StrictHostKeyChecking accept-new\n</code></pre> <ul> <li>Rsync command (used manually + DSM scheduled task):</li> </ul> <pre><code>rsync -av --delete ha-vps:/srv/healtharchive/backups/ /volume1/nobak/healtharchive/backups/db/\n</code></pre>"},{"location":"deployment/production-single-vps/#8-healthchecksio-backup-disk","title":"8) Healthchecks.io (backup + disk)","text":"<p>Secrets file: <code>/etc/healtharchive/healthchecks.env</code> (mode 600)</p> <p>Notes:</p> <ul> <li>This env file may also be used by the newer systemd unit templates under   <code>docs/deployment/systemd/</code> (which use <code>HEALTHARCHIVE_HC_PING_*</code> variable names).   It is OK to keep both sets of variables in the same file.</li> <li>Avoid placeholder values like <code>https://hc-ping.com/&lt;uuid&gt;</code> in this file if you   ever <code>source</code> it from bash; the <code>&lt;</code>/<code>&gt;</code> characters can break shell parsing.</li> </ul> <pre><code>HC_DB_BACKUP_URL=https://hc-ping.com/UUID_HERE\nHC_DISK_URL=https://hc-ping.com/UUID_HERE\nHC_DISK_THRESHOLD=80\n</code></pre> <p>Disk check: - Script: <code>/usr/local/bin/healtharchive-disk-check</code> - Service/Timer: <code>healtharchive-disk-check.service</code> / <code>healtharchive-disk-check.timer</code> (hourly) - Pings success; sends <code>/fail</code> if <code>/</code> or <code>/srv/healtharchive</code> exceeds 80%.</p>"},{"location":"deployment/production-single-vps/#9-synthetic-snapshot-for-smoke-testing","title":"9) Synthetic snapshot for smoke testing","text":"<p>Created a minimal WARC + Snapshot for smoke checks: - WARC: <code>/srv/healtharchive/jobs/manual-warcs/viewer-test.warc.gz</code> - Snapshot ID: <code>1</code> - Raw: <code>https://api.healtharchive.ca/api/snapshots/raw/1</code> - Viewer: <code>https://www.healtharchive.ca/snapshot/1</code></p> <p>Use this to verify end-to-end viewer behavior after deploys.</p>"},{"location":"deployment/production-single-vps/#10-restore-drill-completed","title":"10) Restore drill (completed)","text":"<p>Procedure:</p> <pre><code>latest=\"$(ls -t /srv/healtharchive/backups/healtharchive_*.dump | head -n 1)\"\nsudo -u postgres dropdb --if-exists healtharchive_restore_test\nsudo -u postgres createdb healtharchive_restore_test\nsudo -u postgres pg_restore --no-owner --no-acl -d healtharchive_restore_test &lt; \"$latest\"\nsudo -u postgres psql -d healtharchive_restore_test -c \"select count(*) from snapshots;\"\nsudo -u postgres dropdb healtharchive_restore_test\n</code></pre> <p>Result: restore succeeded, <code>snapshots</code> contained 1 row (the synthetic test snapshot).</p>"},{"location":"deployment/production-single-vps/#11-external-uptime-checks-recommended","title":"11) External uptime checks (recommended)","text":"<p>Configure an external monitor (e.g., UptimeRobot) for: - <code>https://api.healtharchive.ca/api/health</code> - <code>https://www.healtharchive.ca/archive</code> - (Optional) <code>https://replay.healtharchive.ca/</code> (if replay is enabled/in use)</p> <p>Note: some providers use <code>HEAD</code> by default; the backend supports <code>HEAD /api/health</code>.</p> <p>For a more detailed, step-by-step checklist (including branch protection / CI enforcement), see:</p> <ul> <li><code>../operations/monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"deployment/production-single-vps/#12-current-known-defaultsassumptions-2025-12","title":"12) Current known defaults/assumptions (2025-12)","text":"<ul> <li>CORS allowlist: <code>https://healtharchive.ca</code>, <code>https://www.healtharchive.ca</code>, <code>https://healtharchive.vercel.app</code></li> <li>Vercel envs set to use <code>https://api.healtharchive.ca</code> for both Preview and Production</li> <li>No staging backend; Preview and Production frontends point to the same API</li> <li>Public SSH closed; Tailscale required for admin/backup access</li> </ul>"},{"location":"deployment/replay-service-pywb/","title":"HealthArchive \u2013 Replay Service (pywb) runbook","text":"<p>This document covers setting up full\u2011fidelity web replay (HTML + CSS/JS/images/fonts) for HealthArchive using a dedicated pywb service behind Caddy.</p> <p>It is intentionally written so a future operator can follow it without needing additional context.</p>"},{"location":"deployment/replay-service-pywb/#0-what-this-is-and-is-not","title":"0) What this is (and is not)","text":"<p>What this provides</p> <ul> <li>A replay origin: <code>https://replay.healtharchive.ca</code></li> <li>Wayback\u2011style replay from the project\u2019s WARC files</li> <li>Natural browsing: links stay inside replay, and captured assets (CSS/JS/images) load from the archive when present</li> </ul> <p>What this does not provide</p> <ul> <li>Guaranteed completeness. If a page depends on third\u2011party CSS/JS/images that were   not captured into the WARCs, those assets will still be missing at replay time.</li> <li>A custom replay UI in pywb itself. HealthArchive provides the primary browsing   experience via the frontend wrapper pages (see \u201cBackend wiring\u201d below).</li> </ul>"},{"location":"deployment/replay-service-pywb/#1-core-decisions-contract","title":"1) Core decisions (contract)","text":""},{"location":"deployment/replay-service-pywb/#11-collections-are-per-archivejob","title":"1.1 Collections are per ArchiveJob","text":"<ul> <li>Each <code>ArchiveJob</code> becomes a replay \u201cedition\u201d.</li> <li> <p>Collection name is stable and mechanical:</p> </li> <li> <p><code>job-&lt;job_id&gt;</code> (example: <code>job-1</code>)</p> </li> </ul> <p>This makes it easy to generate replay URLs from DB data later.</p>"},{"location":"deployment/replay-service-pywb/#12-replay-url-format","title":"1.2 Replay URL format","text":"<p>We will use pywb\u2019s standard collection routing.</p> <p>Replay latest capture (most common):</p> <pre><code>https://replay.healtharchive.ca/&lt;collection&gt;/&lt;original_url&gt;\n</code></pre> <p>List captures (\u201ccalendar\u201d / capture list UI):</p> <pre><code>https://replay.healtharchive.ca/&lt;collection&gt;/*/&lt;original_url&gt;\n</code></pre> <p>Replay closest capture to a timestamp (14-digit UTC <code>YYYYMMDDhhmmss</code>):</p> <pre><code>https://replay.healtharchive.ca/&lt;collection&gt;/&lt;timestamp&gt;/&lt;original_url&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;collection&gt;</code> is <code>job-&lt;job_id&gt;</code></li> <li><code>&lt;timestamp&gt;</code> is UTC in <code>YYYYMMDDhhmmss</code> (14 digits)</li> </ul> <p>Example (latest capture):</p> <pre><code>https://replay.healtharchive.ca/job-1/https://www.canada.ca/en/health-canada.html\n</code></pre> <p>Note: HealthArchive\u2019s public API generates timestamp-locked replay URLs for snapshots (the <code>&lt;timestamp&gt;</code> form) so the viewer stays anchored to the capture time as you navigate within the backup.</p>"},{"location":"deployment/replay-service-pywb/#13-retention-warning-replay-depends-on-warcs-staying-on-disk","title":"1.3 Retention warning: replay depends on WARCs staying on disk","text":"<p>Replay reads from the WARC files referenced by each job.</p> <p>Important: <code>ha-backend cleanup-job --mode temp</code> currently removes archive_tool temp dirs including WARCs (see <code>src/ha_backend/cli.py:cmd_cleanup_job</code>).</p> <p>If you run cleanup on a replayable job, replay will break.</p> <p>Operational rule for now: do not run <code>cleanup-job --mode temp</code> for any job you want replayable.</p> <p>When replay is enabled (backend env var <code>HEALTHARCHIVE_REPLAY_BASE_URL</code> is set), <code>cleanup-job --mode temp</code> will refuse to run unless you pass <code>--force</code>.</p> <p>This rule is repeated in:</p> <ul> <li><code>docs/deployment/production-single-vps.md</code></li> <li><code>docs/deployment/hosting-and-live-server-to-dos.md</code></li> <li><code>docs/development/live-testing.md</code></li> </ul>"},{"location":"deployment/replay-service-pywb/#2-dns-tls","title":"2) DNS + TLS","text":"<p>Create DNS:</p> <ul> <li><code>A replay.healtharchive.ca -&gt; &lt;VPS_PUBLIC_IP&gt;</code></li> </ul> <p>TLS is handled by Caddy automatically once the site block exists.</p> <p>Before continuing, SSH to the VPS as your admin user (typically over Tailscale):</p> <pre><code>ssh -i ~/.ssh/healtharchive_hetzner haadmin@&lt;VPS_TAILSCALE_IP&gt;\n</code></pre>"},{"location":"deployment/replay-service-pywb/#3-vps-directory-layout","title":"3) VPS directory layout","text":"<p>On the VPS:</p> <ul> <li>WARCs/job outputs already live under:</li> <li><code>/srv/healtharchive/jobs</code></li> <li>Replay service state (config, collections, indexes) will live under:</li> <li><code>/srv/healtharchive/replay</code></li> </ul> <p>Create directories:</p> <pre><code>sudo mkdir -p /srv/healtharchive/replay\nsudo mkdir -p /srv/healtharchive/replay/collections\n</code></pre> <p>Create a dedicated system user for the replay volume:</p> <pre><code>sudo adduser --system --no-create-home --ingroup healtharchive hareplay\n</code></pre> <p>Recommended perms (important):</p> <pre><code>sudo chown -R hareplay:healtharchive /srv/healtharchive/replay\nsudo chmod 2770 /srv/healtharchive/replay /srv/healtharchive/replay/collections\n</code></pre> <p>Why the <code>hareplay</code> ownership matters:</p> <ul> <li>Your WARC files are typically <code>640</code> and group-owned by <code>healtharchive</code>.</li> <li>The pywb container is hardened with <code>--cap-drop=ALL</code>, which means \u201croot\u201d in   the container cannot bypass Unix permissions (no <code>CAP_DAC_OVERRIDE</code>).</li> <li>We will also run the container as the <code>hareplay</code> UID/GID explicitly (below),   so pywb can:</li> <li>write its indexes under <code>/webarchive</code> (owned by <code>hareplay:healtharchive</code>)</li> <li>read group-readable WARCs under <code>/warcs</code> (group <code>healtharchive</code>)</li> </ul>"},{"location":"deployment/replay-service-pywb/#4-pywb-container-deployment-systemd-docker","title":"4) pywb container deployment (systemd + Docker)","text":"<p>We run pywb only on localhost (Caddy is the public edge).</p>"},{"location":"deployment/replay-service-pywb/#41-create-pywb-config","title":"4.1 Create pywb config","text":"<p>Create <code>/srv/healtharchive/replay/config.yaml</code>:</p> <pre><code>debug: false\n\n# We embed replay inside a HealthArchive wrapper UI later; disable pywb\u2019s framed\n# replay chrome so the page itself renders \u201cas captured\u201d.\nframed_replay: false\n\n# Prefer stable URLs once a capture is resolved.\nredirect_to_exact: true\n\n# Optional: expose an aggregate across all on-disk collections at `/all/...`.\n# (This is not required for per-job collections like `/job-1/...`.)\n# collections:\n#   all: $all\n</code></pre>"},{"location":"deployment/replay-service-pywb/#42-create-systemd-service","title":"4.2 Create systemd service","text":"<p>Create <code>/etc/systemd/system/healtharchive-replay.service</code>:</p> <pre><code>[Unit]\nDescription=HealthArchive replay (pywb)\nAfter=network.target docker.service\nRequires=docker.service\n\n[Service]\nType=simple\nRestart=always\nRestartSec=3\n\n# Safety: start clean\nExecStartPre=-/usr/bin/docker rm -f healtharchive-replay\nExecStartPre=/usr/bin/docker pull webrecorder/pywb:2.9.1\n\n# Run on localhost only; Caddy terminates TLS publicly.\nExecStart=/usr/bin/docker run --rm --name healtharchive-replay \\\n  -p 127.0.0.1:8090:8080 \\\n  --user &lt;HAREPLAY_UID&gt;:&lt;HEALTHARCHIVE_GID&gt; \\\n  --cap-drop=ALL \\\n  --security-opt no-new-privileges:true \\\n  -v /srv/healtharchive/replay:/webarchive:rw \\\n  -v /srv/healtharchive/jobs:/warcs:ro,rshared \\\n  webrecorder/pywb:2.9.1\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Notes:</p> <ul> <li><code>HAREPLAY_UID</code> comes from <code>id -u hareplay</code> (often <code>110</code>).</li> <li><code>HEALTHARCHIVE_GID</code> comes from <code>getent group healtharchive</code> (3rd <code>:</code>-separated field).</li> <li>We run as <code>hareplay:healtharchive</code> to avoid the container needing to   <code>useradd</code>/<code>su</code> internally (which fails when <code>--cap-drop=ALL</code> removes   <code>CAP_SETUID</code>/<code>CAP_SETGID</code>).</li> <li>The <code>rshared</code> bind propagation on <code>/srv/healtharchive/jobs</code> helps pywb see   nested mounts under that tree (e.g., Storage Box tiering bind mounts) without   requiring a container restart after mount repairs.</li> </ul> <p>Enable + start:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable --now healtharchive-replay.service\nsudo systemctl status healtharchive-replay.service --no-pager\n</code></pre> <p>Local check (on the VPS):</p> <pre><code>curl -I http://127.0.0.1:8090/ | head\n</code></pre> <p>If <code>wb-manager reindex</code> fails with <code>Permission denied</code>:</p> <ul> <li>Double-check:</li> <li><code>/srv/healtharchive/replay</code> is owned by <code>hareplay:healtharchive</code> (not <code>root:healtharchive</code>)</li> <li>the systemd unit runs with <code>--user &lt;hareplay_uid&gt;:&lt;healtharchive_gid&gt;</code></li> </ul> <p>Then restart:</p> <pre><code>sudo chown -R hareplay:healtharchive /srv/healtharchive/replay\nsudo systemctl restart healtharchive-replay.service\n</code></pre>"},{"location":"deployment/replay-service-pywb/#5-caddy-config-public-https","title":"5) Caddy config (public HTTPS)","text":"<p>Edit <code>/etc/caddy/Caddyfile</code> and add:</p> <pre><code>replay.healtharchive.ca {\n  encode zstd gzip\n\n  # Replay needs to be embeddable by the HealthArchive frontend.\n  # (The frontend wrapper provides the visible banner/controls.)\n  header {\n    -X-Frame-Options\n    Content-Security-Policy \"frame-ancestors https://healtharchive.ca https://www.healtharchive.ca\"\n  }\n\n  reverse_proxy 127.0.0.1:8090\n}\n</code></pre> <p>Validate + reload:</p> <pre><code>sudo caddy fmt --overwrite /etc/caddy/Caddyfile\nsudo caddy validate --config /etc/caddy/Caddyfile\nsudo systemctl reload caddy\n</code></pre> <p>Public check (from your laptop):</p> <pre><code>curl -I https://replay.healtharchive.ca/ | head\n</code></pre>"},{"location":"deployment/replay-service-pywb/#51-optional-healtharchive-banner-on-direct-replay-pages","title":"5.1) Optional: HealthArchive banner on direct replay pages","text":"<p>The HealthArchive frontend provides the primary replay UX via <code>/snapshot/&lt;id&gt;</code> and <code>/browse/&lt;id&gt;</code> (header, navigation, disclaimers). Users may still open <code>replay.healtharchive.ca</code> directly in a new tab.</p> <p>To reduce confusion, you can inject a small HealthArchive banner into pywb\u2019s non-framed replay HTML using pywb\u2019s <code>custom_banner.html</code> hook.</p> <p>Implementation notes:</p> <ul> <li>The banner is inserted only for non-framed replay (our default).</li> <li>When replay is embedded in an iframe, the banner collapses to a minimal UI   (View diff + Details + Hide) to avoid duplicating the HealthArchive wrapper header.</li> <li>When embedded, the script also emits lightweight <code>postMessage</code> events with   the current replay URL/timestamp so the HealthArchive frontend can support   edition switching while you browse.</li> <li>Users can dismiss it via the Hide button (stored in <code>localStorage</code> on the   replay origin).</li> </ul> <p>Deploy on the VPS:</p> <pre><code>sudo mkdir -p /srv/healtharchive/replay/templates\nsudo install -o hareplay -g healtharchive -m 0640 \\\n  /opt/healtharchive-backend/docs/deployment/pywb/custom_banner.html \\\n  /srv/healtharchive/replay/templates/custom_banner.html\n\nsudo systemctl restart healtharchive-replay.service\n</code></pre> <p>Notes:</p> <ul> <li>The banner script calls the HealthArchive public API from the replay origin   (for example, <code>GET /api/replay/resolve</code>) to resolve the snapshot ID and build   the correct \u201cback to snapshot\u201d and compare links. Ensure the backend CORS   allowlist includes <code>https://replay.healtharchive.ca</code> when the banner is   enabled.</li> <li>The direct-replay banner is a compact sticky top bar: title, capture date,   original URL, an always-visible disclaimer line, and action links (View diff,   Details, All snapshots, Raw HTML, Metadata JSON, Cite, Report issue, Hide).   \u201cAll snapshots\u201d opens a right-aligned popover list.</li> <li>On small screens, the banner stacks the action links below the \u201c\u2190 HealthArchive.ca\u201d   button to avoid overlap; when you scroll, it transitions into a more compact   mode that hides the metadata/disclaimer line.</li> <li>The \u201c\u2190 HealthArchive.ca\u201d button returns to the HealthArchive page the user came   from when possible (the frontend passes an explicit return path in the replay   URL fragment). If no return path is available, it falls back to the archive   search page for the current original URL.</li> <li>The banner uses <code>XMLHttpRequest</code> with pywb\u2019s wombat opt-out (<code>xhr._no_rewrite = true</code>)   so API requests are not replay-rewritten. Ensure CORS allows the   <code>X-Pywb-Requested-With</code> header from <code>https://replay.healtharchive.ca</code>.</li> <li>Production expects the public API to be reachable at <code>https://api.&lt;apex&gt;</code> (for   example, <code>https://api.healtharchive.ca</code>). If your deployment instead proxies   <code>/api</code> on the frontend origin, ensure the banner\u2019s API base candidates are   still valid for your hostnames.</li> <li>If you deploy the backend using <code>./scripts/vps-deploy.sh --apply --restart-replay</code>,   the deploy helper will also install <code>custom_banner.html</code> and restart the replay   service as part of that run (single-VPS setup).</li> </ul> <p>Note: the banner can be disabled for screenshot generation by adding a fragment:</p> <pre><code>...#ha_nobanner=1\n</code></pre>"},{"location":"deployment/replay-service-pywb/#6-create-a-collection-and-index-a-jobs-warcs-no-copying","title":"6) Create a collection and index a job\u2019s WARCs (no copying)","text":"<p>pywb\u2019s <code>wb-manager</code> requires WARC files to exist in the collection\u2019s <code>archive/</code> directory. We avoid duplicating data by placing symlinks to the real WARC files (mounted read-only at <code>/warcs</code> in the container).</p>"},{"location":"deployment/replay-service-pywb/#60-recommended-use-the-backend-cli-one-command-per-job","title":"6.0 Recommended: use the backend CLI (one command per job)","text":"<p>If the backend and pywb run on the same VPS, you can make a job replayable via:</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend replay-index-job --id 1\n</code></pre> <p>Dry-run (prints actions without changes):</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend replay-index-job --id 1 --dry-run\n</code></pre>"},{"location":"deployment/replay-service-pywb/#61-initialize-collection-for-job-1","title":"6.1 Initialize collection for job 1","text":"<pre><code>sudo docker exec healtharchive-replay wb-manager init job-1\n</code></pre>"},{"location":"deployment/replay-service-pywb/#62-link-job-1-warcs-into-the-collection","title":"6.2 Link job 1 WARCs into the collection","text":"<p>1) Determine the job output directory:</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend show-job --id 1\n</code></pre> <p>2) Find WARCs under that output directory:</p> <pre><code>OUTPUT_DIR=\"/srv/healtharchive/jobs/imports/legacy-hc-2025-04-21\"  # example; replace\nfind \"$OUTPUT_DIR\" -type f -name '*.warc.gz' | sort &gt; /tmp/job-1-warcs.txt\nwc -l /tmp/job-1-warcs.txt\n</code></pre> <p>3) Convert host paths \u2192 container paths (because we mount <code>/srv/healtharchive/jobs</code> as <code>/warcs</code>):</p> <pre><code>sed 's#^/srv/healtharchive/jobs#\\/warcs#' /tmp/job-1-warcs.txt &gt; /tmp/job-1-warcs.container.txt\n</code></pre> <p>4) Create symlinks in the collection archive directory (prefixing with a stable counter to avoid name collisions):</p> <pre><code>COLL_ARCHIVE_DIR=\"/srv/healtharchive/replay/collections/job-1/archive\"\nsudo mkdir -p \"$COLL_ARCHIVE_DIR\"\n\nnl -ba /tmp/job-1-warcs.container.txt | while read -r n p; do\n  printf -v linkname \"warc-%06d.warc.gz\" \"$n\"\n  sudo ln -sf \"$p\" \"$COLL_ARCHIVE_DIR/$linkname\"\ndone\n</code></pre> <p>Note: the symlink targets are container paths under <code>/warcs/...</code>, so they may appear \u201cbroken\u201d when inspected on the host. They will resolve correctly inside the container because <code>/srv/healtharchive/jobs</code> is mounted as <code>/warcs</code>.</p> <p>5) Index:</p> <pre><code>sudo docker exec healtharchive-replay wb-manager reindex job-1\n</code></pre>"},{"location":"deployment/replay-service-pywb/#63-verify-replay-works","title":"6.3 Verify replay works","text":"<p>Pick a known URL in the job (example HC homepage):</p> <pre><code>curl -I \"https://replay.healtharchive.ca/job-1/https://www.canada.ca/en/health-canada.html\" | head\n</code></pre> <p>In a browser:</p> <ul> <li>Open the same URL and click around.</li> <li>Confirm CSS/images load and links stay under <code>replay.healtharchive.ca/job-1/...</code>.</li> </ul>"},{"location":"deployment/replay-service-pywb/#64-repeat-for-another-job-example-cihr","title":"6.4 Repeat for another job (example: CIHR)","text":"<p>Once the CIHR legacy WARCs are imported and indexed as an <code>ArchiveJob</code> (see <code>docs/operations/legacy-crawl-imports.md</code>), repeat the same steps with that job ID:</p> <ul> <li>Recommended:</li> <li><code>ha-backend replay-index-job --id &lt;id&gt;</code></li> <li><code>wb-manager init job-&lt;id&gt;</code></li> <li>Symlink that job\u2019s WARCs into <code>/srv/healtharchive/replay/collections/job-&lt;id&gt;/archive/</code></li> <li><code>wb-manager reindex job-&lt;id&gt;</code></li> <li>Verify: <code>https://replay.healtharchive.ca/job-&lt;id&gt;/&lt;some captured url&gt;/</code></li> </ul>"},{"location":"deployment/replay-service-pywb/#7-troubleshooting","title":"7) Troubleshooting","text":"<ul> <li>Blank pages / missing styling: the asset was not captured into the WARC set, or the page uses live third\u2011party resources not archived.</li> <li>Replay 404 but snapshot exists in DB: the job\u2019s WARCs were not linked+indexed into pywb (or you ran <code>cleanup-job</code> and deleted WARCs).</li> <li>Replay UI shows \u201cAll-time (0 captures)\u201d: that exact URL (including scheme + host, eg <code>www.</code> vs non-<code>www</code>) likely isn\u2019t present in the WARC set. Confirm via <code>/&lt;collection&gt;/cdx?url=...</code> and try host/scheme variants.</li> <li>Iframe blocked: check <code>frame-ancestors</code> header on <code>replay.healtharchive.ca</code> and ensure you removed <code>X-Frame-Options</code>.</li> <li>Service crash-loop with <code>groupadd/useradd</code> and <code>su: Authentication failure</code>: the container entrypoint is trying to create/switch users, but <code>--cap-drop=ALL</code> removes the capabilities needed. Fix by running the container as the host UID/GID directly via <code>--user &lt;hareplay_uid&gt;:&lt;healtharchive_gid&gt;</code>.</li> </ul>"},{"location":"deployment/replay-service-pywb/#8-backend-wiring-optional-but-recommended","title":"8) Backend wiring (optional, but recommended)","text":"<p>If you want the HealthArchive frontend to embed replay by default, configure the backend to emit a <code>browseUrl</code> for each snapshot.</p> <p>On the VPS (backend host), set in <code>/etc/healtharchive/backend.env</code>:</p> <pre><code>HEALTHARCHIVE_REPLAY_BASE_URL=https://replay.healtharchive.ca\n</code></pre> <p>Then restart the backend service.</p>"},{"location":"deployment/replay-service-pywb/#81-edition-switching-v2-preserve-current-page-across-backups","title":"8.1 Edition switching (v2: \u201cpreserve current page across backups\u201d)","text":"<p>HealthArchive supports switching \u201ceditions\u201d (jobs) while keeping you on the same original URL when possible.</p> <p>This is implemented as:</p> <ul> <li><code>GET /api/sources/{sourceCode}/editions</code></li> <li>lists replayable jobs (editions) for the source, including each job\u2019s     <code>entryBrowseUrl</code> (a good fallback when a specific page wasn\u2019t captured).</li> <li><code>POST /api/replay/resolve</code></li> <li>input: <code>{ \"jobId\": &lt;id&gt;, \"url\": \"&lt;original_url&gt;\", \"timestamp14\": \"YYYYMMDDhhmmss\" | null }</code></li> <li>output: a best-effort <code>browseUrl</code> for the selected job if a capture exists     (or <code>found=false</code> when it does not).</li> </ul> <p>The frontend relies on lightweight <code>postMessage</code> events emitted by the replay banner template (see \u201cOptional: HealthArchive banner on direct replay pages\u201d above) to learn the current original URL while the user clicks around inside replay.</p> <p>Frontend-side details and verification are documented in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md</li> </ul> <p>Frontend verification (recommended):</p> <ul> <li>See https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md for the end-to-end   checks that confirm:</li> <li>snapshot pages embed replay correctly, and</li> <li><code>/browse/&lt;snapshotId&gt;</code> provides a full-screen browsing wrapper with a     persistent HealthArchive banner above the replay iframe.</li> </ul>"},{"location":"deployment/replay-service-pywb/#9-cached-source-preview-images-optional-recommended","title":"9) Cached source preview images (optional, recommended)","text":"<p>The frontend <code>/archive</code> page can show a lightweight \u201chomepage preview\u201d tile for each source\u2019s latest replayable backup.</p> <p>To avoid rendering live iframes on every page load, these previews are served as cached static images generated out-of-band.</p>"},{"location":"deployment/replay-service-pywb/#91-configure-preview-directory-vps","title":"9.1 Configure preview directory (VPS)","text":"<p>Choose a directory on the VPS:</p> <ul> <li>Recommended: <code>/srv/healtharchive/replay/previews</code></li> </ul> <p>Create it with the same ownership model as the replay volume:</p> <pre><code>sudo mkdir -p /srv/healtharchive/replay/previews\nsudo chown -R hareplay:healtharchive /srv/healtharchive/replay/previews\nsudo chmod 2770 /srv/healtharchive/replay/previews\n</code></pre> <p>In <code>/etc/healtharchive/backend.env</code>, set:</p> <pre><code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR=/srv/healtharchive/replay/previews\n</code></pre> <p>Then restart the API:</p> <pre><code>sudo systemctl restart healtharchive-api\n</code></pre>"},{"location":"deployment/replay-service-pywb/#92-generate-previews-vps","title":"9.2 Generate previews (VPS)","text":"<p>Generate (or refresh) previews for all sources with:</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend replay-generate-previews\n</code></pre> <p>This uses a Playwright container to screenshot each source\u2019s <code>entryBrowseUrl</code> (with <code>#ha_nobanner=1</code> so the pywb banner is not captured).</p> <p>Note: The generator caches Playwright\u2019s Node.js dependencies under <code>&lt;preview_dir_parent&gt;/.preview-node/</code>. If you point <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code> at a path inside your repo for local testing, ensure <code>.preview-node/</code> is ignored by git (it is in <code>.gitignore</code>).</p>"},{"location":"deployment/replay-service-pywb/#93-verify-previews-are-available","title":"9.3 Verify previews are available","text":"<p>1) Confirm <code>/api/sources</code> advertises <code>entryPreviewUrl</code> where available:</p> <pre><code>curl -s https://api.healtharchive.ca/api/sources | python3 -m json.tool | rg entryPreviewUrl\n</code></pre> <p>2) Confirm an individual preview serves as an image:</p> <pre><code>curl -I \"https://api.healtharchive.ca/api/sources/hc/preview?jobId=1\" | head\n</code></pre>"},{"location":"deployment/search-rollout/","title":"Search ranking rollout (v2 default)","text":"<p>This is the recommended rollout procedure for enabling the blended search ranking (v2) in production.</p> <p>Decision: use v2 by default via <code>HA_SEARCH_RANKING_VERSION=v2</code>.</p> <p>Rationale (given current project goals/resources): - We\u2019re intentionally staying on Postgres FTS + lightweight heuristics (no separate search service). - v2 materially improves broad-query \u201chub\u201d discovery using signals we already compute (<code>page_signals</code>, <code>snapshot_outlinks</code>). - Rollback is instant and low-risk (flip one env var + restart API).</p>"},{"location":"deployment/search-rollout/#0-preconditions","title":"0) Preconditions","text":"<ul> <li>You have already deployed the authority schema (tables <code>snapshot_outlinks</code> and <code>page_signals</code>) and populated outlinks for your jobs.</li> <li>If <code>snapshot_outlinks</code> is empty, v2 won\u2019t get the link-graph benefits.</li> <li>You have the admin token available (for <code>/api/admin/search-debug</code> verification).</li> </ul>"},{"location":"deployment/search-rollout/#1-recommended-production-rollout-steps-single-vps","title":"1) Recommended production rollout steps (single VPS)","text":"<p>On the VPS (as <code>haadmin</code>) in the backend repo checkout (e.g. <code>/opt/healtharchive-backend</code>):</p> <p>1) Pull the new backend revision. 2) Apply migrations:    - <code>./.venv/bin/alembic upgrade head</code> 3) Recompute link signals (populates <code>inlink_count</code>, <code>outlink_count</code>, and <code>pagerank</code> when present):    - <code>./.venv/bin/ha-backend recompute-page-signals</code> 4) Enable v2 by default:    - Edit <code>/etc/healtharchive/backend.env</code> and set <code>HA_SEARCH_RANKING_VERSION=v2</code> 5) Restart only the API process (worker does not need the ranking env var):    - <code>sudo systemctl restart healtharchive-api</code></p> <p>Notes: - <code>ha-backend recompute-page-signals</code> can take a while on large graphs; run it in <code>tmux</code> and consider off-peak hours. - If you have not yet backfilled outlinks for existing WARCs, do that first (per job):   - <code>./.venv/bin/ha-backend backfill-outlinks --job-id &lt;JOB_ID&gt; --update-signals</code>   - Then run <code>./.venv/bin/ha-backend recompute-page-signals</code> once after the backfills finish.</p>"},{"location":"deployment/search-rollout/#2-verification-checklist-production","title":"2) Verification checklist (production)","text":"<p>1) Health:    - <code>curl -s https://api.healtharchive.ca/api/health | python3 -m json.tool</code> 2) Search v2 is active by default:    - <code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;view=pages&amp;sort=relevance&amp;pageSize=20\" | python3 -m json.tool | head</code> 3) Compare explicitly:    - <code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;view=pages&amp;sort=relevance&amp;pageSize=20&amp;ranking=v1\" | python3 -m json.tool | head</code>    - <code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;view=pages&amp;sort=relevance&amp;pageSize=20&amp;ranking=v2\" | python3 -m json.tool | head</code> 4) Debug a ranking decision (admin token required):    - <code>curl -s \"https://api.healtharchive.ca/api/admin/search-debug?q=covid&amp;view=pages&amp;sort=relevance&amp;ranking=v2&amp;pageSize=10\" -H \"X-Admin-Token: $HEALTHARCHIVE_ADMIN_TOKEN\" | python3 -m json.tool</code></p>"},{"location":"deployment/search-rollout/#3-capture-diff-recommended-smoke-eval","title":"3) Capture + diff (recommended \u201csmoke eval\u201d)","text":"<p>From any machine (your laptop is fine):</p> <p>1) Capture:    - <code>./scripts/search-eval-capture.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval --page-size 20 --ranking v1</code>    - <code>./scripts/search-eval-capture.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval --page-size 20 --ranking v2</code> 2) Diff:    - <code>python ./scripts/search-eval-diff.py --a /tmp/ha-search-eval/&lt;TS_V1&gt; --b /tmp/ha-search-eval/&lt;TS_V2&gt; --top 20</code></p> <p>Or run capture+diff in one command:</p> <ul> <li><code>./scripts/search-eval-run.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval</code></li> </ul>"},{"location":"deployment/search-rollout/#4-rollback-plan-fast","title":"4) Rollback plan (fast)","text":"<p>If something looks off in production:</p> <p>1) Set <code>HA_SEARCH_RANKING_VERSION=v1</code> in <code>/etc/healtharchive/backend.env</code> 2) <code>sudo systemctl restart healtharchive-api</code></p> <p>This reverts default behavior immediately without touching data/migrations. You can still test v2 per-request with <code>&amp;ranking=v2</code> while investigating.</p>"},{"location":"deployment/staging-rollout-checklist/","title":"Staging rollout checklist \u2013 backend + frontend","text":"<p>This file turns the higher\u2011level hosting notes into a step\u2011by\u2011step checklist for bringing a staging environment online. It assumes:</p> <ul> <li>Staging API host: <code>https://api-staging.healtharchive.ca</code></li> <li>Staging frontend/preview: <code>https://healtharchive.vercel.app</code> +   branch\u2011specific preview URLs.</li> <li>Code from <code>main</code> in both repos has been deployed to the staging host /   Vercel.</li> </ul> <p>It does not require or describe changes on your local machine, beyond pushing commits; all steps here are meant to be performed on the staging host and in Vercel / GitHub.</p> <p>Note: the current production deployment intentionally runs without a separate staging backend. If you are following the single\u2011VPS production runbook, you can skip this checklist unless/until you decide to add staging.</p> <p>For background, see:</p> <ul> <li><code>hosting-and-live-server-to-dos.md</code></li> <li><code>environments-and-configuration.md</code></li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md</li> </ul>"},{"location":"deployment/staging-rollout-checklist/#1-backend-staging-environment","title":"1. Backend staging environment","text":""},{"location":"deployment/staging-rollout-checklist/#11-set-env-vars-on-the-staging-backend-host","title":"1.1 Set env vars on the staging backend host","text":"<p>On the staging host (VM/container/PaaS):</p> <ol> <li>Decide where you want jobs and WARCs to live, e.g.:</li> </ol> <pre><code>/srv/healtharchive/jobs-staging\n</code></pre> <ol> <li>Configure env vars for the backend app (via systemd env file, Docker env,    or PaaS UI). Typical values:</li> </ol> <pre><code>export HEALTHARCHIVE_ENV=staging\nexport HEALTHARCHIVE_DATABASE_URL=postgresql+psycopg://user:pass@db-host:5432/healtharchive_staging\nexport HEALTHARCHIVE_ARCHIVE_ROOT=/srv/healtharchive/jobs-staging\nexport HEALTHARCHIVE_ADMIN_TOKEN=\"&lt;staging-long-random-secret&gt;\"\nexport HEALTHARCHIVE_CORS_ORIGINS=\"https://healtharchive.vercel.app\"\nexport HEALTHARCHIVE_LOG_LEVEL=INFO\n</code></pre> <p>Adjust the DB URL, archive root, and CORS origins to match your actual    staging infrastructure. If you want specific branch preview URLs to call    the API directly, add them to <code>HEALTHARCHIVE_CORS_ORIGINS</code> as a    comma\u2011separated list.</p>"},{"location":"deployment/staging-rollout-checklist/#12-run-migrations-and-seed-sources","title":"1.2 Run migrations and seed sources","text":"<p>From a checkout of <code>healtharchive-backend</code> at the deployed revision on the staging host:</p> <pre><code>cd /path/to/healtharchive-backend\n\n# Activate venv or ensure dependencies are installed\nalembic upgrade head\nha-backend seed-sources\n</code></pre> <p>This:</p> <ul> <li>Applies all Alembic migrations to the staging DB.</li> <li>Ensures baseline <code>Source</code> rows for <code>hc</code> and <code>phac</code> exist (idempotent).</li> </ul>"},{"location":"deployment/staging-rollout-checklist/#13-start-api-worker-processes","title":"1.3 Start API + worker processes","text":"<p>Configure your process manager (systemd, Docker Compose, PaaS) to run:</p> <ul> <li>API:</li> </ul> <pre><code>uvicorn ha_backend.api:app --host 0.0.0.0 --port 8001\n</code></pre> <ul> <li>Worker:</li> </ul> <pre><code>ha-backend start-worker --poll-interval 30\n</code></pre> <p>Both processes must see the same <code>HEALTHARCHIVE_*</code> env vars from 1.1.</p>"},{"location":"deployment/staging-rollout-checklist/#14-create-at-least-one-staging-snapshot","title":"1.4 Create at least one staging snapshot","text":"<p>For basic end\u2011to\u2011end testing, you can start with a tiny synthetic WARC + one <code>Snapshot</code> row, using the recipe from the local live\u2011testing guide. On the staging host:</p> <ol> <li> <p>Ensure <code>HEALTHARCHIVE_DATABASE_URL</code> and <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> are set    (as in 1.1).</p> </li> <li> <p>Run the synthetic WARC script from    <code>healtharchive-backend/docs/development/live-testing.md</code> \u00a76.1 (\u201cHappy\u2011path    viewer using a synthetic WARC\u201d). It will:</p> </li> <li> <p>Create a small WARC file under <code>HEALTHARCHIVE_ARCHIVE_ROOT</code>.</p> </li> <li>Insert a <code>Snapshot</code> row in the staging DB.</li> <li> <p>Print:</p> <pre><code>SNAPSHOT_ID &lt;N&gt;\n</code></pre> </li> <li> <p>Record this <code>N</code> as your canonical staging snapshot ID for smoke tests.</p> </li> <li> <p>Quick check (from your own machine):</p> </li> </ol> <pre><code>curl -i \"https://api-staging.healtharchive.ca/api/snapshot/&lt;N&gt;\"\ncurl -i \"https://api-staging.healtharchive.ca/api/snapshots/raw/&lt;N&gt;\"\n</code></pre> <ul> <li>Both should return HTTP 200 with sensible data/HTML.</li> </ul>"},{"location":"deployment/staging-rollout-checklist/#15-api-health-and-cors-checks","title":"1.5 API health and CORS checks","text":"<p>From your own terminal (not on the staging host):</p> <pre><code>curl -i \"https://api-staging.healtharchive.ca/api/health\"\n\ncurl -i \\\n  -H \"Origin: https://healtharchive.vercel.app\" \\\n  \"https://api-staging.healtharchive.ca/api/health\"\n</code></pre> <p>Verify:</p> <ul> <li>HTTP 200 and <code>\"status\":\"ok\"</code> in the JSON body.</li> <li><code>Access-Control-Allow-Origin: https://healtharchive.vercel.app</code></li> <li><code>Vary: Origin</code></li> </ul>"},{"location":"deployment/staging-rollout-checklist/#2-frontend-staging-configuration-vercel-preview","title":"2. Frontend staging configuration (Vercel Preview)","text":""},{"location":"deployment/staging-rollout-checklist/#21-preview-env-vars","title":"2.1 Preview env vars","text":"<p>In the Vercel project for <code>healtharchive-frontend</code>:</p> <ol> <li>Go to Settings \u2192 Environment Variables \u2192 Preview.</li> <li>Set:</li> </ol> <pre><code>NEXT_PUBLIC_API_BASE_URL=https://api-staging.healtharchive.ca\nNEXT_PUBLIC_SHOW_API_HEALTH_BANNER=true\nNEXT_PUBLIC_LOG_API_HEALTH_FAILURE=true\nNEXT_PUBLIC_SHOW_API_BASE_HINT=true\n</code></pre> <ol> <li>Save and trigger a new Preview deployment (e.g. by pushing a commit to    the staging branch or redeploying the latest preview).</li> </ol>"},{"location":"deployment/staging-rollout-checklist/#22-grab-the-preview-url","title":"2.2 Grab the preview URL","text":"<p>After the build completes:</p> <ol> <li>Open the Vercel project\u2019s Deployments tab.</li> <li>Click the latest Preview deployment.</li> <li>Copy its URL, e.g.:</li> </ol> <pre><code>https://healtharchive-git-staging-&lt;hash&gt;.vercel.app\n</code></pre> <p>You will use this URL for the verification steps below.</p>"},{"location":"deployment/staging-rollout-checklist/#3-staging-verification-browser","title":"3. Staging verification (browser)","text":"<p>The following steps mirror https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/deployment/verification.md but framed as a checklist.</p>"},{"location":"deployment/staging-rollout-checklist/#31-frontend-security-headers-csp","title":"3.1 Frontend security headers &amp; CSP","text":"<ol> <li>Open the preview <code>/archive</code> route in a browser:</li> </ol> <pre><code>https://healtharchive-git-staging-&lt;hash&gt;.vercel.app/archive\n</code></pre> <ol> <li>In DevTools \u2192 Network:</li> <li>Select the main document request (<code>/archive</code>).</li> <li>Under Response headers, confirm:<ul> <li><code>Referrer-Policy: strict-origin-when-cross-origin</code></li> <li><code>X-Content-Type-Options: nosniff</code></li> <li><code>X-Frame-Options: SAMEORIGIN</code></li> <li><code>Permissions-Policy: geolocation=(), microphone=(), camera=()</code></li> <li><code>Content-Security-Policy-Report-Only: ...</code> with the expected    <code>connect-src</code> and <code>frame-src</code> entries for    <code>https://api.healtharchive.ca</code> and <code>https://api-staging.healtharchive.ca</code>.</li> </ul> </li> </ol>"},{"location":"deployment/staging-rollout-checklist/#32-backend-headers-cors-via-frontend","title":"3.2 Backend headers &amp; CORS (via frontend)","text":"<ol> <li>Still on <code>/archive</code>, filter the Network tab for requests to    <code>https://api-staging.healtharchive.ca</code>.</li> <li>Inspect <code>GET /api/health</code> and <code>GET /api/search?...</code>:</li> <li>Confirm HTTP 200 + JSON body.</li> <li>Confirm headers:<ul> <li><code>X-Content-Type-Options</code>, <code>Referrer-Policy</code>, <code>X-Frame-Options</code>,    <code>Permissions-Policy</code>.</li> <li><code>Access-Control-Allow-Origin</code> equal to the preview URL.</li> <li><code>Vary: Origin</code>.</li> </ul> </li> </ol>"},{"location":"deployment/staging-rollout-checklist/#33-snapshot-viewer-iframe","title":"3.3 Snapshot viewer iframe","text":"<ol> <li>Navigate to the staging snapshot using the ID from \u00a71.4:</li> </ol> <pre><code>https://healtharchive-git-staging-&lt;hash&gt;.vercel.app/snapshot/&lt;N&gt;\n</code></pre> <ol> <li>In DevTools \u2192 Elements:</li> <li>Locate the <code>&lt;iframe&gt;</code> in the snapshot viewer.</li> <li> <p>Confirm:</p> <ul> <li><code>src</code> is    <code>https://api-staging.healtharchive.ca/api/snapshots/raw/&lt;N&gt;</code> (i.e.    <code>NEXT_PUBLIC_API_BASE_URL</code> + the <code>rawSnapshotUrl</code> path).</li> <li><code>sandbox=\"allow-same-origin allow-scripts\"</code> is present.</li> </ul> </li> <li> <p>In Network:</p> </li> <li>Click the iframe request (<code>GET /api/snapshots/raw/&lt;N&gt;</code>).</li> <li>Confirm:<ul> <li>The request goes to <code>api-staging.healtharchive.ca</code>.</li> <li>Security headers match <code>/api/health</code>, except that    <code>X-Frame-Options</code> is intentionally omitted on this route so the    snapshot can be embedded.</li> </ul> </li> </ol>"},{"location":"deployment/staging-rollout-checklist/#34-console-diagnostics","title":"3.4 Console diagnostics","text":"<ol> <li>On <code>/archive</code>, open the browser console.</li> <li>Confirm you see a log similar to:</li> </ol> <pre><code>[healtharchive] API base URL (from NEXT_PUBLIC_API_BASE_URL or default): https://api-staging.healtharchive.ca\n</code></pre> <ol> <li>If the backend is unreachable or misconfigured, confirm:</li> <li>The health banner appears (when enabled).</li> <li><code>NEXT_PUBLIC_LOG_API_HEALTH_FAILURE</code> causes an appropriate warning.</li> </ol>"},{"location":"deployment/staging-rollout-checklist/#4-staging-sign-off","title":"4. Staging sign-off","text":"<p>Once the steps above pass, you can:</p> <ul> <li>Mark the staging\u2011related items in <code>hosting-and-live-server-to-dos.md</code> as   complete for the staging environment.</li> <li>Use the same patterns (with different env vars and hosts) when bringing   production online.</li> </ul>"},{"location":"deployment/systemd/","title":"Systemd unit templates (single VPS)","text":"<p>These files are templates meant to be copied onto the production VPS under <code>/etc/systemd/system/</code>.</p> <p>They implement:</p> <ul> <li>Annual scheduling timer (Jan 01 UTC)</li> <li>Worker priority lowering during campaign (always-on, low-risk)</li> <li>Storage Box mount (sshfs) for cold WARC storage (optional but recommended for tiering)</li> <li>WARC tiering bind mounts (Storage Box -&gt; canonical paths) (optional; for tiny-SSD setups)</li> <li>Replay reconciliation timer (pywb indexing; capped)</li> <li>Change tracking timer (edition-aware diffs; capped)</li> <li>Baseline drift check timer (policy vs observed; detects config drift)</li> <li>Public surface verification timer (public API + frontend; deeper than uptime checks)</li> <li>Optional \"timer ran\" pings (Healthchecks-style)</li> <li>Annual search verification capture (optional, safe)</li> </ul> <p>Assumptions (adjust paths/user if your VPS differs):</p> <ul> <li>Repo is deployed at: <code>/opt/healtharchive-backend</code></li> <li>Venv exists at: <code>/opt/healtharchive-backend/.venv</code></li> <li>Backend env file: <code>/etc/healtharchive/backend.env</code></li> <li>Backend system user: <code>haadmin</code></li> </ul>"},{"location":"deployment/systemd/#job-lock-directory-recommended","title":"Job lock directory (recommended)","text":"<p>The backend uses per-job <code>flock</code> lock files to prevent double-running a job and to help watchdog scripts classify whether a job is still actively running.</p> <p>By default, lock files live under <code>/tmp/healtharchive-job-locks</code>, which can be fragile on hardened systems and during cross-user incident response.</p> <p>Recommended production lock directory:</p> <ul> <li><code>/srv/healtharchive/ops/locks/jobs</code></li> </ul> <p>Enablement (on the VPS):</p> <ol> <li> <p>Ensure ops dirs exist:</p> </li> <li> <p><code>cd /opt/healtharchive-backend &amp;&amp; sudo ./scripts/vps-bootstrap-ops-dirs.sh</code></p> </li> <li> <p>Set the env var in <code>/etc/healtharchive/backend.env</code>:</p> </li> <li> <p><code>export HEALTHARCHIVE_JOB_LOCK_DIR=/srv/healtharchive/ops/locks/jobs</code></p> </li> <li>Restart the worker and any watchdog timers/services that read <code>backend.env</code>    during a safe window.</li> </ol> <p>Hard requirement: do not restart the worker while crawls are running unless you accept interruption. Confirm <code>ha-backend list-jobs --status running</code> is empty before restarting.</p> <p>Recommended (safe, copy/paste checklist):</p> <ul> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-job-lock-dir-cutover.sh</code></li> </ul> <p>If the script is missing on the VPS, your <code>/opt/healtharchive-backend</code> checkout is behind the repo. You can either deploy/pull first, or stage the cutover manually (no restarts required until your maintenance window):</p> <ul> <li><code>sudo cp -av /etc/healtharchive/backend.env /etc/healtharchive/backend.env.bak.$(date -u +%Y%m%dT%H%M%SZ)</code></li> <li>Add/update:</li> <li><code>export HEALTHARCHIVE_JOB_LOCK_DIR=/srv/healtharchive/ops/locks/jobs</code></li> <li>Ensure the lock dir exists (some older <code>vps-bootstrap-ops-dirs.sh</code> versions did not create it):</li> <li><code>sudo install -d -m 2770 -o root -g healtharchive /srv/healtharchive/ops/locks/jobs</code></li> </ul>"},{"location":"deployment/systemd/#files","title":"Files","text":"<ul> <li><code>healtharchive-schedule-annual.service</code></li> <li>Apply mode: enqueues annual jobs (<code>--apply</code>) for the current UTC year.</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/automation-enabled</code>.</li> <li><code>RefuseManualStart=yes</code> to prevent accidental <code>systemctl start</code> while the     worker is running.</li> <li><code>healtharchive-schedule-annual.timer</code></li> <li>Runs at <code>*-01-01 00:05:00 UTC</code></li> <li><code>Persistent=true</code> (runs on next boot if missed)</li> <li><code>healtharchive-schedule-annual-dry-run.service</code></li> <li>Safe validation service (no DB writes).</li> <li><code>healtharchive-worker.service.override.conf</code></li> <li>Drop-in that lowers worker CPU/IO priority to keep the API responsive.</li> <li><code>healtharchive-replay-reconcile.service</code></li> <li>Apply mode: runs <code>ha-backend replay-reconcile --apply --max-jobs 1</code>.</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/replay-automation-enabled</code>.</li> <li>Uses a lock file under <code>/srv/healtharchive/replay/.locks/</code> to prevent concurrent runs.</li> <li><code>healtharchive-replay-reconcile.timer</code></li> <li>Daily at <code>*-*-* 02:30:00 UTC</code></li> <li><code>Persistent=true</code> (runs on next boot if missed)</li> <li><code>healtharchive-replay-reconcile-dry-run.service</code></li> <li>Safe validation service (no docker exec, no filesystem writes beyond the lock file dir).</li> <li><code>healtharchive-change-tracking.service</code></li> <li>Apply mode: runs <code>ha-backend compute-changes</code> (edition-aware diffs).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/change-tracking-enabled</code>.</li> <li><code>healtharchive-change-tracking.timer</code></li> <li>Daily at <code>*-*-* 03:40:00 UTC</code></li> <li><code>Persistent=true</code> (runs on next boot if missed)</li> <li><code>healtharchive-change-tracking-dry-run.service</code></li> <li>Safe validation service (no DB writes; reports how many diffs would be computed).</li> <li><code>scripts/systemd-healthchecks-wrapper.sh</code></li> <li>Helper for optional Healthchecks-style pings without embedding ping URLs in unit files.</li> <li><code>healtharchive-annual-search-verify.service</code></li> <li>Runs <code>scripts/annual-search-verify.sh</code> daily, but captures once per year (idempotent).</li> <li><code>healtharchive-annual-search-verify.timer</code></li> <li>Daily timer for <code>healtharchive-annual-search-verify.service</code>.</li> <li><code>healtharchive-coverage-guardrails.service</code> + <code>.timer</code></li> <li>Writes coverage regression guardrails to the node_exporter textfile collector.</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/coverage-guardrails-enabled</code>.</li> <li><code>healtharchive-replay-smoke.service</code> + <code>.timer</code></li> <li>Runs replay smoke tests against the latest indexed job per source (node_exporter textfile).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/replay-smoke-enabled</code>.</li> <li><code>healtharchive-cleanup-automation.service</code> + <code>.timer</code></li> <li>Cleans indexed jobs using safe <code>temp-nonwarc</code> mode (keeps WARCs).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/cleanup-automation-enabled</code>.</li> <li><code>healtharchive-disk-threshold-cleanup.service</code> + <code>.timer</code></li> <li>Runs safe <code>temp-nonwarc</code> cleanup in threshold mode (no-op when disk is below threshold).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/cleanup-automation-enabled</code>.</li> <li><code>healtharchive-baseline-drift-check.service</code></li> <li>Runs <code>scripts/check_baseline_drift.py</code> (policy vs observed; writes artifacts under <code>/srv/healtharchive/ops/baseline/</code>).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/baseline-drift-enabled</code>.</li> <li><code>healtharchive-baseline-drift-check.timer</code></li> <li>Weekly timer for <code>healtharchive-baseline-drift-check.service</code>.</li> <li><code>healtharchive-public-surface-verify.service</code></li> <li>Runs <code>scripts/verify_public_surface.py</code> (public API + frontend; includes changes/RSS and partner pages).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/public-verify-enabled</code>.</li> <li>Intended as a deeper \u201csynthetic check\u201d than external uptime monitors.</li> <li><code>healtharchive-public-surface-verify.timer</code></li> <li>Daily timer for <code>healtharchive-public-surface-verify.service</code>.</li> <li><code>healtharchive-tiering-metrics.service</code> + <code>.timer</code></li> <li>Writes a small set of tiering health metrics to the node_exporter textfile collector.</li> <li>Used to alert on Storage Box / tiering failures without needing a systemd collector.</li> <li>Prereq: node_exporter must run with <code>--collector.textfile.directory=/var/lib/node_exporter/textfile_collector</code>     (configured by <code>scripts/vps-install-observability-exporters.sh</code>).</li> <li><code>healtharchive-crawl-metrics.service</code> + <code>.timer</code></li> <li>Writes per-job crawl progress/stall metrics (based on crawlStatus logs) to the node_exporter textfile collector.</li> <li>Used to alert on stalled crawls without manual log tailing.</li> <li>Prereq: node_exporter textfile collector is enabled (same as tiering metrics).</li> <li><code>healtharchive-crawl-auto-recover.service</code> + <code>.timer</code></li> <li>Optional automation to recover stalled crawl jobs by marking stale running jobs as retryable (and restarting the worker when needed).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/crawl-auto-recover-enabled</code>.</li> <li>Disabled by default; enable only after you\u2019re comfortable with the thresholds/caps in <code>scripts/vps-crawl-auto-recover.py</code>.</li> <li><code>healtharchive-worker-auto-start.service</code> + <code>.timer</code></li> <li>Optional automation to ensure the worker is running when it should be (jobs pending + storage OK).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/worker-auto-start-enabled</code>.</li> <li>Conservative by default; prefers a \u201cdo nothing\u201d skip over unsafe starts.</li> <li><code>healtharchive-storage-hotpath-auto-recover.service</code> + <code>.timer</code></li> <li>Optional automation to recover stale/unreadable hot paths caused by <code>sshfs</code>/FUSE mount failures (Errno 107).</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/storage-hotpath-auto-recover-enabled</code>.</li> <li>Disabled by default; enable only after dry-run validation and only if you\u2019re comfortable with the safety caps in <code>scripts/vps-storage-hotpath-auto-recover.py</code>.</li> <li><code>healtharchive-storage-watchdog-burnin-snapshot.service</code> + <code>.timer</code></li> <li>Optional read-only daily snapshot of the storage hot-path watchdog burn-in summary.</li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/storage-watchdog-burnin-enabled</code>.</li> <li>Writes dated JSON artifacts under <code>/srv/healtharchive/ops/burnin/storage-watchdog/</code> (and <code>latest.json</code>).</li> <li><code>healtharchive-storagebox-sshfs.service</code></li> <li>Mounts a Hetzner Storage Box at <code>/srv/healtharchive/storagebox</code> via <code>sshfs</code>.</li> <li>Reads configuration from <code>/etc/healtharchive/storagebox.env</code>.</li> <li>Intended for tiered WARC storage on small SSD hosts.</li> <li><code>healtharchive-warc-tiering.service</code></li> <li>Applies bind mounts from <code>/etc/healtharchive/warc-tiering.binds</code> so canonical     archive paths under <code>/srv/healtharchive/jobs/**</code> resolve to Storage Box data.</li> <li>Runs before the API/worker/replay services start.</li> <li><code>healtharchive-annual-output-tiering.service</code></li> <li>After annual jobs are enqueued, bind-mounts each annual job output_dir onto the Storage Box tier.</li> <li>Triggered via <code>OnSuccess=</code> in <code>healtharchive-schedule-annual.service</code> (template).</li> <li><code>healtharchive-annual-campaign-sentinel.service</code> + <code>.timer</code></li> <li>Runs a \u201cday-of\u201d annual readiness gate automatically: preflight + annual-status + tiering checks.</li> <li>Writes a small Prometheus textfile metric so Alertmanager can notify on failures.</li> </ul>"},{"location":"deployment/systemd/#recommended-enablement-guidance","title":"Recommended enablement guidance","text":"<p>These timers are safe-by-default and gated by sentinel files. Enable only what matches your operational readiness.</p> <ul> <li>Change tracking (<code>healtharchive-change-tracking.timer</code>)</li> <li>Recommended to enable once the <code>snapshot_changes</code> table exists and a dry     run succeeds without errors.</li> <li>Annual scheduling (<code>healtharchive-schedule-annual.timer</code>)</li> <li>Enable only after an annual dry-run succeeds and storage headroom is     confirmed.</li> <li>Replay reconcile (<code>healtharchive-replay-reconcile.timer</code>)</li> <li>Enable only if replay is enabled and stable.</li> <li>Annual search verification (<code>healtharchive-annual-search-verify.timer</code>)</li> <li>Optional; safe to enable if you want a yearly search QA artifact.</li> <li>Coverage guardrails (<code>healtharchive-coverage-guardrails.timer</code>)</li> <li>Recommended once you have at least two annual editions indexed.</li> <li>Replay smoke tests (<code>healtharchive-replay-smoke.timer</code>)</li> <li>Enable only if replay is enabled and stable.</li> <li>Cleanup automation (<code>healtharchive-cleanup-automation.timer</code>)</li> <li>Optional; keep caps conservative and review first dry-run.</li> <li>Disk threshold cleanup (<code>healtharchive-disk-threshold-cleanup.timer</code>)</li> <li>Optional; runs every 30 minutes but only applies cleanup when disk usage exceeds the configured threshold.</li> <li>Baseline drift check (<code>healtharchive-baseline-drift-check.timer</code>)</li> <li>Recommended; low-risk and catches \u201csilent\u201d ops drift.</li> <li>Storage hot-path auto-recover (<code>healtharchive-storage-hotpath-auto-recover.timer</code>)</li> <li>Dangerous if misconfigured; only enable after you\u2019ve validated Phase 1 alerts and run the watchdog in dry-run mode.</li> <li>The unit is gated by a venv presence check and the watchdog skips runs while the deploy lock is held (to avoid flapping during active deploys).</li> <li>Storage watchdog burn-in snapshots (<code>healtharchive-storage-watchdog-burnin-snapshot.timer</code>)</li> <li>Read-only; safe to enable during rollout/burn-in weeks so evidence is captured automatically.</li> <li>Worker auto-start watchdog (<code>healtharchive-worker-auto-start.timer</code>)</li> <li>Recommended once you\u2019re confident in the single-VPS production automation stack.</li> <li>The unit is sentinel-gated and refuses to start the worker if the Storage Box mount is unreadable or if the DB indicates a <code>status=running</code> job while the worker is down.</li> </ul> <p>If a timer is enabled, also ensure its sentinel file exists under <code>/etc/healtharchive/</code> (see the enablement sections below).</p>"},{"location":"deployment/systemd/#install-update-on-the-vps","title":"Install / update on the VPS","text":"<p>Preferred (one command; installs templates + worker priority drop-in):</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker\n</code></pre>"},{"location":"deployment/systemd/#run-a-crawl-job-detached-optional","title":"Run a crawl job detached (optional)","text":"<p>If you need to run a specific DB-backed crawl job manually (for debugging or recovery), prefer launching it as a transient systemd unit so your SSH session doesn\u2019t need to stay open:</p> <pre><code>sudo /opt/healtharchive-backend/scripts/vps-run-db-job-detached.py --id 7 --retry-first\n</code></pre> <p>Follow the printed <code>journalctl -u &lt;unit&gt;.service -f</code> command to tail logs.</p> <p>If you are using WARC tiering with a Storage Box, also create these files on the VPS:</p> <ul> <li><code>/etc/healtharchive/storagebox.env</code></li> <li>Configuration consumed by <code>healtharchive-storagebox-sshfs.service</code>.</li> <li><code>/etc/healtharchive/warc-tiering.binds</code></li> <li>Bind mount manifest consumed by <code>healtharchive-warc-tiering.service</code>.</li> </ul> <p>See: <code>docs/operations/playbooks/storage/warc-storage-tiering.md</code>.</p> <p>Before enabling timers that write artifacts under <code>/srv/healtharchive/ops/</code>, ensure the ops directories exist with the expected permissions:</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-bootstrap-ops-dirs.sh\n</code></pre> <p>Manual install (equivalent):</p> <p>Copy unit files:</p> <pre><code>sudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-schedule-annual.service \\\n  /etc/systemd/system/healtharchive-schedule-annual.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-schedule-annual.timer \\\n  /etc/systemd/system/healtharchive-schedule-annual.timer\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-schedule-annual-dry-run.service \\\n  /etc/systemd/system/healtharchive-schedule-annual-dry-run.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-replay-reconcile.service \\\n  /etc/systemd/system/healtharchive-replay-reconcile.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-replay-reconcile.timer \\\n  /etc/systemd/system/healtharchive-replay-reconcile.timer\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-replay-reconcile-dry-run.service \\\n  /etc/systemd/system/healtharchive-replay-reconcile-dry-run.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-change-tracking.service \\\n  /etc/systemd/system/healtharchive-change-tracking.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-change-tracking.timer \\\n  /etc/systemd/system/healtharchive-change-tracking.timer\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-change-tracking-dry-run.service \\\n  /etc/systemd/system/healtharchive-change-tracking-dry-run.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-annual-search-verify.service \\\n  /etc/systemd/system/healtharchive-annual-search-verify.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-annual-search-verify.timer \\\n  /etc/systemd/system/healtharchive-annual-search-verify.timer\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-baseline-drift-check.service \\\n  /etc/systemd/system/healtharchive-baseline-drift-check.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-baseline-drift-check.timer \\\n  /etc/systemd/system/healtharchive-baseline-drift-check.timer\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-public-surface-verify.service \\\n  /etc/systemd/system/healtharchive-public-surface-verify.service\n\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-public-surface-verify.timer \\\n  /etc/systemd/system/healtharchive-public-surface-verify.timer\n</code></pre> <p>Install the worker priority drop-in:</p> <pre><code>sudo install -d -m 0755 -o root -g root /etc/systemd/system/healtharchive-worker.service.d\nsudo install -m 0644 -o root -g root \\\n  /opt/healtharchive-backend/docs/deployment/systemd/healtharchive-worker.service.override.conf \\\n  /etc/systemd/system/healtharchive-worker.service.d/override.conf\n</code></pre> <p>Reload systemd:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre> <p>Restart worker to pick up priority changes:</p> <pre><code>sudo systemctl restart healtharchive-worker\n</code></pre> <p>Verify the priority values:</p> <pre><code>systemctl show healtharchive-worker -p Nice -p IOSchedulingClass -p IOSchedulingPriority\n</code></pre>"},{"location":"deployment/systemd/#optional-timer-ran-pings-healthchecks-style","title":"Optional: \"timer ran\" pings (Healthchecks-style)","text":"<p>This repo does not commit ping URLs. If you want \"did it run?\" checks, create a root-owned env file on the VPS:</p> <pre><code>sudo install -d -m 0755 -o root -g root /etc/healtharchive\n# Only create the file if missing (don't clobber existing values like HC_DB_BACKUP_URL).\nsudo test -f /etc/healtharchive/healthchecks.env || sudo install -m 0600 -o root -g root /dev/null /etc/healtharchive/healthchecks.env\nsudo chown root:root /etc/healtharchive/healthchecks.env\nsudo chmod 0600 /etc/healtharchive/healthchecks.env\n</code></pre> <p>Edit <code>/etc/healtharchive/healthchecks.env</code> and set (examples):</p> <pre><code># You do NOT need to set every variable listed here.\n# Only set the variables for Healthchecks checks you have actually created.\n# If a variable is missing/empty, the service still runs; it just won't ping.\n#\n# Note: the Healthchecks \"Name\" can be anything; it does not need to match the\n# env var name. The env var is just how systemd finds the ping URL.\nHEALTHARCHIVE_HC_PING_REPLAY_RECONCILE=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_SCHEDULE_ANNUAL=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_ANNUAL_SENTINEL=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_CHANGE_TRACKING=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_BASELINE_DRIFT=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_PUBLIC_VERIFY=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_ANNUAL_SEARCH_VERIFY=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_COVERAGE_GUARDRAILS=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_REPLAY_SMOKE=https://hc-ping.com/UUID_HERE\nHEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION=https://hc-ping.com/UUID_HERE\n</code></pre> <p>Notes:</p> <ul> <li>The unit templates use <code>EnvironmentFile=-/etc/healtharchive/healthchecks.env</code>   so the file is optional.</li> <li>If you also use the legacy Healthchecks-based scripts described in   <code>../production-single-vps.md</code> (DB backup + disk check), keep their variables   in the same file too (<code>HC_DB_BACKUP_URL</code>, <code>HC_DISK_URL</code>, <code>HC_DISK_THRESHOLD</code>).</li> <li>If set, services will best-effort ping:</li> <li><code>&lt;url&gt;/start</code> at the beginning</li> <li><code>&lt;url&gt;</code> on success</li> <li><code>&lt;url&gt;/fail</code> on failure</li> <li>Ping failures do not fail the service.</li> </ul>"},{"location":"deployment/systemd/#audit-healthchecks-alignment-safe","title":"Audit Healthchecks alignment (safe)","text":"<p>This script compares:</p> <ul> <li>What ping env vars are set in <code>/etc/healtharchive/healthchecks.env</code></li> <li>What ping vars are referenced by installed systemd unit files (via <code>--ping-var ...</code>)</li> <li>Which timers exist (for manual cross-check with Healthchecks \u201clast ping\u201d timestamps)</li> </ul> <p>Run on the VPS:</p> <pre><code>cd /opt/healtharchive-backend\nsudo -u haadmin python3 ./scripts/verify_healthchecks_alignment.py\n</code></pre> <p>If it reports \u201creferenced but unset\u201d, you either:</p> <ul> <li>Intentionally have pings disabled for those timers (OK), or</li> <li>Should create the missing checks in Healthchecks and add the missing env vars.</li> </ul> <p>If it reports \u201cset but unused\u201d, you likely have a stale env var (remove it) or the unit that used to reference it was removed/renamed.</p>"},{"location":"deployment/systemd/#validate-the-annual-scheduler-safe","title":"Validate the annual scheduler (safe)","text":"<p>This dry-run service exercises DB connectivity + scheduler output without creating jobs:</p> <pre><code>sudo systemctl start healtharchive-schedule-annual-dry-run.service\nsudo journalctl -u healtharchive-schedule-annual-dry-run.service -n 200 --no-pager\n</code></pre> <p>Do not run <code>healtharchive-schedule-annual.service</code> manually in production; it enqueues jobs and the worker may start crawling immediately.</p>"},{"location":"deployment/systemd/#validate-replay-reconciliation-safe","title":"Validate replay reconciliation (safe)","text":"<p>This dry-run service exercises DB connectivity + filesystem drift detection without running any docker exec commands:</p> <pre><code>sudo systemctl start healtharchive-replay-reconcile-dry-run.service\nsudo journalctl -u healtharchive-replay-reconcile-dry-run.service -n 200 --no-pager\n</code></pre>"},{"location":"deployment/systemd/#validate-change-tracking-safe","title":"Validate change tracking (safe)","text":"<p>This dry-run service exercises DB connectivity and reports how many diffs would be computed:</p> <pre><code>sudo systemctl start healtharchive-change-tracking-dry-run.service\nsudo journalctl -u healtharchive-change-tracking-dry-run.service -n 200 --no-pager\n</code></pre> <p>If you see an error like <code>relation \"snapshot_changes\" does not exist</code>, apply migrations first (idempotent):</p> <pre><code>cd /opt/healtharchive-backend\nsudo -u haadmin /opt/healtharchive-backend/.venv/bin/alembic upgrade head\n</code></pre>"},{"location":"deployment/systemd/#enable-automation-jan-01","title":"Enable automation (Jan 01)","text":"<p>Create the automation sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/automation-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-schedule-annual.timer\nsystemctl list-timers | rg healtharchive-schedule-annual || systemctl list-timers | grep healtharchive-schedule-annual\n</code></pre> <p>Note: do not <code>enable</code> the <code>.service</code> units directly; only the <code>.timer</code> should be enabled.</p>"},{"location":"deployment/systemd/#enable-replay-reconciliation-automation-optional","title":"Enable replay reconciliation automation (optional)","text":"<p>Create the replay automation sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/replay-automation-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-replay-reconcile.timer\nsystemctl list-timers | rg healtharchive-replay-reconcile || systemctl list-timers | grep healtharchive-replay-reconcile\n</code></pre> <p>Note: by default, the timer only reconciles replay indexing. Preview image generation is intentionally left manual/capped until you decide it\u2019s stable enough to automate.</p>"},{"location":"deployment/systemd/#enable-change-tracking-automation-optional","title":"Enable change tracking automation (optional)","text":"<p>Create the change tracking sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/change-tracking-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-change-tracking.timer\nsystemctl list-timers | rg healtharchive-change-tracking || systemctl list-timers | grep healtharchive-change-tracking\n</code></pre>"},{"location":"deployment/systemd/#enable-annual-search-verification-capture-optional","title":"Enable annual search verification capture (optional)","text":"<p>This captures golden-query <code>/api/search</code> JSON once per year after the annual campaign becomes search-ready.</p> <p>The service is idempotent:</p> <ul> <li>If the campaign isn't ready, it exits 0 (no failure spam).</li> <li>If artifacts already exist for the current year/run-id, it exits 0.</li> </ul> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-annual-search-verify.timer\nsystemctl list-timers | rg healtharchive-annual-search-verify || systemctl list-timers | grep healtharchive-annual-search-verify\n</code></pre> <p>Artifacts default to:</p> <ul> <li><code>/srv/healtharchive/ops/search-eval/&lt;year&gt;/final/</code></li> </ul> <p>To force a re-run for the current year, delete that directory and run the service once.</p>"},{"location":"deployment/systemd/#enable-coverage-guardrails-optional","title":"Enable coverage guardrails (optional)","text":"<p>This emits daily metrics comparing the latest indexed annual job to the prior year per source.</p> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/coverage-guardrails-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-coverage-guardrails.timer\nsystemctl list-timers | rg healtharchive-coverage-guardrails || systemctl list-timers | grep healtharchive-coverage-guardrails\n</code></pre>"},{"location":"deployment/systemd/#enable-replay-smoke-tests-optional","title":"Enable replay smoke tests (optional)","text":"<p>This runs lightweight replay checks against the latest indexed job per source.</p> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/replay-smoke-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-replay-smoke.timer\nsystemctl list-timers | rg healtharchive-replay-smoke || systemctl list-timers | grep healtharchive-replay-smoke\n</code></pre>"},{"location":"deployment/systemd/#enable-cleanup-automation-optional","title":"Enable cleanup automation (optional)","text":"<p>This runs safe <code>temp-nonwarc</code> cleanup for older indexed jobs (keeps WARCs).</p> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/cleanup-automation-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-cleanup-automation.timer\nsystemctl list-timers | rg healtharchive-cleanup-automation || systemctl list-timers | grep healtharchive-cleanup-automation\n</code></pre>"},{"location":"deployment/systemd/#enable-disk-threshold-cleanup-optional","title":"Enable disk threshold cleanup (optional)","text":"<p>This is an event-driven safety net for disk pressure:</p> <ul> <li>it runs on a frequent timer (every 30 minutes),</li> <li>but only applies cleanup if disk usage exceeds <code>threshold_trigger_percent</code>   from <code>ops/automation/cleanup-automation.toml</code>,</li> <li>and uses <code>threshold_max_jobs_per_run</code> as the cap when triggered.</li> </ul> <p>It is gated by the same sentinel file as weekly cleanup:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/cleanup-automation-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-disk-threshold-cleanup.timer\nsystemctl list-timers | rg healtharchive-disk-threshold-cleanup || systemctl list-timers | grep healtharchive-disk-threshold-cleanup\n</code></pre> <p>Rollback:</p> <pre><code>sudo systemctl disable --now healtharchive-disk-threshold-cleanup.timer\n</code></pre>"},{"location":"deployment/systemd/#enable-storage-hot-path-auto-recovery-optional-high-impact","title":"Enable storage hot-path auto-recovery (optional; high impact)","text":"<p>This automation attempts conservative self-healing for the specific failure class:</p> <ul> <li><code>OSError: [Errno 107] Transport endpoint is not connected</code></li> </ul> <p>It can unmount stale hot paths and re-apply tiering. It will only stop the worker when either:</p> <ul> <li>a running job output directory is detected as stale (Errno 107), or</li> <li>there are no running jobs (to prevent races while repairing mountpoints for the next jobs).</li> </ul> <p>It also probes the output dirs of the next queued/retryable jobs to prevent infra-error retry storms (a stale mountpoint for a retryable job should be repaired before the worker selects it).</p> <p>If no stale targets are currently eligible but <code>healtharchive-warc-tiering.service</code> is stuck in <code>failed</code>, the watchdog will attempt a conservative reconcile (<code>reset-failed</code> + <code>start</code>) when the base Storage Box mount is readable. This helps clear persistent <code>HealthArchiveWarcTieringFailed</code> alerts caused by stale historical unit state.</p> <p>After successful mount recovery it restarts replay (best-effort) so replay sees a clean view of <code>/srv/healtharchive/jobs</code>.</p> <p>Keep it disabled by default and enable only after:</p> <ul> <li>Phase 1 alerting/metrics are working (you have visibility),</li> <li>you have validated the watchdog in dry-run mode first.</li> </ul> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/storage-hotpath-auto-recover-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-storage-hotpath-auto-recover.timer\nsystemctl list-timers | rg healtharchive-storage-hotpath-auto-recover || systemctl list-timers | grep healtharchive-storage-hotpath-auto-recover\n</code></pre> <p>Rollback:</p> <pre><code>sudo systemctl disable --now healtharchive-storage-hotpath-auto-recover.timer\nsudo rm -f /etc/healtharchive/storage-hotpath-auto-recover-enabled\n</code></pre> <p>The watchdog writes state under:</p> <ul> <li><code>/srv/healtharchive/ops/watchdog/storage-hotpath-auto-recover.json</code></li> </ul> <p>and emits node_exporter textfile metrics via:</p> <ul> <li><code>healtharchive_storage_hotpath_auto_recover.prom</code></li> </ul>"},{"location":"deployment/systemd/#enable-storage-watchdog-burn-in-snapshots-optional-low-impact","title":"Enable storage watchdog burn-in snapshots (optional; low impact)","text":"<p>This automation captures a daily read-only snapshot of the storage watchdog burn-in report so you have evidence artifacts even if nobody remembers to run the command manually.</p> <p>Precondition: ops directories exist (idempotent):</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-bootstrap-ops-dirs.sh\n</code></pre> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/storage-watchdog-burnin-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-storage-watchdog-burnin-snapshot.timer\nsystemctl list-timers | rg healtharchive-storage-watchdog-burnin-snapshot || systemctl list-timers | grep healtharchive-storage-watchdog-burnin-snapshot\n</code></pre> <p>Artifacts:</p> <ul> <li><code>/srv/healtharchive/ops/burnin/storage-watchdog/latest.json</code></li> <li><code>/srv/healtharchive/ops/burnin/storage-watchdog/storage-watchdog-burnin-YYYYMMDD.json</code></li> </ul> <p>Rollback:</p> <pre><code>sudo systemctl disable --now healtharchive-storage-watchdog-burnin-snapshot.timer\nsudo rm -f /etc/healtharchive/storage-watchdog-burnin-enabled\n</code></pre>"},{"location":"deployment/systemd/#enable-worker-auto-start-watchdog-optional-conservative","title":"Enable worker auto-start watchdog (optional; conservative)","text":"<p>This automation exists to prevent \u201ceverything stopped\u201d failures where the system is healthy enough to run, but <code>healtharchive-worker.service</code> is down and jobs are pending.</p> <p>It will only start the worker when all of these are true:</p> <ul> <li>the worker unit is inactive,</li> <li>there are pending crawl jobs (<code>status in (queued, retryable)</code>),</li> <li>the Storage Box mount is readable,</li> <li>the deploy lock is not present (or is stale),</li> <li>and there are no DB jobs in <code>status=running</code> (conservative safety gate).</li> </ul> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/worker-auto-start-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-worker-auto-start.timer\nsystemctl list-timers | rg healtharchive-worker-auto-start || systemctl list-timers | grep healtharchive-worker-auto-start\n</code></pre> <p>Rollback:</p> <pre><code>sudo systemctl disable --now healtharchive-worker-auto-start.timer\nsudo rm -f /etc/healtharchive/worker-auto-start-enabled\n</code></pre> <p>The watchdog writes state under:</p> <ul> <li><code>/srv/healtharchive/ops/watchdog/worker-auto-start.json</code></li> </ul> <p>and emits node_exporter textfile metrics via:</p> <ul> <li><code>healtharchive_worker_auto_start.prom</code></li> </ul>"},{"location":"deployment/systemd/#enable-baseline-drift-checks-recommended","title":"Enable baseline drift checks (recommended)","text":"<p>Baseline drift checks validate that production still matches the project\u2019s expected invariants (security posture, perms, unit enablement).</p> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/baseline-drift-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-baseline-drift-check.timer\nsystemctl list-timers | rg healtharchive-baseline-drift-check || systemctl list-timers | grep healtharchive-baseline-drift-check\n</code></pre> <p>Artifacts are written under:</p> <ul> <li><code>/srv/healtharchive/ops/baseline/</code></li> </ul> <p>If the drift check fails, inspect:</p> <ul> <li><code>/srv/healtharchive/ops/baseline/drift-report-latest.txt</code></li> <li><code>journalctl -u healtharchive-baseline-drift-check.service --no-pager -l</code></li> </ul>"},{"location":"deployment/systemd/#enable-public-surface-verification-optional-recommended","title":"Enable public surface verification (optional, recommended)","text":"<p>This is a deeper synthetic check than external uptime monitors. It validates:</p> <ul> <li>public API health, sources, search, snapshot detail and raw HTML</li> <li>replay browse URL (unless skipped)</li> <li>exports manifest and export endpoint HEADs</li> <li>changes feed + RSS</li> <li>key frontend pages, including <code>/brief</code>, <code>/cite</code>, <code>/methods</code>, and <code>/governance</code></li> </ul> <p>Create the sentinel file:</p> <pre><code>sudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/public-verify-enabled\n</code></pre> <p>Enable the timer:</p> <pre><code>sudo systemctl enable --now healtharchive-public-surface-verify.timer\nsystemctl list-timers | rg healtharchive-public-surface-verify || systemctl list-timers | grep healtharchive-public-surface-verify\n</code></pre>"},{"location":"deployment/systemd/#rollback-disable-quickly","title":"Rollback / disable quickly","text":"<ul> <li>Disable timer immediately:</li> </ul> <pre><code>sudo systemctl disable --now healtharchive-schedule-annual.timer\n</code></pre> <ul> <li>Disable all scheduling automation immediately:</li> </ul> <pre><code>sudo rm -f /etc/healtharchive/automation-enabled\n</code></pre> <ul> <li>Disable replay reconciliation automation immediately:</li> </ul> <pre><code>sudo systemctl disable --now healtharchive-replay-reconcile.timer\nsudo rm -f /etc/healtharchive/replay-automation-enabled\n</code></pre> <ul> <li>Disable annual search verification automation immediately:</li> </ul> <pre><code>sudo systemctl disable --now healtharchive-annual-search-verify.timer\n</code></pre> <ul> <li>Remove the worker priority override:</li> </ul> <pre><code>sudo rm -f /etc/systemd/system/healtharchive-worker.service.d/override.conf\nsudo systemctl daemon-reload\nsudo systemctl restart healtharchive-worker\n</code></pre>"},{"location":"development/","title":"Development docs","text":""},{"location":"development/#start-here","title":"Start Here","text":"<p>New developer? - Setup: Dev Environment Setup \u2014 Local setup guide - Test: Live Testing \u2014 Local testing workflows - Contribute: Testing Guidelines \u2014 Test conventions - Architecture: Architecture \u2014 How the code works</p> <p>Quick reference: | Task | Documentation | |------|---------------| | Run backend locally | Live Testing | | Run tests | Testing Guidelines | | Understand architecture | Architecture | | Deploy changes | Change to Production |</p>"},{"location":"development/#all-development-documentation","title":"All Development Documentation","text":"<ul> <li>Local testing flows (recommended): <code>live-testing.md</code></li> <li>Local + VPS setup (recommended): <code>dev-environment-setup.md</code></li> <li>Backend testing conventions: <code>testing-guidelines.md</code></li> <li>Development playbooks (task workflows): <code>playbooks/README.md</code></li> </ul>"},{"location":"development/#code-annotations-demo","title":"Code Annotations (Demo)","text":"<p>This project uses MkDocs Material code annotations to provide inline context for complex configurations:</p> <pre><code># Example docker-compose.yml\nservices:\n  api:\n    image: healtharchive-backend:latest\n    ports:\n      - \"8001:8001\" # (1)\n    environment:\n      - HEALTHARCHIVE_DATABASE_URL=sqlite:///data.db # (2)\n</code></pre> <ol> <li>Standard FastAPI port for local development.</li> <li>Default SQLite path inside the container.</li> </ol>"},{"location":"development/dev-environment-setup/","title":"Developer environment setup (local + VPS)","text":"<p>This document answers two questions:</p> <p>1) How to set up a local dev environment for HealthArchive (backend + frontend). 2) Where to run which commands (your dev machine vs the production VPS).</p> <p>For full backend live-testing flows, see <code>live-testing.md</code>.</p>"},{"location":"development/dev-environment-setup/#repo-layout","title":"Repo layout","text":"<p>HealthArchive currently lives as multiple repos:</p> <ul> <li>Backend: https://github.com/jerdaw/healtharchive-backend (local dir: <code>healtharchive-backend/</code>)</li> <li>Frontend: https://github.com/jerdaw/healtharchive-frontend (local dir: <code>healtharchive-frontend/</code>)</li> <li>Datasets: https://github.com/jerdaw/healtharchive-datasets (local dir: <code>healtharchive-datasets/</code>)</li> </ul> <p>Many developers keep them in one folder and use the mono-repo root <code>Makefile</code> to run checks across repos.</p>"},{"location":"development/dev-environment-setup/#local-machine-setup-recommended","title":"Local machine setup (recommended)","text":""},{"location":"development/dev-environment-setup/#0-prereqs","title":"0) Prereqs","text":"<ul> <li><code>git</code></li> <li><code>python3</code> (match <code>healtharchive-backend</code> requirements)</li> <li><code>node</code> (match frontend <code>engines.node</code>: https://github.com/jerdaw/healtharchive-frontend/blob/main/package.json)</li> <li><code>make</code></li> </ul> <p>Recommended:</p> <ul> <li><code>pipx</code> (for global Python tools like <code>pre-commit</code>)</li> <li>Docker (only needed for end-to-end crawling runs)</li> </ul>"},{"location":"development/dev-environment-setup/#1-backend-setup-local","title":"1) Backend setup (local)","text":"<p>From <code>healtharchive-backend/</code>:</p> <pre><code>make venv\nmake check\n</code></pre> <p>Then follow <code>docs/development/live-testing.md</code> for running the API locally, running worker flows, and Docker-based crawling tests.</p>"},{"location":"development/dev-environment-setup/#2-frontend-setup-local","title":"2) Frontend setup (local)","text":"<p>From <code>healtharchive-frontend/</code>:</p> <pre><code>npm ci\nnpm run check\n</code></pre>"},{"location":"development/dev-environment-setup/#3-local-guardrails-recommended-for-solo-fast-direct-to-main","title":"3) Local guardrails (recommended for solo-fast direct-to-main)","text":"<p>If you\u2019re moving fast and pushing directly to <code>main</code>, you want local guardrails that reduce \u201coops I forgot to run checks\u201d mistakes.</p>"},{"location":"development/dev-environment-setup/#one-command-check-workspace-root","title":"One-command check (workspace root)","text":"<p>From the mono-repo root:</p> <pre><code>make check\n</code></pre> <p>This runs:</p> <ul> <li>backend <code>make check</code></li> <li>frontend <code>pre-commit run --all-files</code> + <code>npm run check</code></li> <li>datasets checks</li> </ul>"},{"location":"development/dev-environment-setup/#pre-push-hooks-recommended","title":"Pre-push hooks (recommended)","text":"<p>These run automatically on <code>git push</code>:</p> <ul> <li>Backend (runs <code>make check</code>; set <code>HA_PRE_PUSH_FULL=1</code> to run <code>make check-full</code>):</li> <li><code>scripts/install-pre-push-hook.sh</code></li> <li>Frontend (runs <code>pre-commit</code> + <code>npm run check</code>):</li> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/scripts/install-pre-push-hook.sh</li> </ul> <p>Install them on your dev machine:</p> <pre><code>cd healtharchive-backend &amp;&amp; ./scripts/install-pre-push-hook.sh\ncd ../healtharchive-frontend &amp;&amp; ./scripts/install-pre-push-hook.sh\n</code></pre> <p>Bypass once if needed (emergency only):</p> <ul> <li><code>git push --no-verify</code></li> <li>or set <code>HA_SKIP_PRE_PUSH=1</code></li> </ul>"},{"location":"development/dev-environment-setup/#pre-commit-recommended","title":"Pre-commit (recommended)","text":"<p>Both repos include <code>.pre-commit-config.yaml</code>.</p> <ul> <li>Install once: <code>pipx install pre-commit</code></li> <li>Enable \u201crun on commit\u201d inside each repo:</li> <li><code>pre-commit install</code></li> </ul>"},{"location":"development/dev-environment-setup/#vps-usage-production","title":"VPS usage (production)","text":""},{"location":"development/dev-environment-setup/#what-runs-on-the-vps","title":"What runs on the VPS","text":"<p>Run these on the production VPS (typically from <code>/opt/healtharchive-backend</code>):</p> <ul> <li>Deploy + restart services:</li> <li><code>./scripts/vps-deploy.sh --apply</code></li> <li>Production verification gates:</li> <li><code>./scripts/check_baseline_drift.py --mode live</code></li> <li><code>./scripts/verify_public_surface.py</code></li> <li>Ops bootstrap / automation helpers (recommended):</li> <li>one-time: <code>sudo ./scripts/vps-bootstrap-ops-dirs.sh</code></li> <li>install/update systemd templates: <code>sudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker</code></li> <li>verify timers/sentinels: <code>./scripts/verify_ops_automation.sh</code></li> </ul> <p>Recommended deploy flow (single command):</p> <pre><code>./scripts/vps-deploy.sh --apply --baseline-mode live\n</code></pre> <p>Note: systemd timer enablement is explicit and gated by sentinel files under <code>/etc/healtharchive/</code>. For enable/rollback steps, see <code>../deployment/systemd/README.md</code>.</p>"},{"location":"development/dev-environment-setup/#what-should-not-run-on-the-vps","title":"What should not run on the VPS","text":"<p>These are local-developer guardrails and should run on your dev machine:</p> <ul> <li><code>make check</code> (workspace root)</li> <li><code>healtharchive-backend/scripts/install-pre-push-hook.sh</code> (this repo)</li> <li>Frontend hook installer: https://github.com/jerdaw/healtharchive-frontend/blob/main/scripts/install-pre-push-hook.sh</li> </ul> <p>Reason: hooks install into <code>.git/hooks/</code> for the repo you\u2019re pushing from (your laptop/workstation), not on the server.</p>"},{"location":"development/live-testing/","title":"HealthArchive Backend \u2013 Live Testing Guide","text":"<p>This document describes a practical, incremental way to live\u2011test the <code>healtharchive-backend</code> in a local development environment, starting with the smallest checks and working up to more realistic scenarios.</p> <p>It assumes you are working from the repo root and are comfortable with a terminal and Python tooling.</p>"},{"location":"development/live-testing/#0-onetime-setup","title":"0. One\u2011time setup","text":""},{"location":"development/live-testing/#01-create-a-virtualenv-and-install-the-backend","title":"0.1 Create a virtualenv and install the backend","text":"<pre><code>make venv\n# or (manual):\n# python -m venv .venv\n# source .venv/bin/activate\n# pip install -e \".[dev]\"\n</code></pre> <p>This provides:</p> <ul> <li><code>ha-backend</code> \u2013 backend CLI</li> <li><code>archive-tool</code> \u2013 crawler CLI implemented by the in-repo <code>archive_tool</code> package (uses Docker + Zimit)</li> </ul>"},{"location":"development/live-testing/#02-configure-environment-variables","title":"0.2 Configure environment variables","text":"<p>Use a local SQLite DB and archive root so you never touch production paths:</p> <pre><code>export HEALTHARCHIVE_DATABASE_URL=sqlite:///$(pwd)/.dev-healtharchive.db\nexport HEALTHARCHIVE_ARCHIVE_ROOT=$(pwd)/.dev-archive-root\nexport HEALTHARCHIVE_ADMIN_TOKEN=localdev-admin  # optional but recommended\n# Optional: set CORS origins if your frontend runs on a non-default host\n# (defaults already include http://localhost:3000 and https://healtharchive.ca)\n# export HEALTHARCHIVE_CORS_ORIGINS=http://localhost:3000\n\n# Shortcut: copy the sample file and source it\n# cp .env.example .env\n# source .env\n</code></pre> <p>Run Alembic migrations once to create the schema:</p> <pre><code>alembic upgrade head\n</code></pre>"},{"location":"development/live-testing/#1-smallest-checks-no-docker-no-jobs","title":"1. Smallest checks (no Docker, no jobs)","text":"<p>Goal: prove the Python package and DB wiring work in isolation.</p>"},{"location":"development/live-testing/#11-run-the-test-suite","title":"1.1 Run the test suite","text":"<pre><code>make ci\n# or (full suite):\n# make check-full\n# or (tests only):\n# pytest -q\n</code></pre> <p>All tests should pass. (At time of writing, a 422 around <code>/api/admin/jobs/status-counts</code> was fixed so this now passes too.)</p>"},{"location":"development/live-testing/#12-check-db-connectivity","title":"1.2 Check DB connectivity","text":"<pre><code>ha-backend check-db\n</code></pre> <p>You should see:</p> <ul> <li>The <code>HEALTHARCHIVE_DATABASE_URL</code> you set.</li> <li>\u201cDatabase connection OK.\u201d</li> </ul>"},{"location":"development/live-testing/#13-check-environment-archive-root","title":"1.3 Check environment / archive root","text":"<pre><code>ha-backend check-env\n</code></pre> <p>Confirms:</p> <ul> <li><code>HEALTHARCHIVE_ARCHIVE_ROOT</code> exists and is writable.</li> <li>The configured <code>archive_tool</code> command is resolvable.</li> </ul>"},{"location":"development/live-testing/#2-apionly-live-smoke-tests-no-archive_tool-no-jobs","title":"2. API\u2011only live smoke tests (no archive_tool, no jobs)","text":"<p>Goal: run FastAPI + DB with an empty dataset.</p>"},{"location":"development/live-testing/#21-start-the-api","title":"2.1 Start the API","text":"<p>In one terminal (with <code>.venv</code> active and env vars set):</p> <pre><code>uvicorn ha_backend.api:app --reload --port 8001\n</code></pre>"},{"location":"development/live-testing/#22-hit-public-endpoints","title":"2.2 Hit public endpoints","text":"<p>From another terminal:</p> <pre><code>curl http://localhost:8001/api/health\ncurl http://localhost:8001/api/sources\ncurl \"http://localhost:8001/api/search?q=test\"\n</code></pre> <p>Expect:</p> <ul> <li><code>/api/health</code> \u2192 <code>{\"status\":\"ok\", ... \"db\":\"ok\" ...}</code>.</li> <li><code>/api/sources</code> \u2192 <code>[]</code> (no data yet).</li> <li><code>/api/search</code> \u2192 empty results, but HTTP 200.</li> </ul>"},{"location":"development/live-testing/#23-admin-endpoints","title":"2.3 Admin endpoints","text":"<p>With <code>HEALTHARCHIVE_ADMIN_TOKEN</code> unset (local dev only):</p> <pre><code>curl http://localhost:8001/api/admin/jobs\n</code></pre> <p>Admin routes are open (dev mode).</p> <p>With <code>HEALTHARCHIVE_ADMIN_TOKEN=localdev-admin</code> set when starting uvicorn:</p> <pre><code>curl -H \"Authorization: Bearer localdev-admin\" \\\n  http://localhost:8001/api/admin/jobs\n</code></pre> <p>Confirms admin auth + simple bearer token protection.</p>"},{"location":"development/live-testing/#24-admin-access-patterns-local-vs-stagingprod","title":"2.4 Admin access patterns (local vs staging/prod)","text":"<p>In local development it is acceptable to either leave <code>HEALTHARCHIVE_ADMIN_TOKEN</code> unset (open admin endpoints) or to use a simple token like <code>localdev-admin</code> as shown above.</p> <p>In staging and production you should always set a strong, random admin token and treat it as a secret:</p> <pre><code>export HEALTHARCHIVE_ADMIN_TOKEN=\"prod-admin-token-from-secret-store\"\nuvicorn ha_backend.api:app --host 0.0.0.0 --port 8001\n</code></pre> <p>From a trusted machine you can then verify access:</p> <ul> <li>Without a token (should be forbidden when the env var is set):</li> </ul> <pre><code>curl -i \"https://api.healtharchive.ca/api/admin/jobs\"\ncurl -i \"https://api.healtharchive.ca/metrics\"\n</code></pre> <ul> <li>With the correct token:</li> </ul> <pre><code>curl -i \\\n  -H \"Authorization: Bearer $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  \"https://api.healtharchive.ca/api/admin/jobs\"\n\ncurl -i \\\n  -H \"Authorization: Bearer $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  \"https://api.healtharchive.ca/metrics\"\n</code></pre> <p>In staging/prod you should call these endpoints only from operator tooling or monitoring systems (Prometheus, etc.), not from the public frontend.</p>"},{"location":"development/live-testing/#3-minimal-archive_tool-integration-sanity-only","title":"3. Minimal archive_tool integration (sanity only)","text":"<p>Goal: prove Docker + <code>archive_tool</code> wiring work without committing to long crawls.</p>"},{"location":"development/live-testing/#31-verify-archive_tool-docker","title":"3.1 Verify archive_tool &amp; Docker","text":"<pre><code>ha-backend check-archive-tool\n</code></pre> <p>This runs <code>archive-tool --help</code> via the configured command (by default <code>archive-tool</code>, which uses Docker).</p> <p>If this fails, fix Docker or PATH before proceeding.</p>"},{"location":"development/live-testing/#32-optional-direct-archive_tool-dry-run","title":"3.2 Optional: direct archive_tool dry run","text":"<p>From the repo root:</p> <pre><code>archive-tool --seeds https://example.org --name example --output-dir $(pwd)/.dev-archive-root/dry-run --dry-run\n</code></pre> <p>This is not required for backend work, but is a quick sanity check that the integrated crawler CLI works directly and that your configuration (seeds, output directory, workers, monitoring flags) is valid without actually starting Docker containers.</p>"},{"location":"development/live-testing/#4-small-dbbacked-job-pipeline-in-a-dev-sandbox","title":"4. Small DB\u2011backed job pipeline in a dev sandbox","text":"<p>Goal: run a single small job end\u2011to\u2011end (create \u2192 run \u2192 index) using the same flows the worker will use, with the important caveat that the current Zimit image may not leave WARCs accessible (see notes below).</p>"},{"location":"development/live-testing/#41-seed-sources","title":"4.1 Seed sources","text":"<pre><code>ha-backend seed-sources\n</code></pre> <p>This inserts baseline <code>Source</code> rows (e.g., <code>hc</code>, <code>phac</code>).</p> <pre><code>ha-backend list-jobs\n</code></pre> <p>Should still show no <code>ArchiveJob</code> rows initially.</p>"},{"location":"development/live-testing/#42-create-a-job","title":"4.2 Create a job","text":"<p>Start with Health Canada:</p> <pre><code>ha-backend create-job --source hc\n</code></pre> <p>Note the printed job ID (call it <code>JOB_ID</code>). At this point:</p> <ul> <li>A DB row exists with <code>status=\"queued\"</code>.</li> <li>An <code>output_dir</code> path under <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> is reserved.</li> </ul>"},{"location":"development/live-testing/#43-run-the-crawl-once","title":"4.3 Run the crawl once","text":"<pre><code>ha-backend run-db-job --id JOB_ID\n</code></pre> <p>This:</p> <ul> <li>Loads the DB row.</li> <li>Constructs an <code>archive_tool</code> command.</li> <li>Runs Docker + Zimit.</li> </ul> <p>It can take a minute or more depending on seeds and limits. If it fails:</p> <ul> <li>Inspect <code>ha-backend show-job --id JOB_ID</code> for <code>crawler_exit_code</code>,   <code>status</code>, and <code>output_dir</code>.</li> <li>Check logs under that <code>output_dir</code> with <code>ls</code> and <code>less</code>.</li> </ul> <p>Note: With the current Zimit image and defaults, small runs may still end with <code>FAILED_NO_WARCS</code> because no accessible WARCs are left under <code>/output/.tmp*</code>. This is a limitation of the current crawler image and does not block backend/API development (see section 6 for a controlled WARC test).</p>"},{"location":"development/live-testing/#44-index-the-job-best-effort","title":"4.4 Index the job (best effort)","text":"<p>Once a job has <code>status=\"completed\"</code>, you can attempt:</p> <pre><code>ha-backend index-job --id JOB_ID\n</code></pre> <p>This:</p> <ul> <li>Runs WARC discovery based on <code>job.output_dir</code>.</li> <li>Streams WARCs into <code>Snapshot</code> rows.</li> <li>Updates <code>warc_file_count</code> and <code>indexed_page_count</code>.</li> </ul> <p>If no WARCs are discovered, the job is marked <code>index_failed</code>. This is expected when the crawler leaves no accessible WARCs.</p>"},{"location":"development/live-testing/#45-verify-via-cli-and-api","title":"4.5 Verify via CLI and API","text":"<p>CLI:</p> <pre><code>ha-backend show-job --id JOB_ID\n</code></pre> <p>Look for:</p> <ul> <li><code>status=\"indexed\"</code> and <code>indexed_page_count &gt; 0</code> (ideal case), or</li> <li><code>status=\"index_failed\"</code> if no WARCs were found.</li> </ul> <p>API (with uvicorn running):</p> <pre><code>curl http://localhost:8001/api/sources\ncurl \"http://localhost:8001/api/search?q=health&amp;source=hc\"\n</code></pre> <p>If indexing succeeded, these will reflect real crawl data. In practice, for development we often use synthetic snapshots instead (see 6.2).</p>"},{"location":"development/live-testing/#5-worker-loop-tests-background-processing","title":"5. Worker loop tests (background processing)","text":"<p>Goal: test the long\u2011running worker process that automates job execution.</p>"},{"location":"development/live-testing/#51-queue-a-couple-of-jobs","title":"5.1 Queue a couple of jobs","text":"<pre><code>ha-backend create-job --source hc\nha-backend create-job --source phac\nha-backend list-jobs\n</code></pre> <p>You should see the new jobs in <code>status=\"queued\"</code>.</p>"},{"location":"development/live-testing/#52-run-worker-in-singlecycle-mode","title":"5.2 Run worker in single\u2011cycle mode","text":"<pre><code>ha-backend start-worker --once\n</code></pre> <p>The worker:</p> <ul> <li>Picks the oldest <code>queued</code>/<code>retryable</code> job.</li> <li>Runs <code>run_persistent_job(job_id)</code> (archive_tool).</li> <li>Immediately runs <code>index_job(job_id)</code>.</li> <li>Exits after one iteration.</li> </ul> <p>Check transitions:</p> <pre><code>ha-backend list-jobs\n</code></pre> <p>Statuses should move (e.g., <code>queued</code> \u2192 <code>completed</code>/<code>index_failed</code>).</p>"},{"location":"development/live-testing/#53-worker-loop-with-a-harmless-tool-command-optional","title":"5.3 Worker loop with a harmless tool command (optional)","text":"<p>For pure orchestration tests, point <code>archive_tool</code> at <code>echo</code>:</p> <pre><code>export HEALTHARCHIVE_TOOL_CMD=echo\n</code></pre> <p>Then:</p> <pre><code>ha-backend create-job --source hc\nha-backend start-worker --once\nha-backend list-jobs\n</code></pre> <p>You will see jobs flip from <code>queued</code> to <code>completed</code> (crawl RC 0) and then to <code>index_failed</code> (no WARCs), verifying the worker loop and status updates without touching Docker.</p>"},{"location":"development/live-testing/#6-raw-snapshot-viewer-tests","title":"6. Raw snapshot viewer tests","text":"<p>Goal: confirm WARC \u2192 HTML replay is functioning.</p> <p>There are two complementary approaches:</p>"},{"location":"development/live-testing/#61-happypath-viewer-using-a-synthetic-warc","title":"6.1 Happy\u2011path viewer using a synthetic WARC","text":"<p>You can create a tiny WARC file and a corresponding <code>Snapshot</code> in the DB:</p> <pre><code>python - &lt;&lt; 'PY'\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom io import BytesIO\n\nfrom warcio.warcwriter import WARCWriter\n\nfrom ha_backend.db import get_session\nfrom ha_backend.models import Source, Snapshot\n\nroot = Path(\".dev-archive-root\") / \"manual-warcs\"\nwarc_path = root / \"viewer-test.warc.gz\"\nurl = \"https://example.org/page\"\nhtml_body = \"&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hello from WARC&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\"\n\nroot.mkdir(parents=True, exist_ok=True)\nwith warc_path.open(\"wb\") as f:\n    writer = WARCWriter(f, gzip=True)\n    payload = BytesIO(\n        (\n            \"HTTP/1.1 200 OK\\\\r\\\\n\"\n            \"Content-Type: text/html; charset=utf-8\\\\r\\\\n\"\n            f\"Content-Length: {len(html_body.encode('utf-8'))}\\\\r\\\\n\"\n            \"\\\\r\\\\n\" +\n            html_body\n        ).encode(\"utf-8\")\n    )\n    record = writer.create_warc_record(\n        uri=url,\n        record_type=\"response\",\n        payload=payload,\n        warc_headers_dict={\"WARC-Date\": \"2025-01-01T12:00:00Z\"},\n    )\n    writer.write_record(record)\n    record_id = record.rec_headers.get_header(\"WARC-Record-ID\")\n\nwith get_session() as session:\n    src = session.query(Source).filter_by(code=\"test\").one_or_none()\n    if src is None:\n        src = Source(\n            code=\"test\",\n            name=\"Test Source\",\n            base_url=\"https://example.org\",\n            description=\"Test source for viewer\",\n            enabled=True,\n        )\n        session.add(src)\n        session.flush()\n\n    snap = Snapshot(\n        job_id=None,\n        source_id=src.id,\n        url=url,\n        normalized_url_group=url,\n        capture_timestamp=datetime(2025, 1, 1, 12, 0, tzinfo=timezone.utc),\n        mime_type=\"text/html\",\n        status_code=200,\n        title=\"Viewer Test Page\",\n        snippet=\"Hello from WARC\",\n        language=\"en\",\n        warc_path=str(warc_path),\n        warc_record_id=record_id,\n    )\n    session.add(snap)\n    session.flush()\n    print(\"SNAPSHOT_ID\", snap.id)\nPY\n</code></pre> <p>Note the printed <code>SNAPSHOT_ID</code> (for example <code>5</code>), then:</p> <pre><code>curl \"http://localhost:8001/api/snapshots/raw/5\"\n</code></pre> <p>You should see the HTML body with <code>\"Hello from WARC\"</code>, confirming that:</p> <ul> <li><code>warc_path</code> is valid.</li> <li><code>warc_record_id</code> is used for lookup.</li> <li><code>viewer.py</code> can reconstruct and return HTML.</li> </ul>"},{"location":"development/live-testing/#62-error-path-when-warcs-are-missing","title":"6.2 Error path when WARCs are missing","text":"<p>For snapshots whose <code>warc_path</code> points to a non\u2011existent file (e.g., your seeded dev snapshots), the route returns a meaningful error:</p> <pre><code>curl \"http://localhost:8001/api/snapshots/raw/1\"\n</code></pre> <p>Returns HTTP 404 with <code>{\"detail\":\"Underlying WARC file for this snapshot is missing\"}</code>.</p>"},{"location":"development/live-testing/#7-real-warc-indexing-advanced","title":"7. Real WARC indexing (advanced)","text":"<p>Goal: take a small real Zimit crawl, fix any permission issues, and index its WARCs into snapshots for use via the HTTP API.</p> <p>This section assumes you have already run a small crawl with something like:</p> <pre><code>ha-backend run-job \\\n  --name hc-dev-warcs \\\n  --seeds https://www.canada.ca/en/health-canada.html \\\n  --initial-workers 1 \\\n  --log-level INFO \\\n  -- \\\n  --pageLimit 5 \\\n  --depth 1\n</code></pre> <p>and have a job directory such as:</p> <pre><code>.dev-archive-root/20251210T013134Z__hc-dev-warcs\n</code></pre>"},{"location":"development/live-testing/#71-fix-permissions-on-the-temp-dir-if-needed","title":"7.1 Fix permissions on the temp dir (if needed)","text":"<p>Zimit may create <code>.tmp*</code> directories owned by <code>root</code>, which prevents the backend from reading WARCs. In the job directory:</p> <pre><code>cd .dev-archive-root/20251210T013134Z__hc-dev-warcs\nls -ld .tmp*\n</code></pre> <p>If you see <code>drwx------ root root ...</code>, fix ownership:</p> <pre><code>sudo chown -R $(id -u):$(id -g) .tmp*\n</code></pre> <p>Verify you can see WARCs:</p> <pre><code>find . -maxdepth 6 -type f \\( -name '*.warc' -o -name '*.warc.gz' \\) -print\n</code></pre> <p>You should see something like:</p> <pre><code>./.tmpXXXX/collections/crawl-.../archive/rec-...warc.gz\n</code></pre>"},{"location":"development/live-testing/#72-create-a-db-job-pointing-at-this-output_dir","title":"7.2 Create a DB job pointing at this output_dir","text":"<p>From the repo root:</p> <pre><code>python - &lt;&lt; 'PY'\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob, Source\n\njob_dir = Path(\".dev-archive-root/20251210T013134Z__hc-dev-warcs\").resolve()\n\nwith get_session() as session:\n    src = session.query(Source).filter_by(code=\"hc\").one()\n    now = datetime.now(timezone.utc)\n    job = ArchiveJob(\n        source_id=src.id,\n        name=\"hc-dev-warcs\",\n        output_dir=str(job_dir),\n        status=\"completed\",   # ready for indexing\n        queued_at=now,\n        started_at=now,\n        finished_at=now,\n    )\n    session.add(job)\n    session.flush()\n    print(\"JOB_ID\", job.id)\nPY\n</code></pre> <p>Note the printed <code>JOB_ID</code> (e.g. <code>11</code>).</p> <p>Alternative (CLI): you can now do the same with a helper command:</p> <pre><code>ha-backend register-job-dir \\\n  --source hc \\\n  --output-dir .dev-archive-root/20251210T013134Z__hc-dev-warcs \\\n  --name hc-dev-warcs\n</code></pre> <p>This creates a DB row in <code>status=\"completed\"</code> so it is ready for indexing.</p>"},{"location":"development/live-testing/#73-index-the-job-and-verify-via-api","title":"7.3 Index the job and verify via API","text":"<p>Index the WARCs:</p> <pre><code>ha-backend index-job --id JOB_ID\nha-backend show-job --id JOB_ID\n</code></pre> <p>You should see:</p> <ul> <li><code>status=\"indexed\"</code></li> <li><code>warc_file_count &gt; 0</code></li> <li><code>indexed_page_count &gt; 0</code></li> </ul> <p>With uvicorn running:</p> <pre><code>curl http://localhost:8001/api/sources\ncurl \"http://localhost:8001/api/search?q=health&amp;source=hc\"\n</code></pre> <p>You will see the real crawl snapshots alongside any synthetic dev data.</p>"},{"location":"development/live-testing/#8-admin-retry-and-cleanup-flows","title":"8. Admin, retry, and cleanup flows","text":"<p>Goal: exercise non\u2011happy\u2011path and maintenance commands.</p>"},{"location":"development/live-testing/#81-retry-jobs","title":"8.1 Retry jobs","text":"<p>If a job has <code>status=\"failed\"</code> or <code>status=\"index_failed\"</code>:</p> <pre><code>ha-backend retry-job --id JOB_ID\nha-backend show-job --id JOB_ID\n</code></pre> <p>Behavior:</p> <ul> <li><code>status=\"failed\"</code> \u2192 <code>status=\"retryable\"</code> (for another crawl).</li> <li><code>status=\"index_failed\"</code> \u2192 <code>status=\"completed\"</code> (allowing re\u2011indexing).</li> </ul>"},{"location":"development/live-testing/#82-cleanup-temp-dirs-and-state","title":"8.2 Cleanup temp dirs and state","text":"<p>Only allowed for <code>status in {\"indexed\", \"index_failed\"}</code>:</p> <pre><code>ha-backend cleanup-job --id JOB_ID --mode temp\nha-backend show-job --id JOB_ID\n</code></pre> <p>This:</p> <ul> <li>Locates temp dirs and <code>.archive_state.json</code> via <code>archive_tool.state</code>.</li> <li>Deletes <code>.tmp*</code> dirs and the state file.</li> <li>Sets:</li> <li><code>cleanup_status = \"temp_cleaned\"</code></li> <li><code>cleaned_at</code> to the cleanup time</li> <li><code>state_file_path = None</code></li> </ul> <p>Caution: This removes temp crawl artifacts (including WARCs) under <code>.tmp*</code> for that job. Only run it once you are satisfied with indexing and any ZIMs/exports.</p> <p>If you are using the replay service (pywb) to serve this job\u2019s WARCs, do not run <code>cleanup-job --mode temp</code> for that job \u2014 replay depends on the WARCs remaining on disk.</p> <p>If replay is enabled globally (<code>HEALTHARCHIVE_REPLAY_BASE_URL</code> is set), <code>cleanup-job --mode temp</code> will refuse unless you pass <code>--force</code>. Treat <code>--force</code> as an emergency override (it can break replay by deleting WARCs).</p>"},{"location":"development/live-testing/#9-metrics-and-observability","title":"9. Metrics and observability","text":"<p>Goal: validate Prometheus\u2011style metrics.</p> <p>With uvicorn running:</p> <pre><code>curl -H \"Authorization: Bearer localdev-admin\" \\\n  http://localhost:8001/metrics | head\n</code></pre> <p>Look for:</p> <ul> <li>Job status metrics:</li> </ul> <pre><code>healtharchive_jobs_total{status=\"failed\"} 6\nhealtharchive_jobs_total{status=\"indexed\"} 1\n...\n</code></pre> <ul> <li>Cleanup status metrics:</li> </ul> <pre><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"none\"} ...\nhealtharchive_jobs_cleanup_status_total{cleanup_status=\"temp_cleaned\"} ...\n</code></pre> <ul> <li>Snapshot metrics:</li> </ul> <pre><code>healtharchive_snapshots_total 5\nhealtharchive_snapshots_total{source=\"hc\"} 3\nhealtharchive_snapshots_total{source=\"test\"} 2\n</code></pre> <ul> <li>Page-level crawl metrics (best-effort from crawl logs):</li> </ul> <pre><code>healtharchive_jobs_pages_crawled_total 1234\nhealtharchive_jobs_pages_crawled_total{source=\"hc\"} 789\nhealtharchive_jobs_pages_failed_total 12\nhealtharchive_jobs_pages_failed_total{source=\"hc\"} 3\n</code></pre> <p>Counts should roughly match <code>ha-backend list-jobs</code>, <code>/api/sources</code> / <code>/api/search</code>, and the page counters shown in <code>/api/admin/jobs/{id}</code>.</p>"},{"location":"development/live-testing/#10-scaling-up-to-more-realistic-scenarios","title":"10. Scaling up to more realistic scenarios","text":"<p>Once the above is stable, you can incrementally increase realism:</p> <ul> <li>Multiple jobs with the worker running continuously.</li> </ul> <pre><code>ha-backend start-worker --poll-interval 30\n</code></pre> <p>In another terminal, periodically run <code>create-job</code> and watch statuses   transition through <code>queued \u2192 running \u2192 completed \u2192 indexed/index_failed</code>.</p> <ul> <li> <p>Postgres instead of SQLite by pointing   <code>HEALTHARCHIVE_DATABASE_URL</code> at a dev Postgres instance and re\u2011running   <code>alembic upgrade head</code>.</p> </li> <li> <p>Monitoring/adaptive options via <code>job_registry</code> overrides:</p> </li> <li> <p>Enable <code>enable_monitoring</code>, <code>enable_adaptive_workers</code>,     <code>enable_vpn_rotation</code> and confirm they affect archive_tool behavior.</p> </li> <li> <p>Frontend integration by running the separate <code>healtharchive-frontend</code>   against your local backend (<code>NEXT_PUBLIC_BACKEND_URL=http://localhost:8001</code>)   and exercising the full UI \u2192 API \u2192 DB \u2192 WARC stack.</p> </li> </ul> <p>Note on real WARCs: At time of writing, the default Zimit image may not leave WARCs in the expected <code>/output/.tmp*/collections/.../archive</code> path or may create temp directories with restrictive permissions. For backend/API and viewer development, using synthetic WARCs (as in 6.1) and seeded snapshots is sufficient. Integrating with live WARCs may require either adjusting Zimit options or updating WARC discovery to match the crawler\u2019s current layout and permissions.</p>"},{"location":"development/test-coverage/","title":"Test Coverage Requirements","text":"<p>This document defines test coverage requirements and quality gates for the HealthArchive backend.</p>"},{"location":"development/test-coverage/#coverage-targets","title":"Coverage Targets","text":""},{"location":"development/test-coverage/#critical-modules","title":"Critical Modules","text":"<p>The following modules are considered critical for system reliability and must maintain minimum test coverage:</p> Module Current Target Priority <code>ha_backend/api</code> 95.81% / 77.29% 80% High <code>ha_backend/worker</code> 81.76% 80% High <code>ha_backend/indexing</code> Mixed 75% \u2192 80% Medium <p>Overall Critical Modules: 76.96% (target: 75% enforced, 80% goal)</p>"},{"location":"development/test-coverage/#running-coverage-checks","title":"Running Coverage Checks","text":"<pre><code># Full coverage report (all modules)\nmake coverage\n\n# Critical modules only (enforced in CI check-full)\nmake coverage-critical\n\n# View coverage reports\nmake coverage-report\n</code></pre>"},{"location":"development/test-coverage/#ci-enforcement","title":"CI Enforcement","text":""},{"location":"development/test-coverage/#current-enforcement-check-full","title":"Current Enforcement (check-full)","text":"<p>The <code>make check-full</code> target includes <code>coverage-critical</code> which enforces: - 75% minimum coverage on critical modules (API, worker, indexing) - Fails CI if coverage drops below threshold - Prevents coverage regressions</p>"},{"location":"development/test-coverage/#not-enforced-in-daily-ci-checkci","title":"Not Enforced in Daily CI (check/ci)","text":"<p>Coverage is not checked in the daily <code>make ci</code> target to keep PR checks fast. Coverage is only enforced in: - <code>make check-full</code> (pre-deploy checks) - Manual coverage audits</p>"},{"location":"development/test-coverage/#coverage-configuration","title":"Coverage Configuration","text":"<p>Coverage settings are in <code>pyproject.toml</code>:</p> <pre><code>[tool.coverage.run]\nsource = [\"src\"]\nomit = [\"*/tests/*\", \"*/test_*.py\"]\n\n[tool.coverage.report]\nprecision = 2\nshow_missing = true\nexclude_lines = [\n    \"pragma: no cover\",\n    \"if TYPE_CHECKING:\",\n    \"raise NotImplementedError\",\n    # ... more exclusions\n]\n</code></pre>"},{"location":"development/test-coverage/#path-to-80-coverage","title":"Path to 80% Coverage","text":"<p>Current bottleneck: <code>indexing/pipeline.py</code> at 22.55%</p> <p>To reach 80% overall: 1. Add integration tests for indexing pipeline 2. Test error paths in WARC processing 3. Test edge cases in text extraction</p> <p>Why 75% now, 80% later? - 75% is realistic given current test suite - Enforcing 75% prevents regressions - Provides concrete baseline for portfolio - 80% is achievable with ~100 more lines of tests</p>"},{"location":"development/test-coverage/#coverage-best-practices","title":"Coverage Best Practices","text":""},{"location":"development/test-coverage/#what-to-test","title":"What to Test","text":"<p>\u2705 High priority: - API endpoints (request/response validation) - Business logic (search, ranking, deduplication) - Error handling (4xx/5xx responses) - Security middleware (auth, rate limiting, CSP)</p> <p>\u2705 Medium priority: - Worker job lifecycle - WARC indexing pipeline - Database queries (critical paths)</p> <p>\u26a0\ufe0f Low priority: - CLI argument parsing - Logging statements - Configuration getters - Development-only utilities</p>"},{"location":"development/test-coverage/#coverage-exclusions","title":"Coverage Exclusions","text":"<p>Use <code># pragma: no cover</code> sparingly for: - Abstract methods that must be overridden - Defensive assertions that should never trigger - Platform-specific code paths - <code>if __name__ == \"__main__\"</code> blocks</p> <p>Never exclude: - Error handling - Business logic - API endpoints - Security code</p>"},{"location":"development/test-coverage/#viewing-coverage-reports","title":"Viewing Coverage Reports","text":"<p>After running <code>make coverage</code> or <code>make coverage-critical</code>:</p> <pre><code># Full report\nopen htmlcov/index.html\n\n# Critical modules only\nopen htmlcov-critical/index.html\n</code></pre> <p>Reports show: - Line-by-line coverage - Missing lines highlighted in red - Partially covered branches - Excluded lines</p>"},{"location":"development/test-coverage/#coverage-in-ci","title":"Coverage in CI","text":"<p>Coverage enforcement in CI workflow:</p> <pre><code># .github/workflows/backend-ci.yml\n- name: Run full checks (includes coverage)\n  run: make check-full\n</code></pre> <p>When coverage fails: 1. Check which module dropped below threshold 2. Review the diff - did you add untested code? 3. Add tests to cover new functionality 4. Re-run <code>make coverage-critical</code></p>"},{"location":"development/test-coverage/#faq","title":"FAQ","text":"<p>Q: Why not 100% coverage? A: Diminishing returns. 75-80% covers critical paths. Higher coverage often tests trivial code.</p> <p>Q: Why different thresholds per module? A: API and worker are user-facing and easier to test. Indexing has complex file I/O harder to mock.</p> <p>Q: Can I temporarily disable coverage checks? A: No. Use <code># pragma: no cover</code> for specific lines only, with justification in code comments.</p> <p>Q: How do I find what's not covered? A: Run <code>make coverage-critical</code> and open <code>htmlcov-critical/index.html</code> in a browser.</p> <p>Related docs: - Testing Guide (if it exists) - CI/CD Pipeline (if it exists) - Contributing (if it exists)</p>"},{"location":"development/testing-guidelines/","title":"Backend testing guidelines (internal)","text":"<p>This doc describes the backend testing expectations and how to run checks locally.</p> <p>If you want step-by-step \u201crun the app and click it\u201d workflows, use:</p> <ul> <li><code>live-testing.md</code></li> </ul>"},{"location":"development/testing-guidelines/#what-ci-runs-recommended-locally","title":"What CI runs (recommended locally)","text":"<p>From the repo root:</p> <ul> <li><code>make check</code> (fast CI gate: format check, lint, typecheck, tests)</li> <li><code>make check-full</code> (optional: pre-commit, security scan, docs build/lint)</li> </ul> <p><code>make check</code> is intentionally kept low-friction so it can run constantly without blocking development. Use <code>make check-full</code> before deploys or when you want stricter validation.</p> <p>Notes:</p> <ul> <li><code>make check</code> runs <code>make test-fast</code> (a curated subset).</li> <li><code>make test-all</code> runs the full test suite.</li> </ul>"},{"location":"development/testing-guidelines/#end-to-end-smoke-public-surface","title":"End-to-end smoke (public surface)","text":"<p>CI also runs a fast end-to-end smoke check that starts the backend + frontend locally and verifies user-critical routes (no browser automation):</p> <ul> <li><code>./scripts/ci-e2e-smoke.sh --frontend-dir ../healtharchive-frontend</code></li> <li>If the frontend is already built (CI artifact), add: <code>--skip-frontend-build</code></li> </ul> <p>In CI, the smoke check is treated as a post-merge safety net (runs on <code>main</code> pushes / manual runs) rather than a PR gate.</p>"},{"location":"development/testing-guidelines/#running-subsets","title":"Running subsets","text":"<ul> <li>Unit tests: <code>pytest</code></li> <li>One test file: <code>pytest tests/test_something.py</code></li> <li>One test: <code>pytest -k some_keyword</code></li> <li>Lint + format: <code>ruff check .</code> and <code>ruff format --check .</code></li> <li>Type-check: <code>mypy src tests</code></li> </ul>"},{"location":"development/testing-guidelines/#writing-tests","title":"Writing tests","text":"<ul> <li>Put tests in <code>tests/</code> and prefer plain <code>pytest</code> tests (no custom harness).</li> <li>Keep tests deterministic:</li> <li>avoid real network calls</li> <li>avoid wall-clock dependencies</li> <li>avoid global state between tests</li> <li>If you add a new API route, add at least one test that exercises the route and asserts the key behavior.</li> <li>If you change DB behavior, prefer tests that set up a temporary DB using the existing test fixtures/patterns.</li> </ul>"},{"location":"development/testing-guidelines/#scope-what-belongs-in-tests-vs-scripts","title":"Scope (what belongs in tests vs scripts)","text":"<ul> <li>Application behavior belongs in <code>tests/</code>.</li> <li>VPS automation scripts under <code>scripts/</code> should stay simple and safe; when logic grows (parsing, policy evaluation), prefer moving that logic into a small Python module that can be tested.</li> </ul>"},{"location":"development/playbooks/","title":"Development playbooks (task-oriented)","text":"<p>These playbooks describe common developer workflows without duplicating deeper docs.</p> <ul> <li>Local environment setup: <code>../dev-environment-setup.md</code></li> <li>Local end-to-end testing flows: <code>../live-testing.md</code></li> <li>Change \u2192 production workflow (solo-fast): <code>change-to-production.md</code></li> <li>Making database changes (migrations): <code>database-migrations.md</code></li> <li>Adding new CLI commands: <code>add-cli-command.md</code></li> </ul>"},{"location":"development/playbooks/add-cli-command/","title":"Playbook: Add a new CLI command (developers)","text":"<p>Purpose: add a backend CLI command that is testable, documented, and safe to operate.</p>"},{"location":"development/playbooks/add-cli-command/#when-to-use","title":"When to use","text":"<ul> <li>You need a new <code>ha-backend &lt;command&gt;</code> for an operator or contributor workflow.</li> <li>You need to extend an existing command in a way that changes its contract.</li> </ul>"},{"location":"development/playbooks/add-cli-command/#preconditions","title":"Preconditions","text":"<ul> <li>You can run the CLI locally (see: <code>../dev-environment-setup.md</code> and <code>../live-testing.md</code>).</li> <li>You understand whether the command is:</li> <li>developer-only, or</li> <li>an operator command (needs docs under <code>docs/operations/**</code> / <code>docs/deployment/**</code> and careful safety rails).</li> </ul>"},{"location":"development/playbooks/add-cli-command/#safety-guardrails","title":"Safety / guardrails","text":"<ul> <li>Avoid adding \u201cpowerful defaults\u201d (e.g., destructive operations) without explicit flags and clear output.</li> <li>Don\u2019t log secrets (DB URLs, tokens, credentials).</li> <li>If this command will be used on production, ensure it has:</li> <li>dry-run support where reasonable,</li> <li>clear \u201cwhat it changes\u201d output,</li> <li>and tests for key edge cases.</li> </ul>"},{"location":"development/playbooks/add-cli-command/#steps","title":"Steps","text":"<p>1) Implement the command:    - Add/extend the CLI wiring in <code>../../../src/ha_backend/cli.py</code>. 2) Add tests close to the behavior:    - Prefer <code>tests/test_cli_*.py</code> style coverage for parsing and side-effects. 3) Document the command:    - Developer-only: add to <code>../live-testing.md</code> or an appropriate dev doc.    - Operator-facing: add to a playbook/runbook under <code>docs/operations/**</code> or <code>docs/deployment/**</code>. 4) Run the local checks:    - <code>make ci</code></p>"},{"location":"development/playbooks/add-cli-command/#verification-done-criteria","title":"Verification (\u201cdone\u201d criteria)","text":"<ul> <li>Command appears in <code>ha-backend --help</code> and <code>ha-backend &lt;command&gt; --help</code>.</li> <li>Tests cover the expected behavior and key failure modes.</li> <li><code>make ci</code> passes.</li> </ul>"},{"location":"development/playbooks/add-cli-command/#references","title":"References","text":"<ul> <li>Local testing flows: <code>../live-testing.md</code></li> <li>Documentation policy: <code>../../documentation-guidelines.md</code></li> <li>CLI implementation: <code>../../../src/ha_backend/cli.py</code></li> </ul>"},{"location":"development/playbooks/change-to-production/","title":"Change \u2192 production workflow (solo-fast)","text":"<p>Goal: ship a change safely while keeping \u201cgreen main\u201d as the deploy gate.</p> <p>Canonical references:</p> <ul> <li>Docs guidelines: <code>../../documentation-guidelines.md</code></li> <li>Monitoring/CI gate: <code>../../operations/monitoring-and-ci-checklist.md</code></li> <li>Deploy playbook (VPS): <code>../../operations/playbooks/core/deploy-and-verify.md</code></li> </ul>"},{"location":"development/playbooks/change-to-production/#workflow","title":"Workflow","text":"<ol> <li>Make the change locally.</li> <li>Run checks:</li> <li><code>make check</code></li> <li>Commit and push.</li> <li>Wait for CI to pass on <code>main</code>.</li> <li>Deploy on the VPS using the deploy playbook.</li> </ol>"},{"location":"development/playbooks/change-to-production/#cross-repo-guardrails","title":"Cross-repo guardrails","text":"<ul> <li>If you add/change a user-facing frontend route that is part of the production \u201cpublic surface\u201d, update:</li> <li><code>scripts/verify_public_surface.py</code></li> <li>Frontend bilingual rules (in the frontend repo): https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/development/bilingual-dev-guide.md</li> </ul>"},{"location":"development/playbooks/database-migrations/","title":"Playbook: Database migrations (developers)","text":"<p>Purpose: safely introduce schema changes and keep Alembic, tests, and docs aligned.</p>"},{"location":"development/playbooks/database-migrations/#when-to-use","title":"When to use","text":"<ul> <li>You changed ORM models (or need to) and the DB schema must change.</li> <li>You need to apply migrations in local dev before running the API/worker.</li> </ul>"},{"location":"development/playbooks/database-migrations/#preconditions","title":"Preconditions","text":"<ul> <li>You can run the backend locally (see: <code>../dev-environment-setup.md</code>).</li> <li><code>HEALTHARCHIVE_DATABASE_URL</code> points at the database you intend to modify.</li> <li>Local dev example (SQLite): see <code>../live-testing.md</code>.</li> </ul>"},{"location":"development/playbooks/database-migrations/#safety-guardrails","title":"Safety / guardrails","text":"<ul> <li>Never generate or apply migrations against a database you didn\u2019t intend to modify.</li> <li>Prefer testing migrations against a fresh local DB and a \u201crealistic\u201d DB with existing data.</li> <li>For production rollout considerations, follow the production runbook:</li> <li><code>../../deployment/production-single-vps.md</code></li> </ul>"},{"location":"development/playbooks/database-migrations/#steps","title":"Steps","text":"<p>1) Update ORM models (and any related code). 2) Generate a migration:    - <code>alembic revision --autogenerate -m \"describe change\"</code> 3) Review the generated migration file under <code>alembic/versions/</code>.    - Ensure it matches the intended change (constraints, nullable, defaults, indexes). 4) Apply migrations locally:    - <code>alembic upgrade head</code> 5) Run the test suite:    - <code>make ci</code> 6) Run the schema-parity guard explicitly (recommended for schema-sensitive API work):    - <code>pytest -q tests/test_ci_schema_parity.py</code> 7) Run the migration-required guard against your branch diff (recommended before opening PR):    - <code>make migration-guard MIGRATION_GUARD_BASE=origin/main MIGRATION_GUARD_HEAD=HEAD</code> 8) Update docs if the change affects operators or contributors. 9) Commit the migration + any code/docs changes together.</p>"},{"location":"development/playbooks/database-migrations/#temporary-exceptions-false-positive-handling","title":"Temporary exceptions (false-positive handling)","text":"<p>Use this only when <code>make migration-guard</code> fails but you have confirmed there is no real persisted schema change (for example, query code introducing temporary-table SQL that does not alter app schema).</p> <p>1) Add a narrowly-scoped temporary rule in <code>.github/migration-guard-exceptions.txt</code>:    - format: <code>path_glob|signal_regex|expires_yyyy-mm-dd|reason</code> 2) Keep expiry short (the guard enforces max 30 days). 3) Prefer fixing the underlying heuristic quickly and removing the exception. 4) Include the exception rationale in the PR description.</p> <p>Never use this to bypass a real schema migration requirement.</p>"},{"location":"development/playbooks/database-migrations/#verification-done-criteria","title":"Verification (\u201cdone\u201d criteria)","text":"<ul> <li><code>alembic upgrade head</code> succeeds on a clean local DB.</li> <li><code>make ci</code> passes.</li> <li><code>tests/test_ci_schema_parity.py</code> passes for schema-sensitive API/model changes.</li> <li><code>make migration-guard MIGRATION_GUARD_BASE=origin/main MIGRATION_GUARD_HEAD=HEAD</code> passes.</li> <li>Any new/changed behavior is documented in the appropriate canonical doc (dev/deploy/ops).</li> </ul>"},{"location":"development/playbooks/database-migrations/#rollback-recovery-if-needed","title":"Rollback / recovery (if needed)","text":"<ul> <li>In dev: revert via <code>alembic downgrade -1</code> (only when safe for your current DB state).</li> <li>In prod: follow the rollback guidance in the deploy/runbook docs; avoid ad-hoc downgrades.</li> </ul>"},{"location":"development/playbooks/database-migrations/#references","title":"References","text":"<ul> <li>Local dev flows: <code>../live-testing.md</code></li> <li>Production runbook: <code>../../deployment/production-single-vps.md</code></li> <li>Alembic config: <code>../../../alembic.ini</code>, <code>../../../alembic/</code></li> </ul>"},{"location":"frontend-external/","title":"Frontend documentation","text":"<p>The HealthArchive frontend lives in the separate repository:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend</li> </ul> <p>This backend repo\u2019s documentation site includes a minimal \u201cFrontend\u201d section that links out to the canonical docs (we intentionally do not mirror frontend docs into this site).</p> <p>For the canonical, up-to-date frontend docs, see:</p> <ul> <li>Frontend overview: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/README.md</li> <li>I18n: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/i18n.md</li> <li>Implementation guide: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/implementation-guide.md</li> </ul>"},{"location":"frontend-external/i18n/","title":"Frontend i18n","text":"<p>Canonical frontend i18n documentation lives in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/i18n.md</li> </ul>"},{"location":"frontend-external/implementation-guide/","title":"Frontend implementation guide","text":"<p>Canonical frontend implementation documentation lives in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/implementation-guide.md</li> </ul>"},{"location":"meta/documentation-health/","title":"Documentation Health Metrics","text":"<p>This page tracks the health and coverage of HealthArchive documentation.</p> <p>Last Updated: Auto-generated on every docs build</p>"},{"location":"meta/documentation-health/#coverage-metrics","title":"Coverage Metrics","text":""},{"location":"meta/documentation-health/#navigation-coverage","title":"Navigation Coverage","text":"<p>Goal: Key documentation is discoverable via sidebar navigation</p> Category Files on Disk In Navigation Coverage Target Tutorials 4 4 100% 100% Operations 50+ 30+ 60%+ 50% Development 5 4 80% 80% Deployment 15+ 8 53% 60% Reference 5 5 100% 100% Explanation 10+ 8 80% 70% Playbooks 32 32 100% 100% Roadmaps 20+ 4 20% 20% Overall 123+ 74+ 60% 50% <p>Status: \u2705 Above target (60% &gt; 50%)</p> <p>Achievements: - All tutorials in navigation (4/4) - All critical playbooks accessible - Reference documentation complete - Production runbook directly accessible</p> <p>Remaining gaps: - Some historical roadmap documents (intentionally archived) - Some operational logs (reference-only)</p>"},{"location":"meta/documentation-health/#documentation-types-diataxis-framework","title":"Documentation Types (Di\u00e1taxis Framework)","text":""},{"location":"meta/documentation-health/#distribution-by-type","title":"Distribution by Type","text":"Type Count Percentage Target Status Tutorials (Learning) 4 3% 3-5% \u2705 At target How-To Guides (Tasks) 50+ 41% 40-50% \u2705 Within range Reference (Information) 10 8% 10-15% \u26a0\ufe0f Could add more Explanation (Understanding) 25+ 20% 15-25% \u2705 Within range Meta/Templates 10 8% 5-10% \u2705 Good Pointers 5 4% &lt;5% \u2705 Minimal <p>Status: \u2705 Well-balanced according to Di\u00e1taxis principles</p>"},{"location":"meta/documentation-health/#content-quality-indicators","title":"Content Quality Indicators","text":""},{"location":"meta/documentation-health/#documentation-completeness","title":"Documentation Completeness","text":"Indicator Status Notes Quick Start exists \u2705 Yes <code>../quickstart.md</code> Architecture documented \u2705 Yes Comprehensive 1,314-line guide API documented \u2705 Yes OpenAPI spec + consumer guide Contribution guide \u2705 Yes Complete CONTRIBUTING.md Code of Conduct \u2705 Yes In CONTRIBUTING.md Deployment runbook \u2705 Yes <code>deployment/production-single-vps.md</code> Incident response \u2705 Yes <code>operations/playbooks/core/incident-response.md</code> Testing guidelines \u2705 Yes <code>development/testing-guidelines.md</code> <p>Score: 8/8 \u2705 Excellent</p>"},{"location":"meta/documentation-health/#freshness","title":"Freshness","text":""},{"location":"meta/documentation-health/#recently-updated-last-30-days","title":"Recently Updated (Last 30 Days)","text":"<p>Based on recent documentation improvements:</p> <ul> <li>\u2705 Navigation restructure (2026-01-18)</li> <li>\u2705 New tutorials added (3 tutorials)</li> <li>\u2705 API consumer guide created</li> <li>\u2705 Project hub enhanced</li> <li>\u2705 CONTRIBUTING.md updated</li> <li>\u2705 Reference section created</li> </ul> <p>Status: \u2705 Active maintenance</p>"},{"location":"meta/documentation-health/#stale-documentation-check","title":"Stale Documentation Check","text":"<p>Documents not updated in &gt;180 days: TBD (requires git analysis)</p> <p>Action: Review quarterly as part of Ops Cadence</p>"},{"location":"meta/documentation-health/#link-health","title":"Link Health","text":""},{"location":"meta/documentation-health/#internal-links","title":"Internal Links","text":"<p>Check script: <code>scripts/check_docs_references.py</code></p> <p>Run: <code>make docs-refs</code></p> <p>Last status: \u23f3 Run <code>make docs-refs</code> to check</p> <p>Expected: 0 broken internal links</p>"},{"location":"meta/documentation-health/#external-links","title":"External Links","text":"<p>Check tool: Lychee (GitHub Action)</p> <p>Last status: \u26a0\ufe0f Advisory only (doesn't fail build)</p> <p>Action: Review and fix broken external links quarterly</p>"},{"location":"meta/documentation-health/#accessibility","title":"Accessibility","text":""},{"location":"meta/documentation-health/#navigation-depth","title":"Navigation Depth","text":"Metric Value Target Status Max nav depth 4 levels \u22644 \u2705 Good Avg nav depth 2.5 levels 2-3 \u2705 Good Orphaned docs 49 &lt;30% \u2705 Below threshold"},{"location":"meta/documentation-health/#search-effectiveness","title":"Search Effectiveness","text":"<p>Features enabled: - \u2705 Search suggestions - \u2705 Search highlighting - \u2705 Tag-based search (new) - \u2705 Minimum search length: 2 chars - \u2705 Language: English</p> <p>Status: \u2705 Good search experience</p>"},{"location":"meta/documentation-health/#multi-repo-consistency","title":"Multi-Repo Consistency","text":""},{"location":"meta/documentation-health/#cross-repo-references","title":"Cross-Repo References","text":"Repo Documented Linked Status healtharchive-backend \u2705 \u2705 This repo healtharchive-frontend \u2705 \u2705 <code>frontend-external/</code> pointers healtharchive-datasets \u2705 \u2705 <code>datasets-external/</code> pointer <p>Linking standard: GitHub URLs (not workspace-relative)</p> <p>Status: \u2705 Consistent</p>"},{"location":"meta/documentation-health/#documentation-workflows","title":"Documentation Workflows","text":""},{"location":"meta/documentation-health/#build-process","title":"Build Process","text":"<p>Command: <code>make docs-build</code></p> <p>Steps: 1. Generate OpenAPI spec (<code>scripts/export_openapi.py</code>) 2. Generate developer assistant context (<code>scripts/generate_llms_txt.py</code>) 3. Build MkDocs site 4. Run advisory checks (refs, coverage) 5. Link checking (Lychee)</p> <p>CI Status: \u2705 Auto-deploys to docs.healtharchive.ca</p>"},{"location":"meta/documentation-health/#validation-checks","title":"Validation Checks","text":"Check Command Status Reference validation <code>make docs-refs</code> \u23f3 Run to check Coverage reporting <code>make docs-coverage</code> \u23f3 Run to check Link checking Lychee (in CI) \u26a0\ufe0f Advisory Format/lint <code>make check-full</code> \u2705 Part of CI"},{"location":"meta/documentation-health/#templates","title":"Templates","text":""},{"location":"meta/documentation-health/#available-templates","title":"Available Templates","text":"<p>Located in <code>docs/_templates/</code>:</p> Template Purpose Usage Count <code>../_templates/runbook-template.md</code> Deployment procedures 15+ runbooks <code>../_templates/playbook-template.md</code> Operational tasks 32 playbooks <code>../_templates/incident-template.md</code> Post-mortems 4 incidents <code>../_templates/decision-template.md</code> ADR-lite records 1 decision <code>../_templates/restore-test-log-template.md</code> Restore verification VPS logs <code>../_templates/adoption-signals-log-template.md</code> Adoption tracking VPS logs <code>../_templates/mentions-log-template.md</code> Mentions tracking VPS logs <code>../_templates/ops-ui-friction-log-template.md</code> UX issues VPS logs <p>Status: \u2705 Well-used templates ensure consistency</p>"},{"location":"meta/documentation-health/#documentation-improvements-roadmap","title":"Documentation Improvements Roadmap","text":""},{"location":"meta/documentation-health/#completed-2026-01-18","title":"Completed (2026-01-18)","text":"<ul> <li>\u2705 Navigation restructure (Di\u00e1taxis framework)</li> <li>\u2705 Quick start guide</li> <li>\u2705 Tutorial trilogy (first contribution, architecture, debugging)</li> <li>\u2705 API consumer guide</li> <li>\u2705 Enhanced project hub</li> <li>\u2705 CONTRIBUTING.md</li> <li>\u2705 Reference section (data model, CLI, archive-tool)</li> <li>\u2705 Documentation health dashboard (this page)</li> <li>\u2705 Search optimization</li> <li>\u2705 Advanced navigation features</li> </ul>"},{"location":"meta/documentation-health/#planned-improvements","title":"Planned Improvements","text":"<p>Near-term (Next quarter): - [ ] Add more code examples to architecture docs - [ ] Create video walkthroughs for tutorials - [ ] Expand troubleshooting guides - [ ] Add more FAQ entries</p> <p>Medium-term (6 months): - [ ] Multi-format export (PDF, ePub) - [ ] Analytics integration (track popular pages) - [ ] Interactive diagrams (clickable Mermaid) - [ ] Versioned documentation (per release)</p> <p>Long-term (Future): - [ ] Multilingual documentation (French) - [ ] Documentation chatbot (assistant-powered search) - [ ] Automated screenshot updates - [ ] Doc contribution gamification</p>"},{"location":"meta/documentation-health/#quality-assurance","title":"Quality Assurance","text":""},{"location":"meta/documentation-health/#documentation-review-checklist","title":"Documentation Review Checklist","text":"<p>For each new document:</p> <ul> <li> Follows appropriate template</li> <li> Uses clear, concise language</li> <li> Includes code examples (if applicable)</li> <li> Cross-referenced from related docs</li> <li> Added to mkdocs.yml navigation (if key doc)</li> <li> Links verified (<code>make docs-refs</code>)</li> <li> Preview checked (<code>make docs-serve</code>)</li> <li> Spell-checked</li> <li> Grammar-checked</li> <li> Technical accuracy verified</li> </ul>"},{"location":"meta/documentation-health/#quarterly-review","title":"Quarterly Review","text":"<p>Every 3 months, review:</p> <ol> <li>Freshness: Update stale docs (&gt;180 days)</li> <li>Accuracy: Verify technical details match current code</li> <li>Completeness: Check for new features needing docs</li> <li>Gaps: Identify missing documentation</li> <li>Feedback: Incorporate user feedback from issues/discussions</li> </ol> <p>Tracked in: Operations Cadence Checklist</p>"},{"location":"meta/documentation-health/#metrics-over-time","title":"Metrics Over Time","text":""},{"location":"meta/documentation-health/#historical-trends","title":"Historical Trends","text":"Date Total Docs In Nav Coverage Notable Changes 2026-01-17 121 23 19% Baseline before restructure 2026-01-18 123 74 60% Di\u00e1taxis restructure + new content <p>Trend: \u2b06\ufe0f Significant improvement (+41 percentage points)</p>"},{"location":"meta/documentation-health/#contributing-to-documentation","title":"Contributing to Documentation","text":""},{"location":"meta/documentation-health/#how-you-can-help","title":"How You Can Help","text":"<ul> <li>\ud83d\udc1b Report issues: Broken links, unclear instructions, typos</li> <li>\ud83d\udca1 Suggest improvements: Missing topics, better examples</li> <li>\u270f\ufe0f Fix typos: Small PRs welcome!</li> <li>\ud83d\udcdd Write new docs: Fill gaps in coverage</li> <li>\ud83c\udfa8 Improve diagrams: Enhance Mermaid diagrams</li> <li>\ud83d\udd0d Review PRs: Help review documentation changes</li> </ul> <p>See: contributing.md</p>"},{"location":"meta/documentation-health/#tools-infrastructure","title":"Tools &amp; Infrastructure","text":""},{"location":"meta/documentation-health/#documentation-stack","title":"Documentation Stack","text":"Component Technology Purpose Generator MkDocs Material Static site generation Markdown GitHub-flavored Content format Diagrams Mermaid Visual documentation API Docs OpenAPI + Swagger UI Interactive API reference Search MkDocs search plugin Full-text search Hosting GitHub Pages docs.healtharchive.ca CI/CD GitHub Actions Auto-build and deploy"},{"location":"meta/documentation-health/#key-configuration-files","title":"Key Configuration Files","text":"File Purpose <code>mkdocs.yml</code> MkDocs configuration <code>docs/_templates/</code> Document templates <code>scripts/export_openapi.py</code> Generate API spec <code>scripts/generate_llms_txt.py</code> Generate developer assistant context <code>scripts/check_docs_references.py</code> Validate links <code>scripts/check_docs_coverage.py</code> Report coverage"},{"location":"meta/documentation-health/#resources","title":"Resources","text":""},{"location":"meta/documentation-health/#documentation-standards","title":"Documentation Standards","text":"<ul> <li>Documentation Guidelines - Project standards</li> <li>Di\u00e1taxis Framework - Documentation philosophy</li> <li>MkDocs Material - Theme documentation</li> <li>GitHub-Flavored Markdown - Markdown spec</li> </ul>"},{"location":"meta/documentation-health/#related-meta-docs","title":"Related Meta Docs","text":"<ul> <li>Documentation Process Audit - 2026-01-09 audit</li> <li>Documentation Guidelines - Standards and taxonomy</li> <li>Documentation Architecture Improvements - Implementation roadmap</li> </ul>"},{"location":"meta/documentation-health/#summary","title":"Summary","text":"<p>Overall Health: \u2705 Excellent</p> <ul> <li>60% navigation coverage (above 50% target)</li> <li>Well-balanced content types (Di\u00e1taxis-aligned)</li> <li>Complete core documentation (8/8 key docs)</li> <li>Active maintenance and improvement</li> <li>Good search and accessibility features</li> <li>Consistent multi-repo approach</li> </ul> <p>Recent Achievements: - Major restructure completed (2026-01-18) - 51 new docs added to navigation - 4 new tutorials created - Comprehensive reference section - Enhanced user experience</p> <p>Next Steps: - Monitor link health quarterly - Continue quarterly freshness reviews - Gather user feedback - Iterate on improvements</p> <p>Questions or suggestions? Open an issue or discussion on GitHub!</p>"},{"location":"operations/","title":"Operations Documentation","text":""},{"location":"operations/#start-here","title":"Start Here","text":"<p>New operator?</p> <ul> <li>First: Operator Responsibilities \u2014 Core duties</li> <li>Deploy: Deploy &amp; Verify \u2014 Deployment workflow</li> <li>Monitor: Monitoring Checklist \u2014 Monitoring setup</li> <li>Respond: Incident Response \u2014 When something breaks</li> </ul> <p>Quick reference:</p> Task Documentation Daily checks Ops Cadence Deploy changes Deploy &amp; Verify Investigate issues Incident Response Monitor health Monitoring Quarterly tasks Restore Test, Dataset Release"},{"location":"operations/#all-operational-documentation","title":"All Operational Documentation","text":"<ul> <li>Ops playbooks (task-oriented checklists): <code>playbooks/README.md</code></li> <li>Incident notes / postmortems (internal): <code>incidents/README.md</code></li> <li>Observability + private stats contract (public vs private boundaries): <code>observability-and-private-stats.md</code></li> <li>Annual capture campaign (scope + seeds): <code>annual-campaign.md</code></li> <li>Automation index (overview): <code>automation.md</code></li> <li>Automation implementation plan (phased, production-only): <code>automation-implementation-plan.md</code></li> <li>Monitoring + uptime + CI checklist: <code>monitoring-and-ci-checklist.md</code></li> <li>Annual Crawl Alerting Strategy: <code>monitoring-and-alerting.md</code></li> <li>Agent handoff guidelines (internal rules): <code>agent-handoff-guidelines.md</code></li> <li>Claims registry (proof artifacts): <code>claims-registry.md</code></li> <li>Data handling &amp; retention (internal contract): <code>data-handling-retention.md</code></li> <li>Export integrity contract (manifest + immutability): <code>export-integrity-contract.md</code></li> <li>Automation verification rituals (timer checks): <code>automation-verification-rituals.md</code></li> <li>Dataset release runbook (verification checklist): <code>dataset-release-runbook.md</code></li> <li>Risk register (top risks + mitigations): <code>risk-register.md</code></li> <li>Ops cadence checklist (internal routine): <code>ops-cadence-checklist.md</code></li> <li>Ops UI friction log template (internal; ongoing): <code>../_templates/ops-ui-friction-log-template.md</code></li> <li>Growth constraints (storage + scope budgets): <code>growth-constraints.md</code></li> <li>Legacy crawl imports (historical import notes): <code>legacy-crawl-imports.md</code></li> <li>Restore test procedure (quarterly): <code>restore-test-procedure.md</code></li> <li>Restore test log template: <code>../_templates/restore-test-log-template.md</code></li> <li>Adoption signals log template (public-safe, quarterly): <code>../_templates/adoption-signals-log-template.md</code></li> <li>HealthArchive ops roadmap + todo (remaining tasks): <code>healtharchive-ops-roadmap.md</code></li> <li>Partner kit (brief + citation + screenshots): <code>partner-kit.md</code></li> <li>One-page brief (pointer to frontend public asset): <code>one-page-brief.md</code></li> <li>Citation handout (pointer to frontend public asset): <code>citation-handout.md</code></li> <li>Outreach templates (email copy): <code>outreach-templates.md</code></li> <li>Verification packet (verifier handoff): <code>verification-packet.md</code></li> <li>Mentions log (public-safe, link-only): <code>mentions-log.md</code></li> <li>Mentions log template (public-safe): <code>../_templates/mentions-log-template.md</code></li> <li>Exports data dictionary (pointer to public asset): <code>exports-data-dictionary.md</code></li> <li>Methods note outline (poster/preprint scaffold): <code>methods-note-outline.md</code></li> <li>Search relevance evaluation (process + commands): <code>search-quality.md</code></li> <li>Golden queries + expected results (living checklist): <code>search-golden-queries.md</code></li> <li>Replay + preview automation plan (design + guardrails; includes <code>replay-reconcile</code>): <code>replay-and-preview-automation-plan.md</code></li> <li>Production baseline drift checks (policy + snapshot + compare): <code>baseline-drift.md</code></li> </ul>"},{"location":"operations/#mission-reports-logs","title":"Mission Reports &amp; Logs","text":"<ul> <li>2026-01-19: Annual Crawl Hardening Shipment</li> <li>2026-01-19: Investigation: Indexing Delay / Zero Indexed Pages</li> </ul>"},{"location":"operations/agent-handoff-guidelines/","title":"Agent Handoff Guidelines (internal)","text":"<p>This repo contains internal operations documentation. Keep everything public-safe:</p> <ul> <li>Do not include secrets, tokens, private emails, internal IPs, or private names.</li> <li>Prefer stable identifiers over prose (timer names, release tags, file paths, commit SHAs).</li> <li>When recording \u201cfirsts\u201d, treat them as historical facts and include stable IDs (e.g., release tag, log filename).</li> </ul> <p>Related docs:</p> <ul> <li><code>docs/operations/claims-registry.md</code></li> <li><code>docs/operations/data-handling-retention.md</code></li> <li><code>docs/operations/export-integrity-contract.md</code></li> <li><code>docs/operations/automation-verification-rituals.md</code></li> <li><code>docs/operations/dataset-release-runbook.md</code></li> <li><code>docs/operations/risk-register.md</code></li> <li><code>docs/operations/healtharchive-ops-roadmap.md</code> (todo list)</li> </ul>"},{"location":"operations/annual-campaign/","title":"Annual Capture Campaign (Jan 01 UTC) \u2014 Scope, Sources, Seeds","text":"<p>Status: approved (v1 scope) \u2014 single VPS, production-only.</p> <p>This document defines the canonical scope of HealthArchive\u2019s annual crawl campaign:</p> <ul> <li>Runs once per year on Jan 01 (UTC).</li> <li>Uses no page/depth limits (completeness and accuracy are the priority).</li> <li>Targets a small, stable set of sources to keep operations reliable on a     single VPS.</li> <li>Optimizes for getting the annual capture indexed and searchable as soon as     each crawl completes (replay + preview automation is explicitly secondary).</li> </ul> <p>This doc is intentionally focused on \u201cwhat we crawl\u201d and \u201cwhere we start\u201d. Implementation details (scheduler, timers, reconciler, monitoring) live in:</p> <ul> <li><code>automation-implementation-plan.md</code></li> </ul>"},{"location":"operations/annual-campaign/#1-goals-and-non-goals","title":"1) Goals and non-goals","text":""},{"location":"operations/annual-campaign/#goals","title":"Goals","text":"<ul> <li>Annual snapshot semantics: each source gets one \u201cannual edition\u201d per year,     labeled as Jan 01 (UTC) for that year.</li> <li>Completeness and accuracy: do not artificially cap depth/pages. Prefer     broad coverage of each source, even if crawls take days.</li> <li>Search-first readiness: once a crawl finishes, indexing should run next so     results become searchable as quickly as possible on production hardware.</li> <li>WARC-first pipeline: WARCs are the canonical input to search indexing; <code>.zim</code> outputs are optional artifacts and are not required for annual \u201cdone\u201d.</li> <li>Stable scope: only include sources we can realistically crawl and operate     with minimal ongoing tweaking.</li> </ul>"},{"location":"operations/annual-campaign/#non-goals-for-v1","title":"Non-goals (for v1)","text":"<ul> <li>Adding many new sources quickly (scope explosion).</li> <li>Achieving a literally simultaneous capture moment across all sources (single     VPS + limited concurrency makes this unrealistic).</li> <li>Automating cleanup/retention that could delete WARCs (explicitly deferred).</li> <li>Building a separate staging environment.</li> <li>Full-text indexing of non-HTML content (e.g. PDFs) \u2014 captured for completeness, but not searchable in v1.</li> </ul>"},{"location":"operations/annual-campaign/#2-canonical-sources-v1","title":"2) Canonical sources (v1)","text":"<p>For the initial annual campaign, we intentionally crawl only three sources:</p> <ol> <li>Health Canada (<code>hc</code>)</li> <li>Public Health Agency of Canada (<code>phac</code>)</li> <li>Canadian Institutes of Health Research (<code>cihr</code>)</li> </ol> <p>Rationale:</p> <ul> <li>These are core, high-value federal public health sources.</li> <li>They keep the campaign small enough to remain operable on a single VPS.</li> <li>They map cleanly to existing backend concepts (<code>Source</code>, <code>ArchiveJob</code>).</li> </ul>"},{"location":"operations/annual-campaign/#3-canonical-seeds-entry-urls","title":"3) Canonical seeds (entry URLs)","text":"<p>Seeds are the \u201centry points\u201d from which the crawler discovers pages.</p> <p>Important notes:</p> <ul> <li>Seeds must be stable and canonical (avoid ephemeral campaign pages).</li> <li>For bilingual sites, include both English and French entry points so     coverage does not depend on cross-link discovery.</li> <li>Seeds should be chosen to represent \u201cthe main hub\u201d of the source.</li> </ul>"},{"location":"operations/annual-campaign/#31-source-table","title":"3.1 Source table","text":"Code Source Primary host(s) English seed French seed <code>hc</code> Health Canada <code>www.canada.ca</code> <code>https://www.canada.ca/en/health-canada.html</code> <code>https://www.canada.ca/fr/sante-canada.html</code> <code>phac</code> Public Health Agency of Canada <code>www.canada.ca</code> <code>https://www.canada.ca/en/public-health.html</code> <code>https://www.canada.ca/fr/sante-publique.html</code> <code>cihr</code> CIHR <code>cihr-irsc.gc.ca</code> <code>https://cihr-irsc.gc.ca/e/193.html</code> <code>https://cihr-irsc.gc.ca/f/193.html</code>"},{"location":"operations/annual-campaign/#32-scope-boundary-notes-important","title":"3.2 Scope boundary notes (important)","text":"<p>These are policy decisions to prevent crawls from ballooning unexpectedly while still preserving completeness within each source:</p> <ul> <li>Primary scope boundary (required): each source has an explicit, mechanical     \u201cin-scope URL rule\u201d.</li> <li>Cross-domain assets: pages will reference external assets (fonts, JS,     images). Capturing all third-party assets is not required for search indexing.     If replay fidelity requires specific additional domains later, add them     explicitly and sparingly (do not allow arbitrary internet expansion).</li> <li>Canada.ca shared host: <code>hc</code> and <code>phac</code> both live on <code>www.canada.ca</code>.     We must scope by host + path allowlist (not \u201call of <code>www.canada.ca</code>\u201d).</li> </ul>"},{"location":"operations/annual-campaign/#33-in-scope-url-rules-mechanical-v1","title":"3.3 In-scope URL rules (mechanical, v1)","text":"<p>These rules define \u201cwhat counts as Health Canada / PHAC\u201d on a shared host.</p>"},{"location":"operations/annual-campaign/#health-canada-hc-wwwcanadaca","title":"Health Canada (<code>hc</code>) \u2014 <code>www.canada.ca</code>","text":"<p>In scope:</p> <ul> <li>Exactly:<ul> <li><code>https://www.canada.ca/en/health-canada.html</code></li> <li><code>https://www.canada.ca/fr/sante-canada.html</code></li> </ul> </li> <li>Any URL under these path prefixes:<ul> <li><code>https://www.canada.ca/en/health-canada/</code></li> <li><code>https://www.canada.ca/fr/sante-canada/</code></li> </ul> </li> <li>Any URL under these asset path prefixes (captured only when referenced by in-scope pages):<ul> <li><code>https://www.canada.ca/etc/designs/canada/wet-boew/</code></li> <li><code>https://www.canada.ca/content/dam/canada/sitemenu/</code></li> <li><code>https://www.canada.ca/content/dam/themes/health/</code></li> <li><code>https://www.canada.ca/content/dam/hc-sc/</code></li> </ul> </li> </ul> <p>Out of scope (examples):</p> <ul> <li><code>https://www.canada.ca/en/services/</code></li> <li><code>https://www.canada.ca/en/government/</code></li> <li>Any other <code>https://www.canada.ca/&lt;lang&gt;/...</code> that is not the hub page or under     the allowed prefixes above.</li> </ul>"},{"location":"operations/annual-campaign/#phac-phac-wwwcanadaca","title":"PHAC (<code>phac</code>) \u2014 <code>www.canada.ca</code>","text":"<p>In scope:</p> <ul> <li>Exactly:<ul> <li><code>https://www.canada.ca/en/public-health.html</code></li> <li><code>https://www.canada.ca/fr/sante-publique.html</code></li> </ul> </li> <li>Any URL under these path prefixes:<ul> <li><code>https://www.canada.ca/en/public-health/</code></li> <li><code>https://www.canada.ca/fr/sante-publique/</code></li> </ul> </li> <li>Any URL under these asset path prefixes (captured only when referenced by in-scope pages):<ul> <li><code>https://www.canada.ca/etc/designs/canada/wet-boew/</code></li> <li><code>https://www.canada.ca/content/dam/canada/sitemenu/</code></li> <li><code>https://www.canada.ca/content/dam/themes/health/</code></li> <li><code>https://www.canada.ca/content/dam/phac-aspc/</code></li> </ul> </li> </ul> <p>Out of scope (examples):</p> <ul> <li><code>https://www.canada.ca/en/services/</code></li> <li><code>https://www.canada.ca/en/government/</code></li> <li>Any other <code>https://www.canada.ca/&lt;lang&gt;/...</code> that is not the hub page or under     the allowed prefixes above.</li> </ul>"},{"location":"operations/annual-campaign/#cihr-cihr-cihr-irscgcca","title":"CIHR (<code>cihr</code>) \u2014 <code>cihr-irsc.gc.ca</code>","text":"<p>In scope:</p> <ul> <li>Any URL on host <code>cihr-irsc.gc.ca</code> (all paths), starting from the EN+FR seeds     above.</li> </ul> <p>Out of scope:</p> <ul> <li>Any other host.</li> </ul> <p>This is a \u201ccompleteness\u201d project, not an \u201cinfinite crawl\u201d project: completeness means \u201ccomplete within the intended source boundaries.\u201d</p>"},{"location":"operations/annual-campaign/#4-campaign-ordering-single-vps-reality","title":"4) Campaign ordering (single VPS reality)","text":"<p>With a single production VPS and limited parallelism, crawls and indexing will not complete simultaneously across sources.</p> <p>We still want the annual snapshot to feel like \u201cone moment in time\u201d, so we should order annual jobs to minimize the spread between the first and last job to finish indexing.</p> <p>Opinionated default ordering for v1:</p> <ol> <li><code>hc</code> (expected to be largest / slowest)</li> <li><code>phac</code></li> <li><code>cihr</code> (expected to be smallest / fastest)</li> </ol> <p>Rationale:</p> <ul> <li>Running the slowest job first reduces \u201cfinish-time spread\u201d across the set.</li> <li>A fixed ordering makes operations reproducible year over year.</li> </ul> <p>After the first annual campaign completes, revisit ordering based on real job durations and storage growth.</p> <p>Implementation note:</p> <ul> <li><code>ha-backend schedule-annual</code> staggers <code>queued_at</code> by a few seconds across   sources to make the single-worker pick order deterministic (hc \u2192 phac \u2192 cihr),   even when all jobs are enqueued in the same command invocation.</li> </ul>"},{"location":"operations/annual-campaign/#5-what-done-means-per-source","title":"5) What \u201cdone\u201d means (per source)","text":"<p>For each source\u2019s annual job:</p> <ol> <li>Job reaches <code>status=indexed</code> (searchable).</li> <li><code>/api/search</code> returns results for that source and year as expected.</li> </ol> <p>Replay and previews are \u201ceventually consistent\u201d follow-ups and are not part of the \u201csearch is ready\u201d definition for the annual campaign.</p>"},{"location":"operations/annual-campaign/#6-post-campaign-verification","title":"6) Post-campaign verification","text":"<p>Once all annual jobs are <code>indexed</code>, capture \u201csearch readiness\u201d evidence as a timestamped artifact directory:</p> <pre><code>YEAR=2026\nset -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend annual-status --year \"$YEAR\"\n./scripts/annual-search-verify.sh --year \"$YEAR\" --out-root /srv/healtharchive/ops/search-eval --base-url http://127.0.0.1:8001\n</code></pre> <p>This produces:</p> <ul> <li><code>annual-status.json</code> / <code>annual-status.txt</code> (campaign readiness evidence)</li> <li>captured <code>/api/search</code> JSON for the golden query set (for later diffing)</li> </ul>"},{"location":"operations/automation-implementation-plan/","title":"Automation Implementation Plan (Production-Only, Single VPS)","text":"<p>Status: active plan (implementation proceeds in phases).</p> <p>This document is the excruciatingly detailed, sequential implementation plan for HealthArchive automation, tailored to the current operating reality:</p> <ul> <li>One production VPS (no staging backend).</li> <li>Annual capture campaign runs on Jan 01 (UTC).</li> <li>Current annual sources: Health Canada (<code>hc</code>), PHAC (<code>phac</code>), CIHR (<code>cihr</code>).</li> <li>No page/depth caps (completeness/accuracy first).</li> <li>Top priority is making the annual snapshot searchable ASAP once crawls   complete; replay/previews are secondary and eventually consistent.</li> </ul> <p>This plan intentionally minimizes operational complexity:</p> <ul> <li>Every new automation starts as manual + dry-run, then graduates to a   systemd timer only once it is boring and predictable.</li> <li>Every automated action must be:</li> <li>idempotent,</li> <li>allowlistable,</li> <li>rate-limited,</li> <li>observable,</li> <li>and instantly disable-able.</li> </ul> <p>Canonical \u201cwhat we crawl\u201d:</p> <ul> <li><code>annual-campaign.md</code></li> </ul> <p>Related context:</p> <ul> <li>Monitoring/CI guidance: <code>monitoring-and-ci-checklist.md</code></li> <li>Replay/preview automation design: <code>replay-and-preview-automation-plan.md</code></li> <li>Production runbook: <code>../deployment/production-single-vps.md</code></li> </ul>"},{"location":"operations/automation-implementation-plan/#global-invariants-do-not-violate","title":"Global invariants (do not violate)","text":""},{"location":"operations/automation-implementation-plan/#safety","title":"Safety","text":"<ul> <li>Never run heavy automation on request paths. No crawl, indexing, replay   indexing, or screenshotting should be triggered by a public HTTP request.</li> <li>Never automate destructive cleanup of WARCs until retention is designed   and tested (see <code>replay-and-preview-automation-plan.md</code>).</li> <li>No secrets in repo or logs. Timers/services must read secrets from   root-owned env files on the VPS, and logs must not print their contents.</li> </ul>"},{"location":"operations/automation-implementation-plan/#idempotency-and-boundedness","title":"Idempotency and boundedness","text":"<ul> <li>Every scheduled unit must be safe to run multiple times (systemd timers with   <code>Persistent=true</code> can replay missed runs).</li> <li>Every automated loop must have:</li> <li>hard caps (jobs per run, previews per run),</li> <li>a lock (global and/or per-item),</li> <li>and clear refusal rules (disk low, dependency down).</li> </ul>"},{"location":"operations/automation-implementation-plan/#single-vps-discipline","title":"\u201cSingle VPS\u201d discipline","text":"<ul> <li>Treat the worker as a scarce resource. Avoid adding competing heavy   automation during the annual campaign.</li> <li>Prefer \u201cqueue work then let the worker run\u201d over spawning extra parallel   processes.</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-1-of-9-define-annual-scope-seeds-docs-only","title":"Step 1 (of 9) \u2014 Define annual scope + seeds (docs only)","text":"<p>Objective</p> <p>Lock the annual campaign\u2019s scope so automation is deterministic and auditable.</p> <p>Deliverables</p> <ul> <li><code>annual-campaign.md</code>:</li> <li>sources list (<code>hc</code>, <code>phac</code>, <code>cihr</code>),</li> <li>canonical seeds (EN+FR where applicable),</li> <li>scope boundary notes,</li> <li>recommended crawl ordering.</li> </ul> <p>No code and no infrastructure changes in this step.</p> <p>Acceptance criteria</p> <ul> <li>Operators can answer \u201cwhat will Jan 01 crawl?\u201d by pointing at a single file.</li> </ul> <p>Rollback</p> <ul> <li>N/A (docs only).</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-2-of-9-align-backend-registry-seeding-with-v1-sources-code","title":"Step 2 (of 9) \u2014 Align backend registry + seeding with v1 sources (code)","text":"<p>Objective</p> <p>Ensure the backend\u2019s canonical configuration matches the annual campaign:</p> <ul> <li><code>seed-sources</code> creates <code>hc</code>, <code>phac</code>, <code>cihr</code>.</li> <li>Job registry can create annual jobs for these sources consistently.</li> </ul> <p>Key decisions</p> <ul> <li>Registry remains the single source of truth for per-source defaults.</li> <li>We do not add page/depth limits to achieve \u201cfast campaigns\u201d; completeness   remains the priority.</li> <li><code>hc</code> and <code>phac</code> are sections of <code>www.canada.ca</code>, so their job configs must   enforce a path allowlist scope (as defined in <code>annual-campaign.md</code>) to   avoid crawling all of Canada.ca.</li> </ul> <p>Implementation steps</p> <ol> <li>Add/confirm <code>cihr</code> in source seeding (<code>ha-backend seed-sources</code>).</li> <li>Add a <code>SourceJobConfig</code> entry for <code>cihr</code> in <code>job_registry.py</code>:</li> <li>seeds from <code>annual-campaign.md</code></li> <li>conservative safety defaults (monitoring off by default unless you choose      otherwise)</li> <li>Update <code>hc</code> and <code>phac</code> seeds to match the canonical list (likely add FR    entry points if not already).</li> <li>Encode the <code>annual-campaign.md</code> in-scope URL rules into job configs:</li> <li>for <code>hc</code> and <code>phac</code>: host + path allowlist scope on <code>www.canada.ca</code></li> <li>for <code>cihr</code>: host scope on <code>cihr-irsc.gc.ca</code></li> <li>Ensure naming templates can represent annual campaigns (see Step 3).</li> </ol> <p>Tests</p> <ul> <li>Unit tests:</li> <li>seeding includes <code>cihr</code>,</li> <li>job creation for each source works,</li> <li>config JSON contains expected seeds and scope constraints.</li> </ul> <p>Acceptance criteria</p> <ul> <li>Running <code>seed-sources</code> on a fresh DB yields all three sources.</li> <li><code>create-job --source cihr</code> works locally and yields a job row with expected   defaults.</li> </ul> <p>Rollback</p> <ul> <li>Revert code changes; no DB migrations required if only seeding/registry   changes are made.</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-3-of-9-implement-annual-scheduler-cli-production-only-logic-dry-run-first","title":"Step 3 (of 9) \u2014 Implement annual scheduler CLI (production-only logic, dry-run first)","text":"<p>Objective</p> <p>Provide a single, safe command that enqueues the annual campaign jobs for a given year (Jan 01 UTC), exactly once.</p> <p>Proposed CLI</p> <ul> <li><code>ha-backend schedule-annual</code></li> </ul> <p>Flags (opinionated):</p> <ul> <li><code>--apply</code> (otherwise dry-run only)</li> <li><code>--year YYYY</code></li> <li>If omitted: allowed only when running on Jan 01 (UTC), in which     case it schedules the current UTC year.</li> <li><code>--sources hc phac cihr</code> (explicit allowlist; subset selection)</li> <li><code>--max-create-per-run N</code> (defaults to number of selected sources)</li> </ul> <p>Idempotency rules</p> <p>For each source in the allowlist:</p> <ul> <li>If a job exists for the same <code>campaign_year</code> (recorded in <code>ArchiveJob.config</code>)   \u2192 skip.</li> <li>If a job exists with the same would-be annual job name (e.g. <code>hc-20270101</code>)   \u2192 skip (prevents duplicates even if the job predates <code>campaign_year</code> metadata).</li> <li>If an \u201cactive\u201d job exists for that source (queued/running/completed/indexing   /index_failed/retryable) \u2192 skip and report why.</li> </ul> <p>Job labeling</p> <ul> <li>Job name must include the campaign date <code>YYYY0101</code> even if the scheduler runs   late (e.g. after reboot).</li> <li>Record metadata in <code>ArchiveJob.config</code> (no schema change):</li> <li><code>campaign_kind=\"annual\"</code></li> <li><code>campaign_year=YYYY</code></li> <li><code>campaign_date=\"YYYY-01-01\"</code></li> <li><code>campaign_date_utc=\"YYYY-01-01T00:00:00Z\"</code></li> <li><code>scheduler_version=\"v1\"</code></li> </ul> <p>Ordering</p> <ul> <li>Create jobs in the order defined in <code>annual-campaign.md</code> to make queue   processing predictable with a single worker.</li> </ul> <p>Tests</p> <ul> <li>Idempotency: second apply creates 0 jobs.</li> <li>Active-job skip: if a job is in progress, scheduler does not add another.</li> <li>Ordering: created jobs are in the expected order.</li> <li>Year labeling: job name/config reflect the specified year, not \u201cnow\u201d.</li> </ul> <p>Acceptance criteria</p> <ul> <li>Dry-run output is readable and complete (operator can review before applying).</li> <li>Apply mode creates exactly one job per selected source, unless prevented by   the idempotency/active-job guards or <code>--max-create-per-run</code>.</li> </ul> <p>Rollback</p> <ul> <li>If jobs were created incorrectly, use admin tooling to mark them failed or   delete rows only if you have a safe procedure (prefer \u201cleave rows, don\u2019t run   them\u201d over ad-hoc deletion).</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-4-of-9-add-annual-statusreporting-cli-operability","title":"Step 4 (of 9) \u2014 Add annual status/reporting CLI (operability)","text":"<p>Objective</p> <p>Make it trivial to answer:</p> <ul> <li>\u201cIs the annual snapshot searchable yet?\u201d</li> <li>\u201cWhich source is stuck, and where?\u201d</li> </ul> <p>Proposed CLI</p> <ul> <li><code>ha-backend annual-status --year YYYY [--json] [--sources ...]</code></li> </ul> <p>Reports per source:</p> <ul> <li>job id/name</li> <li>job status + timestamps</li> <li>retry_count</li> <li>indexed_page_count</li> <li>crawl/index exit codes if applicable</li> </ul> <p>Campaign-level summary:</p> <ul> <li>total sources, indexed count, failed count, in-progress count</li> <li>\u201cready for search\u201d boolean (all indexed)</li> </ul> <p>Implementation notes (v1)</p> <ul> <li>Uses <code>ArchiveJob.config</code> metadata written by <code>schedule-annual</code>:</li> <li><code>campaign_kind=\"annual\"</code></li> <li><code>campaign_year=YYYY</code></li> <li>Fallback: if metadata is missing, it will also consider the canonical annual   name format (e.g. <code>hc-20270101</code>).</li> <li>If no annual job is found for a source, the command will also surface the   most recent \u201cactive\u201d job for that source (queued/running/completed/indexing/   index_failed/retryable) to help explain why scheduling may have been skipped.</li> <li>If multiple annual candidates are found for a source/year, the command prints   an error for that source (operators must resolve duplicates).</li> </ul> <p>Acceptance criteria</p> <ul> <li>An operator can copy/paste the output into an incident note and it\u2019s   self-explanatory.</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-5-of-9-production-systemd-timer-for-jan-01-scheduling-infrastructure","title":"Step 5 (of 9) \u2014 Production systemd timer for Jan 01 scheduling (infrastructure)","text":"<p>Objective</p> <p>Run the annual scheduler automatically on Jan 01 UTC, reliably.</p> <p>systemd units (draft)</p> <p>Templates live in: <code>../deployment/systemd/</code></p> <ul> <li><code>healtharchive-schedule-annual.service</code> (apply)</li> <li>Runs: <code>ha-backend schedule-annual --apply --year &lt;UTC_YEAR&gt; --sources hc phac cihr</code><ul> <li><code>&lt;UTC_YEAR&gt;</code> is computed at runtime (<code>date -u +%Y</code>) so that   <code>Persistent=true</code> can safely run a missed activation after a reboot.</li> </ul> </li> <li>Uses <code>EnvironmentFile=/etc/healtharchive/backend.env</code></li> <li>Gated by <code>ConditionPathExists=/etc/healtharchive/automation-enabled</code></li> <li>Uses <code>RefuseManualStart=yes</code> to reduce accidental production scheduling.</li> <li><code>healtharchive-schedule-annual-dry-run.service</code> (safe validation)</li> <li>Runs the same scheduler without <code>--apply</code> (no DB writes).</li> <li><code>healtharchive-schedule-annual.timer</code></li> <li><code>OnCalendar=*-01-01 00:05:00 UTC</code></li> <li><code>Persistent=true</code></li> </ul> <p>Why <code>Persistent=true</code></p> <ul> <li>If the VPS reboots or the timer is disabled temporarily, systemd will run the   missed activation on the next boot/start, but the scheduler still labels jobs   as Jan 01 for the target year.</li> </ul> <p>Acceptance criteria</p> <ul> <li>Timer wiring is validated by running the dry-run service manually:   <code>systemctl start healtharchive-schedule-annual-dry-run.service</code></li> <li>Timer is enabled only after manual review.</li> </ul> <p>Rollback</p> <ul> <li>Disable timer: <code>systemctl disable --now healtharchive-schedule-annual.timer</code></li> <li>Remove <code>/etc/healtharchive/automation-enabled</code> to stop all automation quickly.</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-6-of-9-resource-policy-during-campaign-keep-site-up-without-safe-window","title":"Step 6 (of 9) \u2014 Resource policy during campaign (keep site up without \u201csafe window\u201d)","text":"<p>Objective</p> <p>Annual crawls may run for days. We want the public API and frontend to remain available even if performance is degraded.</p> <p>Approach</p> <ul> <li>Prefer systemd-level prioritization over complex in-app throttling.</li> </ul> <p>Actions:</p> <ul> <li>Ensure worker service runs with lower priority than API:</li> <li><code>Nice=5</code> or <code>Nice=10</code></li> <li>optionally <code>IOSchedulingClass=best-effort</code>, <code>IOSchedulingPriority=6</code></li> <li>Keep only one worker process unless you explicitly decide to accept more   contention for a tighter \u201csame moment\u201d capture.</li> </ul> <p>Implementation (v1)</p> <ul> <li>Use a systemd drop-in for the worker:</li> <li>template: <code>../deployment/systemd/healtharchive-worker.service.override.conf</code></li> <li>install path: <code>/etc/systemd/system/healtharchive-worker.service.d/override.conf</code></li> </ul> <p>Acceptance criteria</p> <ul> <li>API stays responsive (no sustained 5xx/timeouts attributable to worker load).</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-7-of-9-post-campaign-search-readiness-verification-light-automation","title":"Step 7 (of 9) \u2014 Post-campaign \u201csearch readiness\u201d verification (light automation)","text":"<p>Objective</p> <p>After all annual jobs are indexed, capture evidence that search is working and stable.</p> <p>Implementation (v1):</p> <ul> <li><code>scripts/search-eval-capture.sh</code> now supports <code>--run-id ID</code> so you can place   captures under a stable, year-tagged path (instead of a nested timestamp you   need to \u201cdiscover\u201d after the fact).</li> <li><code>scripts/annual-search-verify.sh</code> wraps the flow safely:</li> <li>runs <code>ha-backend annual-status --year YYYY --json</code>,</li> <li>refuses to capture unless <code>summary.readyForSearch=true</code> (unless you pass     <code>--allow-not-ready</code>),</li> <li>writes <code>annual-status.json</code>/<code>annual-status.txt</code> into the same capture dir as     the golden query responses,</li> <li>passes optional args through to <code>search-eval-capture.sh</code>.</li> </ul> <p>Recommended artifact layout on the VPS:</p> <ul> <li><code>/srv/healtharchive/ops/search-eval/&lt;year&gt;/&lt;run_id&gt;/</code></li> <li><code>annual-status.json</code></li> <li><code>annual-status.txt</code></li> <li><code>annual-search-verify.meta.txt</code></li> <li><code>meta.txt</code> (from <code>search-eval-capture.sh</code>)</li> <li><code>&lt;query&gt;.(pages|snapshots).json</code></li> </ul> <p>Operator command (production example):</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\ncd /opt/healtharchive-backend\n./scripts/annual-search-verify.sh --year 2026 --out-root /srv/healtharchive/ops/search-eval --base-url http://127.0.0.1:8001\n</code></pre> <p>Optional (Postgres, manual): consider running a one-time <code>VACUUM (ANALYZE)</code> after large ingestion completes. Do this manually, off-peak, and only if you\u2019re confident it won\u2019t starve IO for user traffic.</p> <p>Acceptance criteria</p> <ul> <li>You have a year-tagged artifact directory showing <code>annual-status</code> output plus   captured <code>/api/search</code> JSON for the golden query set.</li> </ul>"},{"location":"operations/automation-implementation-plan/#step-8-of-9-replaypreview-reconciliation-after-search-is-stable","title":"Step 8 (of 9) \u2014 Replay/preview reconciliation (after search is stable)","text":"<p>Objective</p> <p>Make replay and previews converge to correct state without risking core search availability.</p> <p>Implementation (v1) (aligns with <code>replay-and-preview-automation-plan.md</code>):</p> <ul> <li>New ops command: <code>ha-backend replay-reconcile</code></li> <li>default mode is dry-run (safe): prints what it would do.</li> <li><code>--apply</code> performs the actions.</li> <li>global lock file prevents concurrent runs (default:     <code>/srv/healtharchive/replay/.locks/replay-reconcile.lock</code>).</li> <li>caps:<ul> <li><code>--max-jobs N</code> (default 1) limits replay indexing repairs per run.</li> <li>optional <code>--previews --max-previews N</code> (default 1) generates missing   preview images for <code>/archive</code> source cards (still capped).</li> </ul> </li> <li> <p>allowlists:</p> <ul> <li><code>--sources hc phac ...</code></li> <li><code>--job-id 123 456 ...</code></li> <li>optional <code>--campaign-year YYYY</code> for annual-only reconciliation.</li> </ul> </li> <li> <p>Replay indexing metadata:</p> </li> <li><code>ha-backend replay-index-job --id &lt;id&gt;</code> now writes a marker file under the     collection root: <code>replay-index.meta.json</code> (WARC count + hash + timestamps).</li> <li><code>replay-reconcile --verify-warc-hash</code> can use the marker to detect drift     (slower; optional).</li> </ul> <p>Staged rollout:</p> <ol> <li>Dry-run:</li> <li><code>ha-backend replay-reconcile --collections-dir /srv/healtharchive/replay/collections</code></li> <li>Apply for one job (manual allowlist):</li> <li><code>ha-backend replay-reconcile --apply --job-id &lt;JOB_ID&gt; --max-jobs 1</code></li> <li>Timer with caps (templates under <code>docs/deployment/systemd/</code>; disabled by default).</li> <li>Optional previews (still capped; failures are surfaced clearly).</li> </ol>"},{"location":"operations/automation-implementation-plan/#step-9-of-9-deployment-automation-low-cost-low-risk-first","title":"Step 9 (of 9) \u2014 Deployment automation (low-cost, low-risk first)","text":"<p>Objective</p> <p>Reduce operator error in backend deployments without introducing brittle GitHub\u2192VPS automation.</p> <p>Implementation (v1)</p> <ul> <li>A single-VPS deploy helper script now exists:</li> <li><code>scripts/vps-deploy.sh</code></li> <li>It is dry-run by default; use <code>--apply</code> to actually deploy.</li> <li>It supports:</li> <li>fast-forward deploys (<code>git pull --ff-only</code>), or pinned SHAs via <code>--ref</code></li> <li>dependency install (editable) + optional skip flags</li> <li>Alembic migrations (sources <code>/etc/healtharchive/backend.env</code> but does not print it)</li> <li>systemd restarts for API + worker</li> <li>a final <code>/api/health</code> check</li> <li>a deploy lock file to avoid concurrent deploys</li> </ul> <p>Operator usage (production):</p> <pre><code>cd /opt/healtharchive-backend\n\n# Dry-run:\n./scripts/vps-deploy.sh\n\n# Deploy latest main:\n./scripts/vps-deploy.sh --apply\n\n# Deploy pinned SHA:\n./scripts/vps-deploy.sh --apply --ref &lt;GIT_SHA&gt;\n</code></pre> <p>Future (optional, higher-risk without staging):</p> <ul> <li>GitHub Actions deployments can be considered later, but require secrets,   rollback discipline, and careful failure handling. For now, the recommended   posture is \u201cboring manual deploy with a single trusted script\u201d.</li> </ul>"},{"location":"operations/automation-verification-rituals/","title":"Automation Verification Rituals (internal)","text":"<p>Use these checks before claiming automation is \u201con\u201d.</p>"},{"location":"operations/automation-verification-rituals/#systemd-timers","title":"systemd timers","text":"<ul> <li>One-command posture check (recommended): <code>./scripts/verify_ops_automation.sh</code></li> <li>Diff-friendly JSON summary (optional): <code>./scripts/verify_ops_automation.sh --json | python3 -m json.tool</code></li> <li>JSON-only artifact (optional): <code>./scripts/verify_ops_automation.sh --json-only &gt; /srv/healtharchive/ops/automation/posture.json</code></li> <li>Strict checks (optional):</li> <li>all timers present: <code>./scripts/verify_ops_automation.sh --require-all-present</code></li> <li>all timers enabled: <code>./scripts/verify_ops_automation.sh --require-all-enabled</code></li> <li><code>systemctl is-enabled &lt;timer&gt;</code> (should be <code>enabled</code>)</li> <li>sentinel file exists under <code>/etc/healtharchive/*enabled</code></li> <li><code>systemctl list-timers --all | grep healtharchive-</code> (shows next/last run)</li> <li><code>journalctl -u &lt;service&gt; -n 200</code> (shows last run success)</li> </ul>"},{"location":"operations/automation-verification-rituals/#posture-snapshots-optional","title":"Posture snapshots (optional)","text":"<ul> <li>Keep dated JSON under <code>/srv/healtharchive/ops/automation/</code> so you can diff over time.</li> <li>If the directory is missing, run: <code>sudo ./scripts/vps-bootstrap-ops-dirs.sh</code> (idempotent).</li> <li>Diff examples:</li> <li><code>diff -u &lt;(python3 -m json.tool &lt; old.json) &lt;(python3 -m json.tool &lt; new.json)</code></li> </ul>"},{"location":"operations/automation-verification-rituals/#dataset-releases","title":"Dataset releases","text":"<ul> <li>Confirm GitHub Actions are enabled in <code>jerdaw/healtharchive-datasets</code></li> <li>Confirm a release exists for the expected quarter/date</li> <li>Download assets and verify: <code>sha256sum -c SHA256SUMS</code></li> </ul>"},{"location":"operations/automation-verification-rituals/#restore-tests","title":"Restore tests","text":"<ul> <li>Confirm a dated log file exists in <code>/srv/healtharchive/ops/restore-tests/</code></li> <li>Ensure it includes: backup source, schema check, API checks, pass/fail, follow-ups</li> </ul>"},{"location":"operations/automation/","title":"Automation (Operations)","text":"<p>This page is an index to automation-related docs for operators.</p>"},{"location":"operations/automation/#core-docs","title":"Core docs","text":"<ul> <li>Automation implementation plan: automation-implementation-plan.md</li> <li>Automation verification rituals: automation-verification-rituals.md</li> </ul>"},{"location":"operations/automation/#playbooks","title":"Playbooks","text":"<ul> <li>Crawl cleanup automation: playbooks/crawl/cleanup-automation.md</li> <li>Storage hot-path stale mount drills: playbooks/storage/storagebox-sshfs-stale-mount-drills.md</li> <li>Storage hot-path stale mount recovery: playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</li> </ul>"},{"location":"operations/automation/#monitoring","title":"Monitoring","text":"<ul> <li>Monitoring &amp; alerting: monitoring-and-alerting.md</li> <li>Thresholds &amp; tuning: thresholds-and-tuning.md</li> </ul>"},{"location":"operations/baseline-drift/","title":"Production baseline drift checks (internal)","text":"<p>Goal: avoid \u201cconfiguration drift\u201d where production stops matching what the project expects (security posture, perms, service units, etc.).</p> <p>This is implemented as:</p> <p>1) Desired state (in git): <code>production-baseline-policy.toml</code> 2) Observed state (generated on the VPS): JSON snapshots written to <code>/srv/healtharchive/ops/baseline/</code> 3) Drift check: compares observed vs policy and fails on required mismatches</p>"},{"location":"operations/baseline-drift/#files","title":"Files","text":"<ul> <li>Policy (edit in git): <code>production-baseline-policy.toml</code></li> <li>Snapshot generator: <code>../../scripts/baseline_snapshot.py</code></li> <li>Drift checker: <code>../../scripts/check_baseline_drift.py</code></li> </ul>"},{"location":"operations/baseline-drift/#one-shot-usage-recommended-after-any-production-change","title":"One-shot usage (recommended after any production change)","text":"<p>On the VPS (as <code>haadmin</code>):</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/check_baseline_drift.py --mode live\n</code></pre> <p>This writes:</p> <ul> <li><code>observed-&lt;timestamp&gt;.json</code> (machine-readable)</li> <li><code>drift-report-&lt;timestamp&gt;.txt</code> (human-readable)</li> <li>plus <code>observed-latest.json</code> and <code>drift-report-latest.txt</code></li> </ul> <p>All files live under <code>/srv/healtharchive/ops/baseline/</code>.</p>"},{"location":"operations/baseline-drift/#local-only-mode-no-network-dependency","title":"\u201cLocal only\u201d mode (no network dependency)","text":"<p>Use local-only mode when you want checks that don\u2019t depend on DNS/TLS/external routing:</p> <pre><code>./scripts/check_baseline_drift.py --mode local\n</code></pre> <p>In <code>local</code> mode:</p> <ul> <li>HSTS is validated by parsing <code>/etc/caddy/Caddyfile</code> for the API site block.</li> <li>Admin endpoint checks are skipped (warn-only).</li> </ul>"},{"location":"operations/baseline-drift/#optional-weekly-drift-timer-systemd","title":"Optional: weekly drift timer (systemd)","text":"<p>If you want drift checks to run automatically (not just during deploys), this repo includes a weekly systemd timer:</p> <ul> <li>Templates: <code>docs/deployment/systemd/healtharchive-baseline-drift-check.*</code></li> <li>Installer helper (VPS): <code>scripts/vps-install-systemd-units.sh --apply</code></li> <li>Enablement steps: <code>docs/deployment/systemd/README.md</code></li> </ul>"},{"location":"operations/baseline-drift/#cors-validation","title":"CORS validation","text":"<p>The policy enforces a strict production allowlist (no extra origins) via <code>HEALTHARCHIVE_CORS_ORIGINS</code>.</p> <ul> <li><code>--mode local</code> validates the env file value (CSV set comparison).</li> <li><code>--mode live</code> additionally probes the API with an <code>Origin:</code> header and checks   real <code>Access-Control-Allow-Origin</code> behavior.</li> </ul>"},{"location":"operations/baseline-drift/#replay-usage-invariants","title":"Replay + usage invariants","text":"<p>The policy can also pin \u201cpublic UX\u201d toggles that affect what users see:</p> <ul> <li><code>HEALTHARCHIVE_REPLAY_BASE_URL</code> and <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code> (replay browse URLs + previews)</li> <li><code>HEALTHARCHIVE_USAGE_METRICS_ENABLED</code> and <code>HEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS</code> (public <code>/status</code> + <code>/impact</code>)</li> </ul>"},{"location":"operations/baseline-drift/#when-to-update-policy","title":"When to update policy","text":"<p>Update <code>production-baseline-policy.toml</code> only when you intentionally change production invariants:</p> <ul> <li>URL strategy (adding staging, changing canonical domains)</li> <li>security posture (HSTS policy, admin auth policy)</li> <li>directory layout / ownership model</li> <li>systemd service names or enablement expectations</li> </ul> <p>Avoid adding \u201cthings that change often\u201d to policy (package versions, job counts, etc.).</p>"},{"location":"operations/citation-handout/","title":"HealthArchive.ca - Citation handout (pointer)","text":"<p>Canonical public copy lives in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/public/partner-kit/healtharchive-citation.md</li> <li>Live page: https://www.healtharchive.ca/cite</li> </ul> <p>If you need to update the handout, edit the frontend asset and deploy the frontend.</p>"},{"location":"operations/claims-registry/","title":"Claims Registry (internal)","text":"<p>Use this to back any reliability/automation/privacy/reproducibility claims with proof artifacts.</p> <ul> <li>Claim: Quarterly dataset releases run automatically (metadata-only).</li> <li>Evidence:<ul> <li>GitHub Releases: <code>https://github.com/jerdaw/healtharchive-datasets/releases</code> (tags <code>healtharchive-dataset-YYYY-MM-DD</code>)</li> <li>Release assets: <code>manifest.json</code> + <code>SHA256SUMS</code> + <code>healtharchive-*.jsonl.gz</code></li> <li>Workflow: <code>jerdaw/healtharchive-datasets</code> \u2192 Actions \u2192 \u201cPublish dataset release\u201d</li> </ul> </li> <li>Cadence: quarterly (Jan/Apr/Jul/Oct)</li> <li>Recorded in: dataset repo Releases + <code>/srv/healtharchive/ops/adoption/</code></li> <li>Claim: Dataset releases are integrity-verifiable.</li> <li>Evidence:<ul> <li>Download assets into one directory and run <code>sha256sum -c SHA256SUMS</code></li> <li><code>manifest.json</code> includes artifact SHA256s and row counts</li> </ul> </li> <li>Cadence: per release</li> <li>Recorded in: release assets + <code>/srv/healtharchive/ops/adoption/</code></li> <li>Claim: Change tracking is computed on schedule.</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-change-tracking.timer</code> (plus sentinel <code>/etc/healtharchive/change-tracking-enabled</code>)</li> <li>logs: <code>journalctl -u healtharchive-change-tracking.service</code></li> <li>public surface: <code>/changes</code> + <code>/api/changes</code></li> </ul> </li> <li>Cadence: daily</li> <li>Recorded in: journald (+ optional Healthchecks ping)</li> <li>Claim: Replay indexes are reconciled on schedule (when replay enabled).</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-replay-reconcile.timer</code> (plus sentinel <code>/etc/healtharchive/replay-automation-enabled</code>)</li> <li>logs: <code>journalctl -u healtharchive-replay-reconcile.service</code></li> </ul> </li> <li>Cadence: daily</li> <li>Recorded in: journald (+ optional Healthchecks ping)</li> <li>Claim: Annual search verification artifacts are captured (when enabled).</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-annual-search-verify.timer</code> (plus sentinel <code>/etc/healtharchive/automation-enabled</code>)</li> <li>artifacts: <code>/srv/healtharchive/ops/search-eval/&lt;year&gt;/final/</code></li> </ul> </li> <li>Cadence: daily timer, captures once per year when ready</li> <li>Recorded in: ops artifacts (+ optional Healthchecks ping)</li> <li>Claim: Coverage guardrails run for annual editions (when enabled).</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-coverage-guardrails.timer</code> (plus sentinel <code>/etc/healtharchive/coverage-guardrails-enabled</code>)</li> <li>metrics: <code>healtharchive_coverage_*</code> in node_exporter textfile collector</li> </ul> </li> <li>Cadence: daily</li> <li>Recorded in: metrics (+ optional Healthchecks ping)</li> <li>Claim: Replay smoke tests run for latest indexed jobs (when enabled).</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-replay-smoke.timer</code> (plus sentinel <code>/etc/healtharchive/replay-smoke-enabled</code>)</li> <li>metrics: <code>healtharchive_replay_smoke_*</code> in node_exporter textfile collector</li> </ul> </li> <li>Cadence: daily</li> <li>Recorded in: metrics (+ optional Healthchecks ping)</li> <li>Claim: Cleanup automation runs safely on indexed jobs (when enabled).</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-cleanup-automation.timer</code> (plus sentinel <code>/etc/healtharchive/cleanup-automation-enabled</code>)</li> <li>logs: <code>journalctl -u healtharchive-cleanup-automation.service</code></li> </ul> </li> <li>Cadence: weekly</li> <li>Recorded in: journald (+ optional Healthchecks ping)</li> <li>Claim: Annual campaign scheduling is automated and gated.</li> <li>Evidence:<ul> <li>systemd timer: <code>healtharchive-schedule-annual.timer</code> (plus sentinel <code>/etc/healtharchive/automation-enabled</code>)</li> <li>logs: <code>journalctl -u healtharchive-schedule-annual.service</code></li> </ul> </li> <li>Cadence: annually (Jan 01 UTC)</li> <li>Recorded in: journald</li> <li>Claim: Quarterly restore tests are performed (backups are usable).</li> <li>Evidence:<ul> <li>restore-test logs: <code>/srv/healtharchive/ops/restore-tests/restore-test-YYYY-MM-DD.md</code></li> <li>procedure reference: <code>docs/operations/restore-test-procedure.md</code></li> </ul> </li> <li>Cadence: quarterly</li> <li>Recorded in: <code>/srv/healtharchive/ops/restore-tests/</code></li> <li>Claim: Public usage metrics are privacy-preserving and aggregated.</li> <li>Evidence:<ul> <li>DB table: <code>usage_metrics</code> (aggregated daily counts only)</li> <li>API: <code>GET /api/usage</code> (windowed aggregates; no personal identifiers)</li> </ul> </li> <li>Cadence: daily aggregation; public reporting window is configurable</li> <li>Recorded in: DB + <code>/api/usage</code></li> </ul>"},{"location":"operations/data-handling-retention/","title":"Data Handling &amp; Retention (internal)","text":"<p>Prevent accidental collection/retention creep and PHI risk. Keep notes public-safe.</p>"},{"location":"operations/data-handling-retention/#issue-reports-post-apireports","title":"Issue reports (<code>POST /api/reports</code>)","text":"<p>Stored in DB table <code>issue_reports</code> with fields:</p> <ul> <li><code>category</code>, <code>description</code> (free text)</li> <li>optional <code>reporter_email</code>, <code>snapshot_id</code>, <code>original_url</code>, <code>page_url</code></li> <li><code>status</code>, <code>internal_notes</code></li> </ul> <p>Policy:</p> <ul> <li>Public UI must warn users not to submit personal health information.</li> <li>Admin views are operator-only; never expose reports in public UI.</li> <li>If a report includes PHI, do not copy it into other systems/logs; redact/delete and record a public-safe note.</li> <li>Retention: keep minimal; retain only what\u2019s needed to resolve the issue.</li> </ul>"},{"location":"operations/data-handling-retention/#usage-metrics-get-apiusage","title":"Usage metrics (<code>GET /api/usage</code>)","text":"<ul> <li>Stored in DB table <code>usage_metrics</code>: <code>metric_date</code>, <code>event</code>, <code>count</code>.</li> <li>Aggregated daily counts only (no IPs, no user IDs).</li> <li>Public API returns a rolling window (<code>HEALTHARCHIVE_USAGE_METRICS_WINDOW_DAYS</code>).</li> </ul>"},{"location":"operations/data-handling-retention/#backups","title":"Backups","text":"<ul> <li>Postgres dumps (custom-format) are stored on the VPS (see <code>docs/deployment/production-single-vps.md</code>).</li> <li>Treat dumps as sensitive; they may contain report text/emails and should not be shared publicly.</li> </ul>"},{"location":"operations/data-handling-retention/#serverapplication-logs","title":"Server/application logs","text":"<ul> <li>journald and web server logs may include IPs and request paths.</li> <li>Treat logs as sensitive; do not paste raw logs into public issues or git.</li> </ul>"},{"location":"operations/data-handling-retention/#ops-logs-public-safe","title":"Ops logs (public-safe)","text":"<ul> <li>Restore tests: <code>/srv/healtharchive/ops/restore-tests/</code> (public-safe Markdown entries only).</li> <li>Adoption signals: <code>/srv/healtharchive/ops/adoption/</code> (public-safe; quarterly; links + aggregates only).</li> <li>Mentions log: <code>mentions-log.md</code> (public-safe, link-only; no private contact details).</li> </ul>"},{"location":"operations/dataset-release-runbook/","title":"Dataset Release Runbook (internal)","text":"<p>This release is normally hands-off (GitHub Actions). Use this checklist for verification or recovery.</p>"},{"location":"operations/dataset-release-runbook/#checklist","title":"Checklist","text":"<p>1) Check <code>https://github.com/jerdaw/healtharchive-datasets/releases</code> for the latest tag. 2) Download all assets to one directory; run <code>sha256sum -c SHA256SUMS</code>. 3) Inspect <code>manifest.json</code> for <code>truncated=false</code> and plausible row counts. 4) Record a quarterly entry in <code>/srv/healtharchive/ops/adoption/</code> (links + aggregates only).</p> <p>Notes:</p> <ul> <li>The datasets publish workflow also validates the release bundle before publishing:</li> <li><code>manifest.json</code> required fields + invariants (including <code>truncated=false</code>)</li> <li>Checksums match both <code>manifest.json</code> and <code>SHA256SUMS</code></li> </ul>"},{"location":"operations/dataset-release-runbook/#if-a-release-is-missing","title":"If a release is missing","text":"<ul> <li>Manually run the Publish dataset release workflow in GitHub Actions.</li> <li>Confirm it creates a tag <code>healtharchive-dataset-YYYY-MM-DD</code> and uploads assets.</li> </ul>"},{"location":"operations/disk-baseline-and-cleanup/","title":"Disk Baseline and Automated Cleanup","text":"<p>Last Updated: 2026-02-01 VPS: Hetzner 75GB single-VPS production</p>"},{"location":"operations/disk-baseline-and-cleanup/#current-baseline","title":"Current Baseline","text":"<p>Normal operating disk usage: ~82% Available space: ~14GB Alert thresholds: - Warning: &gt;85% for 30m - Critical: &gt;92% for 10m</p>"},{"location":"operations/disk-baseline-and-cleanup/#why-82-baseline","title":"Why 82% Baseline?","text":"<p>The VPS uses a tiered storage architecture: - Local disk (75GB): System, Docker, logs, temp crawl data - Storagebox (1TB): Final WARCs, ZIMs, large job data via SSHFS mounts</p> <p>Local disk breakdown (~61GB used): - System/packages: ~3.1GB (<code>/usr</code>) - Docker: ~7GB (<code>/var/lib/docker</code>) - Logs: ~2GB (<code>/var/log</code>) - Ephemeral data: ~1GB (<code>/srv</code> local, temp crawl dirs) - OS/kernel: ~48GB (includes filesystem metadata, journal, reserves)</p>"},{"location":"operations/disk-baseline-and-cleanup/#automated-cleanup","title":"Automated Cleanup","text":""},{"location":"operations/disk-baseline-and-cleanup/#1-docker-cleanup-weekly","title":"1. Docker Cleanup (Weekly)","text":"<p>Timer: <code>docker-cleanup.timer</code> (weekly) Script: <code>/usr/local/bin/docker-cleanup.sh</code> Actions: <pre><code>docker image prune -a -f  # Remove unused images\ndocker system prune -f    # Remove stopped containers, networks\n</code></pre></p> <p>Expected impact: Frees 2-4GB per week</p>"},{"location":"operations/disk-baseline-and-cleanup/#2-log-rotation","title":"2. Log Rotation","text":"<p>Journald (<code>/etc/systemd/journald.conf</code>): - <code>SystemMaxUse=500M</code> - Cap journal size - <code>SystemKeepFree=2G</code> - Ensure 2GB always free - <code>MaxFileSec=1week</code> - Rotate weekly</p> <p>Docker container logs (<code>/etc/docker/daemon.json</code>): - <code>max-size: 10m</code> - Max 10MB per log file - <code>max-file: 3</code> - Keep 3 rotations (30MB total per container)</p> <p>Expected impact: Prevents runaway log growth, keeps logs &lt;2GB</p>"},{"location":"operations/disk-baseline-and-cleanup/#3-manual-cleanup-commands","title":"3. Manual Cleanup Commands","text":"<p>When disk &gt;85%, run these manually:</p> <pre><code># Clean Docker\ndocker image prune -a -f\ndocker system prune -f\n\n# Rotate logs\nsudo journalctl --vacuum-size=500M\n\n# Truncate large container logs\nsudo truncate -s 0 /var/lib/docker/containers/*/CONTAINER-json.log\n\n# Check what's consuming space\nsudo du -xsh /* 2&gt;/dev/null | sort -hr | head -10\n</code></pre>"},{"location":"operations/disk-baseline-and-cleanup/#worker-pre-crawl-disk-check","title":"Worker Pre-Crawl Disk Check","text":"<p>Threshold: 85% Behavior: Worker skips job selection if disk &gt;85%</p> <p>This prevents starting crawls that would fail mid-flight due to disk pressure.</p>"},{"location":"operations/disk-baseline-and-cleanup/#monitoring","title":"Monitoring","text":"<p>Metrics: <code>node_filesystem_avail_bytes</code>, <code>node_filesystem_size_bytes</code> Dashboard: Grafana \"HealthArchive - Infrastructure\" Status command: <code>ha-backend status</code> (shows disk usage with color coding)</p>"},{"location":"operations/disk-baseline-and-cleanup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"operations/disk-baseline-and-cleanup/#disk-85-sustained","title":"Disk &gt;85% Sustained","text":"<ol> <li>Check Docker images: <code>docker system df</code></li> <li>Check logs: <code>sudo du -sh /var/log</code></li> <li>Check temp crawl dirs: <code>du -xsh /srv/healtharchive/jobs/*/</code></li> <li>Run manual cleanup (see above)</li> </ol>"},{"location":"operations/disk-baseline-and-cleanup/#disk-92-critical","title":"Disk &gt;92% (Critical)","text":"<ol> <li>Stop active crawls if necessary: <code>docker ps</code> \u2192 <code>docker stop &lt;id&gt;</code></li> <li>Run all cleanup commands</li> <li>Consider truncating container logs</li> <li>If still critical, investigate filesystem accounting with <code>sudo du -xsh /</code></li> </ol>"},{"location":"operations/disk-baseline-and-cleanup/#false-alarm-du-reports-100gb","title":"False Alarm: du Reports &gt;100GB","text":"<p>If <code>du -sh /srv/healtharchive/jobs/*</code> reports huge sizes (&gt;100GB), it's traversing SSHFS mounts and reporting remote storagebox data.</p> <p>Fix: Use <code>du -xsh</code> to stay on local filesystem only: <pre><code>sudo du -xsh /srv/healtharchive/jobs/*\n</code></pre></p> <p>Or just use <code>df -h /</code> for filesystem truth.</p>"},{"location":"operations/disk-baseline-and-cleanup/#history","title":"History","text":"<ul> <li>2026-02-01: Established 82% baseline after Docker/log cleanup freed 5.4GB</li> <li>2026-01-31: Disk pressure incident (89% \u2192 cleanup \u2192 82%)</li> <li>2026-01-24: Automated tiering for annual jobs deployed</li> </ul>"},{"location":"operations/escalation-procedures/","title":"Escalation Procedures","text":"<p>Last Updated: 2026-01-18 Status: Active</p> <p>This document defines how to categorize, escalate, and respond to incidents affecting the HealthArchive production environment.</p>"},{"location":"operations/escalation-procedures/#1-severity-levels","title":"1. Severity Levels","text":"<p>We categorize incidents into four levels based on impact and urgency.</p> Level Definition Response Time Actions Sev0 Critical Outage / Data LossSystem is totally unusable, or confirmed data loss is occurring. Immediate 1. Stop all non-recovery work.2. Notify stakeholders (if any).3. Initiate Disaster Recovery. Sev1 Major DegradationCore features (Search, API) are broken or extremely slow. User impact is high. &lt; 1 Hour 1. Engage Primary On-Call.2. Investigate immediately.3. Deploy hotfix or rollback. Sev2 Partial DegradationSecondary features (e.g., Replay) broken, or performance issues with workarounds. &lt; 4 Hours 1. Log incident.2. Investigate within business hours.3. Schedule fix for next release window. Sev3 Minor IssueTrivial bugs, cosmetic issues, or single-page failures. No broad user impact. &lt; 24 Hours 1. Log ticket/issue.2. Prioritize in normal development backlog."},{"location":"operations/escalation-procedures/#2-escalation-path","title":"2. Escalation Path","text":""},{"location":"operations/escalation-procedures/#current-state-single-operator","title":"Current State: Single Operator","text":"<p>In the current single-maintainer topology, the escalation path is flat.</p> <ol> <li>Primary: Operator (You) - Responsible for all triage and resolution.</li> <li>Backup: None (Bus factor = 1).<ul> <li>Mitigation: Comprehensive Runbooks and Disaster Recovery docs to allow a skilled third party to recover the system using \"Break-Glass\" credentials if the primary operator is incapacitated.</li> </ul> </li> </ol>"},{"location":"operations/escalation-procedures/#future-state-multi-operator","title":"Future State: Multi-Operator","text":"<p>When the team grows, follow this hierarchy:</p> <ol> <li>Level 1 (On-Call): Triage, immediate mitigation, and initial investigation.</li> <li>Level 2 (Secondary/Backup): Deep dive debugging, code fixes, and complex recovery.</li> <li>Level 3 (Project Lead): Strategic decisions (e.g., data loss acceptance, major architecture rollback).</li> </ol>"},{"location":"operations/escalation-procedures/#3-dri-assignments-directly-responsible-individuals","title":"3. DRI Assignments (Directly Responsible Individuals)","text":"<p>Since we largely operate as a single unit, the Operator is the DRI for all areas. This matrix serves as a template for future delegation.</p> Area DRI Responsibilities Backend API Operator FastAPI availability, performance, response correctness. Worker / Crawls Operator Job scheduling, zimit/warcio execution, tiering to storage. Database Operator PostgreSQL uptime, backup verification, schema migrations. Storage / WARC Operator Disk space management, Storage Box connectivity, manifest integrity. Replay Service Operator <code>pywb</code> availability and indexing health. Infrastructure Operator VPS provisioning, OS updates, systemd maintenance, Tailscale."},{"location":"operations/escalation-procedures/#4-contact-information-storage","title":"4. Contact Information Storage","text":"<p>For security reasons, do not store phone numbers or sensitive access codes in this git repository.</p>"},{"location":"operations/escalation-procedures/#production-contact-list","title":"Production Contact list","text":"<p>Store a secure, read-only file on the production VPS for emergency reference:</p> <ul> <li>Path: <code>/etc/healtharchive/contacts.env</code></li> <li>Permissions: <code>600</code> (root/owner only)</li> <li>Format: Key-Value pairs</li> </ul> <pre><code># Example content for /etc/healtharchive/contacts.env\nOPERATOR_PHONE=\"+1-555-0100\"\nOPERATOR_EMAIL=\"admin@healtharchive.ca\"\nSECONDARY_CONTACT_PHONE=\"+1-555-0101\" # Backup contact (if any)\nHETZNER_SUPPORT_PIN=\"12345\"\nNAMECHEAP_SUPPORT_PIN=\"67890\"\n</code></pre>"},{"location":"operations/escalation-procedures/#personal-backup","title":"Personal Backup","text":"<p>Mirror this information in your password manager (e.g., 1Password, Bitwarden) under a secure note titled \"HealthArchive Emergency Contacts\".</p>"},{"location":"operations/escalation-procedures/#5-break-glass-procedures","title":"5. Break-Glass Procedures","text":"<p>Quick-reference steps for common critical failures where normal access or services are blocked.</p>"},{"location":"operations/escalation-procedures/#a-api-unresponsive-http-502503timeout","title":"A. API Unresponsive (HTTP 502/503/Timeout)","text":"<ol> <li>Access: SSH to VPS via Tailscale (<code>ssh haadmin@100.x.y.z</code>).</li> <li>Status: Check if the service is running.     <pre><code>username@host:~$ systemctl status healtharchive-api\n</code></pre></li> <li>Logs: specific error messages?     <pre><code>username@host:~$ journalctl -u healtharchive-api -n 100\n</code></pre></li> <li>Action: Restart the service.     <pre><code>username@host:~$ sudo systemctl restart healtharchive-api\n</code></pre></li> <li>Escalation: If restart fails or immediately crashes, check Database connectivity (see B).</li> </ol>"},{"location":"operations/escalation-procedures/#b-database-unreachable","title":"B. Database Unreachable","text":"<ol> <li>Status: Is Postgres running?     <pre><code>username@host:~$ systemctl status postgresql\n</code></pre></li> <li>Resources: Is disk full?     <pre><code>username@host:~$ df -h\n</code></pre></li> <li>Logs: <pre><code>username@host:~$ journalctl -u postgresql -n 100\n</code></pre></li> <li>Action: Restart Postgres.     <pre><code>username@host:~$ sudo systemctl restart postgresql\n</code></pre></li> <li>Escalation: If database won't start due to corruption, proceed to Disaster Recovery Scenario B.</li> </ol>"},{"location":"operations/escalation-procedures/#c-vps-unreachable-ssh-down","title":"C. VPS Unreachable (SSH Down)","text":"<ol> <li>Check Network: Try accessing via different Tailscale node or public IP (if SSH open/testing).</li> <li>Console Access: Log in to Hetzner Cloud Console &gt; Select Server &gt; Console.<ul> <li>This bypasses network/SSH config issues.</li> </ul> </li> <li>Reboot: Use the Hetzner \"Power\" menu to force a reboot ACPI or hard reset if the OS is frozen.</li> <li>Escalation: If the server is deleted or hardware failed, proceed to Disaster Recovery Scenario A.</li> </ol>"},{"location":"operations/escalation-procedures/#6-handoff-procedures","title":"6. Handoff Procedures","text":"<p>When transferring responsibility (e.g., vacation coverage):</p> <ol> <li>Sync: Verify current system health (dashboards, logs).</li> <li>Access: Confirm backup operator has valid SSH/Tailscale access.</li> <li>Docs: Ensure emergency contact info is accessible to the backup.</li> <li>Notify: Inform any stakeholders (if applicable) of the active operator change.</li> </ol>"},{"location":"operations/export-integrity-contract/","title":"Export Integrity Contract (internal)","text":"<p>Exports and dataset releases must be defensible and reproducible over time.</p>"},{"location":"operations/export-integrity-contract/#export-endpoints-ordering-pagination","title":"Export endpoints (ordering + pagination)","text":"<ul> <li><code>GET /api/exports/snapshots</code> is ordered by <code>snapshot_id</code> ascending and paginates via <code>afterId</code>.</li> <li><code>GET /api/exports/changes</code> is ordered by <code>change_id</code> ascending and paginates via <code>afterId</code>.</li> </ul>"},{"location":"operations/export-integrity-contract/#dataset-release-manifest-manifestjson","title":"Dataset release manifest (<code>manifest.json</code>)","text":"<p>Required fields:</p> <ul> <li><code>version</code> (schema version for the manifest itself)</li> <li><code>tag</code> (release tag)</li> <li><code>releasedAtUtc</code> (ISO-8601 UTC timestamp)</li> <li><code>apiBase</code> and <code>exportsManifest</code> (from <code>GET /api/exports</code>)</li> <li><code>artifacts.snapshots</code> and <code>artifacts.changes</code> including:</li> <li><code>rows</code>, <code>minId</code>, <code>maxId</code>, <code>requestsMade</code>, <code>limitPerRequest</code>, <code>truncated</code></li> <li><code>sha256</code> for each artifact</li> </ul> <p>Rules:</p> <ul> <li><code>SHA256SUMS</code> must match all listed files.</li> <li><code>truncated</code> should be <code>false</code> for both exports; if <code>true</code>, treat the release as incomplete and re-run.</li> </ul>"},{"location":"operations/export-integrity-contract/#immutability-corrections","title":"Immutability / corrections","text":"<ul> <li>Treat published dataset release tags as immutable research objects.</li> <li>If a correction is required, document it in release notes and prefer a new tag over silently rewriting history.</li> </ul>"},{"location":"operations/export-integrity-contract/#diff-recomputation-policy","title":"Diff recomputation policy","text":"<ul> <li>Change export rows include <code>diff_version</code> and <code>normalization_version</code>.</li> <li>If change tracking methodology changes, bump versions and document it (methods note/changelog) rather than silently rewriting history.</li> </ul>"},{"location":"operations/exports-data-dictionary/","title":"HealthArchive exports \u2014 data dictionary (pointer)","text":"<p>Canonical public copy lives in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/public/exports/healtharchive-data-dictionary.md</li> <li>Live page: https://www.healtharchive.ca/exports</li> </ul> <p>If you need to update the data dictionary, edit the frontend asset and deploy the frontend.</p>"},{"location":"operations/growth-constraints/","title":"Growth Constraints (internal policy)","text":"<p>Purpose: prevent slow scope creep from undermining reliability on a single-VPS architecture.</p> <p>These constraints are intentionally conservative defaults. Adjust only after a capacity review.</p>"},{"location":"operations/growth-constraints/#storage-budget","title":"Storage budget","text":"<ul> <li>Target: keep total disk usage under 70% of available space.</li> <li>Review threshold: 80% usage triggers a pause on new sources until cleanup or capacity planning is complete.</li> <li>Replay retention: if replay is enabled, WARCs must remain available. Use safe cleanup only (<code>cleanup-job --mode temp-nonwarc</code>).</li> </ul>"},{"location":"operations/growth-constraints/#source-cap-per-annual-edition","title":"Source cap per annual edition","text":"<ul> <li>Default cap: add no more than 2 new sources per annual edition.</li> <li>Any additions beyond the cap require:</li> <li>a successful dry-run capture,</li> <li>indexing verification,</li> <li>and a storage headroom check.</li> </ul>"},{"location":"operations/growth-constraints/#performance-budgets-initial-targets","title":"Performance budgets (initial targets)","text":"<p>These are targets, not guarantees. Use them to detect regressions.</p> <ul> <li>Search (view=pages): p95 response &lt; 2s for common queries.</li> <li>Snapshot detail: p95 response &lt; 1s for metadata payloads.</li> <li>Changes feed: p95 response &lt; 2s for edition-aware queries.</li> </ul> <p>If p95 exceeds targets for multiple weeks, pause new scope additions and prioritize performance fixes.</p>"},{"location":"operations/growth-constraints/#crawl-load-limits","title":"Crawl load limits","text":"<ul> <li>Keep crawler parallelism conservative during annual campaigns to avoid:</li> <li>starving the API,</li> <li>exceeding source rate limits,</li> <li>and filling storage too quickly.</li> <li>If capture jobs start to lag, reduce concurrency before expanding scope.</li> </ul>"},{"location":"operations/growth-constraints/#public-facing-summary-use-in-governance","title":"Public-facing summary (use in governance)","text":"<p>Reliability and provenance take priority over expanding coverage. Sources are added deliberately within storage and operational capacity, and the archive does not attempt to crawl the entire internet.</p>"},{"location":"operations/healtharchive-ops-roadmap/","title":"HealthArchive ops roadmap (internal)","text":"<p>This file tracks the current ops roadmap/todo items only. Keep it short and current.</p> <p>For historical roadmaps and upgrade context, see:</p> <ul> <li><code>docs/planning/README.md</code> (backend repo)</li> </ul> <p>Keep the two synced copies of this file aligned:</p> <ul> <li>Backend repo: <code>docs/operations/healtharchive-ops-roadmap.md</code></li> <li>Optional local working copy (non-git): if you keep a separate ops checklist outside the repo, keep it in sync with this canonical file.</li> </ul>"},{"location":"operations/healtharchive-ops-roadmap/#recurring-ops-non-irl-ongoing","title":"Recurring ops (non-IRL, ongoing)","text":"<ul> <li>Quarterly: run a restore test and record a public-safe log entry in <code>/srv/healtharchive/ops/restore-tests/</code>.</li> <li>Quarterly: add an adoption signals entry in <code>/srv/healtharchive/ops/adoption/</code> (links + aggregates only).</li> <li>Quarterly: confirm dataset release exists and passes checksum verification (<code>sha256sum -c SHA256SUMS</code>).</li> <li>Quarterly: confirm core timers are enabled and succeeding (recommended: on the VPS run <code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/verify_ops_automation.sh</code>; then spot-check <code>journalctl -u &lt;service&gt;</code>).</li> <li>Quarterly: docs drift skim: re-read the production runbook + incident response and fix any drift you notice (keep docs matching reality).</li> </ul>"},{"location":"operations/healtharchive-ops-roadmap/#current-status-as-of-2026-02-07","title":"Current status (as of 2026-02-07)","text":"<ul> <li>2026 annual crawl is actively running on the VPS (jobs: <code>hc</code>/<code>phac</code>/<code>cihr</code>; see <code>./scripts/vps-crawl-status.sh --year 2026</code>).</li> <li>Deploy-lock suppression is cleared (the stale <code>/tmp/healtharchive-backend-deploy.lock</code> was removed; auto-recover apply actions are no longer skipped due to deploy lock).</li> <li>Job lock-dir cutover is staged (non-disruptive) but not fully complete:</li> <li><code>/etc/healtharchive/backend.env</code> now sets <code>HEALTHARCHIVE_JOB_LOCK_DIR=/srv/healtharchive/ops/locks/jobs</code></li> <li><code>/srv/healtharchive/ops/locks/jobs</code> exists with intended perms</li> <li>Maintenance-window restart of services is still required to pick up the env change.</li> <li>Annual output-dir mount topology is currently unexpected (direct <code>sshfs</code> mounts instead of bind mounts) for the active 2026 jobs.</li> <li>We are intentionally deferring conversion to bind mounts until a maintenance window to avoid interrupting in-progress crawls.</li> </ul>"},{"location":"operations/healtharchive-ops-roadmap/#current-ops-tasks-implementation-already-exists-enableverify","title":"Current ops tasks (implementation already exists; enable/verify)","text":"<ul> <li>Maintenance window: complete the job lock-dir cutover by restarting services that read <code>/etc/healtharchive/backend.env</code>.</li> <li>This must wait until crawls are idle unless you explicitly accept interrupting them.</li> <li>Plan + commands: <code>../planning/2026-02-06-crawl-operability-locks-and-retry-controls.md</code> (Phase 4)</li> <li>Maintenance window (after 2026 annual crawl is idle): convert annual output dirs from direct <code>sshfs</code> mounts to bind mounts.</li> <li>Why defer: unmount/re-mount of a live job output dir can interrupt in-progress crawls; benefit is reduced Errno 107 blast radius,     but not worth forced interruption mid-campaign.</li> <li>Detection (crawl-safe): <code>python3 /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py --year 2026</code></li> <li>Repair (maintenance only): stop the worker and ensure crawl containers are stopped, then:<ul> <li><code>sudo python3 /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py --year 2026 --apply --repair-unexpected-mounts --allow-repair-running-jobs</code></li> </ul> </li> <li>After any reboot/rescue/maintenance where mounts may drift:</li> <li>Verify Storage Box mount is active (<code>healtharchive-storagebox-sshfs.service</code>).</li> <li>Re-apply annual output tiering for the active campaign year and confirm job output dirs are on Storage Box (see incident: <code>incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk.md</code>).</li> <li>After deploying new crawl tuning defaults (or if an annual campaign was started before the change):</li> <li>Reconcile already-created annual job configs so retries/restarts adopt the new per-source profiles:<ul> <li>Dry-run: <code>ha-backend reconcile-annual-tool-options --year &lt;YEAR&gt;</code></li> <li>Apply: <code>ha-backend reconcile-annual-tool-options --year &lt;YEAR&gt; --apply</code></li> </ul> </li> <li>Verify the new Docker resource limit environment variables are set appropriately on VPS if defaults need adjustment:</li> <li><code>HEALTHARCHIVE_DOCKER_MEMORY_LIMIT</code> (default: 4g)</li> <li><code>HEALTHARCHIVE_DOCKER_CPU_LIMIT</code> (default: 1.5)</li> <li>Verify the new alerts are firing correctly in Grafana:</li> <li><code>HealthArchiveCrawlRateSlowHC</code></li> <li><code>HealthArchiveCrawlRateSlowPHAC</code></li> <li><code>HealthArchiveCrawlRateSlowCIHR</code></li> <li><code>HealthArchiveCrawlNewPhaseChurn</code></li> <li><code>HealthArchiveInfraErrorsHigh</code></li> <li><code>HealthArchiveCrawlMetricsStale</code></li> </ul>"},{"location":"operations/healtharchive-ops-roadmap/#irl-external-validation-pending","title":"IRL / external validation (pending)","text":"<p>Track external validation/outreach work (partner, verifier, mentions/citations log) in:</p> <ul> <li><code>../planning/roadmap.md</code></li> </ul>"},{"location":"operations/legacy-crawl-imports/","title":"Importing legacy crawls (from old <code>.zim</code> backups)","text":"<p>This doc explains how we imported existing crawl data (originally used to produce <code>.zim</code> files) into the HealthArchive backend so it shows up in:</p> <ul> <li><code>https://api.healtharchive.ca</code> (search, sources, viewer)</li> <li><code>https://www.healtharchive.ca/archive</code> (live results)</li> </ul>"},{"location":"operations/legacy-crawl-imports/#important-clarification-we-did-not-import-the-zim-files","title":"Important clarification: we did not \u201cimport the <code>.zim</code> files\u201d","text":"<p>The backend does not read <code>.zim</code> files today.</p> <p>Instead, we imported the WARC files (the raw web captures) that live next to those <code>.zim</code> outputs in the old crawl directories. The backend indexes WARCs into database rows (<code>Snapshot</code>) and the snapshot viewer replays archived HTML from those WARCs.</p> <p>If you want to keep the <code>.zim</code> files, store them as separate artifacts (NAS / object storage). They\u2019re useful for offline viewing, but they are not part of the backend\u2019s serving path.</p>"},{"location":"operations/legacy-crawl-imports/#terminology","title":"Terminology","text":"<ul> <li>Legacy crawl: a crawl run before this project\u2019s integrated backend existed.</li> <li>WARC: compressed web capture files (<code>*.warc.gz</code>).</li> <li>Import directory: a directory on the VPS under   <code>/srv/healtharchive/jobs/imports/&lt;import-name&gt;</code> that holds the legacy WARCs in   a layout the backend\u2019s WARC discovery can find.</li> <li>Register: create an <code>ArchiveJob</code> row pointing at an existing directory   (<code>ha-backend register-job-dir</code>).</li> <li>Index: parse WARCs and write <code>Snapshot</code> rows (<code>ha-backend index-job</code>).</li> </ul>"},{"location":"operations/legacy-crawl-imports/#prerequisites","title":"Prerequisites","text":"<ul> <li>Backend is already deployed on the VPS and can reach Postgres.</li> <li>You have an SSH path from your NAS to the VPS:</li> <li>We used Tailscale so the NAS can reach the VPS consistently even if home     IP changes.</li> <li>We used a dedicated NAS SSH key and a dedicated VPS user (<code>habackup</code>) for     file transfer.</li> <li>The backend env file exists on the VPS:</li> <li><code>/etc/healtharchive/backend.env</code> (mode <code>0600</code>, owned by <code>root:root</code>).</li> <li>The backend archive root exists:</li> <li><code>/srv/healtharchive/jobs</code></li> </ul>"},{"location":"operations/legacy-crawl-imports/#source-of-truth-locations-nas-vs-dev-vm-mount","title":"Source of truth locations (NAS vs dev VM mount)","text":"<p>Your Synology NAS stores the legacy crawl data at:</p> <ul> <li><code>/volume1/nobak/gov-health-archives/...</code></li> </ul> <p>Your Linux dev VM mounts that NAS path at:</p> <ul> <li><code>/mnt/nasd/nobak/gov-health-archives/...</code></li> </ul> <p>When writing instructions for \u201crun on NAS\u201d, use the <code>/volume1/...</code> path. When running on the dev VM, use the <code>/mnt/nasd/...</code> path.</p>"},{"location":"operations/legacy-crawl-imports/#step-by-step-importing-a-legacy-dataset","title":"Step-by-step: importing a legacy dataset","text":""},{"location":"operations/legacy-crawl-imports/#step-1-identify-the-warc-archive-directory-to-import","title":"Step 1 \u2014 Identify the WARC archive directory to import","text":"<p>In the legacy crawl output, find the directory that contains the WARCs. Example (Health Canada legacy crawl):</p> <ul> <li>NAS:</li> <li><code>/volume1/nobak/gov-health-archives/canada_ca_health_backup_2025-04-21/crawler_data/collections/canada_ca_health_crawl_2025-04/archive/</code></li> <li>Dev VM (same content, mounted):</li> <li><code>/mnt/nasd/nobak/gov-health-archives/canada_ca_health_backup_2025-04-21/crawler_data/collections/canada_ca_health_crawl_2025-04/archive/</code></li> </ul> <p>What you\u2019re looking for:</p> <ul> <li>many files like <code>rec-&lt;id&gt;-&lt;collection&gt;-&lt;timestamp&gt;-N.warc.gz</code></li> </ul>"},{"location":"operations/legacy-crawl-imports/#step-2-create-the-destination-import-directory-structure-on-the-vps","title":"Step 2 \u2014 Create the destination \u201cimport directory\u201d structure on the VPS","text":"<p>We created an import output directory that looks like an <code>archive_tool</code> output, because the backend\u2019s WARC discovery already knows how to find WARCs inside a job directory by looking for temp dirs like <code>.tmp-*</code> and <code>collections/*/archive</code>.</p> <p>Example destination (Health Canada legacy import):</p> <ul> <li><code>/srv/healtharchive/jobs/imports/legacy-hc-2025-04-21/.tmp-legacy/collections/crawl-legacy-hc-2025-04-21/archive/</code></li> </ul> <p>Example destination (CIHR legacy import):</p> <ul> <li><code>/srv/healtharchive/jobs/imports/legacy-cihr-2025-04/.tmp-legacy/collections/crawl-legacy-cihr-2025-04/archive/</code></li> </ul> <p>Notes:</p> <ul> <li>The exact collection directory name doesn\u2019t matter much; what matters is the   presence of:</li> <li>a <code>.tmp-*</code> directory (we used <code>.tmp-legacy</code>)</li> <li>a <code>collections/&lt;anything&gt;/archive/</code> directory inside it</li> <li>WARCs under that archive directory</li> <li>The backend log line you\u2019ll see during indexing is similar to:</li> <li>\u201cFallback found latest temp dir: \u2026/.tmp-legacy\u201d</li> </ul>"},{"location":"operations/legacy-crawl-imports/#step-3-transfer-the-warcs-from-nas-vps-via-rsync","title":"Step 3 \u2014 Transfer the WARCs from NAS \u2192 VPS via rsync","text":"<p>We ran <code>rsync</code> from the NAS, using SSH over Tailscale:</p> <pre><code>rsync -av --info=progress2 --partial --append-verify --bwlimit=5000 \\\n  -e \"ssh -i ~/.ssh/&lt;NAS_SSH_KEY&gt;\" \\\n  \"/volume1/nobak/gov-health-archives/&lt;LEGACY_PATH&gt;/archive/\" \\\n  \"habackup@&lt;VPS_TAILSCALE_IP&gt;:/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;/.tmp-legacy/collections/&lt;COLLECTION_NAME&gt;/archive/\"\n</code></pre> <p>Why these flags:</p> <ul> <li><code>--partial --append-verify</code>: safe-ish resume if the connection drops.</li> <li><code>--bwlimit</code>: avoids saturating your uplink.</li> </ul> <p>After transfer, verify on VPS:</p> <pre><code>sudo find \"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\" -name '*.warc.gz' | wc -l\nsudo du -sh \"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\"\n</code></pre> <p>Real example result (Health Canada legacy import):</p> <ul> <li><code>959</code> WARCs</li> <li><code>~26G</code> total</li> </ul>"},{"location":"operations/legacy-crawl-imports/#step-4-normalize-permissions-rsync-from-nas-can-create-unsafe-modes","title":"Step 4 \u2014 Normalize permissions (rsync from NAS can create unsafe modes)","text":"<p>The rsync upload preserved permissive modes (<code>777</code>) from the source, which is not what we want on the VPS.</p> <p>We normalized permissions on the VPS:</p> <pre><code>IMPORT_DIR=\"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\"\n\nsudo chown -R habackup:healtharchive \"$IMPORT_DIR\"\n\n# Directories: rwx for owner+group, setgid so new files inherit group\nsudo find \"$IMPORT_DIR\" -type d -exec chmod 2770 {} +\n\n# WARCs: readable by owner+group only\nsudo find \"$IMPORT_DIR\" -type f -name '*.warc.gz' -exec chmod 640 {} +\n</code></pre> <p>Why:</p> <ul> <li><code>healtharchive</code> group can be granted controlled read access.</li> <li>We avoid <code>777</code> and other \u201cworld writable\u201d mistakes.</li> </ul>"},{"location":"operations/legacy-crawl-imports/#optional-helper-one-command-to-normalize-register-index","title":"Optional helper: one command to normalize + register + index","text":"<p>This repo includes a helper script that wraps the \u201cVPS-side\u201d steps:</p> <ul> <li>permissions normalization (Step 4)</li> <li>register-job-dir (Step 5)</li> <li>index-job (Step 6)</li> </ul> <p>It is dry-run by default:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/import-legacy-crawl.sh --import-dir \"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\" --source &lt;SOURCE_CODE&gt;\n</code></pre> <p>To apply (real run):</p> <pre><code>./scripts/import-legacy-crawl.sh --apply --import-dir \"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\" --source &lt;SOURCE_CODE&gt; --job-name \"&lt;JOB_NAME&gt;\"\n</code></pre>"},{"location":"operations/legacy-crawl-imports/#step-5-register-the-directory-as-an-archivejob","title":"Step 5 \u2014 Register the directory as an ArchiveJob","text":"<p>Because <code>/etc/healtharchive/backend.env</code> is root-owned and not readable by normal users, we used <code>systemd-run</code> to run the CLI with <code>EnvironmentFile=/etc/healtharchive/backend.env</code>.</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend register-job-dir \\\n  --source &lt;SOURCE_CODE&gt; \\\n  --output-dir \"/srv/healtharchive/jobs/imports/&lt;IMPORT_NAME&gt;\" \\\n  --name \"&lt;JOB_NAME&gt;\"\n</code></pre> <p>Example (Health Canada legacy import):</p> <ul> <li><code>--source hc</code></li> <li><code>--output-dir /srv/healtharchive/jobs/imports/legacy-hc-2025-04-21</code></li> <li><code>--name legacy-hc-2025-04-21</code></li> <li>Created <code>ArchiveJob</code> ID <code>1</code></li> </ul>"},{"location":"operations/legacy-crawl-imports/#step-6-index-warcs-into-snapshot-rows","title":"Step 6 \u2014 Index WARCs into Snapshot rows","text":"<p>Indexing is the expensive part. It scans WARCs and creates one <code>Snapshot</code> row per captured HTML page.</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend index-job \\\n  --id &lt;JOB_ID&gt;\n</code></pre> <p>Expected resource usage (real example: Health Canada legacy import):</p> <ul> <li><code>959</code> WARCs</li> <li><code>~2h 16m</code> wall time</li> <li><code>~1.9G</code> peak RAM</li> <li>CPU pegged near 100% during indexing</li> </ul> <p>You can watch progress indirectly via systemd:</p> <pre><code>sudo systemctl status run-uXXX.service --no-pager -l\n</code></pre>"},{"location":"operations/legacy-crawl-imports/#step-7-verify-the-import-worked-db-api-frontend","title":"Step 7 \u2014 Verify the import worked (DB + API + frontend)","text":"<p>On VPS (DB/job view):</p> <pre><code>sudo systemd-run --wait --pipe \\\n  --property=EnvironmentFile=/etc/healtharchive/backend.env \\\n  /opt/healtharchive-backend/.venv/bin/ha-backend show-job --id &lt;JOB_ID&gt;\n</code></pre> <p>On your laptop (public API):</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?page=1&amp;pageSize=10\"\ncurl -s \"https://api.healtharchive.ca/api/sources\"\n</code></pre> <p>In the browser (frontend):</p> <ul> <li><code>https://www.healtharchive.ca/archive</code> should show a large snapshot count and real results.</li> <li>Clicking \u201cView snapshot\u201d should open <code>https://www.healtharchive.ca/snapshot/&lt;id&gt;</code> and the embedded content should load from:</li> <li><code>https://api.healtharchive.ca/api/snapshots/raw/&lt;id&gt;</code></li> </ul> <p>Optional single command (VPS; requires CIHR to be imported/indexed if you ask for it):</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/verify_public_surface.py --require-source cihr\n</code></pre>"},{"location":"operations/legacy-crawl-imports/#known-gotcha-capture-dates-may-look-wrong-fix-requires-re-index","title":"Known gotcha: capture dates may look wrong (fix requires re-index)","text":"<p>If the indexer fails to parse <code>WARC-Date</code>, the backend may fall back to \u201cnow\u201d, making imported snapshots look like they were captured on import day.</p> <p>When this is fixed in code, you must re-run indexing for the job to update the stored capture timestamps.</p>"},{"location":"operations/legacy-crawl-imports/#what-we-imported-so-far-real-outcomes","title":"What we imported so far (real outcomes)","text":""},{"location":"operations/legacy-crawl-imports/#health-canada-legacy-import","title":"Health Canada legacy import","text":"<ul> <li>Source: legacy Health Canada crawl output (April 2025).</li> <li>Imported to VPS under:</li> <li><code>/srv/healtharchive/jobs/imports/legacy-hc-2025-04-21</code></li> <li>Result:</li> <li><code>ArchiveJob</code> ID <code>1</code></li> <li><code>959</code> WARCs</li> <li><code>123,656</code> snapshots indexed</li> <li>API + frontend show live results.</li> </ul>"},{"location":"operations/legacy-crawl-imports/#cihr-legacy-import-in-progress-next","title":"CIHR legacy import (in progress / next)","text":"<ul> <li>Source: legacy CIHR crawl output (April 2025).</li> <li>WARCs transferred to:</li> <li><code>/srv/healtharchive/jobs/imports/legacy-cihr-2025-04/.../archive/</code></li> <li>Next steps:</li> <li>normalize permissions</li> <li>register-job-dir with <code>--source cihr</code></li> <li>index-job</li> </ul>"},{"location":"operations/mentions-log/","title":"Mentions log (public-safe, link-only)","text":"<p>Purpose: a lightweight log of public mentions, links, or citations for HealthArchive.ca.</p> <p>Hard rules:</p> <ul> <li>Do not add private contact details (emails, phone numbers, private notes).</li> <li>If permission to name is unclear, do not name the organization here.</li> <li>Use <code>Pending</code> in the organization field until permission is explicit.</li> </ul>"},{"location":"operations/mentions-log/#confirmed-mentions-permission-to-name-yes","title":"Confirmed mentions (permission to name: Yes)","text":"Date (UTC) Organization / Outlet Link Context"},{"location":"operations/mentions-log/#unconfirmed-mentions-permission-pending-unknown","title":"Unconfirmed mentions (permission: Pending / Unknown)","text":"Date (UTC) Organization / Outlet Link Context Permission status"},{"location":"operations/mentions-log/#notes","title":"Notes","text":"<ul> <li>Keep descriptions factual and short.</li> <li>Prefer the most specific public link (the actual resources page that mentions HealthArchive).</li> </ul>"},{"location":"operations/methods-note-outline/","title":"Methods Note Outline (public-safe)","text":"<p>This outline is designed to become a poster, short preprint, or blog-style methods write-up that is:</p> <ul> <li>Descriptive (not interpretive).</li> <li>Explicitly not medical advice and not current guidance.</li> <li>Reproducibility-focused: \u201cwhat was published, when, and how we preserve it\u201d.</li> </ul> <p>It is intentionally \u201coutline-first\u201d so it can be quickly adapted to different venues without rewriting the project.</p>"},{"location":"operations/methods-note-outline/#working-title-options","title":"Working title options","text":"<ul> <li>HealthArchive.ca: A provenance-first archive of Canadian public health webpages</li> <li>Preserving temporal provenance in Canadian public health web guidance</li> <li>From snapshots to auditability: indexing and change tracking for public health webpages</li> </ul>"},{"location":"operations/methods-note-outline/#one-sentence-framing-use-everywhere","title":"One-sentence framing (use everywhere)","text":"<p>HealthArchive.ca preserves time-stamped snapshots of selected Canadian public health webpages so changes remain auditable, citable, and reproducible over time.</p>"},{"location":"operations/methods-note-outline/#abstract-structure","title":"Abstract (structure)","text":"<ul> <li>Background / problem: Public health web guidance and surveillance dashboards are \u201cliving documents\u201d that can change or disappear; this complicates reproducibility, journalism, and policy history.</li> <li>Objective: Build an independent, non-authoritative archive that makes historical versions discoverable and citable, with provenance labeling and descriptive change tracking.</li> <li>Methods: Automated capture \u2192 WARC storage \u2192 indexing into a searchable database \u2192 snapshot viewer + optional replay \u2192 edition-aware change tracking.</li> <li>Outputs: Public UI, metadata API, change feed, and metadata-only exports for research use.</li> <li>Limitations: Not authoritative, not current guidance, replay fidelity varies, scope intentionally constrained.</li> </ul>"},{"location":"operations/methods-note-outline/#introduction-what-why","title":"Introduction (what + why)","text":"<ul> <li>Web content in public health matters because it is used operationally (clinicians, journalists, researchers, public).</li> <li>Web guidance changes are normal, but they become hard to reconstruct after updates.</li> <li>Existing general-purpose web archives may not be optimized for:</li> <li>discoverability via structured search,</li> <li>consistent provenance labeling,</li> <li>edition-aware change tracking,</li> <li>research-ready exports and citation workflows.</li> </ul>"},{"location":"operations/methods-note-outline/#scope-and-safety-posture-non-negotiable","title":"Scope and safety posture (non-negotiable)","text":"<ul> <li>What it is: An independent archive of historical public webpages, designed for auditability and reproducibility.</li> <li>What it is not: A source of current guidance, a government site, or medical advice.</li> <li>Primary audiences: researchers, journalists, educators (secondary: clinicians/public with strong labeling).</li> <li>Privacy posture: no accounts, no PHI collection, minimal aggregated usage metrics only.</li> </ul>"},{"location":"operations/methods-note-outline/#system-overview-architecture","title":"System overview (architecture)","text":"<p>Describe at a high level (no sensitive infra details):</p> <ul> <li>Capture pipeline: browser-based crawling to standards-based WARC files.</li> <li>Storage: WARC files retained as the archival source of truth.</li> <li>Indexing: extract title/snippet/text signals into a relational database to enable fast search/browse.</li> <li>Serving: public API + Next.js frontend snapshot viewer; optional higher-fidelity replay.</li> <li>Edition model: annual \u201ceditions\u201d anchored to a capture campaign date (e.g., Jan 01 UTC) with occasional ad-hoc captures.</li> </ul>"},{"location":"operations/methods-note-outline/#capture-methodology-how-snapshots-are-created","title":"Capture methodology (how snapshots are created)","text":"<ul> <li>Seed URL sets and explicit include/exclude rules per source.</li> <li>Capture outputs recorded as WARCs (HTTP responses + timestamps and metadata).</li> <li>Operational constraints:</li> <li>scope boundaries to avoid \u201ccrawl everything\u201d failure modes,</li> <li>reliability &gt; breadth,</li> <li>respect for safe crawling practices (rate limiting, constrained seeds).</li> </ul>"},{"location":"operations/methods-note-outline/#indexing-and-discovery-how-users-find-things","title":"Indexing and discovery (how users find things)","text":"<ul> <li>Convert WARC records into \u201csnapshot\u201d rows with:</li> <li>capture timestamp (UTC),</li> <li>source attribution,</li> <li>original URL,</li> <li>stable snapshot permalink,</li> <li>language + status code where available.</li> <li>Provide:</li> <li>keyword search,</li> <li>source filtering,</li> <li>date range filtering,</li> <li>page-grouping (\u201clatest per page\u201d) vs \u201call captures\u201d views.</li> </ul>"},{"location":"operations/methods-note-outline/#provenance-labeling-how-users-avoid-misuse","title":"Provenance labeling (how users avoid misuse)","text":"<ul> <li>Snapshot pages and changes surfaces display:</li> <li>capture date/time (UTC),</li> <li>source name,</li> <li>original URL,</li> <li>\u201carchival snapshot\u201d warning / \u201cnot current guidance\u201d callout,</li> <li>links back to official sources.</li> </ul>"},{"location":"operations/methods-note-outline/#change-tracking-descriptive-diffing","title":"Change tracking (descriptive diffing)","text":"<p>Goal: make changes visible without interpreting meaning.</p> <ul> <li>Edition-aware change tracking: defaults to \u201cchanges between editions\u201d, not \u201crecent changes\u201d.</li> <li>Normalization: extract readable text and reduce boilerplate noise.</li> <li>Diffs: generate human-readable comparisons and a changes feed.</li> <li>Guardrails: no medical interpretation; the system reports \u201ctext changed\u201d and where.</li> </ul>"},{"location":"operations/methods-note-outline/#research-exports-metadata-only","title":"Research exports (metadata-only)","text":"<ul> <li>Public export manifest describes formats and limits.</li> <li>Exports include:</li> <li>snapshot metadata export (no raw HTML),</li> <li>change event export (no diff bodies).</li> <li>Intended uses:</li> <li>reproducible citations in papers/articles,</li> <li>quantitative analysis of guidance drift without redistributing full copyrighted page bodies.</li> </ul>"},{"location":"operations/methods-note-outline/#limitations-and-failure-modes-be-explicit","title":"Limitations and failure modes (be explicit)","text":"<ul> <li>Coverage gaps: not all pages captured; crawling can fail; scope is limited.</li> <li>Replay fidelity: some JS assets or third-party embeds may not replay.</li> <li>Temporal resolution: annual editions mean \u201cbetween-edition changes\u201d are not real-time updates.</li> <li>Non-authoritative: content is archival and may be outdated or superseded.</li> </ul>"},{"location":"operations/methods-note-outline/#ethics-governance-and-corrections","title":"Ethics, governance, and corrections","text":"<ul> <li>Governance pages define:</li> <li>inclusion criteria,</li> <li>correction workflow and response expectations,</li> <li>takedown/opt-out process,</li> <li>changelog discipline.</li> </ul>"},{"location":"operations/methods-note-outline/#results-descriptive-no-interpretation","title":"Results (descriptive; no interpretation)","text":"<p>Suggested content:</p> <ul> <li>Coverage counts (sources, snapshots, pages).</li> <li>Example: a single URL\u2019s timeline across editions.</li> <li>Example: change feed categories (new page, updated, removed/redirected).</li> <li>Usage signals (aggregated counts only; no personal identifiers).</li> </ul>"},{"location":"operations/methods-note-outline/#discussion-what-this-enables","title":"Discussion (what this enables)","text":"<ul> <li>Reproducibility for researchers (date-stamped citations).</li> <li>Accountability and auditability for journalists and educators.</li> <li>Public-interest infrastructure value without claiming authority.</li> </ul>"},{"location":"operations/methods-note-outline/#conclusion","title":"Conclusion","text":"<ul> <li>Reiterate that the contribution is infrastructure for temporal provenance and discoverability.</li> <li>State planned work at a high level:</li> <li>more sources within scope,</li> <li>improved diff quality,</li> <li>versioned dataset releases (if/when adopted).</li> </ul>"},{"location":"operations/methods-note-outline/#figures-tables-plan","title":"Figures / tables (plan)","text":"<p>1) Architecture diagram (capture \u2192 WARC \u2192 indexing \u2192 API/UI \u2192 exports). 2) Timeline figure for one URL (captures over time). 3) Coverage table (sources + first/last capture + snapshot counts). 4) Example change comparison screenshot (with \u201cdescriptive only\u201d labeling).</p>"},{"location":"operations/methods-note-outline/#appendix-links-public","title":"Appendix / links (public)","text":"<ul> <li>Project site: <code>https://healtharchive.ca</code></li> <li>Governance: <code>https://healtharchive.ca/governance</code></li> <li>Methods: <code>https://healtharchive.ca/methods</code></li> <li>Changes + compare: <code>https://healtharchive.ca/changes</code>, <code>https://healtharchive.ca/compare</code></li> <li>Exports: <code>https://healtharchive.ca/exports</code></li> <li>API manifest: <code>https://api.healtharchive.ca/api/exports</code></li> </ul>"},{"location":"operations/monitoring-and-alerting/","title":"Monitoring &amp; Alerting Strategy - Annual Crawl Campaign","text":"<p>Last Updated: 2026-02-06</p>"},{"location":"operations/monitoring-and-alerting/#overview","title":"Overview","text":"<p>This document defines the monitoring strategy for the HealthArchive annual crawl campaign. We use a combination of systemd timers, Python scripts, and Prometheus <code>node_exporter</code> textfile collectors to expose custom metrics about crawl health, restart stability, and progress.</p>"},{"location":"operations/monitoring-and-alerting/#metric-sources","title":"Metric Sources","text":"<p>Custom metrics are written to the node_exporter textfile collector directory:</p> <ul> <li><code>/var/lib/node_exporter/textfile_collector/</code></li> </ul> <p>Primary files (single-VPS annual campaign):</p> <ul> <li><code>healtharchive_crawl.prom</code></li> <li>Written by <code>scripts/vps-crawl-metrics-textfile.py</code></li> <li>Triggered every 1 minute by <code>healtharchive-crawl-metrics.timer</code></li> <li><code>healtharchive_storage_hotpath_auto_recover.prom</code></li> <li>Written by <code>scripts/vps-storage-hotpath-auto-recover.py</code></li> <li>Triggered every 1 minute by <code>healtharchive-storage-hotpath-auto-recover.timer</code> (sentinel-gated)</li> <li><code>healtharchive_worker_auto_start.prom</code></li> <li>Written by <code>scripts/vps-worker-auto-start.py</code></li> <li>Triggered every 2 minutes by <code>healtharchive-worker-auto-start.timer</code> (sentinel-gated)</li> </ul>"},{"location":"operations/monitoring-and-alerting/#key-metric-families","title":"Key Metric Families","text":"Metric Name Type Description <code>healtharchive_crawl_running_jobs</code> Gauge Count of currently active jobs in the DB. <code>healtharchive_worker_active</code> Gauge 1 = worker systemd unit is active. <code>healtharchive_jobs_pending_crawl</code> Gauge Count of jobs in <code>status in (queued, retryable)</code>. <code>healtharchive_jobs_infra_error_recent_total{minutes=\"10\"}</code> Gauge Count of jobs recently failing due to infra errors (windowed). <code>healtharchive_worker_should_be_running</code> Gauge 1 = pending crawl jobs exist and Storage Box mount is readable. <code>healtharchive_crawl_running_job_state_file_ok</code> Gauge 1 = <code>.archive_state.json</code> is readable and valid. 0 = Probe failed (SSHFS/Permissions issue). <code>healtharchive_crawl_running_job_container_restarts_done</code> Gauge Cumulative count of Zimit container restarts for the current job. <code>healtharchive_crawl_running_job_last_progress_age_seconds</code> Gauge Time since the last \"pages crawled\" increment in the logs. <code>healtharchive_crawl_running_job_stalled</code> Gauge 1 = Progress stalled &gt; 1 hour. <code>healtharchive_crawl_running_job_output_dir_ok</code> Gauge 1 = Output directory is accessible. <code>healtharchive_crawl_annual_pending_job_output_dir_writable{source,job_id,status,year}</code> Gauge 1 = Queued/retryable annual job output dir would be writable by the worker user (permission drift detection). <code>healtharchive_crawl_running_job_log_probe_ok</code> Gauge 1 = Combined log file is readable. <code>healtharchive_crawl_running_job_crawl_rate_ppm</code> Gauge Pages per minute crawl rate (from crawlStatus log window). <code>healtharchive_crawl_running_job_new_crawl_phase_count</code> Gauge Count of <code>New Crawl Phase</code> stage starts seen in the current combined-log tail window. <code>healtharchive_crawl_running_job_progress_known</code> Gauge 1 = Progress metrics parsed from crawlStatus logs. <code>healtharchive_crawl_metrics_timestamp_seconds</code> Gauge Unix timestamp when metrics were last written. <code>healtharchive_jobs_infra_error_recent_total{window=\"10m\"}</code> Gauge Count of jobs with infra errors in rolling window."},{"location":"operations/monitoring-and-alerting/#alerting-thresholds","title":"Alerting Thresholds","text":"<p>Alerts are defined in:</p> <ul> <li><code>ops/observability/alerting/healtharchive-alerts.yml</code></li> </ul>"},{"location":"operations/monitoring-and-alerting/#1-worker-availability-high-signal","title":"1) Worker availability (high-signal)","text":"<p>Alert: <code>HealthArchiveWorkerDownWhileJobsPending</code></p> <ul> <li>Threshold: <code>healtharchive_worker_should_be_running == 1 and healtharchive_worker_active == 0</code> for 10m.</li> <li>Meaning: There is pending crawl work and storage appears usable, but the worker service is down.</li> <li>Action: Check <code>healtharchive-worker.service</code> logs and Storage Box hot-path health. If automation is enabled, check <code>healtharchive-storage-hotpath-auto-recover.timer</code> + state.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#2-sshfsmount-stability","title":"2) SSHFS/Mount Stability","text":"<p>Alert: <code>HealthArchiveCrawlOutputDirUnreadable</code> (and related probe alerts)</p> <ul> <li>Threshold: <code>healtharchive_crawl_running_job_output_dir_ok == 0</code> for 2m.</li> <li>Meaning: A running crawl job cannot access its output directory. Errno 107 typically means a stale SSHFS/FUSE mount.</li> <li>Action: Follow the Storage Box stale mount recovery playbook and/or ensure hot-path auto-recover is enabled and succeeding.</li> </ul> <p>Alert: <code>HealthArchiveAnnualOutputDirNotWritable</code></p> <ul> <li>Threshold: <code>healtharchive_crawl_annual_pending_job_output_dir_writable == 0</code> for 10m (probe-user gated).</li> <li>Meaning: A queued/retryable annual job output dir is not writable for the worker user. This typically indicates permission drift (Errno 13) or a stale mount (Errno 107) and can consume retry budget if not fixed before the job starts.</li> <li>Action: Run crawl preflight checks for the specific job output dir mount and writability; re-apply annual output tiering if needed.</li> </ul> <p>Alert: <code>HealthArchiveStorageHotpathStaleUnrecovered</code></p> <ul> <li>Threshold: <code>healtharchive_storage_hotpath_auto_recover_detected_targets &gt; 0</code> for 10m (when the automation is enabled).</li> <li>Meaning: Hot-path auto-recover still sees stale/unreadable paths after 10 minutes.</li> <li>Action: Inspect <code>/srv/healtharchive/ops/watchdog/storage-hotpath-auto-recover.json</code> and consider manual unmount + tiering re-apply.</li> </ul> <p>Alert: <code>HealthArchiveStorageHotpathApplyFailedPersistent</code></p> <ul> <li>Threshold: watchdog enabled, at least one apply attempt, <code>last_apply_ok == 0</code>, and last apply timestamp older than 24h (for 30m).</li> <li>Meaning: Hot-path auto-recover apply mode has remained in a failed terminal state for over a day.</li> <li>Action: Inspect <code>/srv/healtharchive/ops/watchdog/storage-hotpath-auto-recover.json</code> (<code>last_apply_errors</code>, <code>last_apply_warnings</code>), then follow stale mount recovery playbook steps and re-run a controlled dry-run/apply verification.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#3-restart-stability","title":"3) Restart stability","text":"<p>Alert: <code>HealthArchiveCrawlContainerRestartsHigh</code></p> <ul> <li>Threshold: <code>healtharchive_crawl_running_job_container_restarts_done &gt;= 10</code> (for 15m).</li> <li>Meaning: The crawler is requiring many adaptive container restarts; this can be a normal resiliency mechanism, but sustained growth can indicate timeouts or I/O instability.</li> <li>Action: Review worker logs and combined logs around restarts; check for repeated timeouts on the same URL or storage errors.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#4-progress-stalls","title":"4) Progress Stalls","text":"<p>Alert: <code>HealthArchiveCrawlStalled</code></p> <ul> <li>Threshold: <code>healtharchive_crawl_running_job_stalled == 1</code> (for 30m).</li> <li>Meaning: The crawler is running but hasn't archived a new page in over an hour.</li> <li>Action: Check if the crawler is stuck on a massive PDF or looped trap.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#5-crawl-rate-per-source-throughput","title":"5) Crawl Rate (per-source throughput)","text":"<p>Alerts: <code>HealthArchiveCrawlRateSlowHC</code>, <code>HealthArchiveCrawlRateSlowPHAC</code>, <code>HealthArchiveCrawlRateSlowCIHR</code></p> <ul> <li>Thresholds (30m, when progress is known):</li> <li>HC: <code>healtharchive_crawl_running_job_crawl_rate_ppm{source=\"hc\"} &lt; 1.5</code></li> <li>PHAC: <code>healtharchive_crawl_running_job_crawl_rate_ppm{source=\"phac\"} &lt; 1.5</code></li> <li>CIHR: <code>healtharchive_crawl_running_job_crawl_rate_ppm{source=\"cihr\"} &lt; 3</code></li> <li>Meaning: The crawler is running but source-specific throughput is below expected long-run baselines.</li> <li>Action: Check combined logs for phase churn/retries, verify mount and network health, then tune source profile values in <code>job_registry.py</code> if sustained and reproducible.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#6-infrastructure-errors","title":"6) Infrastructure Errors","text":"<p>Alert: <code>HealthArchiveInfraErrorsHigh</code></p> <ul> <li>Threshold: <code>healtharchive_jobs_infra_error_recent_total{window=\"10m\"} &gt;= 3</code> (for 5m).</li> <li>Meaning: Multiple jobs are failing due to infrastructure errors (errno 107 stale mount, permission denied, etc.) in a short window.</li> <li>Action: Check Storage Box mount health, run hot-path recovery, verify output directory permissions.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#7-new-crawl-phase-churn","title":"7) New Crawl-Phase Churn","text":"<p>Alert: <code>HealthArchiveCrawlNewPhaseChurn</code></p> <ul> <li>Threshold: <code>healtharchive_crawl_running_job_new_crawl_phase_count &gt;= 3</code> (for 30m).</li> <li>Meaning: The crawler is repeatedly falling back to <code>New Crawl Phase</code> in the current log window, which can indicate resume/frontier instability and long-run completeness efficiency risk.</li> <li>Action: Review combined log transitions, resume YAML presence, and storage stability before adjusting crawler thresholds.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#8-metrics-freshness","title":"8) Metrics Freshness","text":"<p>Alert: <code>HealthArchiveCrawlMetricsStale</code></p> <ul> <li>Threshold: <code>(time() - healtharchive_crawl_metrics_timestamp_seconds) &gt; 600</code> (for 5m).</li> <li>Meaning: The crawl metrics textfile hasn't been updated in over 10 minutes.</li> <li>Action: Check if <code>healtharchive-crawl-metrics.timer</code> is running and <code>vps-crawl-metrics-textfile.py</code> is succeeding.</li> </ul>"},{"location":"operations/monitoring-and-alerting/#indexing-monitoring","title":"Indexing Monitoring","text":"<p>Indexing runs after the crawl completes.</p> <ul> <li>Active Indexing: Check worker logs for <code>Indexing for job &lt;ID&gt; completed successfully</code>.</li> <li>Failure Detection: <code>healtharchive_job_crawl_status{status=\"completed\"}</code> AND <code>healtharchive_job_indexed_pages == 0</code> for &gt; 1 hour indicates a broken pipeline.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/","title":"Monitoring, uptime, and CI checklist","text":"<p>This file pulls together the ongoing operations aspects of the project:</p> <ul> <li>Uptime and health monitoring.</li> <li>Metrics and alerting.</li> <li>CI enforcement and branch protection.</li> </ul> <p>It is meant to complement:</p> <ul> <li><code>../deployment/hosting-and-live-server-to-dos.md</code></li> <li><code>../deployment/staging-rollout-checklist.md</code></li> <li><code>../deployment/production-rollout-checklist.md</code></li> <li><code>service-levels.md</code> \u2014 for SLO targets and commitments</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#0-implementation-steps-ci-external-monitoring","title":"0. Implementation steps (CI + external monitoring)","text":"<p>This section is a practical, sequential setup plan for enforcing CI and configuring external monitoring in the real world (GitHub + UptimeRobot, etc.).</p> <p>Important: most of this is not \u201ccode you deploy\u201d \u2014 it is configuration in:</p> <ul> <li>GitHub repository settings (branch protection)</li> <li>Your monitoring provider (UptimeRobot, Healthchecks, etc.)</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-0-baseline-audit-decisions-operator","title":"Step 0 \u2014 Baseline audit + decisions (operator)","text":"<p>Objective: avoid duplicate monitors, avoid alert noise, and avoid \u201cunknown settings drift\u201d.</p> <ol> <li>Inventory current external monitors (UptimeRobot, etc.):</li> <li>Monitor name</li> <li>URL</li> <li>Interval + timeout</li> <li>Alert contacts/routes</li> <li>Any keyword/body assertions</li> <li>Decide alert routing:</li> <li>Which alerts should page you vs. just email (recommended: only \u201csite down\u201d      pages; everything else emails).</li> <li>Decide the <code>main</code> branch policy:</li> <li>Mode A \u2014 Solo-fast (recommended for this project right now): direct pushes to <code>main</code>.<ul> <li>CI still runs on every push to <code>main</code>.</li> <li>Deploys are gated by \u201cgreen main\u201d + VPS verification steps (below).</li> </ul> </li> <li>Mode B \u2014 Multi-committer (defer until needed): PR-only merges into <code>main</code> with required      status checks + code owner review (track in <code>../planning/roadmap.md</code>).</li> <li>Switch to Mode B when there is more than one regular committer, or when you want stricter      enforcement than \u201cgreen main + local hooks\u201d.</li> </ol> <p>Verification:</p> <ul> <li>You can point to a quick note (even in a personal doc) listing current   monitors + what each covers.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-1-verify-ci-runs-on-main-pushes-operator","title":"Step 1 \u2014 Verify CI runs on <code>main</code> pushes (operator)","text":"<p>Objective: ensure CI runs on pushes to <code>main</code> so you can treat \u201cgreen main\u201d as the deploy gate.</p> <ol> <li>Confirm GitHub Actions workflows are enabled:</li> <li>Repo \u2192 Actions \u2192 ensure workflows are enabled (not disabled by org/fork policy).</li> <li>Push a trivial commit to <code>main</code> (e.g. a doc tweak).</li> <li>Confirm the workflow runs and passes on that commit.</li> </ol> <p>Verification:</p> <ul> <li>GitHub Actions shows the backend CI workflow completing successfully on <code>main</code>.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#check-name-inventory-branch-protection","title":"Check name inventory (branch protection)","text":"<p>Use stable workflow/job check names shown in GitHub\u2019s UI. Avoid renaming workflow/job IDs after you start requiring them.</p> <p>As of 2026-02-06 (solo-dev profile), the checks are used as follows:</p> <ul> <li>Backend repo (required on <code>main</code>): <code>Backend CI / test</code></li> <li>Backend repo (not required): <code>Backend CI / e2e-smoke</code> (push/manual only), <code>Backend CI (Full) / test-full</code> (nightly/manual)</li> <li>Frontend repo (when protecting frontend <code>main</code>): <code>Frontend CI / lint-and-test</code> (required), <code>Frontend CI / e2e-smoke</code> (optional)</li> <li>Datasets repo (when protecting datasets <code>main</code>): <code>Datasets CI / lint</code> (required)</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-1b-end-to-end-smoke-checks-ci","title":"Step 1b \u2014 End-to-end smoke checks (CI)","text":"<p>Objective: catch regressions where the apps \u201cbuild\u201d but user\u2011critical paths fail at runtime.</p> <p>What the smoke does:</p> <ul> <li>Starts the backend locally (uvicorn) with a tiny seeded SQLite + WARC dataset.</li> <li>Builds and starts the frontend locally (<code>next start</code>) pointing at that backend.</li> <li>Runs <code>healtharchive-backend/scripts/verify_public_surface.py</code> against:</li> <li>Frontend: <code>/archive</code>, <code>/fr/archive</code>, <code>/snapshot/{id}</code>, <code>/fr/snapshot/{id}</code>, and other key pages</li> <li>API: <code>/api/health</code>, <code>/api/sources</code>, <code>/api/search</code>, <code>/api/snapshot/{id}</code>, <code>/api/usage</code>, <code>/api/exports</code>, <code>/api/changes</code></li> <li>Replay (pywb) is intentionally skipped in CI (<code>--skip-replay</code>).</li> <li>The verifier includes minimal \u201cnot just 200\u201d assertions:</li> <li><code>/archive</code> pages must include a stable Next.js marker (<code>/_next/static/</code>).</li> <li><code>/snapshot/{id}</code> pages must include the snapshot title returned by the API.</li> </ul> <p>Where it runs:</p> <ul> <li>Backend repo CI: <code>.github/workflows/backend-ci.yml</code> job <code>e2e-smoke</code></li> <li>Tests backend changes against latest frontend <code>main</code>.</li> <li>Runs on <code>main</code> pushes / manual runs (kept out of the PR gate to avoid blocking development on flaky cross-repo or environment issues).</li> <li>Frontend repo CI: https://github.com/jerdaw/healtharchive-frontend/blob/main/.github/workflows/frontend-ci.yml (job: <code>e2e-smoke</code>)</li> <li>Tests frontend changes against latest backend <code>main</code>.</li> <li>If cross-repo checkout fails (private repo), set a repo secret:</li> <li><code>HEALTHARCHIVE_CI_READ_TOKEN</code> (PAT with read access)</li> </ul> <p>Local reproduction (from the mono\u2011repo workspace where the repos are siblings):</p> <pre><code>cd healtharchive-frontend &amp;&amp; npm ci\ncd ../healtharchive-backend\nmake venv\n./scripts/ci-e2e-smoke.sh --frontend-dir ../healtharchive-frontend\n</code></pre> <p>On failure, the script prints the tail of the backend/frontend logs that it writes under:</p> <ul> <li><code>healtharchive-backend/.tmp/ci-e2e-smoke/</code></li> </ul> <p>CI also uploads the smoke logs as a GitHub Actions artifact on failure:</p> <ul> <li>Backend repo: <code>backend-e2e-smoke-artifacts</code></li> <li>Frontend repo: <code>frontend-e2e-smoke-artifacts</code></li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-2-solo-fast-deploy-gate-operator-recommended","title":"Step 2 \u2014 Solo-fast deploy gate (operator; recommended)","text":"<p>Objective: prevent broken deploys by only deploying when <code>main</code> is green.</p> <p>Workflow (recommended):</p> <ol> <li>Local guardrails (recommended while branch protections are relaxed):</li> <li>Run checks before you push:<ul> <li>From the mono-repo root: <code>make check</code></li> <li>Or per-repo:</li> <li><code>healtharchive-backend: make check</code></li> <li><code>healtharchive-frontend: npm run check</code></li> <li><code>healtharchive-datasets: make check</code></li> <li>Optional before deploys: <code>healtharchive-backend: make check-full</code></li> </ul> </li> <li>Optional but recommended: install pre-push hooks so you can't forget:<ul> <li>Backend: <code>scripts/install-pre-push-hook.sh</code> (set <code>HA_PRE_PUSH_FULL=1</code> for <code>make check-full</code>)</li> <li>Frontend: https://github.com/jerdaw/healtharchive-frontend/blob/main/scripts/install-pre-push-hook.sh</li> <li>Datasets: https://github.com/jerdaw/healtharchive-datasets/blob/main/scripts/install-pre-push-hook.sh</li> </ul> </li> <li>Push to <code>main</code>.</li> <li>Wait for GitHub Actions to go green on that commit.</li> <li>Deploy on the VPS:</li> <li>Recommended (one command): <code>./scripts/vps-deploy.sh --apply --baseline-mode live</code><ul> <li>Includes baseline drift + public-surface verify by default.</li> </ul> </li> <li>If you use a local alias like <code>dodeploy</code>, ensure you still run:<ul> <li><code>./scripts/check_baseline_drift.py --mode live</code></li> <li><code>./scripts/verify_public_surface.py</code></li> </ul> </li> </ol> <p>Verification:</p> <ul> <li>The VPS deploy completes and both verification scripts pass.</li> </ul> <p>Future (tighten later):</p> <ul> <li>When there are multiple committers or when you want stricter enforcement, switch to PR-only merges   and require the backend/frontend checks in branch protection (track in <code>../planning/roadmap.md</code>).</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-3-external-uptime-monitoring-operator-uptimerobot-settings","title":"Step 3 \u2014 External uptime monitoring (operator; UptimeRobot settings)","text":"<p>Objective: catch real, user-visible failures with minimal noise.</p> <p>Recommended minimal monitor set (HTTP(s) checks):</p> <ol> <li>API health</li> <li>URL: <code>https://api.healtharchive.ca/api/health</code></li> <li>Expected: HTTP 200</li> <li>Interval: 1\u20135 minutes</li> <li>Note: backend supports <code>HEAD /api/health</code> for providers that default to <code>HEAD</code>.</li> <li>Frontend integration</li> <li>URL: <code>https://www.healtharchive.ca/archive</code></li> <li>Expected: HTTP 200</li> <li>Interval: 5 minutes</li> <li>Optional: keyword assertion (stable string that should always appear).</li> <li>Replay base URL (optional but recommended if you rely on replay)</li> <li>URL: <code>https://replay.healtharchive.ca/</code></li> <li>Expected: HTTP 200</li> <li>Interval: 5\u201310 minutes</li> </ol> <p>Optional, higher-signal replay monitoring (recommended later):</p> <ul> <li>After annual jobs exist and are replay-indexed, add 1\u20133 \u201cknown-good replay URL\u201d   monitors (one per source or one total) pointing at a stable capture inside a   <code>job-&lt;id&gt;</code> collection. Update them annually.</li> </ul> <p>Verification:</p> <ul> <li>Optional pre-flight from the VPS (or any machine with internet + <code>curl</code>):</li> <li><code>./scripts/smoke-external-monitors.sh</code></li> <li>All monitors show \u201cUp\u201d.</li> <li>Alerting routes work (optional test: intentionally break a monitor briefly).</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-4-timer-ran-monitoring-healthchecks-style-optional-but-recommended","title":"Step 4 \u2014 Timer ran monitoring (Healthchecks-style; optional but recommended)","text":"<p>Objective: get alerted if systemd timers stop running (even when the site is up).</p> <p>This is intentionally optional: you already have high-value uptime checks in Step 3, but \"timer ran\" alerts are useful for catching silent failures (timer disabled, unit failing, disk low refusal, etc.).</p> <p>Recommended checks to monitor:</p> <ul> <li>Baseline drift check (weekly):</li> <li><code>healtharchive-baseline-drift-check.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_BASELINE_DRIFT</code></li> <li>Public surface verification (daily):</li> <li><code>healtharchive-public-surface-verify.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_PUBLIC_VERIFY</code></li> <li>Replay reconcile (daily):</li> <li><code>healtharchive-replay-reconcile.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_REPLAY_RECONCILE</code></li> <li>Change tracking (daily):</li> <li><code>healtharchive-change-tracking.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_CHANGE_TRACKING</code></li> <li>Annual scheduler (yearly):</li> <li><code>healtharchive-schedule-annual.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_SCHEDULE_ANNUAL</code></li> <li>Annual search verify (daily, idempotent once per year):</li> <li><code>healtharchive-annual-search-verify.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_ANNUAL_SEARCH_VERIFY</code></li> <li>Coverage guardrails (daily):</li> <li><code>healtharchive-coverage-guardrails.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_COVERAGE_GUARDRAILS</code></li> <li>Replay smoke tests (daily):</li> <li><code>healtharchive-replay-smoke.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_REPLAY_SMOKE</code></li> <li>Cleanup automation (weekly):</li> <li><code>healtharchive-cleanup-automation.timer</code></li> <li>Ping variable: <code>HEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION</code></li> </ul> <p>Note: avoid pinging high-frequency timers (e.g., crawl metrics, crawl auto-recover) to reduce noise.</p> <p>Implementation approach (VPS):</p> <ol> <li>Create a check in your Healthchecks provider for each timer you care about.</li> <li>Store ping URLs only on the VPS in a root-owned env file:</li> <li><code>/etc/healtharchive/healthchecks.env</code> (mode 0600, root:root)</li> <li>Note: this file may be shared across multiple automations; it is OK to keep both:<ul> <li>legacy <code>HC_*</code> variables (DB backup + disk check)</li> <li>newer <code>HEALTHARCHIVE_HC_PING_*</code> variables (systemd unit templates)</li> </ul> </li> <li>Ensure the installed systemd units source that env file:</li> <li><code>EnvironmentFile=-/etc/healtharchive/healthchecks.env</code></li> <li>Ensure the unit uses the wrapper so ping URLs never appear in unit files:</li> <li><code>/opt/healtharchive-backend/scripts/systemd-healthchecks-wrapper.sh</code></li> </ol> <p>Safety posture:</p> <ul> <li>Pinging is best-effort; ping failures do not fail jobs.</li> <li>Removing <code>/etc/healtharchive/healthchecks.env</code> disables pings immediately.</li> </ul> <p>Verification (VPS):</p> <ul> <li>Add a temporary ping URL for one service, then run:</li> <li><code>sudo systemctl start healtharchive-replay-reconcile-dry-run.service</code></li> <li>Confirm the check receives a ping.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-5-automated-post-campaign-search-verification-capture-optional","title":"Step 5 \u2014 Automated post-campaign search verification capture (optional)","text":"<p>Objective: once the annual campaign becomes search-ready, automatically capture golden-query <code>/api/search</code> JSON into a year-tagged directory for later diffing and audits.</p> <p>What gets captured (recommended minimal set):</p> <ul> <li><code>annual-status.json</code> and a human-readable <code>annual-status.txt</code></li> <li><code>meta.txt</code> (capture metadata)</li> <li><code>&lt;query&gt;.pages.json</code> + <code>&lt;query&gt;.snapshots.json</code> for your golden query list</li> </ul> <p>Implementation approach (VPS, systemd):</p> <ul> <li>This repo provides an optional daily timer that is idempotent:</li> <li>If the campaign is not ready, it exits 0 (no alert noise).</li> <li>If artifacts already exist for the current year/run-id, it exits 0.</li> </ul> <p>Install and enable:</p> <ol> <li>Copy templates onto the VPS (see <code>../deployment/systemd/README.md</code>):</li> <li><code>healtharchive-annual-search-verify.service</code></li> <li><code>healtharchive-annual-search-verify.timer</code></li> <li>Reload systemd:</li> <li><code>sudo systemctl daemon-reload</code></li> <li>Enable the timer:</li> <li><code>sudo systemctl enable --now healtharchive-annual-search-verify.timer</code></li> </ol> <p>Artifacts:</p> <ul> <li>Default location: <code>/srv/healtharchive/ops/search-eval/&lt;year&gt;/final/</code></li> <li>To force re-run for a year: delete that directory and re-run the service.</li> </ul> <p>Verification (VPS):</p> <ul> <li>Force-run once:</li> <li><code>sudo systemctl start healtharchive-annual-search-verify.service</code></li> <li>Confirm it either:</li> <li>exits 0 quickly (not ready), or</li> <li>creates artifacts under <code>/srv/healtharchive/ops/search-eval/&lt;year&gt;/final/</code>.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#step-6-optional-github-driven-deploys-cd-infrastructure-project","title":"Step 6 \u2014 Optional GitHub-driven deploys (CD) (infrastructure project)","text":"<p>Objective: reduce deploy mistakes without expanding the production attack surface.</p> <p>Recommended posture for this project (single VPS, no staging backend):</p> <ul> <li>Keep deployments manual on the VPS.</li> <li>Use the deploy helper script:</li> <li><code>scripts/vps-deploy.sh</code> (dry-run default; <code>--apply</code> to deploy)</li> </ul> <p>Rationale:</p> <ul> <li>Avoids storing production access secrets in GitHub.</li> <li>Avoids granting passwordless sudo/SSH access to GitHub Actions.</li> <li>Keeps the operational path \u201cboring\u201d and easy to reason about.</li> </ul> <p>Verification (VPS):</p> <ul> <li>Dry-run: <code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-deploy.sh</code></li> <li>Apply: <code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-deploy.sh --apply</code></li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#1-uptime-and-health-checks","title":"1. Uptime and health checks","text":""},{"location":"operations/monitoring-and-ci-checklist/#11-backend-health-endpoint","title":"1.1 Backend health endpoint","text":"<p>Primary health endpoint:</p> <ul> <li><code>GET https://api.healtharchive.ca/api/health</code></li> <li><code>HEAD https://api.healtharchive.ca/api/health</code> (supported; some uptime tools use <code>HEAD</code>)</li> </ul> <p>Some uptime providers issue <code>HEAD</code> requests by default. The backend supports <code>HEAD /api/health</code> so monitors may use either method.</p> <p>Expected behavior:</p> <ul> <li>HTTP 200</li> <li>JSON body like:</li> </ul> <pre><code>{\n  \"status\": \"ok\",\n  \"checks\": {\n    \"db\": \"ok\",\n    \"jobs\": {\n      \"queued\": 0,\n      \"indexed\": 5,\n      \"failed\": 0\n    },\n    \"snapshots\": {\n      \"total\": 123\n    }\n  }\n}\n</code></pre> <p>Suggested uptime monitor:</p> <ul> <li>Configure an external service (UptimeRobot, healthchecks.io, your cloud   provider) to poll:</li> <li><code>https://api.healtharchive.ca/api/health</code></li> <li>If you later add a separate staging API, also poll:</li> <li><code>https://api-staging.healtharchive.ca/api/health</code></li> <li>Alert on:</li> <li>5xx responses.</li> <li>Timeouts.</li> <li>Repeated failures over a short window.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#12-frontend-integration-check","title":"1.2 Frontend + integration check","text":"<p>To verify the frontend and backend integration:</p> <ul> <li><code>GET https://healtharchive.ca/archive</code></li> </ul> <p>Expected behavior:</p> <ul> <li>HTTP 200.</li> <li>Page renders with:</li> <li>Filters header showing <code>Filters (live API)</code> when backend is up.</li> <li>Real search results when snapshots exist.</li> </ul> <p>Suggested uptime monitor:</p> <ul> <li>Configure a separate check that:</li> <li>Downloads <code>https://healtharchive.ca/archive</code>.</li> <li>Optionally asserts presence of a known string in the body (e.g.     \u201cHealthArchive.ca\u201d or \u201cBrowse &amp; search demo snapshots\u201d).</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#13-replay-uptime-check-optional-but-recommended-if-replay-is-in-use","title":"1.3 Replay uptime check (optional but recommended if replay is in use)","text":"<p>To ensure replay is reachable:</p> <ul> <li><code>GET https://replay.healtharchive.ca/</code></li> </ul> <p>If you want a higher-signal check (recommended once you have stable annual jobs):</p> <ul> <li>Monitor a known-good replay URL inside a specific <code>job-&lt;id&gt;</code> collection:</li> <li><code>https://replay.healtharchive.ca/job-&lt;id&gt;/&lt;original_url&gt;</code></li> <li>Choose an original URL that is stable and low-cost to serve.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#2-metrics-and-alerting","title":"2. Metrics and alerting","text":""},{"location":"operations/monitoring-and-ci-checklist/#21-metrics-endpoint","title":"2.1 Metrics endpoint","text":"<p>Metrics are exposed at:</p> <ul> <li><code>GET https://api.healtharchive.ca/metrics</code></li> <li>If you later add a separate staging API:</li> <li><code>GET https://api-staging.healtharchive.ca/metrics</code></li> </ul> <p>This endpoint is protected by <code>HEALTHARCHIVE_ADMIN_TOKEN</code>. In Prometheus or a similar system, you will typically:</p> <ul> <li>Store the token in a secure place (e.g. Prometheus config / secret).</li> <li>Pass it via <code>Authorization: Bearer &lt;token&gt;</code> or <code>X-Admin-Token</code> header.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#22-key-metrics","title":"2.2 Key metrics","text":"<p>The metrics endpoint exposes, among others:</p> <ul> <li>Job counts:</li> </ul> <pre><code>healtharchive_jobs_total{status=\"queued\"} 1\nhealtharchive_jobs_total{status=\"indexed\"} 5\nhealtharchive_jobs_total{status=\"failed\"} 0\n</code></pre> <ul> <li>Cleanup status:</li> </ul> <pre><code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"none\"} 10\nhealtharchive_jobs_cleanup_status_total{cleanup_status=\"temp_cleaned\"} 3\n</code></pre> <ul> <li>Snapshot counts:</li> </ul> <pre><code>healtharchive_snapshots_total 123\nhealtharchive_snapshots_total{source=\"hc\"} 80\nhealtharchive_snapshots_total{source=\"phac\"} 43\n</code></pre> <ul> <li>Page\u2011level crawl metrics:</li> </ul> <pre><code>healtharchive_jobs_pages_crawled_total 45678\nhealtharchive_jobs_pages_crawled_total{source=\"hc\"} 30000\nhealtharchive_jobs_pages_failed_total 120\nhealtharchive_jobs_pages_failed_total{source=\"hc\"} 30\n</code></pre> <ul> <li>Search metrics (per-process; reset on restart):</li> </ul> <pre><code>healtharchive_search_requests_total 123\nhealtharchive_search_errors_total 0\nhealtharchive_search_duration_seconds_bucket{le=\"0.3\"} 100\nhealtharchive_search_mode_total{mode=\"relevance_fts\"} 80\nhealtharchive_search_mode_total{mode=\"relevance_fallback\"} 25\nhealtharchive_search_mode_total{mode=\"relevance_fuzzy\"} 5\nhealtharchive_search_mode_total{mode=\"boolean\"} 2\nhealtharchive_search_mode_total{mode=\"url\"} 3\nhealtharchive_search_mode_total{mode=\"newest\"} 8\n</code></pre>"},{"location":"operations/monitoring-and-ci-checklist/#23-example-alert-ideas-prometheusstyle","title":"2.3 Example alert ideas (Prometheus\u2011style)","text":"<p>These are examples, not full rules, but can guide what you set up:</p> <ul> <li> <p>Crawl state file unhealthy:</p> </li> <li> <p>Alert if <code>healtharchive_crawl_running_job_state_file_ok==1</code> but     <code>healtharchive_crawl_running_job_state_parse_ok==0</code> for &gt;10m.</p> </li> <li> <p>Crawl stalled:</p> </li> <li> <p>Alert if <code>healtharchive_crawl_running_job_stalled==1</code> for &gt;30m.</p> </li> <li> <p>Crawl completed but indexing not starting:</p> </li> <li> <p>Alert if <code>healtharchive_indexing_pending_job_max_age_seconds</code> exceeds your SLA (e.g., &gt;1h).</p> </li> <li> <p>High job failure rate:</p> </li> <li> <p>Alert if <code>healtharchive_jobs_total{status=\"failed\"}</code> jumps unexpectedly     over a sliding window.</p> </li> <li> <p>No new snapshots over time:</p> </li> <li> <p>Alert if <code>increase(healtharchive_snapshots_total[24h]) == 0</code> while jobs     are being created, indicating indexing or crawl issues.</p> </li> <li> <p>Cleanup not happening:</p> </li> <li> <p>Alert if <code>healtharchive_jobs_cleanup_status_total{cleanup_status=\"none\"}</code>     grows without bound while <code>temp_cleaned</code> remains flat.</p> </li> </ul> <p>Tune these based on actual volumes and acceptable thresholds.</p>"},{"location":"operations/monitoring-and-ci-checklist/#3-ci-and-branch-protection","title":"3. CI and branch protection","text":""},{"location":"operations/monitoring-and-ci-checklist/#31-github-actions-workflows","title":"3.1 GitHub Actions workflows","text":"<p>Workflows live at:</p> <ul> <li>Backend: <code>.github/workflows/backend-ci.yml</code></li> <li>Frontend: https://github.com/jerdaw/healtharchive-frontend/blob/main/.github/workflows/frontend-ci.yml</li> </ul> <p>Each should:</p> <ul> <li>Backend:</li> <li>Run <code>make check</code>.</li> <li>Frontend:</li> <li>Install deps via <code>npm ci</code>.</li> <li>Run <code>npm run check</code>.</li> <li>Optionally run <code>npm audit --audit-level=high</code>.</li> </ul> <p>Checklist:</p> <ul> <li> Ensure workflows are enabled in GitHub:</li> <li>Open the repo on GitHub.</li> <li>Go to Actions.</li> <li>If you see \u201cWorkflows are disabled for this fork\u201d, click Enable.</li> <li> Verify that pushing to <code>main</code> or opening a PR triggers the workflows.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#32-branch-protection-on-main-backend-repo-solo-dev-profile","title":"3.2 Branch protection on <code>main</code> (backend repo, solo-dev profile)","text":"<p>Backend enforcement is currently maintained as a GitHub ruleset.</p> <p>As configured on 2026-02-06:</p> <ul> <li>Ruleset name: <code>main-protection</code></li> <li>Target branch: <code>main</code></li> <li>Enforcement status: <code>Active</code></li> <li>Bypass list: <code>Repository admin Role</code> (always allow)</li> <li>Enabled rules:</li> <li><code>Restrict deletions</code></li> <li><code>Require status checks to pass</code> with required check <code>Backend CI / test</code></li> <li><code>Block force pushes</code></li> <li>Disabled rules (intentional for solo-dev speed):</li> <li><code>Require a pull request before merging</code></li> <li>Review/approval requirements</li> <li>Code owner requirements</li> <li>Extra code scanning/code quality gates</li> </ul> <p>Important check-selection notes:</p> <ul> <li>Do not require <code>Backend CI / e2e-smoke</code> in branch protection (it does not run on pull requests).</li> <li>Do not require <code>Backend CI (Full) / test-full</code> (nightly/manual workflow).</li> </ul> <p>Verification ritual (operator, monthly or after workflow edits):</p> <ol> <li>Open GitHub \u2192 Repository \u2192 Settings \u2192 Rules \u2192 Rulesets \u2192 <code>main-protection</code>.</li> <li>Confirm required check list still contains only <code>Backend CI / test</code>.</li> <li>Confirm <code>Block force pushes</code> and <code>Restrict deletions</code> remain enabled.</li> <li>Open Actions and confirm the latest <code>Backend CI</code> run on <code>main</code> is green.</li> <li>Log any changes in an ops report note (date + what changed + why).</li> <li>Review <code>.github/migration-guard-exceptions.txt</code>:</li> <li>remove expired rules,</li> <li>ensure any active rule has a short-lived expiry and clear reason.</li> </ol> <p>Latest evidence snapshot:</p> <ul> <li>2026-02-06: <code>main-protection</code> ruleset verified active with required check <code>Backend CI / test</code>,   <code>Restrict deletions</code> enabled, and <code>Block force pushes</code> enabled.</li> </ul> <p>Future tighten-up trigger:</p> <ul> <li>If a second regular committer is added, enable PR-required merges and approval rules, then update this section.</li> </ul>"},{"location":"operations/monitoring-and-ci-checklist/#4-periodic-operations-review","title":"4. Periodic operations review","text":"<p>On a regular cadence (e.g. monthly or quarterly), review:</p> <ul> <li>Uptime logs:</li> <li>Are there recurring outages at specific times?</li> <li>Metrics:</li> <li>Are job failures spiking?</li> <li>Are snapshots growing at the expected rate?</li> <li>Is cleanup keeping up with new jobs?</li> <li>CI:</li> <li>Are workflows still running on all relevant branches?</li> <li>Do new checks or tooling need to be added?</li> </ul> <p>Recording a short \u201cops state\u201d note alongside these reviews will make future debugging and capacity planning much easier.</p>"},{"location":"operations/observability-and-private-stats/","title":"Observability + private stats (internal contract)","text":"<p>This document defines the public vs private boundaries for HealthArchive observability and \u201cprivate stats\u201d, with a bias toward:</p> <ul> <li>low maintenance / low toil</li> <li>privacy-preserving measurement (aggregate-only; no identifiers)</li> <li>no new public attack surface</li> </ul> <p>This is an internal ops document. Keep it public-safe (no secrets).</p>"},{"location":"operations/observability-and-private-stats/#1-definitions-public-vs-private-surfaces","title":"1) Definitions: public vs private surfaces","text":""},{"location":"operations/observability-and-private-stats/#public-surfaces-intentionally-public","title":"Public surfaces (intentionally public)","text":"<ul> <li>Frontend pages like <code>/status</code> and <code>/impact</code> (public reporting).</li> <li>Public API routes under <code>/api/**</code> (search, sources, snapshots, public usage window).</li> </ul>"},{"location":"operations/observability-and-private-stats/#private-surfaces-operator-only","title":"Private surfaces (operator-only)","text":"<ul> <li>Observability stack UIs (Grafana; optionally Prometheus UI).</li> <li>Admin endpoints:</li> <li><code>/api/admin/**</code></li> <li><code>/metrics</code> (Prometheus-style metrics)</li> </ul> <p>Rule: public web UI must never call or depend on <code>/api/admin/**</code> or <code>/metrics</code>.</p>"},{"location":"operations/observability-and-private-stats/#2-private-access-model-default","title":"2) Private access model (default)","text":"<p>Default approach: tailnet-only access, using Tailscale.</p> <ul> <li>Preferred: access Grafana via an SSH port-forward over Tailscale (simple, private, no Tailscale HTTPS certs required).</li> <li>Optional: use <code>tailscale serve</code> (tailnet-only HTTPS) if you want a shareable URL and you are OK with the tailnet hostname appearing in public certificate logs.</li> <li>Keep Prometheus UI loopback-only unless operators explicitly need it.</li> </ul> <p>Non-goals:</p> <ul> <li>No new public DNS records for ops tools.</li> <li>No Caddy vhosts for ops tools.</li> <li>No new public firewall openings.</li> </ul> <p>If an operator needs access without Tailscale, treat that as a deliberate security change and document it as a separate decision.</p>"},{"location":"operations/observability-and-private-stats/#21-host-footprint-dirs-secrets","title":"2.1 Host footprint (dirs + secrets)","text":"<p>These paths are conventions for operators and automation; they do not imply anything is public.</p>"},{"location":"operations/observability-and-private-stats/#ops-directories-public-safe-by-policy","title":"Ops directories (public-safe by policy)","text":"<ul> <li><code>/srv/healtharchive/ops/observability/</code></li> <li><code>dashboards/</code> \u2014 exported dashboard JSON, provisioning files (no secrets)</li> <li><code>alerting/</code> \u2014 public-safe alert rule templates/notes (no secrets)</li> <li><code>notes/</code> \u2014 public-safe operational notes</li> </ul> <p>Low-maintenance default:</p> <ul> <li>Keep Prometheus/Grafana data in distro defaults (typically <code>/var/lib/prometheus</code> and   <code>/var/lib/grafana</code>) unless you have a strong reason to relocate.</li> </ul>"},{"location":"operations/observability-and-private-stats/#secrets-root-owned-never-under-srvhealtharchiveops","title":"Secrets (root-owned; never under <code>/srv/healtharchive/ops/</code>)","text":"<ul> <li><code>/etc/healtharchive/observability/prometheus_backend_admin_token</code></li> <li><code>/etc/healtharchive/observability/grafana_admin_password</code></li> <li><code>/etc/healtharchive/observability/postgres_grafana_password</code></li> <li><code>/etc/healtharchive/observability/postgres_exporter.env</code></li> <li><code>/etc/healtharchive/observability/postgres_exporter_password</code></li> <li><code>/etc/healtharchive/observability/alertmanager_webhook_url</code> (routes alerts to one operator channel)</li> <li><code>/etc/healtharchive/observability/pushover_app_token</code> (optional; for Pushover relay)</li> <li><code>/etc/healtharchive/observability/pushover_user_key</code> (optional; for Pushover relay)</li> </ul> <p>Bootstrap helper (VPS only):</p> <ul> <li><code>scripts/vps-bootstrap-observability-scaffold.sh</code></li> <li><code>scripts/vps-install-observability-exporters.sh</code></li> <li><code>scripts/vps-install-observability-prometheus.sh</code></li> <li><code>scripts/vps-install-observability-grafana.sh</code></li> <li><code>scripts/vps-enable-tailscale-serve-grafana.sh</code></li> <li><code>scripts/vps-install-observability-dashboards.sh</code></li> <li><code>scripts/vps-install-observability-alerting.sh</code></li> <li><code>scripts/vps-install-observability-pushover-relay.sh</code> (optional; Alertmanager -&gt; Pushover)</li> <li><code>scripts/vps-install-ops-admin-proxy.sh</code> (optional; browser-friendly /api/admin/** + /metrics via SSH tunnel)</li> <li><code>scripts/vps-verify-observability.sh</code> (read-only; quick \u201cis everything up?\u201d check)</li> </ul>"},{"location":"operations/observability-and-private-stats/#3-data-collection-contract-privacy-preserving","title":"3) Data collection contract (privacy-preserving)","text":""},{"location":"operations/observability-and-private-stats/#31-what-we-collect-allowed","title":"3.1 What we collect (allowed)","text":"<ul> <li>Usage metrics: daily aggregate counters only.</li> <li>Storage: <code>usage_metrics(metric_date, event, count)</code></li> <li>No IPs, no user IDs, no per-request identifiers.</li> <li>Ops/service metrics: operational counts and totals needed to keep the service healthy.</li> <li>Examples: job status counts, snapshot totals, storage totals, search error rate.</li> </ul> <p>See also: <code>data-handling-retention.md</code> (retention + PHI risk notes).</p>"},{"location":"operations/observability-and-private-stats/#32-what-we-do-not-collect-explicitly-disallowed","title":"3.2 What we do not collect (explicitly disallowed)","text":"<ul> <li>Query strings / \u201ctop search terms\u201d.</li> <li>IP addresses, user agents, referrers stored into analytics tables.</li> <li>Per-user or per-session identifiers.</li> <li>High-cardinality dimensions (per-URL/page/path tracking).</li> <li>Third-party browser analytics scripts (unless a separate, explicit privacy/security decision is made).</li> </ul>"},{"location":"operations/observability-and-private-stats/#33-public-vs-private-usage-reporting","title":"3.3 Public vs private usage reporting","text":"<ul> <li>Public reporting can show only a curated subset of aggregate usage metrics.</li> <li>Private dashboards (Grafana) may show a broader set of aggregate events from the DB,   as long as they remain aggregate-only and public-safe.</li> </ul> <p>If adding new usage events:</p> <ul> <li>Keep the event set small and stable.</li> <li>Update docs and tests to ensure private-only events do not accidentally appear in public reporting.</li> </ul>"},{"location":"operations/observability-and-private-stats/#4-observability-architecture-intended-shape","title":"4) Observability architecture (intended shape)","text":""},{"location":"operations/observability-and-private-stats/#41-prometheus-scraping","title":"4.1 Prometheus scraping","text":"<ul> <li>Prometheus scrapes backend <code>GET /metrics</code> via loopback and includes the admin token.</li> <li>Exporters (node/postgres) are loopback- or tailnet-only and scraped by Prometheus.</li> <li>Retention must be capped (time and/or size) so Prometheus cannot fill disk.</li> </ul>"},{"location":"operations/observability-and-private-stats/#42-grafana-dashboards-private-stats-page","title":"4.2 Grafana dashboards (\u201cprivate stats page\u201d)","text":"<p>Grafana is the operator-facing surface.</p> <p>Data sources:</p> <ul> <li>Prometheus (time-series).</li> <li>Postgres (tables and long-window aggregates), using a dedicated read-only DB role.</li> </ul> <p>Sensitive tables:</p> <ul> <li>Treat <code>issue_reports</code> as sensitive (may contain free text and emails).</li> <li>Prefer redacted views for Grafana (counts + metadata only), and keep full text for explicit operator workflows.</li> </ul>"},{"location":"operations/observability-and-private-stats/#5-operational-invariants-must-remain-true","title":"5) Operational invariants (must remain true)","text":"<ul> <li>In production/staging, admin token must be configured and admin/metrics endpoints must not be public.</li> <li>Secrets must not be written under <code>/srv/healtharchive/ops/</code> (ops artifacts are public-safe by policy).</li> <li>Anything that changes public vs private boundaries must be documented as a deliberate decision.</li> </ul>"},{"location":"operations/observability-and-private-stats/#6-related-docs-canonical-references","title":"6) Related docs (canonical references)","text":"<ul> <li>Monitoring + CI checklist: <code>monitoring-and-ci-checklist.md</code></li> <li>Data handling &amp; retention: <code>data-handling-retention.md</code></li> <li>Production runbook: <code>../deployment/production-single-vps.md</code></li> <li>Ops playbooks index: <code>playbooks/README.md</code></li> </ul>"},{"location":"operations/one-page-brief/","title":"HealthArchive.ca - One-page brief (pointer)","text":"<p>Canonical public copy lives in:</p> <ul> <li>https://github.com/jerdaw/healtharchive-frontend/blob/main/public/partner-kit/healtharchive-brief.md</li> <li>Live page: https://www.healtharchive.ca/brief</li> </ul> <p>If you need to update the brief, edit the frontend asset and deploy the frontend.</p>"},{"location":"operations/ops-cadence-checklist/","title":"Ops Cadence Checklist (internal)","text":"<p>Purpose: make routine operations repeatable and low-friction so the project can be maintained without heroics.</p> <p>This checklist is intentionally short. If a task feels too heavy to do regularly, it should be moved to a longer cadence or automated safely.</p>"},{"location":"operations/ops-cadence-checklist/#every-deploy-always","title":"Every deploy (always)","text":"<ul> <li>Treat green <code>main</code> as the deploy gate (run local checks, push, wait for CI).</li> <li>Deploy using the VPS helper (safe deploy + verification):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-deploy.sh --apply --baseline-mode live</code></li> <li>Verify observability is still healthy (internal; loopback-only):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-verify-observability.sh</code></li> <li>Update docs if reality changed</li> <li>If you had to do manual steps not captured in a runbook/playbook, update the canonical doc(s) so the next deploy is repeatable.</li> <li>If the deploy script fails, don\u2019t retry blindly:</li> <li>read the drift report / verifier output</li> <li>fix the underlying mismatch (policy vs reality)</li> </ul> <p>Related docs:</p> <ul> <li>Deploy runbook: <code>../deployment/production-single-vps.md</code></li> <li>Verification/monitoring: <code>monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"operations/ops-cadence-checklist/#weekly-1015-minutes","title":"Weekly (10\u201315 minutes)","text":"<ul> <li>Observability sanity check</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-verify-observability.sh</code></li> <li>Service health</li> <li><code>curl -sS http://127.0.0.1:8001/api/health; echo</code></li> <li><code>sudo systemctl status healtharchive-api healtharchive-worker --no-pager -l</code></li> <li>Disk usage trend</li> <li><code>df -h /</code></li> <li>If <code>/srv/healtharchive</code> exists: <code>du -sh /srv/healtharchive/* | sort -h | tail -n 5</code></li> <li>If cleanup is needed: Disk baseline and cleanup</li> <li>Recent errors</li> <li><code>sudo journalctl -u healtharchive-api -n 200 --no-pager</code></li> <li><code>sudo journalctl -u healtharchive-worker -n 200 --no-pager</code></li> <li>Change tracking timer (if enabled)</li> <li><code>systemctl list-timers | rg healtharchive-change-tracking || systemctl list-timers | grep healtharchive-change-tracking</code></li> </ul>"},{"location":"operations/ops-cadence-checklist/#ongoing-automation-maintenance","title":"Ongoing automation maintenance","text":"<ul> <li>Keep systemd unit templates installed/updated on the VPS after repo updates:</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; sudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker</code></li> <li>Treat sentinel files under <code>/etc/healtharchive/</code> as the explicit on/off controls for automation.</li> <li>If you enable Healthchecks pings, keep ping URLs only in the root-owned VPS env file:</li> <li><code>/etc/healtharchive/healthchecks.env</code> (never commit ping URLs)</li> <li>If you use Healthchecks pings, periodically audit for drift (missing or stale checks):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; sudo -u haadmin python3 ./scripts/verify_healthchecks_alignment.py</code></li> <li>If you enable optional automations (coverage guardrails, replay smoke, cleanup), confirm their timers + sentinels are intentional.</li> </ul> <p>See: <code>../deployment/systemd/README.md</code></p>"},{"location":"operations/ops-cadence-checklist/#monthly-3060-minutes","title":"Monthly (30\u201360 minutes)","text":"<ul> <li>Reliability review (can be folded into the impact report)</li> <li>Note any incidents, slowdowns, or crawl failures.</li> <li>Confirm <code>/status</code> and <code>/impact</code> look reasonable and are current.</li> <li>Changelog update</li> <li>Add a short entry in <code>/changelog</code> reflecting meaningful updates (process: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md).</li> <li>Docs drift skim (10 minutes)</li> <li>Skim the production runbook + any playbooks you used recently; fix drift you notice.</li> <li>Search quality spot-check (lightweight)</li> <li>Run a few common queries on <code>/archive</code> and ensure results look plausible.</li> <li>Automation sanity check</li> <li>Verify timers are enabled only where intended.</li> </ul>"},{"location":"operations/ops-cadence-checklist/#quarterly-12-hours","title":"Quarterly (1\u20132 hours)","text":"<ul> <li>Restore test</li> <li>Follow <code>restore-test-procedure.md</code> and record results using <code>../_templates/restore-test-log-template.md</code>.</li> <li>Dataset release integrity</li> <li>Confirm a dataset release exists for the expected quarter/date.</li> <li>Verify checksums: <code>sha256sum -c SHA256SUMS</code> (see <code>dataset-release-runbook.md</code>).</li> <li>Docs maintenance</li> <li>Re-read <code>incidents/severity.md</code> + <code>playbooks/core/incident-response.md</code> and ensure they match current reality.</li> <li>Adoption signals entry (public-safe)</li> <li>Add a dated entry under <code>/srv/healtharchive/ops/adoption/</code> (links + aggregates only).</li> <li>Mentions log refresh (public-safe)</li> <li>Update <code>mentions-log.md</code> with new public links (permission-aware; link-only).</li> <li>Automation posture check</li> <li>On the VPS run: <code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/verify_ops_automation.sh</code></li> <li>Optional (diff-friendly): <code>./scripts/verify_ops_automation.sh --json | python3 -m json.tool</code></li> <li>Optional (JSON-only artifact): <code>./scripts/verify_ops_automation.sh --json-only &gt; /srv/healtharchive/ops/automation/posture.json</code></li> <li>Spot-check logs: <code>journalctl -u &lt;service&gt; -n 200</code></li> <li>Growth constraints review</li> <li>Revisit <code>growth-constraints.md</code> (storage, source caps, performance budgets).</li> <li>Adjust only if you can still support the new limits.</li> </ul>"},{"location":"operations/ops-cadence-checklist/#annual-before-jan-01-utc","title":"Annual (before Jan 01 UTC)","text":"<ul> <li>Annual edition readiness</li> <li>Review <code>annual-campaign.md</code> for scope changes.</li> <li>Ensure enough storage headroom for a full capture cycle.</li> <li>Run the crawl preflight audit:<ul> <li><code>cd /opt/healtharchive-backend &amp;&amp; YEAR=2026 &amp;&amp; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\"</code></li> </ul> </li> <li>Dry-run the scheduler if it is enabled:<ul> <li><code>sudo systemctl start healtharchive-schedule-annual-dry-run.service</code></li> <li><code>sudo journalctl -u healtharchive-schedule-annual-dry-run.service -n 200 --no-pager</code></li> </ul> </li> </ul>"},{"location":"operations/ops-cadence-checklist/#where-to-record-outcomes","title":"Where to record outcomes","text":"<ul> <li>Changelog: public-facing changes and policy updates.</li> <li>Impact report: monthly coverage + reliability + usage snapshot.</li> <li>Incident notes: for outages/degradations/manual interventions: <code>incidents/README.md</code>.</li> <li>Internal ops log: optional private notes (date + key checks + issues).</li> </ul>"},{"location":"operations/outreach-templates/","title":"Outreach Templates (draft, public-safe)","text":"<p>Purpose: Ready-to-send templates for partner outreach. Keep these generic and non-committal until you have permission to name partners publicly.</p> <p>Do NOT add private contact details here.</p>"},{"location":"operations/outreach-templates/#template-a-distribution-partner-libraryresources-page","title":"Template A: Distribution partner (library/resources page)","text":"<p>Subject: HealthArchive.ca (archive + change tracking) for your resources page</p> <p>Hello , <p>I maintain HealthArchive.ca, an independent archive of Canadian public health web pages. It preserves time-stamped snapshots and provides descriptive change tracking so researchers and educators can cite what guidance said at a specific time.</p> <p>It is not medical advice or current guidance; it is a provenance archive.</p> <p>Would you consider adding HealthArchive.ca to your digital scholarship or public health methods resources page? The easiest option is to link to our changes feed or digest:</p> <ul> <li>https://www.healtharchive.ca/changes</li> <li>https://www.healtharchive.ca/digest</li> </ul> <p>If helpful, I can send a short one-page brief and screenshots.</p> <p>Thank you for considering it,"},{"location":"operations/outreach-templates/#template-b-researchteaching-partner","title":"Template B: Research/teaching partner","text":"<p>Subject: Resource for reproducibility of public health guidance (HealthArchive.ca)</p> <p>Hello , <p>I run HealthArchive.ca, an independent archive of Canadian public health web pages with time-stamped snapshots and descriptive change tracking.</p> <p>This can support reproducibility work and teaching about how guidance evolves over time (without interpreting or endorsing content).</p> <p>Would it be useful for your group to include HealthArchive in a methods reading list, teaching module, or research workflow? I can share a one-page brief and suggested citation format.</p> <p>Thanks for your time,"},{"location":"operations/outreach-templates/#template-c-journalism-communication-partner","title":"Template C: Journalism / communication partner","text":"<p>Subject: Archive of Canadian public health guidance (time-stamped snapshots)</p> <p>Hello , <p>HealthArchive.ca is an independent archive of Canadian public health web pages. It preserves time-stamped snapshots and provides a changes feed so journalists can audit how wording evolves over time. It is not current guidance or medical advice.</p> <p>If this could be useful to your work, the digest and changes feed are here:</p> <ul> <li>https://www.healtharchive.ca/changes</li> <li>https://www.healtharchive.ca/digest</li> </ul> <p>I can share a brief overview and citation guidance if helpful.</p> <p>Best,"},{"location":"operations/outreach-templates/#follow-up-1-week","title":"Follow-up (1 week)","text":"<p>Subject: Re: HealthArchive.ca resource link</p> <p>Hello , <p>Just following up in case my note got buried. HealthArchive.ca is an archive of Canadian public health guidance with time-stamped snapshots and descriptive change tracking. It is not current guidance or medical advice.</p> <p>If it is useful, the quickest addition is a link to:</p> <ul> <li>https://www.healtharchive.ca/digest</li> </ul> <p>Thanks again for considering it.</p> <p>"},{"location":"operations/outreach-templates/#follow-up-2-weeks-final","title":"Follow-up (2 weeks, final)","text":"<p>Subject: Closing the loop on HealthArchive.ca</p> <p>Hello , <p>I will close the loop for now, but if HealthArchive.ca becomes useful in the future, I am happy to provide a short brief or citation guidance.</p> <p>Thank you,"},{"location":"operations/outreach-templates/#optional-onboarding-blurb-for-partners-to-paste","title":"Optional: Onboarding blurb (for partners to paste)","text":"<p>HealthArchive.ca is an independent, non-governmental archive of Canadian public health web pages. It preserves time-stamped snapshots and provides descriptive change tracking for reproducibility and auditability. It is not current guidance or medical advice. Always consult official sources for up-to-date guidance.</p>"},{"location":"operations/partner-kit/","title":"Partner Kit (internal ops guide)","text":"<p>Purpose: A lightweight, partner-ready kit that makes it easy to link to HealthArchive without implying endorsement or medical guidance.</p> <p>Do not include private emails or contact lists here.</p> <p>Canonical public assets (do not duplicate copy here):</p> <ul> <li>One-page brief (source file): https://github.com/jerdaw/healtharchive-frontend/blob/main/public/partner-kit/healtharchive-brief.md</li> <li>Citation handout (source file): https://github.com/jerdaw/healtharchive-frontend/blob/main/public/partner-kit/healtharchive-citation.md</li> <li>Live pages: <code>https://www.healtharchive.ca/brief</code> and <code>https://www.healtharchive.ca/cite</code></li> </ul> <p>If you need to update public copy, edit the frontend assets above and deploy the frontend.</p>"},{"location":"operations/partner-kit/#1-distribution-blurb-pasteable","title":"1) Distribution blurb (pasteable)","text":"<p>HealthArchive.ca is an independent, non-governmental archive of Canadian public health web pages. It provides time-stamped snapshots and descriptive change tracking so researchers, journalists, and educators can audit how guidance evolves over time. It is not current guidance or medical advice.</p> <p>Suggested links:</p> <ul> <li>Digest (RSS + overview): https://www.healtharchive.ca/digest</li> <li>Changes feed: https://www.healtharchive.ca/changes</li> <li>One-page brief: https://www.healtharchive.ca/brief</li> <li>Citation guidance: https://www.healtharchive.ca/cite</li> <li>Methods and scope: https://www.healtharchive.ca/methods</li> </ul>"},{"location":"operations/partner-kit/#2-screenshot-checklist-for-partner-kit","title":"2) Screenshot checklist (for partner kit)","text":"<p>Save files with consistent names so they can be attached to outreach emails.</p> <ul> <li>01-home.png (Home page with \"What this is/is not\" block)</li> <li>02-archive.png (/archive with search + filters)</li> <li>03-snapshot.png (snapshot metadata + report link)</li> <li>04-changes.png (/changes feed)</li> <li>05-compare.png (/compare?to=, shows disclaimer) <li>06-digest.png (/digest with RSS links)</li> <li>07-status.png (/status metrics)</li> <li>08-impact.png (/impact monthly report)</li>"},{"location":"operations/partner-kit/#3-rss-links-reference","title":"3) RSS links (reference)","text":"<p>Global RSS:</p> <ul> <li>https://api.healtharchive.ca/api/changes/rss</li> </ul> <p>Per-source RSS (replace  with code, e.g., hc, phac, cihr):</p> <ul> <li>https://api.healtharchive.ca/api/changes/rss?source=</li> </ul>"},{"location":"operations/partner-kit/#4-notes-for-partners","title":"4) Notes for partners","text":"<ul> <li>HealthArchive is an archival record, not a guidance provider.</li> <li>Please avoid phrasing that implies endorsement or official status.</li> <li>Preferred language: \"archive\", \"snapshots\", \"change tracking\",   \"auditability\", \"reproducibility\".</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/","title":"Replay + Preview Automation (Plan Only)","text":"<p>Status: design draft (v1 implemented: <code>ha-backend replay-reconcile</code>; automation timers remain optional).</p> <p>This document is a thorough, safety-first plan for automating the operational steps that make HealthArchive \u201cfeel like a real site archive\u201d:</p> <ul> <li>pywb replay is kept up-to-date for each indexed crawl job (\u201cedition\u201d).</li> <li>cached homepage preview images exist for the <code>/archive</code> source cards.</li> <li>drift is detected and repaired safely (or surfaced clearly when it can\u2019t be repaired).</li> </ul> <p>It intentionally contains no implementation code. The goal is to agree on the what/where/how/why, guardrails, and edge cases before we build anything.</p> <p>Related runbooks and context:</p> <ul> <li>Replay runbook: <code>docs/deployment/replay-service-pywb.md</code></li> <li>Production VPS runbook: <code>docs/deployment/production-single-vps.md</code></li> <li>Legacy imports: <code>docs/operations/legacy-crawl-imports.md</code></li> <li>Architecture overview: <code>docs/architecture.md</code></li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#0-what-done-means-for-automation-high-level","title":"0) What \u201cdone\u201d means for automation (high level)","text":"<p>Automation is considered \u201csuccessful\u201d when:</p> <ol> <li>For every <code>ArchiveJob</code> with <code>status=indexed</code>, replay is eventually available at <code>https://replay.healtharchive.ca/job-&lt;id&gt;/...</code> (or we can point to a specific reason why it isn\u2019t).</li> <li>For every source shown on <code>/archive</code>, a cached preview image is eventually available for the latest edition (or we can point to a specific reason why it isn\u2019t).</li> <li>The system is safe-by-default:</li> <li>read-only / dry-run modes exist,</li> <li>destructive operations are excluded (cleanup),</li> <li>retries are bounded and don\u2019t spam,</li> <li>concurrency is controlled,</li> <li>we can disable automation instantly without affecting core API availability.</li> </ol>"},{"location":"operations/replay-and-preview-automation-plan/#1-scope-what-we-will-and-will-not-automate-yet","title":"1) Scope: what we will and will not automate (yet)","text":""},{"location":"operations/replay-and-preview-automation-plan/#in-scope","title":"In scope","text":"<ul> <li>Replay indexing: making <code>job-&lt;id&gt;</code> collections replayable (symlinks + CDX index).</li> <li>Preview generation: producing cached preview images for the <code>/archive</code> source cards.</li> <li>Reconciliation: a periodic \u201crepair drift\u201d process that converges the system to correctness.</li> <li>Observability: basic monitoring/alerting for replay/previews.</li> <li>Guardrails: locks, refusal rules, throttling, idempotency, backoff.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#out-of-scope-for-now","title":"Out of scope (for now)","text":"<ul> <li>Crawl scheduling / job creation automation (e.g., monthly crawls).</li> <li>Any \u201cautomatic cleanup\u201d that deletes WARCs or temp dirs without a retention policy.</li> <li>CI/CD pipeline automation (PR checks, deployment pipelines) beyond documentation.</li> <li>Anything that requires printing secrets or env files in logs or scripts.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#2-terminology-shared-language","title":"2) Terminology (shared language)","text":"<ul> <li>Job: <code>ArchiveJob</code> row in the DB. An indexed job corresponds to captured WARCs + <code>Snapshot</code> rows.</li> <li>Edition: a user-facing term for a job\u2019s backup (one job \u2192 one edition).</li> <li>Collection: pywb collection name. We use <code>job-&lt;id&gt;</code> for a job\u2019s collection.</li> <li>Replay indexed: pywb has a CDX index for <code>job-&lt;id&gt;</code> and can serve captures from that job\u2019s WARCs.</li> <li>Preview: a cached image (PNG/JPEG/WebP) used in the <code>/archive</code> source cards.</li> <li>Drift: DB says job is indexed, but replay/preview state doesn\u2019t match (missing index, missing WARCs, stale preview, etc.).</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#3-current-reality-constraints-we-must-respect","title":"3) Current reality (constraints we must respect)","text":""},{"location":"operations/replay-and-preview-automation-plan/#replay-depends-on-warcs-staying-on-disk","title":"Replay depends on WARCs staying on disk","text":"<p>Replay reads from job WARCs on disk. If WARCs are deleted, replay will break even if the DB still has snapshots.</p> <ul> <li>Guardrail already exists: <code>ha-backend cleanup-job --mode temp</code> refuses unless <code>--force</code> when replay is enabled.</li> <li>Operational posture: treat WARC retention as \u201ccritical state\u201d until we design cold storage replay.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#pywb-deployment-and-permissions-matter","title":"pywb deployment and permissions matter","text":"<p>On the VPS (per <code>docs/deployment/replay-service-pywb.md</code>):</p> <ul> <li>pywb runs in Docker as container <code>healtharchive-replay</code></li> <li>exposed locally at <code>127.0.0.1:8090</code></li> <li>WARCs are mounted read-only at <code>/warcs</code> (host <code>/srv/healtharchive/jobs</code>)</li> <li>replay state is mounted read-write at <code>/webarchive</code> (host <code>/srv/healtharchive/replay</code>)</li> <li>container runs without Linux capabilities (<code>--cap-drop=ALL</code>) \u2192 file permissions must be correct; \u201croot in container\u201d can\u2019t bypass them.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#preview-files-are-served-by-the-backend-api","title":"Preview files are served by the backend API","text":"<p>The backend supports cached preview images via:</p> <ul> <li><code>GET /api/sources/{source_code}/preview?jobId=&lt;id&gt;</code></li> </ul> <p>Files are expected in:</p> <ul> <li><code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code></li> <li>naming convention: <code>source-&lt;code&gt;-job-&lt;jobId&gt;.webp|.jpg|.jpeg|.png</code></li> </ul> <p>The frontend expects <code>entryPreviewUrl</code> to be present in <code>GET /api/sources</code> once previews exist.</p>"},{"location":"operations/replay-and-preview-automation-plan/#4-non-negotiable-design-principles","title":"4) Non-negotiable design principles","text":""},{"location":"operations/replay-and-preview-automation-plan/#safety-first-defaults","title":"Safety-first defaults","text":"<ul> <li>All automation must support dry-run mode.</li> <li>All automation must support allowlists:</li> <li>allowlist by source code (<code>hc</code>, <code>cihr</code>, \u2026)</li> <li>allowlist by job id range or \u201cnewest N\u201d</li> <li>All automation must refuse to run when required dependencies aren\u2019t healthy (docker down, pywb missing, disk low, etc.).</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#idempotency","title":"Idempotency","text":"<p>Every action must be safe to re-run:</p> <ul> <li>\u201cMake replayable\u201d can run repeatedly; it should converge to correct symlinks + index.</li> <li>\u201cGenerate preview\u201d can run repeatedly; it should be atomic and overwrite safely.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#isolation-from-user-traffic","title":"Isolation from user traffic","text":"<p>Automation must never run on:</p> <ul> <li>an API request path,</li> <li>a frontend request path,</li> <li>or any flow that could block interactive user browsing.</li> </ul> <p>It must run as a background process (manual trigger, timer, or separate worker queue).</p>"},{"location":"operations/replay-and-preview-automation-plan/#observability","title":"Observability","text":"<p>Automation must produce:</p> <ul> <li>machine-readable status (\u201cOK / needs work / blocked\u201d) per job and per source,</li> <li>actionable error messages (without secrets),</li> <li>backoff to avoid repeated failures.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#5-automation-candidate-a-replay-indexing-pywb-collections-cdx","title":"5) Automation candidate A \u2014 Replay indexing (pywb collections + CDX)","text":""},{"location":"operations/replay-and-preview-automation-plan/#51-desired-end-state","title":"5.1 Desired end state","text":"<p>For each job <code>id</code> where <code>ArchiveJob.status == indexed</code>:</p> <ul> <li>A pywb collection exists: <code>/srv/healtharchive/replay/collections/job-&lt;id&gt;/...</code></li> <li>The collection contains symlinks in <code>archive/</code> pointing to <code>/warcs/...</code> WARC paths (container-visible).</li> <li><code>indexes/index.cdxj</code> exists and corresponds to the current WARC set.</li> <li>A basic replay check succeeds for the job\u2019s entry URL:</li> <li>timegate form: <code>https://replay.healtharchive.ca/job-&lt;id&gt;/&lt;original_url&gt;</code></li> <li>(optional) CDX query returns at least one record for the entry URL.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#52-where-this-automation-should-live-options","title":"5.2 Where this automation should live (options)","text":"<p>Option A1: Reconciler timer (recommended first)</p> <ul> <li>A periodic process that looks at \u201cdesired jobs\u201d vs \u201creplay-ready jobs\u201d and repairs drift.</li> </ul> <p>Why this is the best first automation:</p> <ul> <li>decoupled from crawl/indexing,</li> <li>can be disabled instantly,</li> <li>can backfill older jobs,</li> <li>naturally repairs operator mistakes (deleted collections/indexes).</li> </ul> <p>Option A2: Worker hook (later, only if needed)</p> <ul> <li>After indexing completes, automatically trigger replay indexing.</li> </ul> <p>Risk:</p> <ul> <li>couples two heavy operations (index + replay indexing),</li> <li>increases failure surface in the worker loop,</li> <li>needs robust retry/backoff to avoid wedging jobs.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#53-required-guardrails","title":"5.3 Required guardrails","text":"<p>Concurrency</p> <ul> <li>Global lock: only one replay indexing operation at a time.</li> <li>Per-job lock: prevent two reindex attempts for the same <code>job-&lt;id&gt;</code>.</li> </ul> <p>Implementation decision (when we code):</p> <ul> <li>Prefer <code>flock</code>-based lock files under <code>/srv/healtharchive/replay/.locks/</code>.</li> <li>simple, visible, and resilient across process crashes.</li> </ul> <p>Eligibility rules (must be true to proceed)</p> <ul> <li>Job exists and is <code>status=indexed</code>.</li> <li>WARC discovery finds &gt;= 1 <code>.warc.gz</code> file.</li> <li>pywb container is running (or is startable).</li> <li>The process has:</li> <li>write access to <code>/srv/healtharchive/replay/collections/\u2026</code></li> <li>permission to run <code>docker exec</code> for <code>wb-manager</code>.</li> </ul> <p>Refusal rules (stop early, report why)</p> <ul> <li>Disk below a configured threshold (to prevent filling the VPS root disk).</li> <li>WARCs are missing / unreadable (likely cleanup ran) \u2192 mark job \u201creplay blocked: missing data\u201d.</li> <li>pywb container exists but is unhealthy (restarts / crashes repeatedly).</li> </ul> <p>Resource control</p> <ul> <li>Cap \u201cjobs per run\u201d (e.g., 1\u20132 per run initially).</li> <li>Optionally run with <code>nice</code> and/or <code>ionice</code> if indexing impacts API latency.</li> </ul> <p>Failure handling</p> <ul> <li>Classify failures into a small set:</li> <li>\u201cblocked\u201d (needs human action: missing WARCs, permissions)</li> <li>\u201cretryable\u201d (transient: docker restart, pywb busy)</li> <li>\u201cinternal\u201d (bug: unexpected exception)</li> <li>Exponential backoff for retryable failures (and a ceiling).</li> <li>Suppress repeated identical errors from spamming journald.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#54-state-tracking-how-we-know-whats-done","title":"5.4 State tracking (how we know what\u2019s done)","text":"<p>We need to know:</p> <ul> <li>which jobs are already replay indexed,</li> <li>whether their replay index matches their current WARC list,</li> <li>and when we last attempted/failed.</li> </ul> <p>Two viable designs:</p> <p>A) Filesystem marker (minimal, first iteration)</p> <ul> <li>After successful replay indexing, write a small JSON marker file into the collection:</li> <li><code>collections/job-&lt;id&gt;/replay-index.meta.json</code></li> <li>includes:<ul> <li><code>jobId</code></li> <li><code>indexedAt</code></li> <li><code>warcCount</code></li> <li><code>warcListHash</code> (hash of sorted WARC paths)</li> <li><code>pywbVersion</code> (optional)</li> </ul> </li> </ul> <p>Pros:</p> <ul> <li>doesn\u2019t require DB migrations,</li> <li>works even if DB is temporarily unavailable (but replay indexing requires DB anyway).</li> </ul> <p>Cons:</p> <ul> <li>harder to report status through APIs/admin dashboards,</li> <li>harder to query across jobs.</li> </ul> <p>B) DB state (preferred once we implement automation seriously)</p> <ul> <li>Add DB fields or a dedicated table to track replay indexing state.</li> </ul> <p>Pros:</p> <ul> <li>easy observability (admin endpoints, metrics),</li> <li>easier to reconcile at scale,</li> <li>can store retry counts and next-attempt timestamps.</li> </ul> <p>Cons:</p> <ul> <li>requires migration and careful rollout.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#55-edge-cases-to-explicitly-handle","title":"5.5 Edge cases to explicitly handle","text":"<ul> <li>WARCs deleted after indexing: DB says \u201cindexed\u201d, replay 404s.</li> <li>Detect via WARC discovery read failures.</li> <li>Mark \u201cblocked: missing WARCs\u201d; do not retry aggressively.</li> <li>Permissions drift: pywb cannot read WARCs due to chmod/chown changes.</li> <li>Detect by trying to <code>stat</code>/open a sample WARC (host) or via pywb reindex failure.</li> <li>Mark \u201cblocked: permissions\u201d; provide a runbook to fix.</li> <li>Job re-imported / WARC set changes: new WARCs added or paths differ.</li> <li>Detect via <code>warcListHash</code> change; reindex.</li> <li>Disk pressure: CDX can be large.</li> <li>Refuse below threshold; alert.</li> <li>pywb container restart during reindex:</li> <li>Reindex is rerunnable; ensure partial index doesn\u2019t block future runs.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#6-automation-candidate-b-cached-source-preview-generation","title":"6) Automation candidate B \u2014 Cached source preview generation","text":""},{"location":"operations/replay-and-preview-automation-plan/#61-desired-end-state","title":"6.1 Desired end state","text":"<p>For each source code shown on <code>/archive</code>:</p> <ul> <li>For the \u201ccurrent edition\u201d job id (latest by capture date):</li> <li>a preview file exists in <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code> named:<ul> <li><code>source-&lt;code&gt;-job-&lt;jobId&gt;.webp</code> (preferred), or</li> <li>a supported fallback format.</li> </ul> </li> <li>The backend returns <code>entryPreviewUrl</code> in <code>GET /api/sources</code>.</li> <li>The frontend displays the image without embedding live iframes.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#62-where-this-automation-should-live-options","title":"6.2 Where this automation should live (options)","text":"<p>Option B1: On-demand operator command (recommended first)</p> <ul> <li>Run it manually after a job becomes replayable, or when you want to refresh thumbnails.</li> </ul> <p>Why:</p> <ul> <li>preview generation is inherently flaky (dynamic pages, timeouts),</li> <li>it\u2019s easy to overwhelm the VPS if automated too aggressively.</li> </ul> <p>Option B2: Scheduled refresh timer (later)</p> <ul> <li>Daily/weekly, only for \u201clatest edition per source\u201d.</li> </ul> <p>Guardrail:</p> <ul> <li>cap number of previews per run,</li> <li>run during off-peak hours.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#63-guardrails-required","title":"6.3 Guardrails required","text":"<ul> <li>Always generate using a replay URL with <code>#ha_nobanner=1</code> so the screenshot matches the underlying site.</li> <li>Strict timeouts + \u201ccontinue on failure\u201d.</li> <li>Atomic writes:</li> <li>write to <code>*.tmp</code> then <code>rename()</code> to final name.</li> <li>Validate output:</li> <li>file exists and size &gt; minimum threshold.</li> <li>Rate limiting:</li> <li>cap to N previews per run.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#64-edge-cases","title":"6.4 Edge cases","text":"<ul> <li>replay entry URL 404s (job not replay indexed yet, or missing WARCs).</li> <li>replay loads but page never settles (long-running scripts).</li> <li>some pages are heavy and render inconsistently; use fixed viewport and a small settle delay.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#7-automation-candidate-c-reconciliation-loop-converge-to-correctness","title":"7) Automation candidate C \u2014 Reconciliation loop (\u201cconverge to correctness\u201d)","text":"<p>This is the safest automation pattern: a background process that continuously closes the gap between \u201cdesired\u201d and \u201cactual\u201d state.</p>"},{"location":"operations/replay-and-preview-automation-plan/#71-inputs-and-outputs","title":"7.1 Inputs and outputs","text":"<p>Inputs</p> <ul> <li>DB jobs and snapshots (<code>ArchiveJob</code>, <code>Snapshot</code>, <code>Source</code>)</li> <li>filesystem state:</li> <li>pywb collections under <code>/srv/healtharchive/replay/collections</code></li> <li>preview files under <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code></li> </ul> <p>Outputs</p> <ul> <li>replay indexing performed for some jobs (via CLI)</li> <li>preview generation performed for some sources (optional, depending on enablement)</li> <li>status reporting (logs + metrics)</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#72-reconciler-modes-must-exist-when-implemented","title":"7.2 Reconciler modes (must exist when implemented)","text":"<ul> <li><code>dry-run</code>: compute and print planned actions only.</li> <li><code>apply</code>: perform actions.</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#73-recommended-initial-algorithm-when-we-implement","title":"7.3 Recommended initial algorithm (when we implement)","text":"<ol> <li>Acquire a global lock (refuse if already running).</li> <li>Query for <code>ArchiveJob.status=indexed</code> ordered newest-first.</li> <li>For each job (up to a max-per-run):</li> <li>if job is not replay indexed (marker/state missing or warc hash changed):<ul> <li>run replay indexing step</li> </ul> </li> <li>For each source (optional, up to a max-per-run):</li> <li>determine latest edition job id (from <code>/api/sources/{code}/editions</code> or DB query)</li> <li>if preview missing for that job id:<ul> <li>generate preview</li> </ul> </li> <li>Emit a summary report.</li> </ol>"},{"location":"operations/replay-and-preview-automation-plan/#74-guardrails","title":"7.4 Guardrails","text":"<ul> <li>allowlist sources for early rollouts</li> <li>max jobs per run</li> <li>max previews per run</li> <li>backoff on failures</li> <li>refuse when disk low</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#8-automation-candidate-d-monitoring-alerting-replay-aware","title":"8) Automation candidate D \u2014 Monitoring + alerting (replay-aware)","text":"<p>Replay introduces new failure modes that standard API health checks won\u2019t catch.</p>"},{"location":"operations/replay-and-preview-automation-plan/#81-recommended-monitors","title":"8.1 Recommended monitors","text":"<p>External (cheap, stable):</p> <ul> <li><code>GET https://api.healtharchive.ca/api/health</code> (already)</li> <li><code>GET https://replay.healtharchive.ca/</code> (200)</li> <li><code>HEAD https://replay.healtharchive.ca/</code> (200)</li> <li>One \u201cknown good\u201d replay entry URL per major source (200):</li> <li><code>https://replay.healtharchive.ca/job-1/.../https://www.canada.ca/en/health-canada.html</code></li> </ul> <p>Internal (optional):</p> <ul> <li>systemd timer that checks:</li> <li>pywb container is running</li> <li>disk usage below a safe threshold</li> <li>replay CDX exists for newest job</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#82-alert-playbook-what-to-do-when-it-breaks","title":"8.2 Alert playbook (what to do when it breaks)","text":"<ul> <li>Replay origin down:</li> <li>check <code>healtharchive-replay.service</code> status/logs</li> <li>check docker health</li> <li>Replay 404 for a known entry URL:</li> <li>check that the job\u2019s WARCs still exist</li> <li>check that <code>replay-index-job</code> was run and index exists</li> <li>Preview missing:</li> <li>run preview generation manually for latest job</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#9-rollout-strategy-methodical-and-safe","title":"9) Rollout strategy (methodical and safe)","text":"<ol> <li>Agree on this document.</li> <li>Implement reconciler in <code>dry-run</code> mode only.</li> <li>Run it manually and review output.</li> <li>Enable <code>apply</code> mode for a single allowlisted source.</li> <li>Add backoff and failure classification.</li> <li>Only after it\u2019s stable:</li> <li>consider enabling for all sources,</li> <li>consider adding preview generation to the reconciler,</li> <li>(optionally) consider a worker hook if needed.</li> </ol>"},{"location":"operations/replay-and-preview-automation-plan/#10-do-not-automate-cleanup-until-retention-is-designed","title":"10) Do not automate cleanup until retention is designed","text":"<p>Cleanup is the highest-risk automation.</p> <p>Before we automate any cleanup, we need a separate retention design:</p> <ul> <li>Which jobs must remain replayable and for how long?</li> <li>Where do \u201ccold\u201d WARCs live (NAS/object storage)?</li> <li>How do we replay cold WARCs without copying huge data back to the VPS?</li> <li>Can we move WARCs out of temp dirs so \u201ccleanup temp state\u201d is safe?</li> </ul> <p>Until then:</p> <ul> <li>Keep cleanup manual and conservative.</li> <li>Rely on the existing CLI guardrail (<code>cleanup-job</code> refuses unless <code>--force</code> when replay is enabled).</li> </ul>"},{"location":"operations/replay-and-preview-automation-plan/#appendix-a-manual-operator-playbook-current-safe","title":"Appendix A \u2014 Manual operator playbook (current, safe)","text":"<p>When a new job is indexed:</p> <ol> <li>Make it replayable:</li> <li><code>ha-backend replay-index-job --id &lt;id&gt;</code></li> <li>(Optional) Generate preview:</li> <li>produce <code>source-&lt;code&gt;-job-&lt;id&gt;.{webp,png,jpg}</code> in <code>HEALTHARCHIVE_REPLAY_PREVIEW_DIR</code></li> <li>Verify:</li> <li><code>/snapshot/&lt;id&gt;</code> and <code>/browse/&lt;id&gt;</code></li> <li><code>/archive</code> source cards show preview and deep browsing works</li> </ol>"},{"location":"operations/restore-test-procedure/","title":"Restore Test Procedure (quarterly)","text":"<p>Purpose: prove backups are usable by performing a clean restore and verifying core API behavior.</p> <p>This procedure is intentionally minimal and public-safe. It does not require production secrets to be stored in the repo.</p>"},{"location":"operations/restore-test-procedure/#preconditions","title":"Preconditions","text":"<ul> <li>A recent database dump exists (from the normal backup routine).</li> <li>A temporary restore target is available (local Postgres on the VPS or a separate staging host).</li> <li>You have enough disk space to restore the dump.</li> </ul>"},{"location":"operations/restore-test-procedure/#step-1-choose-a-restore-target","title":"Step 1 \u2014 Choose a restore target","text":"<p>Options (pick one):</p> <ul> <li>Local temporary database on the VPS (preferred for speed).</li> <li>Staging database on a separate host.</li> </ul> <p>Record the target in the restore test log.</p>"},{"location":"operations/restore-test-procedure/#step-2-restore-the-database-dump","title":"Step 2 \u2014 Restore the database dump","text":"<p>Follow your standard backup tool instructions. In production, backups are typically created with:</p> <ul> <li><code>pg_dump -Fc</code> (custom-format dump)</li> </ul> <p>Examples (adjust paths):</p> <pre><code># Example: restore into a temporary database named healtharchive_restore_test_YYYYMMDD\nDBNAME=\"healtharchive_restore_test_$(date -u +%Y%m%d)\"\n\n# Pick a backup file (example naming from the production runbook)\nBACKUP=\"/srv/healtharchive/backups/healtharchive_YYYY-MM-DDTHHMMSSZ.dump\"\n\nsudo -u postgres createdb -O healtharchive \"$DBNAME\"\n\n# If backups live under a directory the `postgres` user cannot traverse, copy it first:\nTMPDUMP=\"/var/tmp/${DBNAME}.dump\"\nsudo install -m 600 -o postgres -g postgres \"$BACKUP\" \"$TMPDUMP\"\n\n# Restore the custom-format dump:\nsudo -u postgres pg_restore --no-owner --role=healtharchive -d \"$DBNAME\" \"$TMPDUMP\"\n\nsudo rm -f \"$TMPDUMP\"\n</code></pre> <p>If your backup is plain SQL (not custom-format), you can restore with <code>psql -f</code>, but production defaults are custom-format.</p>"},{"location":"operations/restore-test-procedure/#step-3-point-the-backend-to-the-restored-db","title":"Step 3 \u2014 Point the backend to the restored DB","text":"<p>Run API checks against the restored DB by temporarily overriding <code>HEALTHARCHIVE_DATABASE_URL</code>. Example:</p> <pre><code>export HEALTHARCHIVE_DATABASE_URL=\"postgresql+psycopg://.../healtharchive_restore_test\"\n/opt/healtharchive-backend/.venv/bin/alembic current\n</code></pre> <p>This confirms the restored schema is usable.</p>"},{"location":"operations/restore-test-procedure/#step-4-run-minimal-verification-checks","title":"Step 4 \u2014 Run minimal verification checks","text":"<p>Run these against the restored DB:</p> <ul> <li><code>GET /api/health</code> (DB check must be <code>ok</code>)</li> <li><code>GET /api/stats</code> (counts should be non-zero)</li> <li><code>GET /api/sources</code> (sources list should load)</li> </ul> <p>If you need a quick CLI-only check, run:</p> <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend check-db\n</code></pre>"},{"location":"operations/restore-test-procedure/#step-5-record-results","title":"Step 5 \u2014 Record results","text":"<p>Use <code>../_templates/restore-test-log-template.md</code> and record:</p> <ul> <li>date + operator,</li> <li>backup source used,</li> <li>restore target,</li> <li>pass/fail,</li> <li>any anomalies or follow-up actions.</li> </ul>"},{"location":"operations/restore-test-procedure/#step-6-clean-up","title":"Step 6 \u2014 Clean up","text":"<p>Remove the temporary database when done:</p> <pre><code>dropdb healtharchive_restore_test\n</code></pre> <p>If you used a staging host, remove any temporary credentials or files.</p>"},{"location":"operations/risk-register/","title":"Risk Register (internal)","text":"<p>Track the top operational risks and mitigations.</p> <ul> <li>Misinterpretation risk (archive mistaken for current guidance)</li> <li>Mitigation: strong disclaimers; never add \u201cinterpretation\u201d features; keep high-risk pages (<code>/browse</code>, <code>/snapshot</code>) explicit.</li> <li>PHI submission risk (issue reports)</li> <li>Mitigation: clear warnings; minimize storage; admin-only access; delete/redact if PHI appears.</li> <li>Proxy/CORS misuse risk</li> <li>Mitigation: keep the frontend same-origin report proxy narrow; do not turn it into a general proxy; keep backend CORS allowlist strict.</li> <li>Single-VPS availability risk</li> <li>Mitigation: Disaster Recovery Runbook (RTO/RPO); backups + restore tests; conservative automation caps; disk monitoring; clear rollback procedures.</li> <li>Export integrity / reproducibility risk</li> <li>Mitigation: checksums + manifest; stable ordering/pagination; version fields (<code>diff_version</code>, <code>normalization_version</code>); avoid rewriting releases.</li> </ul>"},{"location":"operations/search-golden-queries/","title":"Golden queries (search relevance)","text":"<p>This file is a living checklist of \u201cgolden queries\u201d used to evaluate whether search ranking changes improve relevance.</p> <p>It complements <code>search-quality.md</code>:</p> <ul> <li><code>search-quality.md</code> explains how to run an evaluation.</li> <li>This file defines what we check (queries + expected outcomes).</li> </ul>"},{"location":"operations/search-golden-queries/#1-how-to-use-this-file","title":"1) How to use this file","text":"<p>For each query below:</p> <ol> <li>Run the API captures (prefer <code>view=pages</code> for user-facing relevance).</li> <li>Compare the top ~10 results against the expectations.</li> <li>Update expectations when the archive\u2019s coverage changes (new sources/pages).</li> </ol> <p>Guiding principle:</p> <ul> <li>Broad queries should surface hub/overview pages near the top.</li> <li>Specific queries should surface the most directly matching documents.</li> </ul>"},{"location":"operations/search-golden-queries/#2-query-set-curated-no-query-logs-required","title":"2) Query set (curated, no query logs required)","text":"<p>We do not have query logs yet, so this list is a curated approximation of:</p> <ul> <li>Broad hub intent (1\u20132 terms).</li> <li>Common refinements (testing, vaccines, isolation).</li> <li>Specific named entities (NACI, antivirals).</li> <li>A few non-COVID queries (to avoid overfitting).</li> <li>A small French \u201csmoke test\u201d set for bilingual content.</li> </ul>"},{"location":"operations/search-golden-queries/#21-broad-head-queries-hub-intent","title":"2.1 Broad \u201chead\u201d queries (hub intent)","text":"<ul> <li><code>covid</code></li> <li><code>influenza</code></li> <li><code>mpox</code></li> <li><code>measles</code></li> <li><code>rsv</code></li> <li><code>food recall</code></li> <li><code>travel advisory</code></li> <li><code>mental health</code></li> <li><code>air quality</code></li> <li><code>wildfire smoke</code></li> <li><code>immunization</code></li> <li><code>vaccines</code></li> </ul>"},{"location":"operations/search-golden-queries/#22-medium-queries-mixed-intent","title":"2.2 Medium queries (mixed intent)","text":"<ul> <li><code>covid vaccine</code></li> <li><code>covid booster</code></li> <li><code>mask</code></li> <li><code>mask guidance</code></li> <li><code>rapid testing</code></li> <li><code>testing</code></li> <li><code>wastewater</code></li> <li><code>isolation</code></li> <li><code>quarantine</code></li> <li><code>symptoms</code></li> <li><code>treatment</code></li> <li><code>prevention risks</code></li> </ul>"},{"location":"operations/search-golden-queries/#23-specific-queries-precision-intent","title":"2.3 Specific queries (precision intent)","text":"<ul> <li><code>long covid</code></li> <li><code>post covid condition</code></li> <li><code>naci</code></li> <li><code>naci booster</code></li> <li><code>myocarditis pericarditis</code></li> <li><code>omicron ventilation</code></li> <li><code>ventilation filtration</code></li> <li><code>paxlovid</code></li> <li><code>nirmatrelvir</code></li> <li><code>remdesivir</code></li> <li><code>health infobase</code></li> </ul>"},{"location":"operations/search-golden-queries/#24-non-covid-queries-overfitting-guardrail","title":"2.4 Non-COVID queries (overfitting guardrail)","text":"<ul> <li><code>opioid overdose</code></li> <li><code>naloxone</code></li> <li><code>vaping</code></li> <li><code>cannabis</code></li> <li><code>antimicrobial resistance</code></li> <li><code>water advisory</code></li> </ul>"},{"location":"operations/search-golden-queries/#25-french-smoke-tests-bilingual-content-no-stemming","title":"2.5 French smoke tests (bilingual content; no stemming)","text":"<ul> <li><code>grippe</code></li> <li><code>variole simienne</code></li> <li><code>vaccin covid</code></li> <li><code>sante mentale</code></li> </ul>"},{"location":"operations/search-golden-queries/#3-expectations-fill-these-in-over-time","title":"3) Expectations (fill these in over time)","text":"<p>Use normalized URL groups where possible (what <code>view=pages</code> groups on). For each query, keep:</p> <ul> <li>\u201cExpected\u201d pages: should appear in top 10 (ideally top 3\u20135 for broad queries).</li> <li>\u201cAnti-results\u201d: should not appear in top 20 for broad queries.</li> </ul>"},{"location":"operations/search-golden-queries/#31-covid","title":"3.1 <code>covid</code>","text":"<p>Expected (top 10):</p> <ul> <li><code>https://www.canada.ca/en/public-health/services/diseases/coronavirus-disease-covid-19.html</code></li> <li><code>https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection.html</code></li> </ul> <p>Nice-to-have (top 20):</p> <ul> <li><code>https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection/prevention-risks.html</code></li> <li><code>https://travel.gc.ca/travel-covid</code></li> </ul> <p>Anti-results (avoid in top 20 for broad <code>covid</code> unless explicitly requested):</p> <ul> <li>Titles beginning with <code>Archived</code> (e.g., <code>Archived - ...</code>, <code>Archived 50: ...</code>)</li> <li>Narrow deep pages that win solely by repeating \u201cCOVID\u201d many times in title/snippet</li> </ul>"},{"location":"operations/search-golden-queries/#32-long-covid","title":"3.2 <code>long covid</code>","text":"<p>Expected (top 10):</p> <ul> <li><code>https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection/symptoms/post-covid-19-condition.html</code></li> <li><code>https://www.canada.ca/en/public-health/services/diseases/2019-novel-coronavirus-infection/health-professionals/post-covid-19-condition.html</code></li> </ul> <p>Anti-results:</p> <ul> <li>Broad COVID hubs outranking long-COVID pages (unless query is just <code>covid</code>)</li> </ul>"},{"location":"operations/search-golden-queries/#33-other-queries","title":"3.3 Other queries","text":"<p>Fill these in as coverage grows and you learn what \u201cgood\u201d looks like:</p> <ul> <li><code>influenza</code>: expected hub pages, PHAC/HC overview pages</li> <li><code>mpox</code>: expected public health hub pages</li> <li><code>food recall</code>: expected recall hub pages (e.g., CFIA)</li> <li><code>travel advisory</code>: expected Travel.gc.ca hub pages</li> <li><code>mental health</code>: expected PHAC/HC hub pages</li> </ul>"},{"location":"operations/search-golden-queries/#4-capture-commands-recommended","title":"4) Capture commands (recommended)","text":"<p>Prefer capturing API responses (fast + deterministic).</p> <p>Production examples:</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=20&amp;sort=relevance&amp;view=pages\" | python3 -m json.tool\ncurl -s \"https://api.healtharchive.ca/api/search?q=long%20covid&amp;page=1&amp;pageSize=20&amp;sort=relevance&amp;view=pages\" | python3 -m json.tool\n</code></pre> <p>To force a specific ranking version (useful for comparisons):</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=20&amp;sort=relevance&amp;view=pages&amp;ranking=v2\" | python3 -m json.tool\n</code></pre> <p>Local examples (backend on <code>127.0.0.1:8001</code>):</p> <pre><code>curl -s \"http://127.0.0.1:8001/api/search?q=covid&amp;page=1&amp;pageSize=20&amp;sort=relevance&amp;view=pages\" | python3 -m json.tool\n</code></pre> <p>For a repeatable \u201cbefore/after\u201d capture directory, use the script:</p> <pre><code>./scripts/search-eval-capture.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval\n</code></pre> <p>To capture using ranking v2 explicitly:</p> <pre><code>./scripts/search-eval-capture.sh --ranking v2 --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval\n</code></pre> <p>To additionally generate corpus-derived queries from the configured database and merge them with the curated list:</p> <pre><code>./scripts/search-eval-capture.sh --generate-from-db --out-dir /tmp/ha-search-eval\n</code></pre> <p>Notes:</p> <ul> <li>Keep captures out of git unless you explicitly want them committed.</li> <li>Prefer <code>view=pages</code> for ranking evaluation; use <code>view=snapshots</code> to debug capture-level issues.</li> </ul>"},{"location":"operations/search-quality/","title":"Search quality &amp; relevance evaluation","text":"<p>This project intentionally runs on a single VPS with Postgres, and does not introduce a separate search service (Elasticsearch/Meilisearch/etc.) unless and until we outgrow Postgres FTS + light heuristics.</p> <p>This document is a lightweight, repeatable way to evaluate whether search results \u201cfeel better\u201d after changes.</p>"},{"location":"operations/search-quality/#1-goals-what-better-means","title":"1) Goals (what \u201cbetter\u201d means)","text":"<p>For broad, common queries (e.g. <code>covid</code>):</p> <ul> <li>No obvious error/garbage captures in the top results (404/5xx \u201cNot Found\u201d pages,   missing assets, etc.).</li> <li>Hub/overview pages rise near the top (titles that clearly match the query).</li> <li>Snippets look human-readable (not \u201cSkip to main content \u2026 Search \u2026 Menu \u2026\u201d).</li> <li>API response time stays reasonable as the dataset grows.</li> </ul>"},{"location":"operations/search-quality/#2-golden-queries-starter-list","title":"2) Golden queries (starter list)","text":"<p>Start with ~10\u201330 queries and expand over time:</p> <ul> <li><code>covid</code></li> <li><code>covid vaccine</code></li> <li><code>long covid</code></li> <li><code>mask guidance</code></li> <li><code>rapid testing</code></li> <li><code>influenza</code></li> <li><code>mpox</code></li> <li><code>food recall</code></li> <li><code>travel advisory</code></li> <li><code>mental health</code></li> </ul> <p>For each query, define:</p> <ul> <li>2\u20135 \u201cexpected\u201d page titles/URLs that should appear in the top 10.</li> <li>2\u20135 \u201canti-results\u201d you never want in the top 20 (assets, obvious error pages).</li> </ul> <p>Keep the list as a simple note, or add a small markdown checklist in this repo if you want it versioned.</p>"},{"location":"operations/search-quality/#3-how-to-run-a-quick-evaluation-local-or-prod","title":"3) How to run a quick evaluation (local or prod)","text":""},{"location":"operations/search-quality/#31-use-the-api-directly-fastest","title":"3.1 Use the API directly (fastest)","text":"<p>Local dev (backend running on <code>http://127.0.0.1:8001</code>):</p> <pre><code>curl -s \"http://127.0.0.1:8001/api/search?q=covid&amp;page=1&amp;pageSize=10&amp;sort=relevance\" | python3 -m json.tool\n</code></pre> <p>Production:</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=10&amp;sort=relevance\" | python3 -m json.tool\n</code></pre> <p>To de-duplicate repeated captures of the same URL (show latest snapshot per page):</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=10&amp;sort=relevance&amp;view=pages\" | python3 -m json.tool\n</code></pre> <p>To include non\u20112xx captures for research/debugging:</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=10&amp;sort=relevance&amp;includeNon2xx=true\" | python3 -m json.tool\n</code></pre> <p>To inspect why a result ranks where it does (admin-only score breakdown):</p> <pre><code>curl -s \"https://api.healtharchive.ca/api/admin/search-debug?q=covid&amp;view=pages&amp;sort=relevance&amp;ranking=v2&amp;pageSize=10\" \\\n  -H \"X-Admin-Token: ${HEALTHARCHIVE_ADMIN_TOKEN}\" \\\n  | python3 -m json.tool\n</code></pre>"},{"location":"operations/search-quality/#32-capture-beforeafter-snapshots-recommended","title":"3.2 Capture \u201cbefore/after\u201d snapshots (recommended)","text":"<p>For a small set of key queries (e.g. <code>covid</code>, <code>mpox</code>, <code>food recall</code>), capture page 1 JSON to files so you can compare later:</p> <pre><code>mkdir -p /tmp/ha-search-eval\ncurl -s \"https://api.healtharchive.ca/api/search?q=covid&amp;page=1&amp;pageSize=10&amp;sort=relevance\" \\\n  &gt; /tmp/ha-search-eval/covid.after.json\n</code></pre> <p>Keep these captures out of git unless you explicitly want them committed.</p> <p>For repeatable, multi-query captures, use the helper scripts in <code>scripts/</code>:</p> <pre><code>./scripts/search-eval-capture.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval --page-size 20 --ranking v1\n./scripts/search-eval-capture.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval --page-size 20 --ranking v2\npython ./scripts/search-eval-diff.py --a /tmp/ha-search-eval/&lt;TS_A&gt; --b /tmp/ha-search-eval/&lt;TS_B&gt; --top 20\n</code></pre> <p>To capture v1 + v2 and generate a diff report in one command:</p> <pre><code>./scripts/search-eval-run.sh --base-url https://api.healtharchive.ca --out-dir /tmp/ha-search-eval\n</code></pre> <p>On the production VPS, prefer a persistent output directory:</p> <pre><code>./scripts/search-eval-run.sh --base-url https://api.healtharchive.ca --out-dir /srv/healtharchive/ops/search-eval\n</code></pre>"},{"location":"operations/search-quality/#4-minimal-passfail-checklist-for-releases","title":"4) Minimal pass/fail checklist for releases","text":"<p>For each key query:</p> <ul> <li> Top 10 contains at least a few clear title matches (\u201chub/overview\u201d pages).</li> <li> No obvious 404/asset/error pages in top 20 (unless <code>includeNon2xx=true</code>).</li> <li> Snippets look readable and are not mostly boilerplate.</li> <li> Pagination behaves correctly (<code>total</code> stable; out-of-range pages return empty).</li> </ul>"},{"location":"operations/search-quality/#5-operational-tools","title":"5) Operational tools","text":"<p>These commands are intended for production maintenance on the VPS (Postgres), but can also be used in local dev environments.</p>"},{"location":"operations/search-quality/#51-backfill-postgres-fts-vectors","title":"5.1 Backfill Postgres FTS vectors","text":"<p>After deploying a schema update that adds <code>snapshots.search_vector</code>, populate it for existing rows:</p> <pre><code>ha-backend backfill-search-vector\n</code></pre>"},{"location":"operations/search-quality/#511-enable-fuzzy-search-postgres-only","title":"5.1.1 Enable fuzzy search (Postgres only)","text":"<p>Fuzzy matching for misspellings relies on the <code>pg_trgm</code> extension and trigram GIN indexes (see Alembic migration <code>0007_pg_trgm_fuzzy_search</code>).</p> <p>Notes:</p> <ul> <li><code>CREATE EXTENSION</code> may require elevated DB privileges on some hosts.</li> <li>If <code>pg_trgm</code> is unavailable, search still works (FTS + substring fallback), but   the fuzzy similarity fallback is disabled.</li> <li>The fuzzy fallback is intentionally conservative for performance: it uses   pg_trgm word similarity (not whole-field similarity) and tuned thresholds to   avoid huge candidate sets on broad queries.</li> </ul>"},{"location":"operations/search-quality/#52-refresh-snippetstitles-from-warcs-in-place","title":"5.2 Refresh snippets/titles from WARCs (in place)","text":"<p>After improving HTML extraction logic, update snapshot metadata without re-indexing (IDs remain stable):</p> <pre><code>ha-backend refresh-snapshot-metadata --job-id &lt;JOB_ID&gt;\n</code></pre>"},{"location":"operations/search-quality/#521-backfill-normalized-url-groups-page-de-duplication","title":"5.2.1 Backfill normalized URL groups (page de-duplication)","text":"<p>If older snapshots are missing <code>Snapshot.normalized_url_group</code>, <code>view=pages</code> may show duplicate \u201cpages\u201d for the same URL (especially when query parameters vary). Backfill the column:</p> <pre><code>ha-backend backfill-normalized-url-groups\n</code></pre>"},{"location":"operations/search-quality/#522-snapshot-view-hide-same-day-content-duplicates-ui","title":"5.2.2 Snapshot view: hide same-day content duplicates (UI)","text":"<p>In <code>view=snapshots</code>, the API can hide same-day duplicates of the exact same URL when the content is identical (same <code>content_hash</code>), which helps reduce noise from repeated tracker / redirect captures while keeping the underlying data intact.</p> <ul> <li>Default: duplicates are hidden.</li> <li>To include them: <code>GET /api/search?...&amp;view=snapshots&amp;includeDuplicates=true</code></li> </ul> <p>Future (storage-only, must preserve trustworthiness): if we can prove the HTML payload is identical (and preserve provenance), consider pruning same-day duplicates from storage to save disk space. Track this work in:</p> <ul> <li><code>../planning/roadmap.md</code></li> </ul>"},{"location":"operations/search-quality/#53-backfill-outlinks-authority-signals","title":"5.3 Backfill outlinks + authority signals","text":"<p>If you have applied the authority schema (tables <code>snapshot_outlinks</code> and <code>page_signals</code>), you can populate link edges for a job by re-reading its WARCs:</p> <pre><code>ha-backend backfill-outlinks --job-id &lt;JOB_ID&gt; --update-signals\n</code></pre> <p>To rebuild all link signals from the full outlink graph (includes <code>inlink_count</code>, <code>outlink_count</code>, and <code>pagerank</code> when present):</p> <pre><code>ha-backend recompute-page-signals\n</code></pre>"},{"location":"operations/service-levels/","title":"Service Levels","text":"<p>This document describes the service level objectives and commitments for HealthArchive.ca. These are targets, not contractual guarantees.</p> <p>Last Updated: 2026-01-18</p>"},{"location":"operations/service-levels/#scope-and-context","title":"Scope and Context","text":"<p>HealthArchive is a public-interest research archive operated as a best-effort service. These service levels reflect commitments appropriate for the project's resources and mission:</p> <ul> <li>Infrastructure: Single VPS (Hetzner cx33: 4 vCPU / 8GB RAM / 80GB SSD)</li> <li>Staffing: Solo operator, no 24/7 coverage</li> <li>Purpose: Public good research tool, not a commercial service</li> </ul> <p>All targets are measured and reviewed on a best-effort basis. Incidents outside business hours may see delayed response.</p>"},{"location":"operations/service-levels/#availability","title":"Availability","text":""},{"location":"operations/service-levels/#target","title":"Target","text":"<p>99.5% monthly availability</p> <p>This allows for approximately 3.6 hours of downtime per month, which is realistic for: - Single-server architecture (no redundancy) - Manual maintenance operations - Solo operator response times</p>"},{"location":"operations/service-levels/#measurement","title":"Measurement","text":"<ul> <li>Primary endpoint: <code>GET /api/health</code> (https://api.healtharchive.ca/api/health)</li> <li>Monitoring method: External uptime monitoring (Healthchecks.io, UptimeRobot)</li> <li>Measurement window: Calendar month</li> <li>Exclusions: Planned maintenance with advance notice (see Maintenance Windows)</li> </ul>"},{"location":"operations/service-levels/#review","title":"Review","text":"<ul> <li>Semi-annual review of actuals vs target</li> <li>Adjust target if infrastructure or staffing changes significantly</li> </ul>"},{"location":"operations/service-levels/#response-times","title":"Response Times","text":"<p>Target response times for key API endpoints, measured server-side (excludes network latency):</p> Endpoint p50 Target p95 Target p99 Target Notes <code>GET /api/health</code> 50ms 100ms 200ms Minimal processing <code>GET /api/search</code> 500ms 2000ms 5000ms Complex queries, database-dependent <code>GET /api/sources</code> 100ms 300ms 500ms Lightweight, typically cached <code>GET /api/snapshot/{id}</code> 100ms 300ms 500ms Single record lookup <code>GET /api/changes</code> 200ms 500ms 1000ms Precomputed change feed"},{"location":"operations/service-levels/#degradation-criteria","title":"Degradation Criteria","text":"<p>The service is considered degraded when: - p95 latency exceeds target by 2\u00d7 for 5+ consecutive minutes - p99 latency exceeds target by 3\u00d7 for 5+ consecutive minutes - Any endpoint timeout rate exceeds 1%</p>"},{"location":"operations/service-levels/#exclusions","title":"Exclusions","text":"<p>These targets do not apply to: - Attack traffic or abusive request patterns - Bulk export operations (<code>/api/exports/*</code>) - Replay operations (separate service: <code>replay.healtharchive.ca</code>)</p>"},{"location":"operations/service-levels/#data-freshness","title":"Data Freshness","text":""},{"location":"operations/service-levels/#crawl-cadence","title":"Crawl Cadence","text":"<p>Primary sources (Health Canada, PHAC): Crawled at least annually, with ad-hoc updates as resources permit</p> <ul> <li>Major annual crawl campaign: typically January</li> <li>Ad-hoc crawls: triggered by significant health events or policy changes</li> <li>Schedule is best-effort and subject to operator availability</li> </ul>"},{"location":"operations/service-levels/#indexing-latency","title":"Indexing Latency","text":"<ul> <li>Crawl-to-indexed: Within 24 hours of crawl completion, subject to operator availability</li> <li>Indexed-to-searchable: Immediate (same database transaction)</li> </ul>"},{"location":"operations/service-levels/#change-tracking","title":"Change Tracking","text":"<ul> <li>Changes computed: Within 24 hours of new snapshots being indexed, subject to operator availability</li> <li>Change feed updated: On next <code>compute-changes</code> run (automated via systemd timer)</li> </ul>"},{"location":"operations/service-levels/#exceptions","title":"Exceptions","text":"<ul> <li>Manual crawls may have different timelines based on urgency</li> <li>Emergency updates (e.g., public health crises) prioritized on case-by-case basis</li> </ul>"},{"location":"operations/service-levels/#maintenance-windows","title":"Maintenance Windows","text":""},{"location":"operations/service-levels/#window-types","title":"Window Types","text":""},{"location":"operations/service-levels/#routine-maintenance","title":"Routine Maintenance","text":"<ul> <li>Examples: Security updates, dependency patches, configuration changes</li> <li>Advance Notice: 24 hours (via changelog)</li> <li>Maximum Duration: 30 minutes</li> <li>Typical Downtime: &lt; 15 minutes</li> </ul>"},{"location":"operations/service-levels/#major-maintenance","title":"Major Maintenance","text":"<ul> <li>Examples: Database migrations, infrastructure changes, new feature deployments</li> <li>Advance Notice: 72 hours (via changelog + announcement if user-facing)</li> <li>Maximum Duration: 4 hours</li> <li>Typical Downtime: 1-2 hours</li> </ul>"},{"location":"operations/service-levels/#emergency-maintenance","title":"Emergency Maintenance","text":"<ul> <li>Examples: Critical security patches, severe bug fixes</li> <li>Advance Notice: ASAP (post-hoc notification if required immediately)</li> <li>Duration: As needed</li> <li>Communication: Documented in changelog after completion</li> </ul>"},{"location":"operations/service-levels/#preferred-timing","title":"Preferred Timing","text":"<ul> <li>Weekdays, off-peak hours: Early morning (00:00-06:00 UTC) or late evening (22:00-24:00 UTC)</li> <li>Avoid: Business hours (14:00-22:00 UTC), weekends, holidays</li> </ul>"},{"location":"operations/service-levels/#post-maintenance-verification","title":"Post-Maintenance Verification","text":"<p>After all maintenance: - Health check validation (<code>/api/health</code>, <code>/archive</code>) - Smoke test (search query, snapshot retrieval) - External uptime monitor confirmation - Documented in changelog</p>"},{"location":"operations/service-levels/#communication-commitments","title":"Communication Commitments","text":""},{"location":"operations/service-levels/#channels","title":"Channels","text":"<p>Public Channels: - Changelog: https://healtharchive.ca/changelog - primary source for planned changes and incidents - Status: https://healtharchive.ca/status - service status overview - No dedicated status page (updates via changelog)</p> <p>Internal/Operator: - Incident notes (selected public-safe summaries published) - Operations logs (private)</p>"},{"location":"operations/service-levels/#incident-communication","title":"Incident Communication","text":"<p>Following the incident disclosure policy (Option B):</p> <p>Sev0/Sev1 (Service Down / Major Degradation): - Communicate within 48 hours of resolution, or as soon as practical - Public-safe summary published to changelog - Includes impact, timeline, resolution, and prevention measures</p> <p>Sev2/Sev3 (Minor Issues): - Include in regular changelog if user-facing - Internal documentation only if operator-only impact</p>"},{"location":"operations/service-levels/#changelog-cadence","title":"Changelog Cadence","text":"<ul> <li>Major changes: Immediate entry</li> <li>Minor changes: Batched weekly or monthly</li> <li>Security updates: Published as appropriate (may delay for responsible disclosure)</li> </ul>"},{"location":"operations/service-levels/#limitations","title":"Limitations","text":"<p>Communication timelines are best-effort and depend on: - Solo operator availability (no 24/7 coverage) - Incident severity and complexity - Need for coordination with external parties (e.g., infrastructure provider)</p>"},{"location":"operations/service-levels/#performance-baselines","title":"Performance Baselines","text":""},{"location":"operations/service-levels/#purpose","title":"Purpose","text":"<p>Baselines provide reference points for detecting performance degradation and validating improvements. They are not targets but rather observations of typical performance under normal conditions.</p>"},{"location":"operations/service-levels/#baseline-measurement-approach","title":"Baseline Measurement Approach","text":"<p>Baselines should be measured: - On production hardware (single VPS, current configuration) - Under typical load (not during crawls or heavy operations) - Multiple samples to account for variance - Documented with measurement date and conditions</p>"},{"location":"operations/service-levels/#current-baselines","title":"Current Baselines","text":"<p>[!NOTE] Initial baselines to be measured and documented during implementation. This section will be updated with actual measurements.</p> <p>API Response Times (server-side, measured via curl timing):</p> Endpoint Baseline p50 Baseline p95 Measured Date <code>GET /api/health</code> TBD TBD TBD <code>GET /api/search?q=covid</code> TBD TBD TBD <code>GET /api/sources</code> TBD TBD TBD <code>GET /api/snapshot/{id}</code> TBD TBD TBD <p>Operational Baselines:</p> Operation Baseline Throughput Measured Date Crawl (pages/hour) TBD TBD Indexing (records/second) TBD TBD Change computation (changes/minute) TBD TBD"},{"location":"operations/service-levels/#baseline-review","title":"Baseline Review","text":"<ul> <li>Semi-annual review: Compare current performance against baselines</li> <li>After major changes: Re-baseline if infrastructure or architecture changes</li> <li>Drift documentation: Document and investigate significant baseline drift (&gt;20%)</li> </ul>"},{"location":"operations/service-levels/#review-and-update-process","title":"Review and Update Process","text":""},{"location":"operations/service-levels/#review-cadence","title":"Review Cadence","text":"<p>Annual Review: - Assess targets vs actuals for the past year - Evaluate appropriateness of commitments - Update targets if resources or infrastructure change significantly</p> <p>Triggered Reviews: - After infrastructure changes (e.g., VPS upgrade, migration) - After staffing changes (e.g., additional operators) - After major architectural changes (e.g., HA implementation)</p>"},{"location":"operations/service-levels/#update-process","title":"Update Process","text":"<ol> <li>Propose changes: Document in roadmap or ADR</li> <li>Review against actuals: Compare proposed targets to historical data</li> <li>Update documentation: Revise this document</li> <li>Communicate changes: Announce via changelog if user-facing impact</li> <li>Update monitoring: Adjust alerts and dashboards to match new targets</li> </ol>"},{"location":"operations/service-levels/#document-maintenance","title":"Document Maintenance","text":"<ul> <li>Location: <code>docs/operations/service-levels.md</code></li> <li>Owner: Primary operator</li> <li>Format: Markdown, version-controlled in healtharchive-backend repo</li> <li>Navigation: Linked from operations index and docs site navigation</li> </ul>"},{"location":"operations/service-levels/#references","title":"References","text":"<ul> <li>Production Runbook - Infrastructure details and deployment procedures</li> <li>Incident Response Playbook - Incident classification and response procedures</li> <li>Monitoring Checklist - Monitoring setup and external checks</li> <li>Disaster Recovery - Recovery procedures and RTO/RPO targets</li> <li>Ops Cadence Checklist - Routine operational tasks</li> </ul>"},{"location":"operations/service-levels/#changelog","title":"Changelog","text":"Date Change Rationale 2026-01-18 Initial version Established baseline service level documentation"},{"location":"operations/thresholds-and-tuning/","title":"Operational Thresholds and Tuning Guide","text":"<p>This document centralizes all operational thresholds used in HealthArchive automation, monitoring, and safeguards.</p> <p>Last updated: 2026-02-06</p>"},{"location":"operations/thresholds-and-tuning/#overview","title":"Overview","text":"<p>HealthArchive uses conservative thresholds to prevent runaway automation and ensure system stability. All automation is opt-in via sentinel files and includes multiple safety caps.</p> <p>General principles: - Automation is safe-by-default (dry-run unless explicitly enabled) - Rate limits prevent flapping (cooldowns + per-hour/day caps) - Deploy locks prevent conflicts during maintenance - All thresholds are tunable but have sensible defaults</p>"},{"location":"operations/thresholds-and-tuning/#disk-management-thresholds","title":"Disk Management Thresholds","text":""},{"location":"operations/thresholds-and-tuning/#worker-pre-crawl-disk-check","title":"Worker Pre-Crawl Disk Check","text":"Parameter Value Location Rationale Threshold 85% <code>src/ha_backend/worker/main.py</code> (<code>DISK_HEADROOM_THRESHOLD_PERCENT</code>) Allows ~11GB buffer for multi-GB annual crawls Check frequency Every job selection Worker loop Prevents mid-crawl disk-full failures Action Skip job selection Worker logs warning Jobs remain queued until space is freed <p>Tuning guidance: - Lower to 80% if crawls are consistently small (e.g., test jobs) - Raise to 88% if disk is oversized and buffer is excessive - Don't raise above 90% - leaves too little margin for error</p> <p>Related alerts: <code>HealthArchiveDiskUsageHigh</code> (warning), <code>HealthArchiveDiskUsageCritical</code></p>"},{"location":"operations/thresholds-and-tuning/#alerting-thresholds","title":"Alerting Thresholds","text":"Severity Threshold Duration Location Action Warning &gt;85% 30 minutes <code>ops/observability/alerting/healtharchive-alerts.yml</code> (<code>HealthArchiveDiskUsageHigh</code>) Page on-call during business hours Critical &gt;92% 10 minutes <code>ops/observability/alerting/healtharchive-alerts.yml</code> (<code>HealthArchiveDiskUsageCritical</code>) Page on-call immediately <p>Tuning guidance: - Warning duration (30min) gives time to react without false positives - Critical threshold (92%) leaves ~6GB for emergency response - Don't raise critical above 95% - risk of sudden disk-full</p> <p>See: <code>docs/operations/disk-baseline-and-cleanup.md</code> (current baseline + cleanup posture)</p>"},{"location":"operations/thresholds-and-tuning/#cleanup-automation","title":"Cleanup Automation","text":"Parameter Value Location Purpose Min age 14 days <code>ops/automation/cleanup-automation.toml</code> (<code>min_age_days</code>) Avoid cleaning recent jobs Keep latest per source 2 <code>ops/automation/cleanup-automation.toml</code> (<code>keep_latest_per_source</code>) Preserve recent snapshots Max jobs per run (weekly) 1 <code>ops/automation/cleanup-automation.toml</code> (<code>max_jobs_per_run</code>) Conservative incremental cleanup Threshold trigger 80% <code>ops/automation/cleanup-automation.toml</code> (<code>threshold_trigger_percent</code>) Only run threshold cleanup when disk exceeds this Max jobs per run (threshold) 5 <code>ops/automation/cleanup-automation.toml</code> (<code>threshold_max_jobs_per_run</code>) More aggressive cleanup under disk pressure Cleanup mode <code>temp-nonwarc</code> <code>scripts/vps-cleanup-automation.py</code> (<code>ha-backend cleanup-job --mode temp-nonwarc</code>) Preserves WARCs (safe) <p>Tuning guidance: - Increase <code>threshold_max_jobs_per_run</code> to 7-10 only if disk pressure is chronic and the cleanup is consistently safe - Decrease <code>min_age_days</code> to 7 if disk pressure is chronic - Increase <code>keep_latest_per_source</code> to 3+ if operators need more history</p> <p>Implementation notes: - Weekly cleanup: <code>docs/deployment/systemd/healtharchive-cleanup-automation.service</code> + <code>docs/deployment/systemd/healtharchive-cleanup-automation.timer</code> - Disk threshold cleanup: <code>docs/deployment/systemd/healtharchive-disk-threshold-cleanup.service</code> + <code>docs/deployment/systemd/healtharchive-disk-threshold-cleanup.timer</code> (runs every 30 min; no-op when below threshold)</p>"},{"location":"operations/thresholds-and-tuning/#crawl-recovery-thresholds","title":"Crawl Recovery Thresholds","text":""},{"location":"operations/thresholds-and-tuning/#stall-detection","title":"Stall Detection","text":"Parameter Value Location Rationale Stall threshold 3600s (60 min) <code>scripts/vps-crawl-auto-recover.py</code> (<code>--stall-threshold-seconds</code>, default: 3600) Balance between false positives and timely recovery Progress metric <code>crawled</code> count unchanged Parsed from combined log Reliable indicator of actual progress Guard window 600s (10 min) <code>scripts/vps-crawl-auto-recover.py</code> (<code>--skip-if-any-job-progress-within-seconds</code>, default: 600) Avoid interrupting healthy crawls <p>Tuning guidance: - Lower to 1800s (30min) for fast sites (e.g., small test crawls) - Raise to 5400s (90min) or 7200s (120min) for very slow sites or flaky networks - Don't lower below 1800s (30min) \u2014 risks false positives during normal slow periods</p>"},{"location":"operations/thresholds-and-tuning/#recovery-rate-limits","title":"Recovery Rate Limits","text":"Parameter Value Location Rationale Per-job daily cap 3 <code>scripts/vps-crawl-auto-recover.py</code> (<code>--max-recoveries-per-job-per-day</code>, default: 3) Prevents restart loops for fundamentally broken jobs Soft recovery enabled True <code>scripts/vps-crawl-auto-recover.py</code> (<code>--soft-recover-when-guarded</code>, default: true) Mark stalled jobs retryable without stopping healthy crawls <p>Tuning guidance: - Increase per-job cap to 5 for known-flaky sources (e.g., sites with frequent timeouts) - Disable soft recovery (<code>--no-soft-recover-when-guarded</code>) only for debugging</p> <p>Recovery enhancements (auto-applied): - <code>enable_adaptive_restart=True</code> - <code>max_container_restarts</code> floor from source profile (<code>hc=24</code>, <code>phac=30</code>, <code>cihr=20</code>) - See: <code>scripts/vps-crawl-auto-recover.py</code> (<code>_ensure_recovery_tool_options</code>)</p>"},{"location":"operations/thresholds-and-tuning/#crawl-auto-start-queue-fill","title":"Crawl Auto-Start (Queue Fill)","text":"<p>When enabled, the crawl auto-recover watchdog can also act as a queue fill mechanism: if there are no stalled jobs, but the annual campaign is running fewer than N jobs, it can auto-start one queued/retryable annual job via <code>systemd-run</code>.</p> <p>This is designed to avoid the operational failure mode where a stalled job gets marked <code>retryable</code> but never returns to running because the worker is already busy with another crawl.</p>"},{"location":"operations/thresholds-and-tuning/#auto-start-thresholds","title":"Auto-Start Thresholds","text":"Parameter Value Location Rationale Min running jobs 3 <code>docs/deployment/systemd/healtharchive-crawl-auto-recover.service</code> (<code>--ensure-min-running-jobs</code>) Keep annual campaign concurrency stable Per-job daily cap 3 <code>scripts/vps-crawl-auto-recover.py</code> (<code>--max-starts-per-job-per-day</code>, default: 3) Prevent auto-start loops Disk safety limit 88% <code>docs/deployment/systemd/healtharchive-crawl-auto-recover.service</code> (<code>--start-max-disk-usage-percent</code>) Avoid starting new crawls when disk is near full <p>Implementation notes: - Auto-start only considers jobs with <code>config.campaign_kind=\"annual\"</code> and matching <code>config.campaign_year</code>. - Auto-start runs the job using <code>systemd-run</code> (detached) and applies Docker caps via env vars:   - <code>HEALTHARCHIVE_DOCKER_CPU_LIMIT</code> (default: 1.0; configurable via <code>--start-docker-cpu-limit</code>)   - <code>HEALTHARCHIVE_DOCKER_MEMORY_LIMIT</code> (default: 3g; configurable via <code>--start-docker-memory-limit</code>)</p>"},{"location":"operations/thresholds-and-tuning/#storage-hot-path-recovery-thresholds","title":"Storage Hot-Path Recovery Thresholds","text":""},{"location":"operations/thresholds-and-tuning/#stale-mount-detection","title":"Stale Mount Detection","text":"Parameter Value Location Rationale Min failure age 120s (2 min) <code>scripts/vps-storage-hotpath-auto-recover.py</code> (<code>--min-failure-age-seconds</code>, default: 120) Avoid acting on transient failures Confirm runs 2 consecutive <code>scripts/vps-storage-hotpath-auto-recover.py</code> (<code>--confirm-runs</code>, default: 2) Require persistence before acting Detection signal Errno 107 Probed via <code>os.stat()</code> \"Transport endpoint is not connected\" <p>Probed locations: 1. Running job output dirs 2. Next queued/retryable job output dirs (prevents retry storms) 3. Manifest hot paths (tiering bind mounts)</p> <p>Tuning guidance: - Don't lower <code>min_failure_age</code> - transient failures are common - Don't reduce <code>confirm_runs</code> - single observations may be false positives</p>"},{"location":"operations/thresholds-and-tuning/#recovery-rate-limits_1","title":"Recovery Rate Limits","text":"Parameter Value Location Rationale Cooldown 15 minutes <code>scripts/vps-storage-hotpath-auto-recover.py</code> (<code>--cooldown-seconds</code>, default: 900) Prevent flapping after recovery Hourly cap 2 <code>scripts/vps-storage-hotpath-auto-recover.py</code> (<code>--max-recoveries-per-hour</code>, default: 2) Global safety limit Daily cap 6 global, 3/job <code>scripts/vps-storage-hotpath-auto-recover.py</code> (<code>--max-recoveries-per-day</code>, default: 6; <code>--max-recoveries-per-job-per-day</code>, default: 3) Prevent runaway automation <p>Tuning guidance: - Increase cooldown to 30min if recovery attempts fail repeatedly - Increase hourly/daily caps cautiously - investigate root cause instead - Don't bypass caps in automation - they prevent pathological loops</p>"},{"location":"operations/thresholds-and-tuning/#persistent-failed-apply-alert","title":"Persistent Failed-Apply Alert","text":"Parameter Value Location Rationale Failed-apply age threshold &gt;24h <code>ops/observability/alerting/healtharchive-alerts.yml</code> (<code>HealthArchiveStorageHotpathApplyFailedPersistent</code>) Catch long-lived failed recovery state Startup guard <code>apply_total &gt; 0</code> Same alert expr Avoid first-run/startup false positives Alert duration 30m Same alert rule (<code>for: 30m</code>) Avoid transient signal noise Initial severity warning Same alert rule Tune in burn-in before considering escalation <p>Tuning guidance: - Keep age threshold at 24h until burn-in confirms clear signal quality. - If alert is too noisy, investigate watchdog state churn first; do not hide failures by increasing to multiple days. - Escalate to <code>critical</code> only after at least one week of clean behavior and verified operator response path.</p>"},{"location":"operations/thresholds-and-tuning/#sshfs-mount-options","title":"SSHFS Mount Options","text":"Option Value Location Purpose reconnect Enabled <code>docs/deployment/systemd/healtharchive-storagebox-sshfs.service</code> Auto-reconnect on connection loss ServerAliveInterval 15s systemd service Send keepalive every 15 seconds ServerAliveCountMax 3 systemd service Disconnect after 3 missed keepalives (45s total) kernel_cache Enabled systemd service Performance optimization <p>Tuning guidance: - Lower <code>ServerAliveInterval</code> to 10s if mounts go stale frequently - Don't raise <code>ServerAliveCountMax</code> - delays detection of stale connections - <code>reconnect</code> should always be enabled</p> <p>Known issue: Stale mounts still occur despite hardened options (root cause under investigation).</p> <p>See: <code>docs/planning/implemented/2026-02-01-operational-resilience-improvements.md</code></p>"},{"location":"operations/thresholds-and-tuning/#deploy-lock-protection","title":"Deploy Lock Protection","text":"Parameter Value Location Purpose Max age 2 hours <code>scripts/vps-crawl-auto-recover.py</code> + <code>scripts/vps-storage-hotpath-auto-recover.py</code> (<code>--deploy-lock-max-age-seconds</code>, default: 2h) Stale lock detection Lock file <code>/tmp/healtharchive-backend-deploy.lock</code> Deploy script + watchdogs Prevent watchdog/deploy conflicts Lock mechanism <code>flock</code> <code>scripts/vps-deploy.sh</code> Atomic lock acquisition <p>Tuning guidance: - Increase max age if deploys routinely take &gt;2 hours (investigate why) - Don't decrease below 1 hour - normal deploys can take 30-45 minutes</p>"},{"location":"operations/thresholds-and-tuning/#infra-error-cooldown","title":"Infra Error Cooldown","text":"Parameter Value Location Rationale Cooldown 10 minutes <code>src/ha_backend/worker/main.py</code> (<code>INFRA_ERROR_RETRY_COOLDOWN_MINUTES</code>) Prevent retry storms when infra is unhealthy Infra errors Errno 107, Errno 5, <code>OSError</code> during job launch <code>src/ha_backend/infra_errors.py</code> Infrastructure failures (not crawl failures) <p>Tuning guidance: - Increase to 20min if infrastructure is persistently unstable - Decrease to 5min if false positives are common (careful!)</p> <p>See: <code>docs/planning/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening.md</code></p>"},{"location":"operations/thresholds-and-tuning/#annual-output-dir-writability-probe","title":"Annual Output Dir Writability Probe","text":"<p>These checks detect permission drift for queued/retryable annual jobs before a crawl attempt consumes retries.</p> Parameter Value Location Rationale Probe cadence every 1 minute <code>docs/deployment/systemd/healtharchive-crawl-metrics.timer</code> Early warning without paging storms Probe target queued/retryable annual jobs <code>scripts/vps-crawl-metrics-textfile.py</code> Bounded cardinality (annual jobs only) Probe identity <code>haadmin</code> <code>scripts/vps-crawl-metrics-textfile.py</code> (<code>--annual-writability-probe-user</code>) Matches worker runtime user Alert duration 10m <code>ops/observability/alerting/healtharchive-alerts.yml</code> (<code>HealthArchiveAnnualOutputDirNotWritable</code>) Avoid transient noise Severity warning same alert rule Fix before retries are consumed <p>Triage signals:</p> <ul> <li><code>..._errno == 13</code>: permission drift (output dir not writable for worker user)</li> <li><code>..._errno == 107</code>: stale sshfs hot path (follow storage hot-path recovery)</li> </ul>"},{"location":"operations/thresholds-and-tuning/#archive-tool-crawler-adaptive-thresholds","title":"Archive Tool (Crawler) Adaptive Thresholds","text":""},{"location":"operations/thresholds-and-tuning/#annual-per-source-profiles","title":"Annual Per-Source Profiles","text":"<p>Annual jobs are source-tuned (not one-size-fits-all). Canonical values live in <code>src/ha_backend/job_registry.py</code> and are reconciled by <code>scripts/vps-crawl-auto-recover.py</code> during recovery/auto-start flows.</p> Source Initial workers Stall timeout Timeout/HTTP threshold Backoff Max restarts Rationale <code>hc</code> 2 75 min 55 / 55 2 min 24 Moderate tolerance for canada.ca long-tail behavior. <code>phac</code> 2 90 min 65 / 65 3 min 30 Highest tolerance due historically high restart churn. <code>cihr</code> 3 45 min 35 / 35 1 min 20 Faster/cleaner profile to improve throughput and fault detection. <p>Tuning guidance: - Change source profiles in <code>job_registry</code> first; keep watchdog reconciliation aligned. - For completeness-first posture, increase tolerance (stall/restart budget) before lowering scope. - Only reduce thresholds when repeated evidence shows low false-positive restart risk.</p>"},{"location":"operations/thresholds-and-tuning/#one-time-annual-backfillreconciliation","title":"One-Time Annual Backfill/Reconciliation","text":"<p>When migrating an existing campaign from shared defaults to per-source tuning, reconcile existing annual jobs in-place:</p> <pre><code># Review changes first (dry-run)\nha-backend reconcile-annual-tool-options --year 2026\n\n# Apply changes\nha-backend reconcile-annual-tool-options --year 2026 --apply\n</code></pre> <p>What this command does: - Reconciles baseline annual values to source profile values (<code>hc</code>, <code>phac</code>, <code>cihr</code>) - Preserves explicit non-baseline overrides (except restart floor enforcement) - Ensures annual safety defaults (<code>enable_monitoring</code>, <code>enable_adaptive_restart</code>, <code>skip_final_build</code>, <code>docker_shm_size=1g</code>)</p> <p>See: <code>src/archive_tool/constants.py</code>, <code>scripts/vps-crawl-auto-recover.py</code></p>"},{"location":"operations/thresholds-and-tuning/#summary-table-all-thresholds","title":"Summary Table: All Thresholds","text":"Category Threshold Value Priority Location Disk Worker headroom 85% P0 <code>worker/main.py</code> Alert warning 85% for 30m P1 alerting YAML Alert critical 92% for 10m P0 alerting YAML Crawl Stall threshold 60 min P1 <code>vps-crawl-auto-recover.py</code> Recovery cap 3/job/day P1 <code>vps-crawl-auto-recover.py</code> New-crawl-phase churn &gt;=3 (30m) P1 alerting YAML Slow-rate alert (HC) &lt;1.5 ppm (30m) P1 alerting YAML Slow-rate alert (PHAC) &lt;1.5 ppm (30m) P1 alerting YAML Slow-rate alert (CIHR) &lt;3 ppm (30m) P1 alerting YAML Storage Stale mount age 120s P1 <code>vps-storage-hotpath-auto-recover.py</code> Recovery cooldown 15 min P1 <code>vps-storage-hotpath-auto-recover.py</code> Recovery cap 6/day global P1 <code>vps-storage-hotpath-auto-recover.py</code> Failed apply persistence alert &gt;24h + 30m P1 alerting YAML Infra Retry cooldown 10 min P1 <code>worker/main.py</code> SSHFS Keepalive interval 15s P1 systemd service"},{"location":"operations/thresholds-and-tuning/#tuning-workflow","title":"Tuning Workflow","text":"<p>When adjusting thresholds:</p> <ol> <li>Document the change: Update this file with new values and rationale</li> <li>Test in staging (if available): Validate behavior before production</li> <li>Monitor metrics: Watch Prometheus/Grafana for impact</li> <li>Iterate conservatively: Small adjustments, measure, repeat</li> <li>Update automation: Adjust watchdog caps if needed</li> </ol> <p>Anti-patterns: - Disabling safety caps to \"fix\" underlying issues - Tuning based on single incidents without trend analysis - Raising thresholds indefinitely instead of fixing root cause</p>"},{"location":"operations/thresholds-and-tuning/#related-documentation","title":"Related Documentation","text":"<ul> <li>Disk baseline: <code>docs/operations/disk-baseline-and-cleanup.md</code></li> <li>Alerting strategy: <code>docs/operations/monitoring-and-alerting.md</code></li> <li>Stale mount playbook: <code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> <li>Crawl stall playbook: <code>docs/operations/playbooks/crawl/crawl-stalls.md</code></li> <li>Operational resilience improvements: <code>docs/planning/implemented/2026-02-01-operational-resilience-improvements.md</code></li> </ul>"},{"location":"operations/verification-packet/","title":"Verification Packet (draft outline)","text":"<p>Purpose: A short, shareable summary for a verifier (librarian, researcher, or editor) to review and potentially confirm your role and the project's value.</p> <p>Do NOT include private emails or personal data. Keep it factual.</p>"},{"location":"operations/verification-packet/#1-project-overview-one-paragraph","title":"1) Project overview (one paragraph)","text":"<p>HealthArchive.ca is an independent, non-governmental archive of Canadian public health web pages. It preserves time-stamped snapshots and provides descriptive change tracking so researchers, journalists, and educators can audit how guidance evolves over time. It is not current guidance or medical advice.</p>"},{"location":"operations/verification-packet/#2-what-is-live-now-key-links","title":"2) What is live now (key links)","text":"<ul> <li>Home: https://www.healtharchive.ca/</li> <li>Archive search: https://www.healtharchive.ca/archive</li> <li>Snapshot viewer: https://www.healtharchive.ca/snapshot/ <li>Changes feed (edition-aware): https://www.healtharchive.ca/changes</li> <li>Compare view: https://www.healtharchive.ca/compare?to= <li>Digest + RSS: https://www.healtharchive.ca/digest</li> <li>Methods &amp; scope: https://www.healtharchive.ca/methods</li> <li>Governance: https://www.healtharchive.ca/governance</li> <li>Status/impact: https://www.healtharchive.ca/status and /impact</li>"},{"location":"operations/verification-packet/#3-what-i-built-operate-short-bullets","title":"3) What I built / operate (short bullets)","text":"<ul> <li>Archive capture pipeline (annual editions, scope rules).</li> <li>Snapshot indexing and search.</li> <li>Public changes feed and compare views (descriptive diffs).</li> <li>Governance, takedown, and issue-reporting process.</li> <li>Ongoing operations (monitoring, backups, scheduled change tracking).</li> </ul>"},{"location":"operations/verification-packet/#4-current-metrics-fill-from-status","title":"4) Current metrics (fill from /status)","text":"<ul> <li>Sources tracked: [N]</li> <li>Snapshots: [N]</li> <li>Pages: [N]</li> <li>Latest capture date (UTC): [YYYY-MM-DD]</li> </ul>"},{"location":"operations/verification-packet/#5-what-verification-would-mean-sample-wording","title":"5) What verification would mean (sample wording)","text":"<p>\"I can verify that  built and operates HealthArchive.ca, and that the project is used as a resource for reproducibility / auditability of Canadian public health guidance. I understand it is an archival tool, not medical advice.\""},{"location":"operations/verification-packet/#6-permission-request-explicit","title":"6) Permission request (explicit)","text":"<ul> <li>Are you willing to verify my role and the project's utility?</li> <li>May I list your name and title publicly as a verifier?</li> <li>Preferred wording (if any)?</li> </ul>"},{"location":"operations/verification-packet/#7-notes-for-the-verifier","title":"7) Notes for the verifier","text":"<ul> <li>The site is explicitly non-authoritative and not affiliated with any public   health agency.</li> <li>Change tracking is descriptive only; it does not interpret meaning.</li> </ul>"},{"location":"operations/incidents/","title":"Incident notes (internal)","text":"<p>This folder contains incident notes / lightweight postmortems for production and operations issues.</p> <p>Goals:</p> <ul> <li>Capture what happened (timeline + impact) while it\u2019s fresh.</li> <li>Record the root cause and recovery steps (so we can repeat them safely).</li> <li>Track \u201cstill to do\u201d work that reduces repeat incidents (docs, guardrails, automation).</li> </ul> <p>Related:</p> <ul> <li>Operator recovery steps: <code>../playbooks/core/incident-response.md</code></li> <li>Ops playbooks index: <code>../playbooks/README.md</code></li> <li>Severity rubric: <code>severity.md</code></li> </ul>"},{"location":"operations/incidents/#what-goes-here","title":"What goes here","text":"<p>Use an incident note when any of the following are true:</p> <ul> <li>Public site/API degraded or down.</li> <li>Crawl/indexing is stuck, repeatedly failing, or risking data integrity.</li> <li>Storage/mount issues (e.g. Errno 107) or \u201chot path\u201d problems.</li> <li>You had to do manual intervention beyond routine operations.</li> </ul> <p>Do not use incident notes for planned maintenance; record that in the changelog (and/or a runbook update) instead.</p>"},{"location":"operations/incidents/#naming","title":"Naming","text":"<p>One file per incident:</p> <ul> <li><code>YYYY-MM-DD-&lt;short-slug&gt;.md</code> (use UTC date of incident start).</li> <li>If multiple incidents share a date, add a suffix: <code>...-a</code>, <code>...-b</code>.</li> </ul> <p>Example:</p> <ul> <li><code>2026-01-09-annual-crawl-phac-output-dir-permission-denied.md</code></li> <li><code>2026-01-16-replay-smoke-503-and-warctieringfailed.md</code></li> </ul>"},{"location":"operations/incidents/#incident-notes-index","title":"Incident notes index","text":"<ul> <li>API search/changes 500 due to missing dedupe migration (2026-02-06)</li> <li>Auto-recover stall detection bugs (2026-02-06)</li> <li>Annual crawl output dirs on root disk (2026-02-04)</li> <li>Storage watchdog unmount filter bug (2026-02-02)</li> <li>Infra error 107 + hot-path thrash + worker stop (2026-01-24)</li> <li>Replay smoke 503 + tiering failures (2026-01-16)</li> <li>Annual crawl stalled (HC) (2026-01-09)</li> <li>PHAC output dir permission denied (2026-01-09)</li> <li>Storage hot-path sshfs stale mount (2026-01-08)</li> </ul>"},{"location":"operations/incidents/#how-to-write-one","title":"How to write one","text":"<p>1) Copy the template: <code>../../_templates/incident-template.md</code> 2) Fill the top metadata and a short summary immediately. 3) Add a timeline (UTC) as you work. 4) After recovery, fill root cause + follow-ups. 5) Link any follow-up playbooks/runbooks/roadmaps you touched. 6) If this incident changes user expectations (outage/degradation, integrity risk, security posture, policy change), add a public-safe note in <code>/changelog</code> and/or <code>/status</code> (no sensitive details; changelog process: https://github.com/jerdaw/healtharchive-frontend/blob/main/docs/changelog-process.md).</p> <p>The template includes an Action items (TODOs) section; use checkboxes so it\u2019s obvious what work remains.</p> <p>If the incident requires engineering work (automation, new scripts, behavior changes), capture it as a follow-up and create a focused implementation plan under <code>docs/planning/</code> (then link it from the incident note).</p>"},{"location":"operations/incidents/#what-not-to-include","title":"What not to include","text":"<ul> <li>Secrets (tokens, passwords, Healthchecks URLs).</li> <li>Private emails or non-public IPs/hostnames.</li> <li>Full logs. Prefer:</li> <li>the exact log path(s),</li> <li>the most relevant ~20\u201350 lines, and</li> <li>one or two <code>vps-*.sh</code> snapshots.</li> </ul>"},{"location":"operations/incidents/#style","title":"Style","text":"<ul> <li>Keep it blameless: focus on systems, invariants, and guardrails (not individuals).</li> <li>Prefer concrete facts over speculation; if something is unknown, label it as such.</li> <li>Record commands that changed state (DB writes, mounts, restarts) and what they affected.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/","title":"Incident: Annual crawl \u2014 Storage hot-path sshfs mounts went stale (Errno 107) (2026-01-08)","text":"<p>Status: closed</p>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-01-08</li> <li>Severity (see <code>severity.md</code>): sev1</li> <li>Environment: production (single VPS)</li> <li>Primary area: storage</li> <li>Owner: (unassigned)</li> <li>Start (UTC): 2026-01-08T06:31:43Z (approx; first observed Errno 107 in worker logs)</li> <li>End (UTC): 2026-01-08T20:38:39Z (approx; crawler restarted and hot paths readable again)</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#summary","title":"Summary","text":"<p>Several Storage Box \u201chot path\u201d <code>sshfs</code> mountpoints under <code>/srv/healtharchive/jobs/**</code> became stale and started returning <code>OSError: [Errno 107] Transport endpoint is not connected</code>. This caused the worker to throw exceptions when reading/writing job output dirs, the crawl metrics textfile writer to fail repeatedly, and annual crawl jobs (HC/PHAC/CIHR) to fail/retry without making forward progress.</p> <p>Recovery required stopping the worker, lazily unmounting the stale hot-path mountpoints, re-applying tiering bind mounts, and marking affected jobs as <code>retryable</code> so they could safely restart. After recovery, the worker successfully restarted the HC crawl and resumed writing WARCs to the output directory.</p>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#impact","title":"Impact","text":"<ul> <li>User-facing impact: none directly observed, but annual campaign remained <code>Ready for search: NO</code> while jobs were blocked/failing.</li> <li>Internal impact (ops burden, automation failures, etc):</li> <li>Manual operator intervention required (mount recovery + job recovery).</li> <li><code>healtharchive-crawl-metrics.service</code> failed repeatedly (reduced visibility during the incident).</li> <li>Worker loop repeatedly hit <code>Errno 107</code> and could not safely proceed with affected jobs.</li> <li>Data impact:</li> <li>Data loss: unknown (no evidence of WARC deletion; risk was primarily loss of crawl continuity and partial/aborted crawl attempts).</li> <li>Data integrity risk: medium (stale mounts can interrupt writes and break assumptions about output dir readability; risk reduced after later WARC verification).</li> <li>Recovery completeness: complete for mount recovery; annual campaign completion remained in-progress.</li> <li>Duration: ~14 hours (approx; first Errno 107 observed in morning logs \u2192 successful crawl restart in the evening).</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#detection","title":"Detection","text":"<ul> <li>Operator status snapshot:</li> <li><code>./scripts/vps-crawl-status.sh --year 2026</code> showed <code>WARN job output dir not found/readable</code> and missing running-job log tails.</li> <li>Direct filesystem symptom:</li> <li><code>ls -la /srv/healtharchive/jobs/hc/</code> returned <code>Transport endpoint is not connected</code> and showed <code>d?????????</code> for the affected job dir.</li> <li>Monitoring symptom:</li> <li><code>systemctl status healtharchive-crawl-metrics.timer healtharchive-crawl-metrics.service</code> showed the metrics writer exiting non-zero.</li> <li><code>journalctl -u healtharchive-crawl-metrics.service</code> showed a traceback ending in <code>OSError: [Errno 107] Transport endpoint is not connected: '&lt;job output dir&gt;'</code>.</li> <li>Worker symptom:</li> <li><code>journalctl -u healtharchive-worker.service</code> showed <code>Unexpected error in worker iteration: [Errno 107] ...</code> while picking jobs 6/7/8.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#most-relevant-excerpts-redacted","title":"Most relevant excerpts (redacted)","text":"<p>Worker journal (error propagation into the worker loop):</p> <pre><code>Jan 08 06:31:43 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:31:43,663 [WARNING] healtharchive.worker: Crawl for job 6 failed (RC=1). Marking as retryable (retry_count=1).\nJan 08 06:31:43 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:31:43,675 [INFO] healtharchive.worker: Worker picked job 6 for source hc (Health Canada) with status retryable and retry_count 1\nJan 08 06:31:43 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:31:43,684 [ERROR] healtharchive.worker: Unexpected error in worker iteration: [Errno 107] Transport endpoint is not connected: '/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101'\nJan 08 06:32:13 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:32:13,694 [INFO] healtharchive.worker: Worker picked job 7 for source phac (Public Health Agency of Canada) with status queued and retry_count 0\nJan 08 06:32:13 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:32:13,702 [ERROR] healtharchive.worker: Unexpected error in worker iteration: [Errno 107] Transport endpoint is not connected: '/srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101'\nJan 08 06:32:43 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:32:43,711 [INFO] healtharchive.worker: Worker picked job 8 for source cihr (Canadian Institutes of Health Research) with status queued and retry_count 0\nJan 08 06:32:43 &lt;vps&gt; ha-backend[302894]: 2026-01-08 06:32:43,718 [ERROR] healtharchive.worker: Unexpected error in worker iteration: [Errno 107] Transport endpoint is not connected: '/srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101'\n</code></pre> <p>Crawl metrics writer failure (systemd service repeatedly failing due to <code>Errno 107</code> during output-dir probing):</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/healtharchive-backend/scripts/vps-crawl-metrics-textfile.py\", line 174, in main\n    log_path = _find_job_log(job)\n  File \"/opt/healtharchive-backend/scripts/vps-crawl-metrics-textfile.py\", line 33, in _find_latest_combined_log\n    if not output_dir.is_dir():\n  File \"/usr/lib/python3.12/pathlib.py\", line 842, in stat\n    return os.stat(self, follow_symlinks=follow_symlinks)\nOSError: [Errno 107] Transport endpoint is not connected: '/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101'\n</code></pre> <p>Filesystem symptom (stale FUSE mountpoint):</p> <pre><code>$ ls -la /srv/healtharchive/jobs/hc/\nls: cannot access '/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101': Transport endpoint is not connected\nd????????? ? ? ? ? ? 20260101T000502Z__hc-20260101\n</code></pre>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#decision-log-optional-but-recommended-for-sev0sev1","title":"Decision log (optional but recommended for sev0/sev1)","text":"<ul> <li>2026-01-08T20:17Z (approx) \u2014 Decision: stop <code>healtharchive-worker.service</code> before unmounting hot paths (why: avoid concurrent reads/writes against a stale FUSE mount; risks: temporarily halts all crawl work).</li> <li>2026-01-08T20:18Z (approx) \u2014 Decision: use <code>umount -l</code> (lazy) for stale mountpoints (why: avoid blocking on FUSE teardown; risks: processes holding FDs continue referencing the old mount until released).</li> <li>2026-01-08T20:22Z (approx) \u2014 Decision: mark jobs as <code>retryable</code> (and later <code>retry-job</code>) after storage recovery (why: allow the worker to restart crawls cleanly; risks: consumes retry budget if repeated).</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-01-08T06:20:00Z \u2014 Worker monitoring logged repeated HTTP/Network errors (many <code>net::ERR_HTTP2_PROTOCOL_ERROR</code>) during the HC crawl (context for the long-running crawl).</li> <li>2026-01-08T06:25:24Z \u2014 CrawlMonitor thread logged: \u201cDocker logs stream ended\u201d (crawl stage ended).</li> <li>2026-01-08T06:31:43Z \u2014 <code>archive_tool</code> and the worker encountered <code>OSError: [Errno 107] Transport endpoint is not connected</code> on the HC job output dir; worker then hit the same error when attempting PHAC and CIHR output dirs.</li> <li>2026-01-08T19:52:58Z \u2014 Operator ran <code>./scripts/vps-crawl-status.sh --year 2026</code> and observed job output dir unreadable and crawl jobs failing/retrying.</li> <li>2026-01-08T20:09:02Z \u2014 <code>healtharchive-crawl-metrics.service</code> repeatedly failed with <code>Errno 107</code> while probing output dirs/logs.</li> <li>2026-01-08T20:17Z (approx) \u2014 Operator stopped worker, unmounted stale hot-path mountpoints for job output dirs.</li> <li>2026-01-08T20:18Z (approx) \u2014 First attempt to re-apply tiering bind mounts failed due to additional stale mounts under <code>/srv/healtharchive/jobs/imports/**</code>.</li> <li>2026-01-08T20:21Z (approx) \u2014 Operator unmounted stale imports mountpoints and re-applied tiering bind mounts successfully.</li> <li>2026-01-08T20:22Z (approx) \u2014 Operator ran <code>ha-backend recover-stale-jobs --apply</code> and restarted the worker; crawl metrics writer started succeeding again.</li> <li>2026-01-08T20:34:34Z \u2014 Status snapshot showed annual jobs in <code>failed</code> (no running jobs); operator re-marked jobs <code>retryable</code> via <code>ha-backend retry-job</code>.</li> <li>2026-01-08T20:38:39Z \u2014 Worker picked job 6 and successfully launched a new <code>zimit</code> container; crawl resumed and began producing new WARCs.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger: one or more <code>sshfs</code> \u201chot path\u201d mountpoints under <code>/srv/healtharchive/jobs/**</code> became stale, causing <code>stat(2)</code> and directory reads to fail with <code>Errno 107</code> (\u201cTransport endpoint is not connected\u201d).</li> <li>Underlying cause(s): unknown.</li> <li>Hypothesis: transient network disruption between the VPS and Storage Box left multiple nested <code>sshfs</code> mounts in a stale-but-mounted state; the base Storage Box mount remained active, but hot-path submounts did not recover automatically.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#contributing-factors","title":"Contributing factors","text":"<ul> <li>The system had multiple per-job/per-path <code>sshfs</code> mountpoints (\u201chot paths\u201d), multiplying the surface area for FUSE staleness.</li> <li>Several code paths treated output-dir probes as infallible:</li> <li><code>archive_tool</code> attempted to <code>stat()</code> combined logs and raised an unhandled exception when the mount was stale.</li> <li>The crawl metrics writer crashed rather than emitting a \u201cprobe failed\u201d metric.</li> <li>No hot-path auto-recovery timer/sentinel was enabled at the time, so stale mountpoints persisted until manual intervention.</li> <li>The crawl was long-running and noisy (frequent HTTP2 protocol errors/timeouts), increasing the chance of being mid-operation when storage became unavailable.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#resolution-recovery","title":"Resolution / Recovery","text":""},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#1-confirm-the-symptom-and-scope","title":"1) Confirm the symptom and scope","text":"<ul> <li>Confirmed filesystem error:</li> <li><code>ls -la /srv/healtharchive/jobs/hc/</code> \u2192 <code>Transport endpoint is not connected</code></li> <li>Confirmed the affected paths were <code>sshfs</code> mountpoints:</li> <li><code>mount | rg '/srv/healtharchive/jobs/(hc|phac|cihr)/20260101T000502Z__'</code></li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#2-stop-the-worker-to-prevent-concurrent-io-against-stale-mounts","title":"2) Stop the worker to prevent concurrent I/O against stale mounts","text":"<pre><code>sudo systemctl stop healtharchive-worker.service\n</code></pre>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#3-lazily-unmount-stale-job-output-dir-hot-paths","title":"3) Lazily unmount stale job output-dir hot paths","text":"<pre><code>sudo umount -l /srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101\nsudo umount -l /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101\nsudo umount -l /srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101\n</code></pre> <p>What this changed:</p> <ul> <li>Removed stale FUSE mountpoints so the tiering scripts could remount cleanly.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#4-re-apply-tiering-bind-mounts-and-clear-any-additional-stale-mounts","title":"4) Re-apply tiering bind mounts (and clear any additional stale mounts)","text":"<p>First attempt surfaced additional stale mounts under legacy imports (same symptom):</p> <pre><code>sudo /opt/healtharchive-backend/scripts/vps-warc-tiering-bind-mounts.sh --apply\n</code></pre> <p>Then unmounted the stale imports mountpoints and re-ran the bind-mount script:</p> <pre><code>mount | rg '/srv/healtharchive/jobs/imports'\nsudo umount -l /srv/healtharchive/jobs/imports/legacy-hc-2025-04-21\nsudo umount -l /srv/healtharchive/jobs/imports/legacy-cihr-2025-04\nsudo /opt/healtharchive-backend/scripts/vps-warc-tiering-bind-mounts.sh --apply\n</code></pre> <p>What this changed:</p> <ul> <li>Restored canonical tiered WARC paths and removed stale \u201cimports\u201d hot paths blocking the bind-mount installer.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#5-requeue-stale-jobs-in-the-db","title":"5) Requeue stale jobs in the DB","text":"<pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 5 --apply --limit 10\n</code></pre> <p>What this changed:</p> <ul> <li>Marked jobs 6/7/8 as <code>retryable</code> so the worker could safely restart them after storage recovery.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#6-restart-the-worker-and-confirm-metrics-writer-success","title":"6) Restart the worker and confirm metrics writer success","text":"<pre><code>sudo systemctl start healtharchive-worker.service\nsystemctl status healtharchive-crawl-metrics.service --no-pager -l\n</code></pre>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#7-explicitly-retry-annual-jobs-and-restart-worker-loop","title":"7) Explicitly retry annual jobs and restart worker loop","text":"<pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend retry-job --id 6\n/opt/healtharchive-backend/.venv/bin/ha-backend retry-job --id 7\n/opt/healtharchive-backend/.venv/bin/ha-backend retry-job --id 8\nsudo systemctl restart healtharchive-worker.service\n</code></pre> <p>What this changed:</p> <ul> <li>Ensured the jobs were eligible for immediate pickup and restarted the worker to pick a retryable job promptly.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#post-incident-verification","title":"Post-incident verification","text":"<ul> <li>Public surface checks:</li> <li>Not performed as part of the storage recovery; incident scope was internal pipeline health.</li> <li>Worker/job health checks:</li> <li><code>sudo systemctl status healtharchive-worker.service --no-pager</code></li> <li><code>./scripts/vps-crawl-status.sh --year 2026</code> (confirm jobs are running/retryable and output dirs readable)</li> <li><code>docker ps | rg 'ghcr.io/openzim/zimit'</code> (confirm active crawl container)</li> <li>Storage/mount checks (if relevant):</li> <li><code>mount | rg '/srv/healtharchive/jobs/(hc|phac|cihr)/20260101T000502Z__'</code></li> <li><code>ls -la /srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101 | head</code></li> <li>Integrity checks (if relevant):</li> <li>After recovery, ran WARC verification (sampling) to reduce integrity uncertainty:<ul> <li><code>/opt/healtharchive-backend/.venv/bin/ha-backend verify-warcs --job-id 6 --level 0 --limit-warcs 20</code></li> </ul> </li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li>What was the underlying trigger for the <code>sshfs</code> hot-path staleness (network instability, server-side disconnect, local FUSE behavior, or sshfs option mismatch)?</li> <li>Why did multiple independent hot paths go stale at once (shared failure mode), while the base mount remained active?</li> <li>Should we treat <code>Errno 107</code> as a first-class \u201cinfra error\u201d everywhere (worker, archive_tool, metrics) so it never consumes retry budget and never crashes the worker loop?</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> Create a focused roadmap and implement guardrails/automation: <code>docs/planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code> (owner=eng, priority=high, due=2026-01-15)</li> <li> Add \u201chot path unreadable\u201d metrics + alerting rules (owner=eng, priority=high, due=2026-01-15)</li> <li> Add operator drill tooling for alert pipeline and stale-mount recovery (owner=eng, priority=medium, due=2026-01-20)</li> <li> Enable <code>healtharchive-storage-hotpath-auto-recover.timer</code> + sentinel on production after a maintenance window (ensure it will not interrupt active crawls unexpectedly). (owner=ops, priority=high, due=2026-01-20, done=2026-01-16)</li> <li> Add an operator runbook step to clear \u201cfailed\u201d systemd unit state after recovery (<code>systemctl reset-failed ...</code>) so warning alerts don\u2019t linger. (owner=ops, priority=medium, due=2026-01-20, done=2026-01-16)</li> <li> Investigate (and document) why hot-path mounts can become stale while the base mount remains OK; adjust sshfs options if needed. (owner=ops, priority=medium, due=unknown)</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>Safe automation implemented post-incident:</li> <li><code>scripts/vps-storage-hotpath-auto-recover.py</code> can detect <code>Errno 107</code> and perform a conservative recovery sequence (stop worker \u2192 unmount stale hot paths \u2192 re-apply tiering \u2192 requeue stale jobs \u2192 start worker), with safeguards (cooldowns, caps, \u201cconfirm runs\u201d).</li> <li>Risk/false positives to consider:</li> <li>Stopping the worker while a crawl is legitimately progressing can cause unnecessary job restarts and reduce annual coverage.</li> <li>Unmount/remount operations are destructive if targeted at the wrong mountpoint; the detector must be confident (Errno 107) and scoped.</li> <li>Automation should remain opt-in via a sentinel file and should be enabled only once its posture matches the desired operational risk tolerance.</li> </ul>"},{"location":"operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Status snapshots:</li> <li><code>scripts/vps-crawl-status.sh</code> (see operator runs around 2026-01-08 19:52Z and 20:34Z)</li> <li>Relevant logs / error excerpts:</li> <li><code>sudo journalctl -u healtharchive-worker.service --since '2026-01-08 06:20' --until '2026-01-08 06:45' --no-pager -l</code></li> <li><code>sudo journalctl -u healtharchive-crawl-metrics.service --since '2026-01-08 20:00' --no-pager -l</code></li> <li>Job output dirs impacted:</li> <li><code>/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101</code></li> <li><code>/srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101</code></li> <li><code>/srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101</code></li> <li>Tiering / mounts:</li> <li><code>scripts/vps-warc-tiering-bind-mounts.sh</code></li> <li><code>scripts/vps-annual-output-tiering.py</code></li> <li>Playbook: <code>../playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> <li>Drill playbook: <code>../playbooks/storage/storagebox-sshfs-stale-mount-drills.md</code></li> <li>Follow-up implementation plan:</li> <li><code>docs/planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/","title":"Incident: Annual crawl \u2014 HC job stalled (2026-01-09)","text":"<p>Status: draft (ongoing)</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-01-09</li> <li>Severity: sev1</li> <li>Environment: production</li> <li>Primary area: crawl</li> <li>Owner: (unassigned)</li> <li>Start (UTC): 2026-01-09T07:34:37Z (last observed crawl progress)</li> <li>End (UTC): ongoing</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#summary","title":"Summary","text":"<p>The annual crawl job for <code>hc</code> (job 6) entered a stalled state: <code>crawlStatus</code> stopped advancing and the crawl metrics exporter flagged it as stalled. The stall correlated with repeated <code>Navigation timeout</code> warnings on canada.ca pages.</p> <p>Manual recovery (stop worker + recover stale jobs) was intentionally deferred while <code>cihr</code> (job 8) was actively crawling, to avoid turning an in-progress crawl into a <code>failed</code> job at max retries.</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#impact","title":"Impact","text":"<ul> <li>User-facing impact: annual campaign remained <code>Ready for search: NO</code>.</li> <li>Internal impact: operator attention required; <code>hc</code> crawl not progressing.</li> <li>Data impact:</li> <li>Data loss: unknown (WARCs exist in temp dirs, but crawl completeness is unknown until completion).</li> <li>Data integrity risk: low/unknown (no specific corruption signals observed; primarily a progress/stall problem).</li> <li>Recovery completeness: not recovered at time of write-up.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#detection","title":"Detection","text":"<ul> <li><code>./scripts/vps-crawl-status.sh --year 2026 --job-id 6</code>:</li> <li><code>healtharchive_crawl_running_job_stalled{job_id=\"6\",source=\"hc\"} 1</code></li> <li><code>last_progress_age_seconds</code> climbed into multi-hour range.</li> <li><code>crawlStatus tail</code> stopped advancing.</li> <li><code>recent timeouts</code> showed repeated <code>Navigation timeout of 90000 ms exceeded</code>.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#decision-log","title":"Decision log","text":"<ul> <li>2026-01-09 \u2014 Deferred the \u201cstop worker + recover stale jobs\u201d procedure while job 8 (<code>cihr</code>) was actively crawling to reduce the risk of interrupting it at max retries.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-01-09T06:05:14Z \u2014 Job 6 started (latest observed start time in status snapshot).</li> <li>2026-01-09T07:34:37Z \u2014 Last observed <code>crawlStatus</code> progress for job 6 (<code>crawled=437</code>, <code>total=3209</code>, <code>pending=1</code>).</li> <li>2026-01-09T12:57:17Z \u2014 Status snapshot shows multi-hour no-progress and <code>stalled=1</code>.</li> <li>2026-01-09T13:33:23Z \u2014 Status snapshot still shows <code>stalled=1</code>.</li> <li>2026-01-16T02:56:12Z \u2014 Manual recovery performed (stop worker + recover stale jobs). Job 6 restarted and began a new crawl attempt.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#root-cause","title":"Root cause","text":"<p>Unknown. Strong signals point to crawl progress blocked by repeated page load failures/timeouts and/or a crawler worker getting stuck on a specific URL.</p> <p>As of 2026-01-16, the job showed many <code>net::ERR_HTTP2_PROTOCOL_ERROR</code> failures on canada.ca and <code>archive_tool</code> applied repeated backoff delays after hitting its HTTP/network error threshold.</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#contributing-factors","title":"Contributing factors","text":"<ul> <li>Many canada.ca pages timed out (90s navigation timeouts), increasing the chance of long \u201cpending page\u201d windows.</li> <li><code>hc</code> and <code>cihr</code> were both running; the safest recovery approach (stopping the worker) would interrupt both.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#decision-manual-recovery-option-c","title":"Decision: Manual Recovery (Option C)","text":"<p>We elected to stick with the manual recover-stale-jobs procedure (documented in <code>../playbooks/crawl/crawl-stalls.md</code>) rather than automating granular per-job stops. The risk of interrupting a healthy concurrent job is acceptable given the rarity of stalls, and stopping the worker is the safest way to ensure no partial state corruption.</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#resolution-recovery","title":"Resolution / Recovery","text":"<p>Performed on 2026-01-16 (VPS):</p> <ul> <li>Followed <code>docs/operations/playbooks/crawl/crawl-stalls.md</code>:</li> <li><code>sudo systemctl stop healtharchive-worker.service</code></li> <li><code>set -a; source /etc/healtharchive/backend.env; set +a</code></li> <li><code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 5 --apply --source hc --limit 1</code></li> <li><code>sudo systemctl start healtharchive-worker.service</code></li> <li>Verified the job restarted (<code>Started at</code> updated) and a new combined log was created.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#post-incident-verification","title":"Post-incident verification","text":"<p>TBD (once recovered).</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li>What exact URL/work unit is the crawler stuck on (if any), and does it repeat across retries?</li> <li>Are timeouts driven by site performance, network issues, headless browser instability, or scope rules?</li> <li>Would changing timeouts/adaptive restart thresholds reduce repeat stalls without harming completeness?</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> After <code>cihr</code> completes (or during a maintenance window), perform the planned recovery steps and update this note with outcomes. (priority=high)</li> <li> (Pending Operator Check) If the stall repeats, capture the specific repeated URL(s) and assess whether scope/timeout tuning is warranted. (owner=ops, priority=medium)</li> <li> Consider tightening/clarifying automation boundaries: per-job recovery without stopping unrelated active crawls.</li> <li>Decision: Explicitly deferred/rejected in favor of manual \"stop worker + recover\" procedure (Option C) to minimize complexity.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>Improve \u201cstalled crawl\u201d detection to include the most recent pending URL and age as part of operator output (snapshot script) and/or alert annotations.</li> <li>Investigate whether recovery can be scoped to a single crawl process/container without stopping the entire worker loop (risk: false positives and partial state).</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-hc-job-stalled/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Operator snapshot script: <code>scripts/vps-crawl-status.sh</code></li> <li>Latest combined log (as of 2026-01-09 12:57Z snapshot): <code>/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101/archive_new_crawl_phase_-_attempt_1_20260109_060517.combined.log</code></li> <li>Latest combined log after 2026-01-16 recovery: <code>/srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101/archive_new_crawl_phase_-_attempt_1_20260116_025617.combined.log</code></li> <li>Playbook: <code>../playbooks/crawl/crawl-stalls.md</code></li> <li>Playbook: <code>../playbooks/core/incident-response.md</code></li> <li>Related: <code>2026-01-09-annual-crawl-phac-output-dir-permission-denied.md</code></li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/","title":"Incident: Annual crawl \u2014 PHAC output dir not writable (2026-01-09)","text":"<p>Status: draft</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-01-09</li> <li>Severity: sev2</li> <li>Environment: production</li> <li>Primary area: crawl + storage</li> <li>Owner: (unassigned)</li> <li>Start (UTC): 2026-01-08T20:22:04Z</li> <li>End (UTC): 2026-01-09T13:39:52Z (mitigated; awaiting successful retry)</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#summary","title":"Summary","text":"<p>The annual crawl job for <code>phac</code> (job 7) repeatedly failed immediately because its job <code>output_dir</code> was not writable (<code>PermissionError</code> while creating a <code>.writable_test_*</code> file). The job produced no WARCs and consumed its retry budget.</p> <p>Recovery restored a writable output directory and reset the job\u2019s retry budget (<code>retry_count=0</code>) so the worker can safely reattempt it when capacity is available.</p>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#impact","title":"Impact","text":"<ul> <li>User-facing impact: none directly, but annual campaign remained <code>Ready for search: NO</code> while jobs were incomplete.</li> <li>Internal impact: operator intervention required; <code>phac</code> job blocked; retry budget consumed.</li> <li>Data impact:</li> <li>Data loss: no (no WARCs were produced).</li> <li>Data integrity risk: low (failure-to-start; no partial WARC writes).</li> <li>Recovery completeness: partial (job left <code>retryable</code>; not yet re-run at time of write-up).</li> <li>Duration: ~17 hours (first failure \u2192 operator repair + retry reset).</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#detection","title":"Detection","text":"<ul> <li><code>./scripts/vps-crawl-status.sh --year 2026</code> showed:</li> <li><code>phac</code> job 7: <code>status=retryable</code>, <code>crawl_rc=1</code>, <code>crawl_status=failed</code>, <code>WARC files=0</code></li> <li>Worker journal showed the root symptom during job startup:</li> <li><code>CRITICAL ... Output directory ... is invalid or not writable: [Errno 13] Permission denied: .../.writable_test_&lt;pid&gt;</code></li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#decision-log","title":"Decision log","text":"<ul> <li>2026-01-09 \u2014 Avoided interventions that stop <code>healtharchive-worker.service</code> while <code>cihr</code> was actively crawling (to reduce the risk of turning an in-progress crawl into a <code>failed</code> job at max retries).</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-01-08T20:22:04Z \u2014 Worker picked job 7 (<code>phac</code>); job failed immediately due to <code>output_dir</code> not writable (Errno 13).</li> <li>2026-01-09T05:21:15Z \u2014 Status snapshot: job 7 still <code>retryable</code>/failed with <code>0</code> WARCs.</li> <li>2026-01-09T13:10Z \u2014 Confirmed the job output dir is an <code>sshfs</code> hot path mountpoint (<code>findmnt -T &lt;output_dir&gt;</code> shows <code>fstype=fuse.sshfs</code>).</li> <li>2026-01-09T13:10Z \u2014 Attempted <code>chown</code> of the output dir failed (<code>Permission denied</code>) because the path is on <code>sshfs</code>.</li> <li>2026-01-09T13:26:21Z \u2014 <code>ha-backend validate-job-config --id 7</code> confirmed crawler command construction and output dir resolution.</li> <li>2026-01-09T13:39:52Z \u2014 Reset <code>retry_count</code> to <code>0</code> via Python + SQLAlchemy session so job can be retried safely.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger: <code>archive_tool</code> refused to start because <code>output_dir</code> was not writable.</li> <li>Underlying cause(s): job output directory mount/permissions were not compatible with the worker/crawler runtime user (details TBD).</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#contributing-factors","title":"Contributing factors","text":"<ul> <li>Direct <code>psql</code> access from the operator account failed due to missing local DB role mapping (e.g., <code>role \"haadmin\" does not exist</code>).</li> <li>The <code>output_dir</code> is on <code>sshfs</code>, so ownership fixes via <code>chown</code> are not available on the VPS; recovery requires \u201cmake the mount writable\u201d rather than \u201cchange owner\u201d.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#resolution-recovery","title":"Resolution / Recovery","text":"<ul> <li>Diagnosed job output dir mount + permissions:</li> <li>Confirmed job config and path:<ul> <li><code>ha-backend show-job --id 7</code> \u2192 <code>Output dir: /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101</code></li> </ul> </li> <li>Confirmed it is an <code>sshfs</code> hot path mountpoint:<ul> <li><code>findmnt -T /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101 -o TARGET,SOURCE,FSTYPE,OPTIONS</code></li> </ul> </li> <li>Confirmed the worker user:<ul> <li><code>systemctl show -p User -p Group healtharchive-worker.service</code> \u2192 <code>User=haadmin</code>, <code>Group=haadmin</code></li> </ul> </li> <li>Attempted to fix ownership failed (<code>Permission denied</code>) because the output dir is on <code>sshfs</code>:<ul> <li><code>sudo chown &lt;worker_user&gt;:&lt;worker_group&gt; &lt;output_dir&gt;</code></li> </ul> </li> <li>Ensured a writable output dir:</li> <li>Verified writability with a host-level probe:<ul> <li><code>touch /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101/.writable_test &amp;&amp; rm /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101/.writable_test</code></li> </ul> </li> <li>Validated annual tiering state for <code>phac</code>:<ul> <li><code>sudo /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py --year 2026 --sources phac --apply</code></li> </ul> </li> <li>Validated job configuration:</li> <li><code>ha-backend validate-job-config --id 7</code></li> <li>Reset the job retry budget:</li> <li>Direct <code>psql</code> access failed due to missing DB roles for the operator account (<code>role \"haadmin\" does not exist</code>, <code>role \"root\" does not exist</code>).</li> <li> <p>Used a small Python snippet with <code>ha_backend.db.get_session()</code> to set <code>retry_count=0</code> for <code>job_id=7</code>:</p> <ul> <li>```bash   /opt/healtharchive-backend/.venv/bin/python3 - &lt;&lt;'PY'   from ha_backend.db import get_session   from ha_backend.models import ArchiveJob</li> </ul> <p>job_id = 7   with get_session() as session:       job = session.get(ArchiveJob, job_id)       if job is None:           raise SystemExit(f\"job {job_id} not found\")       old = job.retry_count       job.retry_count = 0       session.commit()       print(f\"OK job_id={job_id} retry_count {old} -&gt; {job.retry_count}\")   PY   ```</p> </li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#post-incident-verification","title":"Post-incident verification","text":"<ul> <li>Confirmed output dir is writable on the host.</li> <li>Confirmed job config dry-run passes.</li> <li>Confirmed job shows <code>Status: retryable</code>, <code>Retry count: 0</code>.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li>Why did this <code>sshfs</code> hot path mount become non-writable for the worker user?</li> <li>Is there any automation that can proactively detect \u201coutput dir not writable\u201d before a crawl attempt consumes retries?</li> <li>Should the worker/crawler user be changed to a dedicated service account (instead of an operator user) to reduce permission drift?</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> Identify why this job\u2019s <code>output_dir</code> was not writable (mount type + UID/GID expectations) and document the invariant we rely on. (priority=high)</li> <li> Add an operator-safe command to reset a crawl job\u2019s retry budget: <code>ha-backend reset-retry-count</code> (dry-run by default; <code>--apply</code> required; skips running/lock-held jobs). (implemented 2026-02-06)</li> <li> Consider treating \u201coutput dir not writable\u201d as an <code>infra_error</code> class so it does not consume retry budget. (priority=medium)</li> <li> Add a short ops note: when <code>psql</code> roles are missing, use the DB session method (Python snippet) rather than forcing <code>psql</code> as root. (priority=low)</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>Add a periodic \u201cjob output dir writability probe\u201d (metrics + alert) for queued/running annual jobs.</li> <li>Expand tiering/repair automation to ensure hot-path output dirs are consistently mounted/writable before a crawl starts.</li> </ul>"},{"location":"operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Operator snapshot script: <code>scripts/vps-crawl-status.sh</code></li> <li>Incident response playbook: <code>../playbooks/core/incident-response.md</code></li> <li>Crawl stalls playbook: <code>../playbooks/crawl/crawl-stalls.md</code></li> <li>Storage hot-path incidents: <code>../playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> <li>Note: job 7 produced no combined crawl logs because it failed before <code>archive_tool</code> started.</li> <li>Related: <code>2026-01-09-annual-crawl-hc-job-stalled.md</code></li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/","title":"Incident: Replay smoke tests failed (503) due to stale mounts + warc-tiering service failed (2026-01-16)","text":"<p>Status: closed</p>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-01-16</li> <li>Severity (see <code>severity.md</code>): sev1</li> <li>Environment: production</li> <li>Primary area: replay + storage</li> <li>Owner: (unassigned)</li> <li>Start (UTC): 2026-01-15T04:20:00Z (first observed failing replay-smoke metrics)</li> <li>End (UTC): 2026-01-16T02:51:56Z (replay-smoke metrics OK)</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#summary","title":"Summary","text":"<p>The daily replay smoke tests began returning <code>503</code> for the legacy imported jobs (HC + CIHR), even though <code>https://replay.healtharchive.ca/</code> itself was up (<code>200</code>). The underlying issue was that the replay container could not reliably read WARCs under <code>/srv/healtharchive/jobs/imports/**</code> due to stale mountpoints (<code>Transport endpoint is not connected</code>) and the replay container\u2019s mount namespace not reflecting repaired/updated mounts. Separately, <code>healtharchive-warc-tiering.service</code> had been left in a <code>failed</code> state since 2026-01-08, preventing tiered imports from being reliably mounted.</p> <p>Recovery: re-apply WARC tiering, clear the failed systemd state, and restart the replay service to refresh its mounts; then re-run replay smoke tests.</p>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#impact","title":"Impact","text":"<ul> <li>User-facing impact: replay for legacy jobs intermittently failed (HTTP 503 responses from pywb for snapshot requests).</li> <li>Internal impact: <code>ReplaySmokeFailed</code> monitoring noise and operator intervention required.</li> <li>Data impact:</li> <li>Data loss: no evidence</li> <li>Data integrity risk: low/unknown (symptom was read failures, not WARC corruption)</li> <li>Recovery completeness: complete (smoke tests returned <code>200</code>)</li> <li>Duration: ~22h (first failing metric to confirmed recovery)</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#detection","title":"Detection","text":"<ul> <li>node_exporter metrics:</li> <li><code>healtharchive_replay_smoke_ok{job_id=\"1\",source=\"hc\"} 0</code> + <code>status_code ... 503</code></li> <li><code>healtharchive_replay_smoke_ok{job_id=\"2\",source=\"cihr\"} 0</code> + <code>status_code ... 503</code></li> <li>systemd state:</li> <li><code>healtharchive-warc-tiering.service</code> was <code>failed</code> since 2026-01-08 with <code>Transport endpoint is not connected</code>.</li> <li>Container symptom:</li> <li><code>docker exec healtharchive-replay ... ls -la /warcs/imports/...</code> showed <code>d?????????</code> and <code>Transport endpoint is not connected</code>.</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#decision-log-recommended-for-sev1","title":"Decision log (recommended for sev1)","text":"<ul> <li>2026-01-16T02:51:00Z \u2014 Decision: restart replay after fixing tiering mounts (why: quickest way to ensure the pywb container sees a clean view of <code>/srv/healtharchive/jobs</code> and can read WARCs; risks: brief replay downtime, but no data mutation).</li> <li>2026-01-16T16:00:00Z \u2014 Decision (post-incident hardening): run pywb with <code>rshared</code> bind propagation for <code>/srv/healtharchive/jobs</code> (why: allow the container to observe repaired nested mounts without requiring an additional restart; risks: broader mount propagation surface, but still read-only inside the container).</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-01-08T06:25:23Z \u2014 <code>healtharchive-warc-tiering.service</code> failed while attempting to operate on <code>/srv/healtharchive/jobs/imports/...</code> (stale mount: <code>Transport endpoint is not connected</code>).</li> <li>2026-01-15T04:20:00Z \u2014 Replay smoke test metrics show <code>503</code> for legacy jobs (first observed failing <code>healtharchive_replay_smoke_*</code> timestamp).</li> <li>2026-01-16T02:25Z \u2014 Verified replay root is up (<code>curl -I https://replay.healtharchive.ca/</code> returns <code>200</code>), but snapshot requests return <code>503</code>.</li> <li>2026-01-16T02:30Z \u2014 Confirmed the replay container cannot read tiered import directories (<code>docker exec healtharchive-replay ...</code> shows <code>Transport endpoint is not connected</code>).</li> <li>2026-01-16T02:51Z \u2014 Recovered by re-applying tiering + restarting replay:</li> <li><code>sudo systemctl reset-failed healtharchive-warc-tiering.service</code></li> <li><code>sudo systemctl start healtharchive-warc-tiering.service</code></li> <li><code>sudo systemctl restart healtharchive-replay.service</code></li> <li><code>sudo systemctl start healtharchive-replay-smoke.service</code></li> <li>2026-01-16T02:51:56Z \u2014 Replay smoke metrics return to <code>200</code>:</li> <li><code>healtharchive_replay_smoke_ok{job_id=\"1\",source=\"hc\"} 1</code></li> <li><code>healtharchive_replay_smoke_ok{job_id=\"2\",source=\"cihr\"} 1</code></li> <li>2026-01-16T16:00Z \u2014 Post-incident hardening: updated replay systemd unit to mount <code>/srv/healtharchive/jobs</code> with <code>rshared</code> bind propagation so pywb can observe nested mount repairs without a restart (see: <code>../../deployment/replay-service-pywb.md</code>).</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger: one or more tiered paths under <code>/srv/healtharchive/jobs/imports/**</code> were stale/unreadable (<code>Errno 107: Transport endpoint is not connected</code>), causing WARC reads inside pywb to fail.</li> <li>Underlying cause(s):</li> <li><code>healtharchive-warc-tiering.service</code> remained <code>failed</code> after a prior storage incident, so tiered import mountpoints were not being applied/validated by systemd.</li> <li>The replay service is a long-running Docker container bind-mounting <code>/srv/healtharchive/jobs</code> into <code>/warcs</code>. Mount changes/repairs on the host can require a container restart for the container to observe a clean view of the mountpoints.</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#contributing-factors","title":"Contributing factors","text":"<ul> <li>Tiered import jobs are critical to replay smoke (legacy jobs are used as smoke targets).</li> <li>Stale mount symptoms were partly masked because:</li> <li>the Storage Box base mount looked healthy, and</li> <li>replay root <code>/</code> still returned <code>200</code>.</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#resolution-recovery","title":"Resolution / Recovery","text":"<p>1) Ensure WARC tiering mounts are applied and systemd is not stuck in a failed state:</p> <pre><code>sudo systemctl reset-failed healtharchive-warc-tiering.service\nsudo systemctl start healtharchive-warc-tiering.service\nsudo systemctl status healtharchive-warc-tiering.service --no-pager -l\n</code></pre> <p>2) Restart replay so the container sees a clean view of <code>/srv/healtharchive/jobs</code>:</p> <pre><code>sudo systemctl restart healtharchive-replay.service\nsudo systemctl status healtharchive-replay.service --no-pager -l\n</code></pre> <p>3) Re-run replay smoke and verify metrics:</p> <pre><code>sudo systemctl start healtharchive-replay-smoke.service\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_replay_smoke_'\n</code></pre>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#post-incident-hardening-durable-fixes","title":"Post-incident hardening (durable fixes)","text":"<ul> <li>Replay service mount propagation:</li> <li>Updated <code>/etc/systemd/system/healtharchive-replay.service</code> to mount <code>/srv/healtharchive/jobs</code> as <code>ro,rshared</code> so nested bind-mount repairs (tiering/hot-path recovery) are visible inside the container.</li> <li>Canonical doc: <code>../../deployment/replay-service-pywb.md</code></li> <li>Tiering service resilience:</li> <li>Updated the tiering systemd unit template to run <code>vps-warc-tiering-bind-mounts.sh --apply --repair-stale-mounts</code> so it can automatically unmount stale <code>Errno 107</code> mountpoints and re-apply binds on start.</li> <li>Canonical playbook: <code>../playbooks/storage/warc-storage-tiering.md</code></li> <li>Storage hot-path auto-recovery:</li> <li>Enabled <code>healtharchive-storage-hotpath-auto-recover.timer</code> (opt-in via sentinel file) so stale mounts are detected and recovered without requiring a manual incident response for common <code>Errno 107</code> cases.</li> <li>Canonical playbook: <code>../playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#post-incident-verification","title":"Post-incident verification","text":"<ul> <li>Public surface checks:</li> <li><code>curl -I https://replay.healtharchive.ca/ | head</code> returns <code>200</code>.</li> <li>Storage/mount checks:</li> <li><code>systemctl status healtharchive-warc-tiering.service --no-pager -l</code> is successful.</li> <li>Replay job checks:</li> <li><code>healtharchive_replay_smoke_ok{job_id=\"1\",source=\"hc\"} 1</code> and <code>...{job_id=\"2\",source=\"cihr\"} 1</code></li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#public-communication-optional","title":"Public communication (optional)","text":"<ul> <li>Public status update: not posted (incident was internal and did not change public-facing expectations beyond the replay smoke targets).</li> <li>Public-safe summary: keep on file; if replay becomes a user-facing guarantee in future, revisit whether sev1 incidents should trigger a public note.</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li>Can we make replay smoke targets independent of tiered-import mounts (e.g., keep a tiny always-local \u201ccanary replay\u201d job) so storage tiering issues don\u2019t mask replay regressions?</li> <li>Decision: Deferred to backlog. Tiering alerting (now implemented) addresses the immediate need for better detection. Canary replay is a future enhancement.</li> <li>Should replay smoke include an explicit \u201cWARC file exists + readable\u201d check to disambiguate pywb failures vs storage failures?</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> Update playbooks to call out \u201crestart replay after mount/tiering repairs\u201d when smoke returns <code>503</code> but replay root is <code>200</code>. (owner=eng, priority=high, due=2026-01-16)</li> <li> Enable the storage hot-path auto-recover watchdog (<code>healtharchive-storage-hotpath-auto-recover.timer</code>) after validating thresholds. (owner=eng, priority=medium, due=2026-01-16)</li> <li> Document and apply <code>rshared</code> bind propagation for the replay service so nested mount repairs are visible without restarting pywb. (owner=eng, priority=high, due=2026-01-16)</li> <li> Enable tiering health metrics + alerting so <code>healtharchive-warc-tiering.service</code> failures are visible quickly. (owner=eng, priority=medium, due=2026-01-18)</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>Automate \u201ctiering failed\u201d detection with metrics + alerting:</li> <li>Enable <code>healtharchive-tiering-metrics.timer</code> and alert on a sustained unhealthy signal (e.g., <code>healtharchive_tiering_metrics_ok==0</code> or a \u201ctiering applied\u201d check failing).</li> <li>Keep replay smoke meaningful but safe:</li> <li>Prefer smoke probes that are read-only and low-cost.</li> <li>Treat <code>Errno 107</code> as an infra/storage failure class, and route recovery through the storage/tiering watchdogs rather than marking replay itself \u201cbroken\u201d.</li> </ul>"},{"location":"operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Tiering manifest (VPS): <code>/etc/healtharchive/warc-tiering.binds</code></li> <li>Tiering script (VPS): <code>scripts/vps-warc-tiering-bind-mounts.sh</code></li> <li>Replay smoke playbook: <code>../playbooks/validation/replay-smoke-tests.md</code></li> <li>Storage recovery playbook: <code>../playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/","title":"Incident: Annual crawl \u2014 Errno 107 hot-path issue triggered infra-error thrash and worker stopped (2026-01-24)","text":"<p>Status: draft</p>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-01-24</li> <li>Severity (see <code>severity.md</code>): sev1</li> <li>Environment: production (single VPS)</li> <li>Primary area: storage</li> <li>Owner: (unassigned)</li> <li>Start (UTC): 2026-01-24T06:17:10Z (approx; storage hot-path auto-recover began running repeatedly)</li> <li>End (UTC): 2026-01-24T12:31:03Z (approx; worker restarted and annual crawl resumed)</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#summary","title":"Summary","text":"<p>During the annual 2026 campaign, Storage Box \u201chot path\u201d mountpoints under <code>/srv/healtharchive/jobs/**</code> intermittently became stale and returned <code>OSError: [Errno 107] Transport endpoint is not connected</code>. The worker then repeatedly picked the PHAC annual job (job 7), immediately failed with an infra error, and re-picked it in a tight loop. Shortly after, the worker service became inactive, leaving the campaign with no running jobs until manual intervention restarted the worker and re-launched the HC crawl (job 6).</p>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#impact","title":"Impact","text":"<ul> <li>User-facing impact: none directly observed, but annual campaign remained <code>Ready for search: NO</code> and made no progress while the worker was down.</li> <li>Internal impact (ops burden, automation failures, etc):</li> <li>Alert noise (repeated infra errors and hot-path auto-recover runs).</li> <li>Worker stopped; no jobs ran until manual restart.</li> <li>Increased risk of \u201cnew crawl phase\u201d restarts (loss of frontier continuity) after recovery.</li> <li>Data impact:</li> <li>Data loss: unknown (no evidence of WARC deletion in this incident record).</li> <li>Data integrity risk: medium (stale mounts can interrupt writes; \u201cnew crawl phase\u201d can reduce completeness by losing crawl frontier).</li> <li>Recovery completeness: partial (worker restarted and crawl resumed; underlying Errno 107 trigger not fully understood).</li> <li>Duration: ~6h 14m (approx; 06:17Z \u2192 12:31Z).</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#detection","title":"Detection","text":"<ul> <li>Operator reported overnight warning/error notifications (approx 5 hours before 12:22Z).</li> <li><code>./scripts/vps-crawl-status.sh --year 2026</code> at ~12:22Z showed:</li> <li><code>FAIL worker service is not active</code></li> <li><code>healtharchive_crawl_running_jobs 0</code></li> <li><code>phac</code> job marked <code>crawl_status=infra_error</code></li> <li>storage hot-path watchdog recorded tiering failures mentioning <code>Errno 107</code> for PHAC/CIHR hot paths.</li> <li>Worker journal around 06:28Z showed rapid repetition of the same infra error for job 7.</li> <li><code>healtharchive-crawl-auto-recover.service</code> was running every ~5 minutes throughout the window and consistently completed successfully (\u201cDeactivated successfully\u201d), which suggests it was likely not the component that stopped the worker. (See timeline and artifacts.)</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-01-24T05:45:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs (timer-driven), completes successfully.</li> <li>2026-01-24T05:50:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T05:55:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:00:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:05:01Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:10:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:15:07Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:17:10Z \u2014 <code>healtharchive-storage-hotpath-auto-recover.service</code> begins running repeatedly (timer-driven).</li> <li>2026-01-24T06:28:01Z \u2014 Worker picks job 7 (PHAC) and immediately raises <code>Errno 107</code> for the job output dir; the same job is re-picked repeatedly within the same second (infra-error thrash).</li> <li>2026-01-24T06:28:02Z \u2014 <code>healtharchive-worker.service</code> becomes inactive (stopped); no running jobs remain.</li> <li>2026-01-24T06:30:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:35:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:40:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:45:05Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:50:01Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T06:55:10Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T07:00:05Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T07:05:01Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T07:10:01Z \u2014 <code>healtharchive-crawl-auto-recover.service</code> runs, completes successfully.</li> <li>2026-01-24T12:22:11Z \u2014 Operator status snapshot confirms worker inactive and no running jobs.</li> <li>2026-01-24T12:31:03Z \u2014 Manual recovery: worker started; job 6 (HC) restarts a crawl container and begins a \u201cnew crawl phase\u201d.</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger:</li> <li>One or more hot-path mountpoints returned <code>Errno 107</code> during job execution (observed for PHAC output dir; watchdog also reported PHAC/CIHR paths).</li> <li>Underlying cause(s):</li> <li>Likely intermittent <code>sshfs</code>/FUSE hot-path instability (similar failure mode to <code>2026-01-08-storage-hotpath-sshfs-stale-mount.md</code>), but root trigger remains unconfirmed.</li> <li>Worker behavior on infra errors allowed immediate re-pick of the same job with no effective cooldown, causing log spam and increased operational risk.</li> <li>The worker stop event at <code>2026-01-24T06:28:02Z</code> was a <code>systemd</code> stop; based on available logs, <code>healtharchive-crawl-auto-recover.service</code> does not appear to be the direct cause (it continued running successfully before/after).</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#contributing-factors","title":"Contributing factors","text":"<ul> <li>Infra error handling did not appear to enforce a cooldown/backoff for fast-failing jobs, enabling a tight loop.</li> <li>Hot-path auto-recovery ran frequently but did not resolve the condition before the worker stopped.</li> <li>Operator CLI confusion: running <code>ha-backend show-job --id 6</code> without exporting <code>/etc/healtharchive/backend.env</code> defaulted to SQLite and failed with <code>no such table: archive_jobs</code> (not causal, but slowed diagnosis).</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#resolution-recovery","title":"Resolution / Recovery","text":"<p>Manual recovery steps performed (state-changing):</p> <p>1) Confirm mounts were readable again (spot-check):</p> <pre><code>timeout 5 ls -la /srv/healtharchive/storagebox &gt;/dev/null\ntimeout 5 ls -la /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101 &gt;/dev/null\ntimeout 5 ls -la /srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101 &gt;/dev/null\n</code></pre> <p>2) Restart the worker:</p> <pre><code>sudo systemctl start healtharchive-worker.service\n</code></pre> <p>3) Verify the crawl restarted and a zimit container is running:</p> <pre><code>./scripts/vps-crawl-status.sh --year 2026\nsudo systemctl --no-pager --full status healtharchive-worker.service\n</code></pre> <p>Observed outcome: job 6 (HC) restarted at ~12:31Z and began writing new crawl temp dirs/WARCs under the existing job output directory.</p>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#additional-recovery-work-2026-01-25","title":"Additional recovery work (2026-01-25)","text":"<ul> <li>Reset the retry budget for jobs 7 (PHAC) and 8 (CIHR) by writing <code>retry_count=0</code> directly via the backend ORM so the re-runs behaved like fresh attempts.</li> <li>Fixed the output-directory permissions (<code>chown -R haadmin:haadmin</code>, <code>chmod 755</code>) and confirmed writability by touching <code>.writable_test_manual</code> inside each job dir as <code>haadmin</code>.</li> <li>Launched the jobs through the transient <code>systemd-run</code>-based helper (<code>scripts/vps-run-db-job-detached.py / systemd-run \u2026 run-db-job --id \u2026</code>) so the crawls kept running while our SSH sessions closed.</li> <li>Relaxed permissions on the existing <code>.tmpt*</code> directories (<code>docker run --rm -v \"\u2026:/output\" alpine sh -c 'chmod -R a+rX /output/.tmp*'</code>) so the hot-path watchdog/ops scripts could read the WARCs without manual chmods.</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#post-incident-verification","title":"Post-incident verification","text":"<ul> <li>Worker/job health checks:</li> <li><code>sudo systemctl status healtharchive-worker.service --no-pager -l</code> (worker active)</li> <li><code>./scripts/vps-crawl-status.sh --year 2026</code> (running job detected; metrics OK; crawlStatus advancing)</li> <li>Storage/mount checks:</li> <li><code>findmnt -T /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101</code></li> <li><code>findmnt -T /srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101</code></li> <li>Integrity checks:</li> <li>Not performed as part of this initial incident note; consider WARC sampling on job 6 after stabilization.</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#open-questions-still-unknown","title":"Open questions (still unknown)","text":"<ul> <li>What is the underlying trigger for hot-path mount staleness (network blip, Storage Box behavior, sshfs option mismatch, local FUSE behavior)?</li> <li>Why did the worker service stop shortly after the infra-error thrash (explicit stop by automation vs worker exit)?</li> <li>Does the current infra-error cooldown/backoff logic actually prevent tight re-pick loops in practice, and is it observable via metrics/logs?</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> Implement mitigations and recovery improvements in <code>docs/planning/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening.md</code>. (owner=eng, priority=high, due=2026-02-01)</li> <li> Investigate underlying cause of the hot-path staleness (Storage Box/sshfs/network/FUSE). (owner=eng, priority=high, due=2026-02-01)</li> <li> Add a worker-side guardrail to prevent tight \u201cpick same job instantly\u201d loops on infra errors (cooldown + metric). (owner=eng, priority=high, due=2026-02-01)</li> <li> Add a playbook section for \u201cCLI shows sqlite/no such table\u201d (env export reminder) to reduce operator confusion. (owner=ops, priority=low, due=2026-02-01)</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#follow-up-implementation-details","title":"Follow-up implementation details","text":"<ul> <li>Added <code>scripts/vps-run-db-job-detached.py</code> and updated <code>docs/deployment/systemd/README.md</code> to point operators at this helper so specific jobs can be re-run within transient <code>systemd-run</code> units without keeping a shell session attached.</li> <li>Extended <code>scripts/vps-storage-hotpath-auto-recover.py</code> to probe the output directories of queued/retryable jobs (not just the currently running ones) so stale hot paths can be detected before the worker picks them; detection still strictly unmounts only the specific stale targets.</li> <li>Extended the archive tool to discover <code>.tmp*</code> temp directories immediately after the container starts and periodically throughout the crawl. When <code>--relax-perms</code> is enabled the helper now runs during the crawl (configurable interval) so host commands can read WARCs without manual <code>chmod</code>, not just after the job finishes.</li> <li>Added a <code>last_healthy_*</code> timestamp to the storage hot-path watchdog state and Prometheus textfile metrics, while continuing to gate actual recovery attempts via the existing <code>last_apply_*</code> cooldown/cap fields. This gives dashboards a clearer signal when the watchdog has seen no stale targets.</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>Safe automation:</li> <li>Detect <code>Errno 107</code> at the job output-dir boundary and pause/park affected jobs with a cooldown rather than thrashing the worker loop.</li> <li>If <code>Errno 107</code> is detected with high confidence, automate a conservative recovery sequence (stop worker \u2192 unmount stale hot paths only \u2192 re-apply tiering/bind mounts \u2192 restart worker), but only when crawl integrity risk is acceptable.</li> <li>What should stay manual:</li> <li>Any automation that unmounts paths used by an actively running crawl container should be guarded heavily (risk of interrupting writes / frontier continuity).</li> </ul>"},{"location":"operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Related incident: <code>docs/operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount.md</code></li> <li>Follow-up roadmap: <code>docs/planning/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening.md</code></li> <li><code>healtharchive-crawl-auto-recover.service</code> journal window:</li> <li><code>sudo journalctl -u healtharchive-crawl-auto-recover.service --since '2026-01-24 05:45:00' --until '2026-01-24 07:15:00' --no-pager | tail -n 200</code></li> <li>Most relevant worker log excerpt (redacted):</li> </ul> <pre><code>Jan 24 06:28:01 &lt;vps&gt; ha-backend[...]: 2026-01-24 06:28:01,050 [INFO] healtharchive.worker: Worker picked job 7 for source phac (...) with status retryable and retry_count 0\nJan 24 06:28:01 &lt;vps&gt; ha-backend[...]: 2026-01-24 06:28:01,056 [WARNING] healtharchive.jobs: Job 7 raised during archive_tool execution: [Errno 107] Transport endpoint is not connected: '/srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101'\nJan 24 06:28:01 &lt;vps&gt; ha-backend[...]: 2026-01-24 06:28:01,063 [WARNING] healtharchive.worker: Crawl for job 7 failed due to infra error (RC=1). Not consuming retry budget (retry_count=0).\nJan 24 06:28:01 &lt;vps&gt; ha-backend[...]: 2026-01-24 06:28:01,068 [INFO] healtharchive.worker: Worker picked job 7 for source phac (...) with status retryable and retry_count 0\nJan 24 06:28:02 &lt;vps&gt; systemd[1]: Stopping healtharchive-worker.service - HealthArchive Worker...\nJan 24 06:28:02 &lt;vps&gt; systemd[1]: healtharchive-worker.service: Deactivated successfully.\n</code></pre>"},{"location":"operations/incidents/2026-02-02-storage-watchdog-unmount-filter-bug/","title":"Storage Watchdog Unmount Filter Bug \u2014 2026-02-02","text":"<p>Status: Resolved Severity: Medium (automation broken, manual recovery needed) Detection: Manual observation during routine crawl status check Duration: ~9 days (2026-01-24 last successful watchdog run \u2192 2026-02-02 fix deployed)</p>"},{"location":"operations/incidents/2026-02-02-storage-watchdog-unmount-filter-bug/#summary","title":"Summary","text":"<p>The storage hotpath auto-recover watchdog (<code>vps-storage-hotpath-auto-recover.py</code>) had a bug that prevented it from unmounting stale SSHFS mounts. The watchdog could detect stale mounts (Errno 107) but silently skipped them during the unmount phase due to an overly-strict filter condition. This required manual intervention for every stale mount occurrence.</p>"},{"location":"operations/incidents/2026-02-02-storage-watchdog-unmount-filter-bug/#root-cause","title":"Root Cause","text":"<p>The watchdog's stale mount detection calls <code>_get_mount_info()</code> to retrieve mount details. For stale FUSE mounts (Errno 107), <code>_get_mount_info()</code> often returns <code>None</code> or incomplete data because the mount endpoint is inaccessible.</p> <p>The filter logic at lines 1041-1053 (dry-run) and 1184-1206 (apply) checked: <pre><code>if target != path:\n    continue  # Skip this mount\n</code></pre></p> <p>When <code>_get_mount_info()</code> returned <code>None</code>, the <code>target</code> variable became an empty string (<code>\"\"</code>), making <code>target != path</code> always <code>True</code>. This caused all stale mounts to be filtered out, leaving the <code>stale_mountpoints</code> list empty, and the unmount step never executed.</p>"},{"location":"operations/incidents/2026-02-02-storage-watchdog-unmount-filter-bug/#why-it-wasnt-caught-earlier","title":"Why It Wasn't Caught Earlier","text":"<ol> <li>Stale mounts are rare - The watchdog was only enabled after the initial incident on 2026-01-08</li> <li>Manual recovery worked - Operators could manually <code>umount -l</code> to fix the issue</li> <li>Dry-run was misleading - The dry-run showed intent to unmount but the apply phase diverged</li> <li>No test coverage - No integration test simulated stale mount scenarios in the watchdog</li> </ol>"},{"location":"operations/incidents/2026-02-02-storage-watchdog-unmount-filter-bug/#resolution","title":"Resolution","text":"<p>Fix deployed: 2026-02-02 03:00 UTC (commit <code>5a48b22</code>)</p> <p>Changed the filter logic to accept paths where either: 1. Confirmed mountpoint (<code>target == path</code> from findmnt), OR 2. Errno 107 detected (strong evidence of stale FUSE mount)</p> <p>The Errno 107 detection itself is sufficient evidence that a path under <code>jobs_root</code> is a stale bind mount, since normal directories don't return Errno 107.</p> <p>Changes: - Updated stale mount filter in both dry-run and apply phases - Improved dry-run output to show errno when mount info unavailable - Fixed post-check to prioritize readability over mount info</p>"},{"location":"operations/incidents/2026-02-02-storage-watchdog-unmount-filter-bug/#verification","title":"Verification","text":"<ul> <li>All 7 storage hotpath tests pass</li> <li>Full test suite: 431 tests passed</li> <li>Watchdog timers confirmed active (running every 1 minute)</li> <li>Manual test: Successfully detected and resolved current stale mounts</li> </ul>"},{"location":"operations/incidents/2026-02-02-storage-watchdog-unmount-filter-bug/#impact","title":"Impact","text":"<p>Before fix: - Watchdog detected stale mounts but failed to unmount them - Manual <code>umount -l</code> required for every stale mount occurrence - Last successful watchdog run: 2026-01-24 06:28 UTC</p> <p>After fix: - Watchdog automatically detects and unmounts stale mounts - No manual intervention needed - Confirmed healthy run: 2026-02-02 02:44 UTC (detected 0 stale targets)</p>"},{"location":"operations/incidents/2026-02-02-storage-watchdog-unmount-filter-bug/#related-incidents","title":"Related Incidents","text":"<ul> <li>2026-01-08 Storage Hotpath SSHFS Stale Mount - Initial incident</li> <li>2026-01-24 Infra Error 107 Hotpath Thrash - Retry storm incident</li> </ul>"},{"location":"operations/incidents/2026-02-02-storage-watchdog-unmount-filter-bug/#related-implementation-plans","title":"Related Implementation Plans","text":"<ul> <li>2026-01-08 Storage Box Stale Mount Recovery - Initial watchdog implementation</li> <li>2026-01-24 Infra Error and Storage Hotpath Hardening - Watchdog improvements</li> <li>2026-02-01 Operational Resilience Improvements - Latest hardening work</li> </ul>"},{"location":"operations/incidents/2026-02-02-storage-watchdog-unmount-filter-bug/#lessons-learned","title":"Lessons Learned","text":"<ol> <li>Test stale mount scenarios - Add integration test that simulates Errno 107 conditions</li> <li>Verify dry-run/apply parity - Ensure dry-run accurately predicts apply behavior</li> <li>Monitor automation health - The watchdog was failing silently for 9 days</li> <li>Trust detection signals - When Errno 107 is explicitly detected, trust it over missing mount info</li> </ol>"},{"location":"operations/incidents/2026-02-02-storage-watchdog-unmount-filter-bug/#follow-up-actions","title":"Follow-up Actions","text":"<ul> <li> Add integration test for stale mount recovery scenarios</li> <li> Add alerting if watchdog runs complete but <code>last_apply_ok=0</code> for &gt;24 hours</li> <li> Document expected failure modes in storage watchdog playbook</li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/","title":"Incident: Annual crawl \u2014 job output dirs on root disk caused disk pressure + crawl pauses (2026-02-04)","text":"<p>Status: closed</p>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-02-04</li> <li>Severity (see <code>severity.md</code>): sev1</li> <li>Environment: production (single VPS)</li> <li>Primary area: storage</li> <li>Owner: (unassigned)</li> <li>Start (UTC): 2026-02-04T13:23:39Z (first operator snapshot showing disk pressure)</li> <li>End (UTC): 2026-02-04T16:47:12Z (operator snapshot showing jobs resumed + root disk healthy)</li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#summary","title":"Summary","text":"<p>The annual 2026 crawl campaign hit sustained root-disk pressure on the VPS (<code>/dev/sda1</code> reached ~84\u201386% used), which triggered the worker\u2019s disk safety guardrail (\u226585% usage) and prevented new crawl progress. Investigation showed that annual crawl output directories for CIHR (~50GB) and PHAC (~1.2GB) were on the local root filesystem under <code>/srv/healtharchive/jobs/**</code> instead of being tiered to the Storage Box. We paused crawls to avoid a disk-full failure, migrated the output directories to the Storage Box, re-established the expected mounts under <code>/srv/healtharchive/jobs/**</code>, and resumed automation.</p>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#impact","title":"Impact","text":"<ul> <li>User-facing impact:</li> <li>Public API/site remained healthy.</li> <li>Annual 2026 campaign was <code>Ready for search: NO</code> and made little/no crawl progress while paused.</li> <li>Internal impact (ops burden, automation failures, etc):</li> <li>Operator time to pause long-running crawls and perform storage tiering.</li> <li>Risk of a disk-full incident avoided by pausing early.</li> <li>Data impact:</li> <li>Data loss: unknown (no evidence observed).</li> <li>Data integrity risk: low-to-medium (risk was primarily \u201cdisk full\u201d / interrupted writes if left running).</li> <li>Recovery completeness: complete (output dirs remounted to Storage Box; jobs resumed).</li> <li>Duration:</li> <li>Disk pressure was present before 2026-02-04T13:23:39Z and resolved by ~16:xxZ; campaign was running again by 16:47Z.</li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#detection","title":"Detection","text":"<ul> <li>Operator ran <code>./scripts/vps-crawl-status.sh --year 2026</code> and saw:</li> <li>Root disk at ~84\u201386% used.</li> <li>Worker log warnings indicating the disk guardrail was active (\u201cDisk usage at 85% exceeds threshold\u2026\u201d).</li> <li><code>sudo du -xhd3 /srv/healtharchive/jobs | sort -h | tail</code> showed ~47\u201350GB under CIHR annual output on the local disk.</li> </ul> <p>Most useful signals:</p> <ul> <li><code>df -h /</code> (root usage)</li> <li><code>sudo du -xhd3 /srv/healtharchive/jobs | sort -h | tail -40</code> (who is using local disk)</li> <li><code>findmnt -T /srv/healtharchive/jobs/&lt;source&gt;/&lt;job_dir&gt; -o SOURCE,FSTYPE,OPTIONS</code> (is this actually on the Storage Box?)</li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#decision-log","title":"Decision log","text":"<ul> <li>2026-02-04T14:4xZ \u2014 Decision: pause all long-running annual crawls (why: avoid disk-full failure; preserve service and data integrity).</li> <li>2026-02-04T15:3xZ \u2014 Decision: migrate annual output dirs with <code>rsync</code> (why: keep WARC/state artifacts; fastest path to reclaim root disk without losing crawl progress).</li> <li>2026-02-04T16:0xZ \u2014 Decision: resume automation only after mounts validated (why: prevent immediately writing back to local disk).</li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-02-04T13:23:39Z \u2014 <code>vps-crawl-status</code> snapshot: root disk ~86% used; annual campaign running (HC+CIHR), PHAC retryable.</li> <li>2026-02-04T13:52:xxZ \u2014 Worker logs show disk guardrail active (\u226585%), skipping crawl starts.</li> <li>2026-02-04T14:4xZ \u2014 Automation disabled and crawls stopped; jobs recovered to <code>retryable</code> in DB for safe restart later.</li> <li>2026-02-04T14:52:45Z \u2014 Fresh Postgres DB backup taken and copied to the Storage Box (with rsync flags adjusted to avoid sshfs <code>chown</code> failures).</li> <li>2026-02-04T15:32:26Z \u2014 Storage Box <code>sshfs</code> mount confirmed active.</li> <li>2026-02-04T15:3xZ \u2192 16:0xZ \u2014 CIHR (~50GB) and PHAC (~1.2GB) annual output directories copied to the Storage Box via <code>rsync</code>.</li> <li>2026-02-04T16:0xZ \u2014 Annual output tiering applied; output dirs mounted back under <code>/srv/healtharchive/jobs/**</code>; local copies deleted; root disk returned to ~19% used.</li> <li>2026-02-04T16:15:33Z \u2014 Services restarted; worker resumed.</li> <li>2026-02-04T16:47:12Z \u2014 <code>vps-crawl-status</code> snapshot shows 3 running annual jobs and root disk healthy.</li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger:</li> <li>Annual crawl output for at least CIHR and PHAC was written to the VPS root filesystem under <code>/srv/healtharchive/jobs/**</code>, consuming ~50GB locally and pushing root above the worker\u2019s safety threshold.</li> <li>Underlying cause(s):</li> <li>Annual output tiering/mounts were not in place for those job output dirs at the time the crawls ran (post-reboot / maintenance window).</li> <li>Manual ops workflows are easy to run in an \u201cunsafe order\u201d (worker running while mounts not validated).</li> <li>The annual output tiering script can be run with missing env exports / DB offline, which causes confusing failures (SQLite \u201cno such table\u201d) that can delay recovery.</li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#contributing-factors","title":"Contributing factors","text":"<ul> <li>Long-running crawls reduce opportunities for a clean maintenance window.</li> <li>Root disk is fixed size and close to the worker\u2019s disk threshold when any large output dir lands locally.</li> <li><code>rsync</code> to <code>sshfs</code> mountpoints can fail on ownership/permissions by default (requires explicit flags).</li> <li>CIHR job (job 8) config drift: missing annual campaign metadata made annual tooling less reliable until patched.</li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#resolution-recovery","title":"Resolution / Recovery","text":""},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#0-pausestop-automation-and-crawls-make-a-maintenance-window","title":"0) Pause/stop automation and crawls (make a maintenance window)","text":"<p>Disable the automations so they don\u2019t immediately restart jobs while mounts are in flux:</p> <pre><code># Disable crawl auto-recover and worker auto-start\nsudo mv /etc/healtharchive/crawl-auto-recover-enabled{,.disabled} 2&gt;/dev/null || true\nsudo mv /etc/healtharchive/worker-auto-start-enabled{,.disabled} 2&gt;/dev/null || true\n\nsudo systemctl stop healtharchive-crawl-auto-recover.timer || true\nsudo systemctl stop healtharchive-worker-auto-start.timer || true\n\n# Stop worker (and any transient crawl units)\nsudo systemctl stop healtharchive-worker.service || true\nsystemctl list-units --all 'healtharchive-job*' --no-pager\nsudo systemctl stop &lt;healtharchive-jobX-...&gt;.service\n</code></pre> <p>Mark stopped jobs restartable:</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 1 --apply --source &lt;source&gt;\n</code></pre>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#1-ensure-backups-exist","title":"1) Ensure backups exist","text":"<pre><code>sudo systemctl start healtharchive-db-backup.service\nls -lt /srv/healtharchive/backups/healtharchive_*.dump | head -n 3\n</code></pre>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#2-restoreverify-storage-box-mount","title":"2) Restore/verify Storage Box mount","text":"<pre><code>sudo systemctl start healtharchive-storagebox-sshfs.service\ndf -h /srv/healtharchive/storagebox\nfindmnt -T /srv/healtharchive/storagebox -o SOURCE,FSTYPE,OPTIONS\n</code></pre>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#3-migrate-large-local-output-dirs-to-storage-box","title":"3) Migrate large local output dirs to Storage Box","text":"<p>Use <code>rsync</code> flags that don\u2019t try to preserve ownership/perms on sshfs:</p> <pre><code>sudo rsync -rtv --info=progress2 --partial --inplace \\\n  --no-owner --no-group --no-perms \\\n  /srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101/ \\\n  /srv/healtharchive/storagebox/jobs/cihr/20260101T000502Z__cihr-20260101/\n</code></pre> <p>Optional \u201csanity dry-run\u201d to see drift (but do not delete without thinking):</p> <pre><code>sudo rsync -rtvn --delete \\\n  --no-owner --no-group --no-perms \\\n  /srv/healtharchive/jobs/&lt;source&gt;/&lt;job_dir&gt;/ \\\n  /srv/healtharchive/storagebox/jobs/&lt;source&gt;/&lt;job_dir&gt;/\n</code></pre>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#4-re-establish-the-expected-mounts-under-srvhealtharchivejobs","title":"4) Re-establish the expected mounts under <code>/srv/healtharchive/jobs/**</code>","text":"<p>Key gotcha: the tiering script must target Postgres. Make sure env vars are exported and Postgres is running, otherwise you may see SQLite errors like <code>no such table: sources</code>.</p> <pre><code>sudo systemctl start postgresql.service\nsudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; \\\n  /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py --apply --year 2026'\n</code></pre> <p>Validate mountpoints:</p> <pre><code>findmnt -T /srv/healtharchive/jobs/hc/20260101T000502Z__hc-20260101 -o SOURCE,FSTYPE\nfindmnt -T /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101 -o SOURCE,FSTYPE\nfindmnt -T /srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101 -o SOURCE,FSTYPE\n</code></pre>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#5-delete-local-copies-and-verify-disk-health","title":"5) Delete local copies and verify disk health","text":"<pre><code>sudo rm -rf /srv/healtharchive/jobs/*/*__*.local-*\ndf -h /\nsudo du -xhd3 /srv/healtharchive/jobs | sort -h | tail -40\n</code></pre>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#6-resume-services-and-automation","title":"6) Resume services and automation","text":"<pre><code># Re-enable sentinels\nsudo mv /etc/healtharchive/crawl-auto-recover-enabled.disabled /etc/healtharchive/crawl-auto-recover-enabled 2&gt;/dev/null || sudo touch /etc/healtharchive/crawl-auto-recover-enabled\nsudo mv /etc/healtharchive/worker-auto-start-enabled.disabled /etc/healtharchive/worker-auto-start-enabled 2&gt;/dev/null || sudo touch /etc/healtharchive/worker-auto-start-enabled\n\nsudo systemctl enable --now healtharchive-crawl-auto-recover.timer\nsudo systemctl enable --now healtharchive-worker-auto-start.timer\n\nsudo systemctl start healtharchive-api.service healtharchive-replay.service postgresql.service\nsudo systemctl start healtharchive-worker.service\n</code></pre>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#post-incident-verification","title":"Post-incident verification","text":"<ul> <li>Public surface checks:</li> <li><code>curl -s http://127.0.0.1:8001/api/health &amp;&amp; echo</code></li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/verify_public_surface.py</code> (when appropriate)</li> <li>Worker/job health checks:</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-crawl-status.sh --year 2026</code></li> <li><code>systemctl list-units --all 'healtharchive-job*' --no-pager</code></li> <li>Storage/mount checks:</li> <li><code>df -h / /srv/healtharchive/storagebox</code></li> <li><code>findmnt -T /srv/healtharchive/jobs/&lt;source&gt;/&lt;job_dir&gt; -o SOURCE,FSTYPE,OPTIONS</code></li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#public-communication","title":"Public communication","text":"<p>None. (No observed user-facing downtime; annual campaign internal pipeline issue.)</p>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#open-questions","title":"Open questions","text":"<ul> <li>What is the \u201csource of truth\u201d workflow after reboot/rescue to ensure annual output tiering is restored before the worker runs?</li> <li>Should we add a boot-time (or worker-start-time) invariant check that annual output dirs are mounted to the Storage Box?</li> <li>Can we reduce the likelihood of needing a rescue-mode window for \u201cdisk mystery\u201d investigations by improving on-host diagnostics and documentation?</li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> Add a runbook section: \u201cAnnual output tiering after reboot/rescue\u201d (owner=ops, priority=high, due=2026-02-08)</li> <li> Add a guardrail: worker refuses to start annual crawls when output dir is on <code>/dev/sda1</code> (owner=eng, priority=high, due=2026-02-15)</li> <li> Improve <code>vps-annual-output-tiering.py</code> UX:</li> <li>detect \u201cPostgres not running / env not exported\u201d and print a single-line fix. (owner=eng, priority=medium, due=2026-02-15)</li> <li> Ensure annual job configs always include <code>campaign_kind/year</code> metadata (owner=eng, priority=medium, due=2026-02-15)</li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>Safe automation:</li> <li>On boot (or before starting the worker), run an idempotent annual tiering \u201censure\u201d pass for the current campaign year.</li> <li>Alert when <code>/srv/healtharchive/jobs/**</code> output dirs are on the root filesystem while a campaign is active.</li> <li>What should stay manual:</li> <li>Any automated deletion of local <code>.local-*</code> directories should remain manual unless preceded by a strong integrity check (to avoid data loss).</li> </ul>"},{"location":"operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Related investigation: <code>../../planning/implemented/2026-02-01-disk-usage-investigation.md</code></li> <li>Related playbooks:</li> <li><code>../playbooks/storage/warc-storage-tiering.md</code></li> <li><code>../playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> <li>Commands used during recovery:</li> <li><code>./scripts/vps-crawl-status.sh --year 2026</code></li> <li><code>scripts/vps-annual-output-tiering.py</code></li> <li><code>healtharchive-storagebox-sshfs.service</code></li> </ul>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/","title":"Incident: API search/changes 500 due to missing dedupe migration (2026-02-06)","text":"<p>Status: closed</p>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-02-06</li> <li>Severity (see <code>operations/incidents/severity.md</code>): sev2</li> <li>Environment: production</li> <li>Primary area: api</li> <li>Owner: jerdaw</li> <li>Start (UTC): 2026-02-06T14:55:33Z</li> <li>End (UTC): 2026-02-06T15:05:15Z</li> </ul>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/#summary","title":"Summary","text":"<p>After deploying a backend change that referenced <code>Snapshot.deduplicated</code>, production API endpoints <code>/api/search</code> and <code>/api/changes</code> returned HTTP 500 because the Postgres schema was missing the new <code>snapshots.deduplicated</code> column. The issue was detected by the deploy verification script and confirmed via loopback repro + API logs. Recovery was completed by adding the missing Alembic migration and re-deploying, which applied the migration and restored API functionality.</p>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/#impact","title":"Impact","text":"<ul> <li>User-facing impact: <code>/api/search</code> and <code>/api/changes</code> returned 500 during the incident window.</li> <li>Internal impact: deploy verification failed; required a follow-up deploy.</li> <li>Data impact:</li> <li>Data loss: no</li> <li>Data integrity risk: low (read queries failing, not silent corruption)</li> <li>Recovery completeness: complete</li> <li>Duration: ~10 minutes</li> </ul>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/#detection","title":"Detection","text":"<ul> <li>Detected via deploy verification (<code>scripts/verify_public_surface.py</code>) failing on <code>/api/search</code> and <code>/api/changes</code>.</li> <li>Confirmed via loopback repro and <code>journalctl -u healtharchive-api</code> showing:</li> <li><code>psycopg.errors.UndefinedColumn: column snapshots.deduplicated does not exist</code></li> </ul>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-02-06T14:55:33Z \u2014 Deploy verification reports <code>/api/search</code> 500; loopback repro confirms 500.</li> <li>2026-02-06T14:55:34Z \u2014 API logs show UndefinedColumn error for <code>snapshots.deduplicated</code>.</li> <li>2026-02-06T15:04:45Z \u2014 Follow-up deploy pulls migration change and runs Alembic upgrade.</li> <li>2026-02-06T15:05:15Z \u2014 Loopback <code>/api/search</code> and <code>/api/changes</code> return 200; public API verification passes.</li> </ul>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/#root-cause","title":"Root cause","text":"<ul> <li>Immediate trigger: application code queried <code>snapshots.deduplicated</code> before the corresponding migration existed/applied.</li> <li>Underlying cause(s): the deploy introduced a schema-dependent feature without an accompanying migration in the same deployable unit.</li> </ul>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/#contributing-factors","title":"Contributing factors","text":"<ul> <li>The public surface verifier correctly detected the regression, but it was run after the first deploy had already restarted the API.</li> </ul>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/#resolution-recovery","title":"Resolution / Recovery","text":"<ul> <li>Added the missing Alembic migration for dedupe schema:</li> <li><code>alembic/versions/0014_snapshot_deduplication.py</code></li> <li>Re-deployed backend so Alembic applied the migration and the API restarted cleanly.</li> </ul>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/#post-incident-verification","title":"Post-incident verification","text":"<ul> <li>Local on VPS:</li> <li><code>curl -sS -i \"http://127.0.0.1:8001/api/search?pageSize=1\" | head</code></li> <li><code>curl -sS -i \"http://127.0.0.1:8001/api/changes?pageSize=1\" | head</code></li> <li>Public surface:</li> <li><code>python3 ./scripts/verify_public_surface.py --api-base https://api.healtharchive.ca --frontend-base https://www.healtharchive.ca --skip-frontend</code></li> </ul>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/#action-items-todos","title":"Action items (TODOs)","text":"<ul> <li> Add a lightweight CI guard that fails if an API query references a missing column in the test DB schema (owner=jerdaw, priority=medium, due=2026-03)</li> <li> Update the dedupe feature checklist to explicitly require an Alembic migration in the same PR (owner=jerdaw, priority=low, due=2026-03)</li> </ul>"},{"location":"operations/incidents/2026-02-06-api-search-changes-500-missing-migration/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Deploy script: <code>scripts/vps-deploy.sh</code></li> <li>Public verifier: <code>scripts/verify_public_surface.py</code></li> <li>Migration: <code>alembic/versions/0014_snapshot_deduplication.py</code></li> </ul>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/","title":"Incident: Auto-recover stall detection bugs (2026-02-06)","text":"<p>Status: closed</p>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#metadata","title":"Metadata","text":"<ul> <li>Date (UTC): 2026-02-06</li> <li>Severity: sev2</li> <li>Environment: production</li> <li>Primary area: crawl</li> <li>Owner: jerdaw</li> <li>Start (UTC): 2026-02-05T06:07:24Z (hc job 6 last progress)</li> <li>End (UTC): 2026-02-06T02:40:07Z (job 6 recovered and restarted)</li> </ul>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#summary","title":"Summary","text":"<p>The crawl auto-recover watchdog failed to detect and recover a stalled hc job (job 6) for ~20 hours due to a chain of four bugs. The watchdog ran every 5 minutes but incorrectly reported \"no_stalled_jobs\" despite the metrics exporter correctly flagging the stall. Manual investigation revealed bugs in stale log detection, runner detection, and cross-user lock file access. After fixing all four bugs, the watchdog successfully recovered the job automatically.</p>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#impact","title":"Impact","text":"<ul> <li>User-facing impact: No 2026 annual search data available during the ~20h stall period.</li> <li>Internal impact: Auto-recovery automation broken; required manual debugging.</li> <li>Data impact:</li> <li>Data loss: no (WARCs preserved on disk)</li> <li>Data integrity risk: no</li> <li>Recovery completeness: complete (job resumed from last checkpoint)</li> <li>Duration: ~20 hours of stalled crawl; auto-recovery restored in ~2 hours of debugging/fixes.</li> </ul>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#detection","title":"Detection","text":"<ul> <li>Detected via manual crawl status check (<code>./scripts/vps-crawl-status.sh --year 2026</code>)</li> <li>Metrics showed disconnect: <code>healtharchive_crawl_running_job_stalled{job_id=\"6\"} = 1</code> but auto-recover reported <code>reason=\"no_stalled_jobs\"</code></li> <li>Most useful signals:</li> <li>Metrics exporter vs auto-recover discrepancy</li> <li>Auto-recover service logs showing PermissionError crashes</li> <li><code>sysctl fs.protected_regular = 2</code> kernel setting</li> </ul>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#timeline-utc","title":"Timeline (UTC)","text":"<ul> <li>2026-02-05T06:07:24Z \u2014 hc job 6 last progress (crawled 299 pages)</li> <li>2026-02-06T00:00:00Z \u2014 Operator checks crawl status, notices 18+ hour stall</li> <li>2026-02-06T00:15:00Z \u2014 Investigation begins: metrics show stall, auto-recover doesn't</li> <li>2026-02-06T00:30:00Z \u2014 Root cause 1 found: <code>_find_job_log</code> using stale DB path</li> <li>2026-02-06T00:45:00Z \u2014 Fix 1 deployed (4bddb7c)</li> <li>2026-02-06T01:00:00Z \u2014 Auto-recover now detects stall but crashes on PermissionError</li> <li>2026-02-06T01:15:00Z \u2014 Root cause 2 found: <code>_detect_job_runner</code> not checking job locks</li> <li>2026-02-06T01:30:00Z \u2014 Fix 2 deployed (1648f74)</li> <li>2026-02-06T01:45:00Z \u2014 Auto-recover still crashes: PermissionError on lock file</li> <li>2026-02-06T02:00:00Z \u2014 Root cause 3 found: OSError not caught in recovery CLI</li> <li>2026-02-06T02:10:00Z \u2014 Fix 3 deployed (e77212b)</li> <li>2026-02-06T02:25:00Z \u2014 Still fails: <code>fs.protected_regular=2</code> blocks O_CREAT</li> <li>2026-02-06T02:30:00Z \u2014 Root cause 4 found: O_CREAT on existing file in /tmp</li> <li>2026-02-06T02:35:00Z \u2014 Fix 4 deployed (e073749), recovery succeeds</li> <li>2026-02-06T02:40:07Z \u2014 Job 6 automatically restarted, crawling resumed</li> </ul>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#root-cause","title":"Root cause","text":"<p>Four cascading bugs in the auto-recovery chain:</p> <ol> <li> <p>Stale log detection: Auto-recover's <code>_find_job_log</code> returned the DB <code>combined_log_path</code> immediately without checking for newer logs on disk. If the DB path pointed to an old log (from a previous attempt), the watchdog parsed stale data and skipped the job.</p> </li> <li> <p>Runner detection gap: <code>_detect_job_runner</code> didn't check held job locks. A dead crawl subprocess + live worker lock = incorrectly classified as runner=\"none\", triggering soft-recovery instead of full recovery.</p> </li> <li> <p>Permission error handling: Lock probes only caught <code>JobAlreadyRunningError</code>, not <code>OSError</code>. When cross-user permission issues occurred (root auto-recover vs haadmin worker), the command crashed instead of skipping gracefully.</p> </li> <li> <p>fs.protected_regular kernel protection: The kernel sysctl <code>fs.protected_regular=2</code> blocks <code>O_CREAT</code> on existing files in world-writable sticky directories (<code>/tmp</code>) when the caller doesn't own the file. The <code>_job_lock</code> function used <code>O_CREAT</code> unconditionally, causing EACCES for cross-user probes.</p> </li> </ol>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#contributing-factors","title":"Contributing factors","text":"<ul> <li>Metrics exporter had already been fixed for bug #1 (stale log detection), but auto-recover was not updated in sync</li> <li>No integration tests covering cross-user lock file scenarios (root vs non-root)</li> <li><code>fs.protected_regular=2</code> is a modern security feature not documented in the job lock implementation</li> </ul>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#resolution-recovery","title":"Resolution / Recovery","text":"<ol> <li>Fixed stale log detection (commit 4bddb7c):</li> <li>Updated <code>_find_job_log</code> in <code>scripts/vps-crawl-auto-recover.py</code> to match metrics exporter logic</li> <li> <p>Added tests for newest-by-mtime selection</p> </li> <li> <p>Fixed runner detection (commit 1648f74):</p> </li> <li>Added job lock probe as final fallback in <code>_detect_job_runner</code></li> <li> <p>Classify as \"worker\" when lock is held and worker is running</p> </li> <li> <p>Fixed permission error handling (commit e77212b):</p> </li> <li>Catch <code>OSError</code> in <code>cmd_recover_stale_jobs</code> lock probe</li> <li> <p>Treat <code>PermissionError</code> as \"potentially held\" in auto-recover</p> </li> <li> <p>Fixed O_CREAT issue (commit e073749):</p> </li> <li>Try <code>O_RDWR</code> first in <code>_job_lock</code>, fall back to <code>O_CREAT | O_RDWR</code> only for new files</li> <li> <p>Bypasses <code>fs.protected_regular</code> restrictions on existing files</p> </li> <li> <p>Fixed <code>/tmp/healtharchive-job-locks/</code> permissions:</p> </li> <li>One-time: <code>chmod 1777 /tmp/healtharchive-job-locks/</code></li> <li>One-time: <code>chmod 666 /tmp/healtharchive-job-locks/job-*.lock</code></li> </ol>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#post-incident-verification","title":"Post-incident verification","text":"<ul> <li>Auto-recover successfully detected and recovered job 6 at 02:35 UTC</li> <li>Worker automatically picked up retryable job 6 at 02:40 UTC</li> <li>New crawl log created, crawlStatus advancing normally (5 pages/3 min)</li> <li>All 238 tests pass, CI green</li> <li>Auto-recover timer running every 5 min without errors</li> </ul>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#action-items","title":"Action items","text":"<ul> <li> Fix stale log detection in auto-recover (completed: 4bddb7c)</li> <li> Fix runner detection to check job locks (completed: 1648f74)</li> <li> Handle PermissionError in lock probes (completed: e77212b)</li> <li> Avoid O_CREAT on existing lock files (completed: e073749)</li> <li> Document fs.protected_regular interaction in MEMORY.md (completed)</li> <li> Create incident record (this file)</li> <li> Consider moving job locks out of /tmp to dedicated directory (priority=low, future improvement)</li> </ul>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#automation-opportunities","title":"Automation opportunities","text":"<ul> <li>The fixes enable fully automatic recovery for future stalls</li> <li>No additional automation needed \u2014 the bugs prevented existing automation from working</li> <li>Keep metrics exporter and auto-recover <code>_find_job_log</code> logic in sync (ongoing maintenance)</li> </ul>"},{"location":"operations/incidents/2026-02-06-auto-recover-stall-detection-bugs/#references-artifacts","title":"References / Artifacts","text":"<ul> <li>Playbook: <code>docs/operations/playbooks/crawl/crawl-stalls.md</code></li> <li>Auto-recover script: <code>scripts/vps-crawl-auto-recover.py</code></li> <li>Metrics exporter: <code>scripts/vps-crawl-metrics-textfile.py</code></li> <li>Test suite: <code>tests/test_ops_crawl_auto_recover_find_job_log.py</code></li> <li>Operator scratch notes: (local, not in repo)</li> <li>Commits: 4bddb7c, 1648f74, e77212b, e073749</li> </ul>"},{"location":"operations/incidents/severity/","title":"Incident severity rubric (internal)","text":"<p>Severity is a shared shorthand for priority and urgency, not blame.</p> <p>Use this rubric to decide how quickly to respond, what to pause, and what level of documentation/verification is appropriate.</p> <p>Notes:</p> <ul> <li>Severity can change as you learn more; update the incident note accordingly.</li> <li>When in doubt, start higher and downgrade later.</li> <li>Data integrity risk should push severity up.</li> </ul>"},{"location":"operations/incidents/severity/#sev0-critical-outage-integrity-security","title":"sev0 \u2014 Critical outage / integrity / security","text":"<p>Criteria (any one is enough):</p> <ul> <li>Public site/API is effectively down for most users, or returning incorrect/unsafe data.</li> <li>Credible data integrity compromise (corruption, missing/cross-linked WARCs, replay integrity loss).</li> <li>Security incident (credential leak, unauthorized access, suspicious behavior).</li> </ul> <p>Response expectations:</p> <ul> <li>Immediate response.</li> <li>Prefer conservative actions that preserve integrity (pause/stop destructive jobs).</li> <li>Capture a complete timeline and include post-incident verification.</li> <li>Public note is optional but recommended when it changes user expectations (outage, integrity risk, security posture, policy change). Keep it public-safe (no sensitive details).</li> </ul>"},{"location":"operations/incidents/severity/#sev1-major-degradation-time-critical-capture-risk","title":"sev1 \u2014 Major degradation / time-critical capture risk","text":"<p>Criteria (any one is enough):</p> <ul> <li>Public site/API is usable but severely degraded (major routes broken, errors widespread).</li> <li>Annual capture campaign is blocked or likely to miss the window (or lose meaningful coverage) without intervention.</li> <li>Storage/mount instability that threatens crawl/indexing continuity even if the public surface is OK.</li> </ul> <p>Response expectations:</p> <ul> <li>Same-day response.</li> <li>Record the recovery steps precisely (commands + what they changed).</li> <li>Public note is optional but recommended when it changes user expectations (outage/degradation, integrity risk, public posture). Keep it public-safe (no sensitive details).</li> </ul>"},{"location":"operations/incidents/severity/#sev2-partial-degradation-contained-pipeline-failure","title":"sev2 \u2014 Partial degradation / contained pipeline failure","text":"<p>Criteria (typical examples):</p> <ul> <li>One source crawl repeatedly failing but others are healthy.</li> <li>Crawl/indexing slowdown or intermittent errors with a known workaround.</li> <li>Non-critical automation failing (metrics, watchdogs, timers) with manual fallback.</li> </ul> <p>Response expectations:</p> <ul> <li>Next-business-day response is usually acceptable.</li> <li>Document what happened and track follow-ups that reduce repeat issues.</li> </ul>"},{"location":"operations/incidents/severity/#sev3-minor-issue-operational-friction","title":"sev3 \u2014 Minor issue / operational friction","text":"<p>Criteria (typical examples):</p> <ul> <li>Internal-only issue with low urgency and a simple workaround.</li> <li>Documentation gaps discovered during ops.</li> <li>Cosmetic or non-impactful errors that should still be cleaned up.</li> </ul> <p>Response expectations:</p> <ul> <li>Fix opportunistically.</li> <li>Still record the incident if it required manual intervention or could recur.</li> </ul>"},{"location":"operations/playbooks/","title":"Ops playbooks (task-oriented)","text":"<p>Playbooks are short, task-oriented checklists for recurring operator work.</p> <p>If you only read one thing first:</p> <ul> <li>operator-responsibilities.md (what you must do to keep the site healthy)</li> </ul> <p>Rules:</p> <ul> <li>Keep them brief and procedural.</li> <li>Avoid duplicating canonical docs; link to the runbook/checklist that owns the details.</li> <li>Prefer stable command entrypoints (scripts) so steps don't drift.</li> <li>Use the template for new playbooks: <code>../../_templates/playbook-template.md</code></li> </ul>"},{"location":"operations/playbooks/#directory-structure","title":"Directory Structure","text":"<p>Playbooks are organized by category:</p> <pre><code>playbooks/\n\u251c\u2500\u2500 core/           # Essential daily operator work\n\u251c\u2500\u2500 observability/  # Monitoring and alerting setup\n\u251c\u2500\u2500 crawl/          # Crawl and archive lifecycle\n\u251c\u2500\u2500 storage/        # WARC storage and integrity\n\u251c\u2500\u2500 validation/     # Quality assurance and verification\n\u2514\u2500\u2500 external/       # External-facing operations\n</code></pre>"},{"location":"operations/playbooks/#core-operations","title":"Core Operations","text":"<p>Essential playbooks for daily operator work:</p> <ul> <li>Operator responsibilities: core/operator-responsibilities.md \u2014 Core operator duties</li> <li>Deploy &amp; verify: core/deploy-and-verify.md \u2014 Deployment workflow</li> <li>Incident response: core/incident-response.md \u2014 When something breaks</li> <li>Admin proxy: core/admin-proxy.md \u2014 Browser-friendly ops triage</li> <li>Replay service: core/replay-service.md \u2014 Replay service operations</li> </ul>"},{"location":"operations/playbooks/#observability","title":"Observability","text":"<p>Monitoring infrastructure setup and maintenance:</p> <ul> <li>Setup guide: observability/observability-guide.md \u2014 Complete observability stack setup (Prometheus, Grafana, Alertmanager)</li> <li>Monitoring setup: observability/monitoring-and-alerting.md \u2014 External monitors and Healthchecks</li> </ul>"},{"location":"operations/playbooks/#crawl-archive-operations","title":"Crawl &amp; Archive Operations","text":"<p>Managing crawls and archive lifecycle:</p> <ul> <li>Crawl preflight: crawl/crawl-preflight.md \u2014 Pre-crawl audit (before annual/large crawls)</li> <li>Crawl stalls: crawl/crawl-stalls.md \u2014 Stalled progress + status snapshot</li> <li>Crawl auto-recover drills: crawl/crawl-auto-recover-drills.md \u2014 Safe dry-run drills (production)</li> <li>Annual campaign: crawl/annual-campaign.md \u2014 Seasonal campaign operations</li> <li>Controlled restart: crawl/2026-01-annual-campaign-controlled-restart.md \u2014 2026 annual crawl restart procedure</li> <li>Cleanup automation: crawl/cleanup-automation.md \u2014 Safe temp cleanup</li> </ul>"},{"location":"operations/playbooks/#storage-management","title":"Storage Management","text":"<p>WARC storage and integrity:</p> <ul> <li>WARC storage tiering: storage/warc-storage-tiering.md \u2014 SSD + Storage Box tiering</li> <li>WARC integrity: storage/warc-integrity-verification.md \u2014 Verify WARCs</li> <li>Stale mount recovery: storage/storagebox-sshfs-stale-mount-recovery.md \u2014 Errno 107 recovery</li> <li>Recovery drills: storage/storagebox-sshfs-stale-mount-drills.md \u2014 Safe production drills</li> </ul>"},{"location":"operations/playbooks/#validation-testing","title":"Validation &amp; Testing","text":"<p>Quality assurance and verification:</p> <ul> <li>Restore test: validation/restore-test.md \u2014 Quarterly restore test</li> <li>Dataset release: validation/dataset-release.md \u2014 Dataset release integrity (quarterly)</li> <li>Coverage guardrails: validation/coverage-guardrails.md \u2014 Annual regression checks</li> <li>Replay smoke tests: validation/replay-smoke-tests.md \u2014 Daily replay validation</li> <li>Healthchecks parity: validation/healthchecks-parity.md \u2014 Env/systemd/Healthchecks sync</li> <li>Security posture: validation/security-posture.md \u2014 Ongoing security checks</li> <li>Automation maintenance: validation/automation-maintenance.md \u2014 Keep automation healthy</li> </ul>"},{"location":"operations/playbooks/#external-outreach","title":"External &amp; Outreach","text":"<p>External-facing operations:</p> <ul> <li>Outreach &amp; verification: external/outreach-and-verification.md \u2014 External outreach workflow</li> <li>Adoption signals: external/adoption-signals.md \u2014 Quarterly adoption signals entry</li> </ul>"},{"location":"operations/playbooks/#quick-reference","title":"Quick Reference","text":"Frequency Tasks Playbooks Daily Service health, crawl status ops-cadence-checklist.md, crawl/crawl-stalls.md Weekly Monitoring review, automation posture ops-cadence-checklist.md, validation/automation-maintenance.md Monthly Reliability review, docs drift ops-cadence-checklist.md Quarterly Restore test, dataset release, adoption signals validation/restore-test.md, validation/dataset-release.md, external/adoption-signals.md Annual Campaign readiness, coverage guardrails crawl/annual-campaign.md, validation/coverage-guardrails.md <p>For the complete operations cadence: <code>../ops-cadence-checklist.md</code></p>"},{"location":"operations/playbooks/core/admin-proxy/","title":"Admin proxy (browser-friendly ops triage; VPS)","text":"<p>Goal: make it easy to browse admin/metrics endpoints in a browser without copying tokens into the browser.</p> <p>This is a lightweight alternative to building a bespoke admin UI.</p>"},{"location":"operations/playbooks/core/admin-proxy/#design","title":"Design","text":"<ul> <li>Runs a tiny reverse proxy on the VPS, bound to loopback only (<code>127.0.0.1</code>).</li> <li>Proxies read-only GET requests to:</li> <li><code>/api/admin/**</code></li> <li><code>/metrics</code></li> <li>Adds the backend admin token server-side from:</li> <li><code>/etc/healtharchive/observability/prometheus_backend_admin_token</code></li> <li>You access it from your laptop via SSH port-forwarding (tailnet-only SSH).</li> </ul> <p>Security notes:</p> <ul> <li>No new public ports.</li> <li>Browser never sees the admin token.</li> <li>Anyone with shell access to the VPS can reach <code>127.0.0.1</code>, so treat VPS access as privileged.</li> </ul>"},{"location":"operations/playbooks/core/admin-proxy/#install-apply-vps","title":"Install / apply (VPS)","text":"<p>1) Pull the latest repo:</p> <ul> <li><code>cd /opt/healtharchive-backend</code></li> <li><code>git pull</code></li> </ul> <p>2) Dry-run:</p> <ul> <li><code>./scripts/vps-install-ops-admin-proxy.sh</code></li> </ul> <p>3) Apply:</p> <ul> <li><code>sudo ./scripts/vps-install-ops-admin-proxy.sh --apply</code></li> </ul>"},{"location":"operations/playbooks/core/admin-proxy/#verify-vps","title":"Verify (VPS)","text":"<ul> <li><code>curl -s http://127.0.0.1:8002/-/health</code></li> <li><code>curl -s http://127.0.0.1:8002/api/admin/jobs?limit=1 | head</code></li> <li><code>curl -s http://127.0.0.1:8002/metrics | head</code></li> </ul>"},{"location":"operations/playbooks/core/admin-proxy/#use-from-your-laptop-ssh-port-forward","title":"Use from your laptop (SSH port-forward)","text":"<p>1) Start a tunnel:</p> <ul> <li><code>ssh -N -L 8002:127.0.0.1:8002 haadmin@&lt;vps-tailscale-ip&gt;</code></li> </ul> <p>2) Open in your browser:</p> <ul> <li><code>http://127.0.0.1:8002/</code></li> </ul> <p>Useful endpoints:</p> <ul> <li><code>http://127.0.0.1:8002/api/admin/jobs</code></li> <li><code>http://127.0.0.1:8002/api/admin/jobs/status-counts</code></li> <li><code>http://127.0.0.1:8002/api/admin/reports</code></li> <li><code>http://127.0.0.1:8002/api/admin/search-debug?q=covid</code></li> <li><code>http://127.0.0.1:8002/metrics</code></li> </ul>"},{"location":"operations/playbooks/core/admin-proxy/#rollback","title":"Rollback","text":"<ul> <li><code>sudo systemctl disable --now healtharchive-admin-proxy.service</code></li> <li><code>sudo rm -f /etc/systemd/system/healtharchive-admin-proxy.service</code></li> <li><code>sudo rm -f /usr/local/bin/healtharchive-admin-proxy</code></li> <li><code>sudo systemctl daemon-reload</code></li> </ul>"},{"location":"operations/playbooks/core/deploy-and-verify/","title":"Deploy + verify playbook (production VPS)","text":"<p>Goal: deploy a known-good <code>main</code> and verify production matches policy.</p> <p>Canonical references:</p> <ul> <li>Production runbook: <code>../../../deployment/production-single-vps.md</code></li> <li>Monitoring/CI gate: <code>../../monitoring-and-ci-checklist.md</code></li> <li>Baseline drift: <code>../../baseline-drift.md</code></li> </ul>"},{"location":"operations/playbooks/core/deploy-and-verify/#preconditions","title":"Preconditions","text":"<ul> <li>CI is green on the commit you intend to deploy.</li> <li>You are on the production VPS and can <code>sudo</code>.</li> </ul>"},{"location":"operations/playbooks/core/deploy-and-verify/#procedure","title":"Procedure","text":"<ol> <li> <p>Update the repo on the VPS:</p> </li> <li> <p><code>cd /opt/healtharchive-backend &amp;&amp; git pull</code></p> </li> <li> <p>Run the deploy gate (recommended one command):</p> </li> <li> <p><code>./scripts/vps-deploy.sh --apply --baseline-mode live</code></p> </li> </ol> <p>Recommended wrapper (routine use):</p> <ul> <li><code>./scripts/vps-hetzdeploy.sh</code></li> </ul> <p>This includes:</p> <ul> <li>DB migrations</li> <li>service restarts (API always; worker may be skipped during active crawls)</li> <li>baseline drift verification</li> <li>public surface verification</li> </ul> <p>If your change updates systemd unit templates or Prometheus alert rules, you can    apply those as part of the deploy:</p> <ul> <li><code>./scripts/vps-deploy.sh --apply --baseline-mode live --install-systemd-units --apply-alerting</code></li> </ul> <p>If the public frontend is externally down (e.g., Vercel <code>402 Payment required</code>), you can deploy backend-only:</p> <ul> <li><code>./scripts/vps-hetzdeploy.sh --mode backend-only</code></li> </ul> <p>Optional: install the wrapper outside the repo so it never dirties <code>/opt/healtharchive-backend</code>:</p> <ul> <li><code>sudo ./scripts/vps-install-hetzdeploy.sh --apply</code></li> <li>Then run: <code>hetzdeploy</code> or <code>hetzdeploy --mode backend-only</code></li> </ul> <p>Notes:</p> <ul> <li>Prefer a real command over an alias; aliases can accidentally persist <code>set -euo pipefail</code> in your interactive shell.</li> <li>If <code>hetzdeploy --mode backend-only</code> errors with <code>syntax error near unexpected token</code>, you probably still have an alias named <code>hetzdeploy</code>.<ul> <li>Check: <code>type hetzdeploy</code></li> <li>Remove: <code>unalias hetzdeploy 2&gt;/dev/null || true</code> and delete the alias line from your shell startup files.</li> </ul> </li> <li><code>--apply-alerting</code> requires alerting to be configured on the VPS (webhook secret present at      <code>/etc/healtharchive/observability/alertmanager_webhook_url</code>).</li> </ul> <p>If you are updating the replay banner/template or replay service config on a    single-VPS deployment, include replay restart + banner install:</p> <ul> <li><code>./scripts/vps-deploy.sh --apply --baseline-mode live --restart-replay</code></li> </ul> <p>Crawl safety:</p> <ul> <li>If any jobs are <code>status=running</code>, the deploy helper will skip restarting <code>healtharchive-worker</code>      by default to avoid SIGTERMing an active crawl.</li> <li>When you need to force a worker restart (only when safe): <code>./scripts/vps-deploy.sh --apply --baseline-mode live --force-worker-restart</code></li> <li> <p>If you want to explicitly keep the worker untouched regardless of job status: <code>./scripts/vps-deploy.sh --apply --baseline-mode live --skip-worker-restart</code></p> </li> <li> <p>If the deploy gate fails:</p> </li> <li> <p>Do not retry blindly.</p> </li> <li>Read the failure output:<ul> <li>drift report artifacts under <code>/srv/healtharchive/ops/baseline/</code></li> <li>verifier output from <code>verify_public_surface.py</code></li> </ul> </li> <li>Fix the underlying mismatch (production state vs policy) or intentionally update policy.</li> </ul>"},{"location":"operations/playbooks/core/deploy-and-verify/#quick-follow-ups-optional","title":"Quick follow-ups (optional)","text":"<ul> <li>Confirm timers/sentinels posture (if you operate automation):</li> <li><code>./scripts/verify_ops_automation.sh</code></li> </ul>"},{"location":"operations/playbooks/core/incident-response/","title":"Incident response playbook (operators)","text":"<p>Goal: restore service safely and capture enough context to prevent repeat incidents.</p> <p>Canonical references:</p> <ul> <li>Production runbook: <code>../../../deployment/production-single-vps.md</code></li> <li>Monitoring checklist: <code>../../monitoring-and-ci-checklist.md</code></li> <li>Service levels: <code>../../service-levels.md</code> \u2014 for communication commitments and SLOs</li> <li>Escalation procedures: <code>../../escalation-procedures.md</code></li> <li>Disaster recovery runbook: <code>../../../deployment/disaster-recovery.md</code></li> <li>Baseline drift: <code>../../baseline-drift.md</code></li> <li>Incident notes (template + where to file): <code>../../incidents/README.md</code></li> <li>Ops runbooks (quick response procedures): <code>../../runbooks/README.md</code></li> </ul>"},{"location":"operations/playbooks/core/incident-response/#first-start-an-incident-note","title":"First: start an incident note","text":"<p>As soon as you suspect this is \u201can incident\u201d (not routine maintenance), start a note so you can record a timeline and the exact recovery steps.</p> <ul> <li>Create a new file: <code>docs/operations/incidents/YYYY-MM-DD-short-slug.md</code></li> <li>Copy the template: <code>docs/_templates/incident-template.md</code></li> <li>Pick an initial severity using: <code>docs/operations/incidents/severity.md</code></li> </ul> <p>If you can\u2019t easily edit the repo on the VPS, capture the note in a local scratchpad and copy it into the repo later.</p>"},{"location":"operations/playbooks/core/incident-response/#when-the-siteapi-looks-broken","title":"When the site/API looks broken","text":"<ol> <li>Confirm what\u2019s failing (public surface):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/verify_public_surface.py</code></li> <li>Check services:</li> <li><code>sudo systemctl status healtharchive-api healtharchive-worker --no-pager -l</code></li> <li>Check recent logs:</li> <li><code>sudo journalctl -u healtharchive-api -n 200 --no-pager</code></li> <li><code>sudo journalctl -u healtharchive-worker -n 200 --no-pager</code></li> <li>Check baseline drift (production correctness):</li> <li><code>./scripts/check_baseline_drift.py --mode live</code></li> </ol>"},{"location":"operations/playbooks/core/incident-response/#when-jobs-are-stuck-crawlindexing-pipeline","title":"When jobs are stuck (crawl/indexing pipeline)","text":"<p>If the worker is running but jobs never advance, check for a job stuck in <code>status=running</code> after a reboot or unexpected termination.</p> <ol> <li>Load production environment (so the CLI targets Postgres):</li> <li><code>set -a; source /etc/healtharchive/backend.env; set +a</code></li> <li>Inspect recent jobs:</li> <li><code>/opt/healtharchive-backend/.venv/bin/ha-backend list-jobs --limit 50</code></li> <li>Recover stale running jobs (safe dry-run first):</li> <li><code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 180</code></li> <li>Apply (sets <code>status=retryable</code>): <code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 180 --apply</code></li> <li>Verify the worker picks them up:</li> <li><code>sudo journalctl -u healtharchive-worker -n 200 --no-pager</code></li> </ol>"},{"location":"operations/playbooks/core/incident-response/#if-you-need-to-deploy-a-fix","title":"If you need to deploy a fix","text":"<ul> <li>Follow <code>deploy-and-verify.md</code> (don\u2019t skip the deploy gate).</li> </ul>"},{"location":"operations/playbooks/core/incident-response/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>The public surface verification passes again.</li> <li>The underlying cause is identified (config drift, failed migration, disk, external dependency, etc.).</li> </ul>"},{"location":"operations/playbooks/core/operator-responsibilities/","title":"Operator responsibilities (must-do list)","text":"<p>Goal: keep HealthArchive operating safely and predictably over time.</p> <p>This file is intentionally brief; it points to canonical docs when you need details.</p>"},{"location":"operations/playbooks/core/operator-responsibilities/#always-every-deploy","title":"Always (every deploy)","text":"<ul> <li>Treat green <code>main</code> as the deploy gate (run checks, push, wait for CI).</li> <li>Canonical: <code>../../monitoring-and-ci-checklist.md</code></li> <li>Run the deploy helper on the VPS (safe deploy + verify):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-deploy.sh --apply --baseline-mode live</code></li> <li>Playbook: <code>deploy-and-verify.md</code></li> <li>Prefer the wrapper for routine deploys (strict git hygiene; safer shell behavior):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/vps-hetzdeploy.sh</code></li> <li>Recommended: install <code>hetzdeploy</code> as a real command (avoid brittle aliases; enables <code>--mode backend-only</code>):</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; sudo ./scripts/vps-install-hetzdeploy.sh --apply</code></li> <li>If the deploy script fails: don\u2019t retry blindly.</li> <li>Read the drift report and verifier output and fix the underlying mismatch.</li> <li>Canonical: <code>../../baseline-drift.md</code></li> <li>If you had to do manual steps or discovered drift, update the canonical runbook/playbook so the next operator can follow reality.</li> <li>Canonical: <code>../../ops-cadence-checklist.md</code></li> </ul>"},{"location":"operations/playbooks/core/operator-responsibilities/#ongoing-automation-maintenance","title":"Ongoing automation maintenance","text":"<ul> <li>Keep systemd unit templates installed/updated on the VPS after repo updates:</li> <li><code>sudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker</code></li> <li>Playbook: <code>../validation/automation-maintenance.md</code></li> <li>Maintain sentinel files under <code>/etc/healtharchive/</code> (explicit automation on/off controls).</li> <li>Canonical: <code>../../../deployment/systemd/README.md</code></li> <li>If you enable Healthchecks pings:</li> <li>keep ping URLs only in the root-owned VPS env file (never in git):<ul> <li><code>/etc/healtharchive/healthchecks.env</code></li> </ul> </li> <li>Note: this file may contain both legacy <code>HC_*</code> variables (DB backup + disk)     and newer <code>HEALTHARCHIVE_HC_PING_*</code> variables (systemd unit templates).</li> <li>Canonical: <code>../../monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"operations/playbooks/core/operator-responsibilities/#quarterly-ops-cadence-sustainability-loop","title":"Quarterly ops cadence (sustainability loop)","text":"<ul> <li>Run a restore test and write a public-safe log entry.</li> <li>Playbook: <code>../validation/restore-test.md</code></li> <li>Verify dataset release checksum integrity (<code>SHA256SUMS</code>).</li> <li>Playbook: <code>../validation/dataset-release.md</code></li> <li>Add an adoption signals entry (links + aggregate counts only).</li> <li>Playbook: <code>../external/adoption-signals.md</code></li> <li>Confirm timers are still enabled and not silently failing.</li> <li><code>./scripts/verify_ops_automation.sh</code> and spot-check <code>journalctl</code></li> <li>Playbook: <code>../validation/automation-maintenance.md</code></li> <li>Do a quick docs drift skim (production runbook + incident response) and fix anything stale.</li> <li>Canonical: <code>../../ops-cadence-checklist.md</code></li> </ul>"},{"location":"operations/playbooks/core/operator-responsibilities/#security-posture-always","title":"Security posture (always)","text":"<ul> <li>Keep secrets (admin token, DB URL, ping URLs) out of git forever.</li> <li>Canonical: <code>../../../deployment/production-single-vps.md</code></li> <li>Maintain HSTS at Caddy for <code>api.healtharchive.ca</code>.</li> <li>Canonical: <code>../../../deployment/hosting-and-live-server-to-dos.md</code></li> <li>Maintain a strict CORS allowlist; treat widening it as a deliberate security decision.</li> <li>Canonical: <code>../../../deployment/environments-and-configuration.md</code></li> </ul>"},{"location":"operations/playbooks/core/replay-service/","title":"Replay service playbook (operators)","text":"<p>Goal: keep replay (<code>replay.healtharchive.ca</code>) available when the project relies on it.</p> <p>Canonical references:</p> <ul> <li>Replay runbook: <code>../../../deployment/replay-service-pywb.md</code></li> <li>Production runbook: <code>../../../deployment/production-single-vps.md</code></li> <li>Replay automation design: <code>../../replay-and-preview-automation-plan.md</code></li> </ul>"},{"location":"operations/playbooks/core/replay-service/#setup-recovery-if-replay-is-missing","title":"Setup / recovery (if replay is missing)","text":"<p>Follow <code>../../../deployment/replay-service-pywb.md</code>.</p>"},{"location":"operations/playbooks/core/replay-service/#verify-replay-is-working","title":"Verify replay is working","text":"<ol> <li>Check the base URL is up:</li> <li><code>curl -I https://replay.healtharchive.ca/ | head</code></li> <li>Verify the public surface script can resolve a replay <code>browseUrl</code> for a known snapshot:</li> <li><code>cd /opt/healtharchive-backend &amp;&amp; ./scripts/verify_public_surface.py</code></li> <li>Verify the replay banner works on a direct replay page:</li> <li>Open a known <code>browseUrl</code> on <code>https://replay.healtharchive.ca/</code> and confirm the banner loads quickly, shows the page title + meta line (capture date + original URL) + disclaimer, and that the action links (View diff, Details, All snapshots, Raw HTML, Metadata JSON, Cite, Report issue, Hide) behave as expected.</li> <li>From HealthArchive search results, click <code>View</code> and confirm \u201c\u2190 HealthArchive.ca\u201d returns to the same search results page.</li> </ol>"},{"location":"operations/playbooks/core/replay-service/#retention-warning","title":"Retention warning","text":"<p>Replay depends on WARCs staying on disk. Do not delete WARCs for jobs you expect to replay.</p>"},{"location":"operations/playbooks/core/replay-service/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li><code>https://replay.healtharchive.ca/</code> responds successfully.</li> <li><code>./scripts/verify_public_surface.py</code> reports a working replay <code>browseUrl</code> where expected.</li> </ul>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/","title":"2026 Annual Campaign Controlled Restart Plan","text":"<p>Date: 2026-01-27 Severity: Sev1 (major degradation - crawls running 27 days without completion) Status: Planning</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#executive-summary","title":"Executive Summary","text":"<p>The 2026 annual campaign (3 jobs: hc, phac, cihr) has been running since Jan 1 with persistent stability issues:</p> <ul> <li>Job 6 (HC): 67 temp dirs, 30 auto-recoveries, state file 2.4 days stale</li> <li>Job 7 (PHAC): stalled 36+ minutes, 11 container restarts, 12 temp dirs</li> <li>Job 8 (CIHR): regressed from 45% to 38%, permission errors, 4 temp dirs</li> </ul> <p>Goal: Controlled restart that preserves all captured WARCs while fixing infrastructure issues.</p> <p>Risk: Data loss if WARCs in <code>.tmp*</code> directories are deleted before consolidation.</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#pre-flight-current-state-assessment","title":"Pre-Flight: Current State Assessment","text":""},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#known-issues","title":"Known Issues","text":"<ol> <li>Stale sshfs mounts (Errno 107) on jobs 7 and 8 output directories</li> <li>Permission denied on Job 8's <code>.tmp_zcuywum/collections</code></li> <li>Disk usage at 68% (9GB/12.8h burn rate)</li> <li>State file staleness (Job 6: 2.4 days, suggesting state not persisting)</li> </ol>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#data-to-preserve","title":"Data to Preserve","text":"Job Temp Dirs Est. WARCs Notes 6 (hc) 67 Unknown (many) Excessive temp dirs suggest many restart phases 7 (phac) 12 Unknown 11 container restarts 8 (cihr) 4 11 discovered 38% complete on latest run"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#phase-1-snapshot-and-document-current-state-15-min","title":"Phase 1: Snapshot and Document Current State (15 min)","text":"<p>Purpose: Create recovery point before any changes.</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#11-full-status-snapshot","title":"1.1 Full Status Snapshot","text":"<pre><code>cd /opt/healtharchive-backend\n\n# Save full status to file\n./scripts/vps-crawl-status.sh --year 2026 &gt; /tmp/crawl-status-$(date -u +%Y%m%dT%H%M%SZ).txt 2&gt;&amp;1\n\n# Document disk state\ndf -h | tee /tmp/disk-state-$(date -u +%Y%m%dT%H%M%SZ).txt\n\n# Document mount state\nmount | grep healtharchive | tee /tmp/mounts-$(date -u +%Y%m%dT%H%M%SZ).txt\n\n# Document running containers\ndocker ps --format 'table {{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.Status}}' | tee /tmp/docker-ps-$(date -u +%Y%m%dT%H%M%SZ).txt\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#12-inventory-existing-warcs-critical","title":"1.2 Inventory Existing WARCs (Critical)","text":"<p>IMPORTANT: Record all WARC locations BEFORE any changes.</p> <pre><code># For each job, document WARCs found\nfor job_dir in /srv/healtharchive/jobs/*/20260101*; do\n  echo \"=== $job_dir ===\" &gt;&gt; /tmp/warc-inventory.txt\n  find \"$job_dir\" -name \"*.warc.gz\" -o -name \"*.warc\" 2&gt;/dev/null | \\\n    while read f; do\n      stat --format=\"%s %Y %n\" \"$f\" 2&gt;/dev/null || echo \"STAT_FAILED: $f\"\n    done &gt;&gt; /tmp/warc-inventory.txt\ndone\n\n# Count totals\necho \"=== WARC Counts ===\" &gt;&gt; /tmp/warc-inventory.txt\nfor job_dir in /srv/healtharchive/jobs/*/20260101*; do\n  count=$(find \"$job_dir\" -name \"*.warc.gz\" -o -name \"*.warc\" 2&gt;/dev/null | wc -l)\n  echo \"$job_dir: $count WARCs\" &gt;&gt; /tmp/warc-inventory.txt\ndone\n\ncat /tmp/warc-inventory.txt\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#13-document-state-files","title":"1.3 Document State Files","text":"<pre><code># Copy state files for reference\nmkdir -p /tmp/state-backup-$(date -u +%Y%m%d)\nfor job_dir in /srv/healtharchive/jobs/*/20260101*; do\n  job_name=$(basename \"$job_dir\")\n  cp -v \"$job_dir/.archive_state.json\" \"/tmp/state-backup-$(date -u +%Y%m%d)/${job_name}.state.json\" 2&gt;/dev/null || echo \"No state file: $job_dir\"\ndone\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#phase-2-stop-all-crawl-activity-10-min","title":"Phase 2: Stop All Crawl Activity (10 min)","text":"<p>Purpose: Prevent further changes while we assess and repair.</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#21-stop-worker-service","title":"2.1 Stop Worker Service","text":"<pre><code>sudo systemctl stop healtharchive-worker.service\nsudo systemctl status healtharchive-worker.service --no-pager -l\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#22-disable-auto-recovery-timers-temporarily","title":"2.2 Disable Auto-Recovery Timers (Temporarily)","text":"<pre><code># Disable crawl auto-recover\nsudo mv /etc/healtharchive/crawl-auto-recover-enabled /etc/healtharchive/crawl-auto-recover-enabled.disabled 2&gt;/dev/null || echo \"Already disabled\"\n\n# Disable storage hotpath auto-recover\nsudo mv /etc/healtharchive/storage-hotpath-auto-recover-enabled /etc/healtharchive/storage-hotpath-auto-recover-enabled.disabled 2&gt;/dev/null || echo \"Already disabled\"\n\n# Disable worker auto-start\nsudo mv /etc/healtharchive/worker-auto-start-enabled /etc/healtharchive/worker-auto-start-enabled.disabled 2&gt;/dev/null || echo \"Already disabled\"\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#23-stop-any-running-zimit-containers","title":"2.3 Stop Any Running Zimit Containers","text":"<pre><code># List zimit containers\ndocker ps --format '{{.ID}} {{.Image}} {{.Names}}' | grep -E 'zimit|openzim'\n\n# Stop them gracefully (adjust IDs as needed)\ndocker ps --format '{{.ID}} {{.Image}}' | grep -E 'zimit|openzim' | awk '{print $1}' | xargs -r docker stop\n\n# Verify stopped\ndocker ps | grep -E 'zimit|openzim' || echo \"No running zimit containers\"\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#phase-3-fix-infrastructure-issues-20-min","title":"Phase 3: Fix Infrastructure Issues (20 min)","text":""},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#31-fix-stale-mounts","title":"3.1 Fix Stale Mounts","text":"<p>Follow <code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code>:</p> <pre><code># Check Storage Box base mount\nls -la /srv/healtharchive/storagebox &gt;/dev/null &amp;&amp; echo \"OK: storagebox readable\" || echo \"BAD: storagebox unreadable\"\n\n# Identify stale job mounts\nfor job_dir in /srv/healtharchive/jobs/*/20260101*; do\n  ls \"$job_dir\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; echo \"OK: $job_dir\" || echo \"STALE: $job_dir\"\ndone\n\n# Unmount stale hot paths (adjust paths based on actual findings)\nsudo umount -l /srv/healtharchive/jobs/phac/20260101T000502Z__phac-20260101 2&gt;/dev/null || true\nsudo umount -l /srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101 2&gt;/dev/null || true\n\n# Re-apply tiering with repair\nsudo ./scripts/vps-warc-tiering-bind-mounts.sh --apply --repair-stale-mounts\n\n# Verify mounts are healthy\nfor job_dir in /srv/healtharchive/jobs/*/20260101*; do\n  ls \"$job_dir\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; echo \"OK: $job_dir\" || echo \"STILL_BROKEN: $job_dir\"\ndone\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#32-fix-permission-issues","title":"3.2 Fix Permission Issues","text":"<pre><code># For Job 8 (cihr) with permission denied on .tmp_zcuywum\njob_dir=\"/srv/healtharchive/jobs/cihr/20260101T000502Z__cihr-20260101\"\n\n# Use Docker alpine container to fix perms (avoids needing host sudo on files)\ndocker run --rm -v \"$job_dir:/output\" alpine chmod -R a+rX /output/.tmp* 2&gt;/dev/null || {\n  # Fallback: use sudo if Docker approach fails\n  sudo chmod -R a+rX \"$job_dir\"/.tmp* 2&gt;/dev/null || echo \"Permission fix may need manual intervention\"\n}\n\n# Verify\nls -la \"$job_dir\"/.tmp*/collections/ 2&gt;/dev/null | head -5\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#33-verify-disk-space","title":"3.3 Verify Disk Space","text":"<pre><code>df -h /srv/healtharchive/jobs\n\n# If above 70%, identify large temp dirs for later cleanup\ndu -sh /srv/healtharchive/jobs/*/20260101*/.tmp* 2&gt;/dev/null | sort -h | tail -20\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#phase-4-assess-and-consolidate-existing-warcs-30-min","title":"Phase 4: Assess and Consolidate Existing WARCs (30 min)","text":"<p>Purpose: Secure all captured data before any restart decisions.</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#41-verify-warcs-per-job","title":"4.1 Verify WARCs Per Job","text":"<pre><code>source /etc/healtharchive/backend.env\ncd /opt/healtharchive-backend\nsource .venv/bin/activate\n\n# Job 6 (HC)\nha-backend show-job --id 6\necho \"--- WARC Discovery ---\"\npython3 -c \"\nfrom ha_backend.indexing.warc_discovery import discover_temp_warcs_for_job\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get(6)\nwarcs = discover_temp_warcs_for_job(job)\nprint(f'Job 6 (HC): {len(warcs)} WARCs discovered')\nfor w in warcs[:5]: print(f'  {w}')\nif len(warcs) &gt; 5: print(f'  ... and {len(warcs)-5} more')\n\"\n\n# Job 7 (PHAC)\nha-backend show-job --id 7\necho \"--- WARC Discovery ---\"\npython3 -c \"\nfrom ha_backend.indexing.warc_discovery import discover_temp_warcs_for_job\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get(7)\nwarcs = discover_temp_warcs_for_job(job)\nprint(f'Job 7 (PHAC): {len(warcs)} WARCs discovered')\nfor w in warcs[:5]: print(f'  {w}')\nif len(warcs) &gt; 5: print(f'  ... and {len(warcs)-5} more')\n\"\n\n# Job 8 (CIHR)\nha-backend show-job --id 8\necho \"--- WARC Discovery ---\"\npython3 -c \"\nfrom ha_backend.indexing.warc_discovery import discover_temp_warcs_for_job\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get(8)\nwarcs = discover_temp_warcs_for_job(job)\nprint(f'Job 8 (CIHR): {len(warcs)} WARCs discovered')\nfor w in warcs[:5]: print(f'  {w}')\nif len(warcs) &gt; 5: print(f'  ... and {len(warcs)-5} more')\n\"\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#42-verify-warc-integrity-level-1","title":"4.2 Verify WARC Integrity (Level 1)","text":"<p>CRITICAL: Do this BEFORE any consolidation or cleanup.</p> <pre><code># Verify WARCs for each job (Level 1 = gzip integrity check)\nha-backend verify-warcs --job-id 6 --level 1 --json-out /tmp/verify-warcs-6.json\nha-backend verify-warcs --job-id 7 --level 1 --json-out /tmp/verify-warcs-7.json\nha-backend verify-warcs --job-id 8 --level 1 --json-out /tmp/verify-warcs-8.json\n\n# Review results\nfor f in /tmp/verify-warcs-*.json; do\n  echo \"=== $f ===\"\n  python3 -c \"import json; d=json.load(open('$f')); print(f\\\"passed={d.get('passed',0)} failed={d.get('failed',0)}\\\")\"\ndone\n</code></pre> <p>If verification shows failures: Use <code>--apply-quarantine</code> to move corrupt WARCs aside (only if job is NOT running):</p> <pre><code># ONLY if failures detected and you want to quarantine corrupt files:\n# ha-backend verify-warcs --job-id &lt;ID&gt; --level 1 --apply-quarantine\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#43-consolidate-warcs-to-stable-location","title":"4.3 Consolidate WARCs to Stable Location","text":"<p>Purpose: Move WARCs from <code>.tmp*</code> to stable <code>warcs/</code> directory with hardlinks.</p> <pre><code># Dry-run first for each job\nha-backend consolidate-warcs --job-id 6 --dry-run\nha-backend consolidate-warcs --job-id 7 --dry-run\nha-backend consolidate-warcs --job-id 8 --dry-run\n\n# If dry-run looks good, apply consolidation\nha-backend consolidate-warcs --job-id 6\nha-backend consolidate-warcs --job-id 7\nha-backend consolidate-warcs --job-id 8\n\n# Verify stable WARCs exist\nfor job_id in 6 7 8; do\n  job_dir=$(python3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get($job_id)\nprint(job.output_dir)\n\")\n  echo \"=== Job $job_id: $job_dir/warcs/ ===\"\n  ls -la \"$job_dir/warcs/\" 2&gt;/dev/null | head -5 || echo \"No stable warcs dir\"\n  ls -la \"$job_dir/warcs/manifest.json\" 2&gt;/dev/null || echo \"No manifest\"\ndone\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#phase-5-decision-point-restart-strategy","title":"Phase 5: Decision Point - Restart Strategy","text":"<p>At this point, you have: - All WARCs consolidated to stable <code>warcs/</code> directories - Infrastructure issues fixed - Worker stopped</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#option-a-resume-existing-crawls-lower-risk-slower","title":"Option A: Resume Existing Crawls (Lower Risk, Slower)","text":"<p>Use this if: - WARCs are mostly complete for some sources - You want to preserve crawl continuity - Time is not critical</p> <p>Pros: Preserves existing progress, zimit may resume from checkpoint Cons: May hit same stability issues, existing adaptation budgets may be exhausted</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#option-b-fresh-crawls-with-warc-consolidation-recommended","title":"Option B: Fresh Crawls with WARC Consolidation (Recommended)","text":"<p>Use this if: - Crawls have been unstable for weeks - Adaptation budgets are exhausted - You want a clean slate but keep existing WARCs</p> <p>Pros: Fresh adaptation budgets, cleaner state, more predictable behavior Cons: Starts crawl from scratch (but WARCs consolidate in final build)</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#option-c-mark-as-completed-and-index-partial-data","title":"Option C: Mark as Completed and Index Partial Data","text":"<p>Use this if: - Time pressure to have SOME data available - Current WARCs represent meaningful coverage - You plan to run a supplemental crawl later</p> <p>Pros: Immediate availability of partial data Cons: Incomplete coverage, may need follow-up crawl</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#phase-6a-resume-existing-crawls","title":"Phase 6A: Resume Existing Crawls","text":""},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#6a1-recover-stale-jobs","title":"6A.1 Recover Stale Jobs","text":"<pre><code># Mark jobs as retryable (preserves state for resume)\nha-backend recover-stale-jobs --older-than-minutes 5 --dry-run\nha-backend recover-stale-jobs --older-than-minutes 5 --apply\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#6a2-reset-retry-budgets-if-needed","title":"6A.2 Reset Retry Budgets (If Needed)","text":"<p>If jobs have exhausted container restart budgets:</p> <pre><code># Check current restart counts\nfor job_id in 6 7 8; do\n  job_dir=$(python3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get($job_id)\nprint(job.output_dir)\n\")\n  echo \"=== Job $job_id ===\"\n  cat \"$job_dir/.archive_state.json\" 2&gt;/dev/null | python3 -m json.tool | grep -E \"restarts|reductions|rotations\"\ndone\n\n# If restart budget exhausted, reset state (preserves temp_dirs)\nfor job_id in 6 7 8; do\n  job_dir=$(python3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get($job_id)\nprint(job.output_dir)\n\")\n  # Edit state to reset counts but keep temp_dirs\n  python3 -c \"\nimport json\nfrom pathlib import Path\nstate_file = Path('$job_dir/.archive_state.json')\nif state_file.exists():\n    state = json.loads(state_file.read_text())\n    state['container_restarts_done'] = 0\n    state['worker_reductions_done'] = 0\n    state['vpn_rotations_done'] = 0\n    state_file.write_text(json.dumps(state, indent=2))\n    print(f'Reset adaptation counts for job $job_id')\n\"\ndone\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#6a3-restart-worker","title":"6A.3 Restart Worker","text":"<pre><code># Re-enable automation\nsudo mv /etc/healtharchive/crawl-auto-recover-enabled.disabled /etc/healtharchive/crawl-auto-recover-enabled 2&gt;/dev/null || true\nsudo mv /etc/healtharchive/storage-hotpath-auto-recover-enabled.disabled /etc/healtharchive/storage-hotpath-auto-recover-enabled 2&gt;/dev/null || true\n\n# Start worker\nsudo systemctl start healtharchive-worker.service\nsudo systemctl status healtharchive-worker.service --no-pager -l\n\n# Monitor for 5 minutes\nsleep 60 &amp;&amp; ./scripts/vps-crawl-status.sh --year 2026 | head -40\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#phase-6b-fresh-crawls-with-warc-consolidation-recommended","title":"Phase 6B: Fresh Crawls with WARC Consolidation (Recommended)","text":""},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#6b1-prepare-jobs-for-fresh-start","title":"6B.1 Prepare Jobs for Fresh Start","text":"<p>CRITICAL: Do NOT use <code>--overwrite</code> flag - it deletes prior WARCs!</p> <pre><code># For each job, delete state file but KEEP temp dirs and WARCs\nfor job_id in 6 7 8; do\n  job_dir=$(python3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nsession = next(get_session())\njob = session.query(ArchiveJob).get($job_id)\nprint(job.output_dir)\n\")\n\n  echo \"=== Job $job_id: $job_dir ===\"\n\n  # Backup state file first\n  cp \"$job_dir/.archive_state.json\" \"/tmp/state-backup-$(date -u +%Y%m%d)/job-$job_id-pre-fresh.json\" 2&gt;/dev/null || true\n\n  # Remove state file (crawl will start fresh but discover existing WARCs)\n  rm -f \"$job_dir/.archive_state.json\"\n  rm -f \"$job_dir/.zimit_resume.yaml\"\n\n  # Verify temp dirs still exist (DO NOT DELETE)\n  echo \"Temp dirs preserved:\"\n  ls -d \"$job_dir\"/.tmp* 2&gt;/dev/null | wc -l\ndone\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#6b2-reset-job-status-in-database","title":"6B.2 Reset Job Status in Database","text":"<pre><code># Reset jobs to queued status with fresh retry count\nsource /etc/healtharchive/backend.env\ncd /opt/healtharchive-backend\nsource .venv/bin/activate\n\npython3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nfrom datetime import datetime, timezone\n\nsession = next(get_session())\n\nfor job_id in [6, 7, 8]:\n    job = session.query(ArchiveJob).get(job_id)\n    if job:\n        print(f'Resetting job {job_id} ({job.source.code}): {job.status} -&gt; queued')\n        job.status = 'queued'\n        job.retry_count = 0\n        job.crawler_exit_code = None\n        job.crawler_status = None\n        job.started_at = None\n        job.finished_at = None\n        job.queued_at = datetime.now(timezone.utc)\n\nsession.commit()\nprint('Jobs reset successfully')\n\"\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#6b3-consider-staggered-restarts","title":"6B.3 Consider Staggered Restarts","text":"<p>To reduce resource contention:</p> <pre><code># Option: Run one job at a time instead of all three\n# Start with the smallest (CIHR)\nha-backend run-db-job --id 8  # Run synchronously to monitor\n\n# Or use detached mode\npython3 ./scripts/vps-run-db-job-detached.py --job-id 8\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#6b4-restart-worker-for-automated-processing","title":"6B.4 Restart Worker for Automated Processing","text":"<pre><code># Re-enable automation\nsudo mv /etc/healtharchive/crawl-auto-recover-enabled.disabled /etc/healtharchive/crawl-auto-recover-enabled 2&gt;/dev/null || true\nsudo mv /etc/healtharchive/storage-hotpath-auto-recover-enabled.disabled /etc/healtharchive/storage-hotpath-auto-recover-enabled 2&gt;/dev/null || true\n\n# Start worker\nsudo systemctl start healtharchive-worker.service\n\n# Worker will pick up queued jobs automatically\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#phase-6c-mark-as-completed-and-index-partial-data","title":"Phase 6C: Mark as Completed and Index Partial Data","text":""},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#6c1-set-jobs-to-completed","title":"6C.1 Set Jobs to Completed","text":"<pre><code>source /etc/healtharchive/backend.env\ncd /opt/healtharchive-backend\nsource .venv/bin/activate\n\npython3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import ArchiveJob\nfrom datetime import datetime, timezone\n\nsession = next(get_session())\n\nfor job_id in [6, 7, 8]:\n    job = session.query(ArchiveJob).get(job_id)\n    if job:\n        print(f'Marking job {job_id} ({job.source.code}) as completed')\n        job.status = 'completed'\n        job.crawler_exit_code = 0\n        job.crawler_status = 'completed_partial'  # Indicates manual completion\n        job.finished_at = datetime.now(timezone.utc)\n\nsession.commit()\nprint('Jobs marked as completed')\n\"\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#6c2-run-indexing-manually","title":"6C.2 Run Indexing Manually","text":"<pre><code># Index each job\nha-backend index-job --id 6\nha-backend index-job --id 7\nha-backend index-job --id 8\n\n# Verify indexing\nfor job_id in 6 7 8; do\n  echo \"=== Job $job_id ===\"\n  ha-backend show-job --id $job_id | grep -E \"status|indexed_page\"\ndone\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#phase-7-post-restart-verification","title":"Phase 7: Post-Restart Verification","text":""},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#71-monitor-crawl-progress","title":"7.1 Monitor Crawl Progress","text":"<pre><code># Check every 30 minutes for the first 2 hours\nwatch -n 1800 './scripts/vps-crawl-status.sh --year 2026 | head -60'\n\n# Or manual checks\n./scripts/vps-crawl-status.sh --year 2026\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#72-verify-disk-space-trend","title":"7.2 Verify Disk Space Trend","text":"<pre><code># Monitor disk usage\nwatch -n 300 'df -h /srv/healtharchive/jobs &amp;&amp; echo \"---\" &amp;&amp; du -sh /srv/healtharchive/jobs/*/20260101*/.tmp* 2&gt;/dev/null | sort -h | tail -10'\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#73-check-for-recurring-issues","title":"7.3 Check for Recurring Issues","text":"<pre><code># Check auto-recovery frequency\ncat /srv/healtharchive/ops/watchdog/crawl-auto-recover.json | python3 -m json.tool | tail -30\n\n# Check for new Errno 107 errors\njournalctl -u healtharchive-worker --since \"1 hour ago\" | grep -i \"errno 107\" || echo \"No Errno 107 errors\"\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#phase-8-post-indexing-cleanup","title":"Phase 8: Post-Indexing Cleanup","text":"<p>ONLY after jobs are <code>indexed</code>:</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#81-safe-temp-cleanup","title":"8.1 Safe Temp Cleanup","text":"<pre><code># Verify jobs are indexed\nfor job_id in 6 7 8; do\n  ha-backend show-job --id $job_id | grep -E \"^Status:\"\ndone\n\n# Dry-run cleanup\nha-backend cleanup-job --id 6 --mode temp-nonwarc --dry-run\nha-backend cleanup-job --id 7 --mode temp-nonwarc --dry-run\nha-backend cleanup-job --id 8 --mode temp-nonwarc --dry-run\n\n# Apply cleanup (consolidates WARCs, rewrites snapshot paths, deletes .tmp*)\nha-backend cleanup-job --id 6 --mode temp-nonwarc\nha-backend cleanup-job --id 7 --mode temp-nonwarc\nha-backend cleanup-job --id 8 --mode temp-nonwarc\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#82-verify-replay-works","title":"8.2 Verify Replay Works","text":"<pre><code># Test replay URLs for a few snapshots\n./scripts/vps-replay-smoke-textfile.py --dry-run\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#rollback-procedures","title":"Rollback Procedures","text":""},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#if-crawl-immediately-fails-after-restart","title":"If Crawl Immediately Fails After Restart","text":"<pre><code># Stop worker\nsudo systemctl stop healtharchive-worker.service\n\n# Check logs\njournalctl -u healtharchive-worker --since \"10 minutes ago\" | tail -100\n\n# Recover stale jobs\nha-backend recover-stale-jobs --older-than-minutes 5 --apply\n\n# Investigate before restarting\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#if-warcs-are-accidentally-lost","title":"If WARCs Are Accidentally Lost","text":"<pre><code># Check if stable WARCs exist\nls -la /srv/healtharchive/jobs/*/20260101*/warcs/\n\n# If no stable WARCs and .tmp* deleted, recovery options:\n# 1. Restore from Storage Box cold tier (if tiered)\n# 2. Restore from backup (if available)\n# 3. Re-crawl from scratch (last resort)\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#data-quality-verification-checklist","title":"Data Quality Verification Checklist","text":"<p>After indexing completes, verify:</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#coverage-check","title":"Coverage Check","text":"<pre><code># Compare indexed pages to previous annual campaigns\nsource /etc/healtharchive/backend.env\ncd /opt/healtharchive-backend\nsource .venv/bin/activate\n\npython3 -c \"\nfrom ha_backend.db import get_session\nfrom ha_backend.models import Snapshot, ArchiveJob\nfrom sqlalchemy import func\n\nsession = next(get_session())\n\n# 2026 campaign stats\nfor job_id in [6, 7, 8]:\n    job = session.query(ArchiveJob).get(job_id)\n    count = session.query(Snapshot).filter(Snapshot.job_id == job_id).count()\n    print(f'{job.source.code} (2026): {count} snapshots, indexed_page_count={job.indexed_page_count}')\n\"\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#search-quality-spot-check","title":"Search Quality Spot Check","text":"<pre><code># Run golden queries\n./scripts/search-eval-capture.sh --out-dir /tmp/ha-search-eval-2026 --page-size 20\n\n# Check results\nls -la /tmp/ha-search-eval-2026/\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#api-health","title":"API Health","text":"<pre><code>curl -s https://api.healtharchive.ca/api/health | python3 -m json.tool\ncurl -s https://api.healtharchive.ca/api/stats | python3 -m json.tool\ncurl -s \"https://api.healtharchive.ca/api/search?q=vaccines&amp;source=hc\" | python3 -c \"import sys,json; d=json.load(sys.stdin); print(f'Results: {d.get(\\\"totalResults\\\", 0)}')\"\n</code></pre>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#timeline-estimates","title":"Timeline Estimates","text":"Phase Duration Notes Phase 1: Snapshot 15 min Document current state Phase 2: Stop activity 10 min Stop worker and containers Phase 3: Fix infra 20 min Mounts, permissions Phase 4: Assess/Consolidate 30 min WARC verification and consolidation Phase 5: Decision 10 min Choose restart strategy Phase 6: Execute restart 15 min Apply chosen strategy Phase 7: Monitor Ongoing Until crawls complete Phase 8: Cleanup 15 min After indexing <p>Total active time: ~2 hours Crawl completion: 2-7 days depending on strategy and source sizes</p>"},{"location":"operations/playbooks/crawl/2026-01-annual-campaign-controlled-restart/#references","title":"References","text":"<ul> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> <li><code>docs/operations/playbooks/crawl/crawl-stalls.md</code></li> <li><code>docs/operations/playbooks/core/incident-response.md</code></li> <li><code>docs/operations/playbooks/storage/warc-integrity-verification.md</code></li> <li><code>docs/operations/playbooks/crawl/cleanup-automation.md</code></li> <li><code>docs/tutorials/debug-crawl.md</code></li> <li><code>src/archive_tool/docs/documentation.md</code></li> </ul>"},{"location":"operations/playbooks/crawl/annual-campaign/","title":"Annual campaign playbook (operators)","text":"<p>Goal: keep the annual capture cycle predictable and operationally boring.</p> <p>Canonical references:</p> <ul> <li>Annual scope/seeds (source of truth): <code>../../annual-campaign.md</code></li> <li>Automation implementation plan: <code>../../automation-implementation-plan.md</code></li> <li>systemd units + enablement: <code>../../../deployment/systemd/README.md</code></li> </ul>"},{"location":"operations/playbooks/crawl/annual-campaign/#before-jan-01-utc-readiness","title":"Before Jan 01 UTC (readiness)","text":"<ul> <li>Review and update scope/seeds in <code>../../annual-campaign.md</code> (docs-only change).</li> <li>Ensure you have storage headroom and backups are healthy.</li> <li>Run the crawl preflight audit (recommended):</li> <li><code>YEAR=2026; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\"</code></li> <li>See: <code>crawl-preflight.md</code></li> <li>If the annual scheduler is enabled, dry-run it:</li> <li><code>sudo systemctl start healtharchive-schedule-annual-dry-run.service</code></li> <li><code>sudo journalctl -u healtharchive-schedule-annual-dry-run.service -n 200 --no-pager</code></li> <li>Enable the annual campaign sentinel (recommended; sends notification on failure):</li> <li><code>sudo systemctl enable --now healtharchive-annual-campaign-sentinel.timer</code></li> <li>Optional: configure Healthchecks ping URL at <code>/etc/healtharchive/healthchecks.env</code>:<ul> <li><code>HEALTHARCHIVE_HC_PING_ANNUAL_SENTINEL=https://hc-ping.com/UUID_HERE</code></li> <li>Note: this env file may also contain legacy <code>HC_*</code> variables (DB backup + disk check).</li> </ul> </li> </ul>"},{"location":"operations/playbooks/crawl/annual-campaign/#duringafter-the-campaign-high-level","title":"During/after the campaign (high level)","text":"<ul> <li>Use the automation plan (<code>../../automation-implementation-plan.md</code>) to decide what is enabled and what is manual.</li> <li>Prefer safe, idempotent entrypoints (systemd services/timers or the provided scripts).</li> <li>Annual jobs are scheduled with crawler monitoring enabled so stalls / error storms can trigger adaptive worker reduction.</li> </ul>"},{"location":"operations/playbooks/crawl/annual-campaign/#if-a-crawl-stalls-or-is-interrupted","title":"If a crawl stalls or is interrupted","text":"<ul> <li>If a crawl is stalled, check the job logs under its <code>output_dir</code> (look for <code>archive_*_attempt_*_*.combined.log</code>) and the worker journal:</li> <li><code>sudo journalctl -u healtharchive-worker -n 200 --no-pager</code></li> <li>If the VPS reboots (or the worker/service is killed) mid-crawl, a job can be left in <code>status=running</code>. Recover safely:</li> <li>Load env: <code>set -a; source /etc/healtharchive/backend.env; set +a</code></li> <li>Dry-run: <code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 180</code></li> <li>Apply: <code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 180 --apply</code></li> </ul>"},{"location":"operations/playbooks/crawl/annual-campaign/#manual-trigger-day-of","title":"Manual trigger (day-of)","text":"<p>If you want to run the annual sentinel immediately (safe; read-only except for metrics output):</p> <pre><code>sudo systemctl start healtharchive-annual-campaign-sentinel.service\nsudo journalctl -u healtharchive-annual-campaign-sentinel.service -n 200 --no-pager\n</code></pre>"},{"location":"operations/playbooks/crawl/annual-campaign/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>Annual scope is current in <code>../../annual-campaign.md</code>.</li> <li>If automation is enabled, the scheduler and follow-up tasks run as intended and are verifiable in logs/artifacts.</li> </ul>"},{"location":"operations/playbooks/crawl/cleanup-automation/","title":"Cleanup automation (safe temp cleanup)","text":"<p>Goal: remove <code>.tmp*</code> crawl directories from older indexed jobs without breaking replay.</p> <p>Canonical refs:</p> <ul> <li>cleanup command: <code>ha-backend cleanup-job --mode temp-nonwarc</code></li> <li>systemd unit templates: <code>../../../deployment/systemd/README.md</code></li> <li>replay retention note: <code>../../growth-constraints.md</code></li> </ul>"},{"location":"operations/playbooks/crawl/cleanup-automation/#what-this-does","title":"What this does","text":"<ul> <li>Picks indexed jobs older than a minimum age.</li> <li>Keeps the latest N per source.</li> <li>Runs safe cleanup (<code>temp-nonwarc</code>) to preserve WARCs.</li> <li>Emits node_exporter metrics:</li> <li><code>healtharchive_cleanup_applied_total</code></li> </ul>"},{"location":"operations/playbooks/crawl/cleanup-automation/#enablement-vps","title":"Enablement (VPS)","text":"<pre><code>sudo touch /etc/healtharchive/cleanup-automation-enabled\nsudo systemctl enable --now healtharchive-cleanup-automation.timer\n</code></pre>"},{"location":"operations/playbooks/crawl/cleanup-automation/#manual-dry-run","title":"Manual dry-run","text":"<p>Warning: starting <code>healtharchive-cleanup-automation.service</code> will apply cleanup (it is the automation entrypoint). Use the script directly for a dry-run preview.</p> <pre><code>sudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-cleanup-automation.py --config /opt/healtharchive-backend/ops/automation/cleanup-automation.toml --out-dir /tmp --out-file healtharchive_cleanup_dryrun.prom'\ncat /tmp/healtharchive_cleanup_dryrun.prom\n</code></pre>"},{"location":"operations/playbooks/crawl/cleanup-automation/#if-cleanup-fails","title":"If cleanup fails","text":"<ol> <li>Check the job output directory exists and is readable:    <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend show-job --id &lt;JOB_ID&gt;\n</code></pre></li> <li>Run the cleanup command manually:    <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend cleanup-job --id &lt;JOB_ID&gt; --mode temp-nonwarc --dry-run\n</code></pre></li> </ol>"},{"location":"operations/playbooks/crawl/cleanup-automation/#config","title":"Config","text":"<p>Edit <code>ops/automation/cleanup-automation.toml</code> to adjust age, caps, and retain count.</p>"},{"location":"operations/playbooks/crawl/crawl-auto-recover-drills/","title":"Crawl auto-recover drills (safe on production)","text":"<p>Goal: periodically prove that:</p> <ul> <li>the crawl auto-recover watchdog is installed and runnable, and</li> <li>the watchdog would take sensible actions for a stalled job,</li> </ul> <p>\u2026without actually stopping services or writing to the production watchdog state/metrics.</p>"},{"location":"operations/playbooks/crawl/crawl-auto-recover-drills/#0-safety-rules","title":"0) Safety rules","text":"<ul> <li>Never run the crawl auto-recover watchdog with <code>--apply</code> as part of a drill.</li> <li>For drills, always override:</li> <li><code>--state-file</code> (use a <code>/tmp/...</code> path)</li> <li><code>--lock-file</code> (use a <code>/tmp/...</code> path)</li> <li><code>--textfile-out-dir</code> (use <code>/tmp</code>)</li> <li><code>--textfile-out-file</code> (use a drill filename)</li> </ul> <p>The watchdog enforces this automatically when you use drill flags.</p>"},{"location":"operations/playbooks/crawl/crawl-auto-recover-drills/#1-pick-a-job-id-to-simulate","title":"1) Pick a job ID to simulate","text":"<p>Pick a real job ID from the database (it does not need to be stalled).</p> <p>For the \u201cguard window\u201d drill below, it helps if at least one other job is currently running and making progress.</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend list-jobs --status running --limit 10\n</code></pre> <p>Also pick a job ID that is not currently running (queued/retryable is fine):</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend list-jobs --limit 20\n</code></pre> <p>Pick one <code>job_id</code> from the output, for example <code>7</code>.</p>"},{"location":"operations/playbooks/crawl/crawl-auto-recover-drills/#2-drill-simulate-a-stalled-job-soft-recovery-path","title":"2) Drill: simulate a stalled job (soft recovery path)","text":"<p>This exercises the common \u201csafe\u201d path where another job is still making progress, so the watchdog would avoid worker restarts.</p> <p>Important: soft recovery is only allowed when the watchdog can confirm the stalled job has no active runner (i.e., it\u2019s a zombie <code>status=running</code> DB row). In drill mode we force that classification with:</p> <ul> <li><code>--simulate-stalled-job-runner none</code></li> </ul> <pre><code>cd /opt/healtharchive-backend\nsudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; \\\n  /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-crawl-auto-recover.py \\\n    --simulate-stalled-job-id 7 \\\n    --simulate-stalled-job-runner none \\\n    --state-file /tmp/healtharchive-crawl-auto-recover.drill.state.json \\\n    --lock-file /tmp/healtharchive-crawl-auto-recover.drill.lock \\\n    --textfile-out-dir /tmp \\\n    --textfile-out-file healtharchive_crawl_auto_recover.drill.prom'\n</code></pre> <p>Expected output includes:</p> <ul> <li><code>DRILL: simulate-stalled-job-id active</code></li> <li><code>Planned actions (dry-run):</code></li> <li><code>recover-stale-jobs ... --apply --source ...</code></li> </ul> <p>Confirm the drill metrics were written:</p> <pre><code>cat /tmp/healtharchive_crawl_auto_recover.drill.prom\n</code></pre>"},{"location":"operations/playbooks/crawl/crawl-auto-recover-drills/#3-drill-simulate-a-stalled-job-full-recovery-path","title":"3) Drill: simulate a stalled job (full recovery path)","text":"<p>This forces the watchdog to show the \u201cfull recovery\u201d plan by disabling the guard window.</p>"},{"location":"operations/playbooks/crawl/crawl-auto-recover-drills/#3a-full-recovery-job-is-running-under-the-worker","title":"3a) Full recovery (job is running under the worker)","text":"<pre><code>cd /opt/healtharchive-backend\nsudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; \\\n  /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-crawl-auto-recover.py \\\n    --skip-if-any-job-progress-within-seconds 0 \\\n    --simulate-stalled-job-id 7 \\\n    --simulate-stalled-job-runner worker \\\n    --state-file /tmp/healtharchive-crawl-auto-recover.full-drill.state.json \\\n    --lock-file /tmp/healtharchive-crawl-auto-recover.full-drill.lock \\\n    --textfile-out-dir /tmp \\\n    --textfile-out-file healtharchive_crawl_auto_recover.full-drill.prom'\n</code></pre> <p>Expected output includes:</p> <ul> <li><code>systemctl stop healtharchive-worker.service</code></li> <li><code>recover-stale-jobs ... --apply --source ...</code></li> <li><code>systemctl start healtharchive-worker.service</code></li> </ul>"},{"location":"operations/playbooks/crawl/crawl-auto-recover-drills/#3b-full-recovery-job-is-running-in-a-systemd-run-transient-unit","title":"3b) Full recovery (job is running in a <code>systemd-run</code> transient unit)","text":"<p>Use any realistic transient unit name (this is a drill-only override):</p> <pre><code>cd /opt/healtharchive-backend\nsudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; \\\n  /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-crawl-auto-recover.py \\\n    --skip-if-any-job-progress-within-seconds 0 \\\n    --simulate-stalled-job-id 7 \\\n    --simulate-stalled-job-runner systemd_unit \\\n    --simulate-stalled-job-runner-unit healtharchive-job7-phac-3way.service \\\n    --state-file /tmp/healtharchive-crawl-auto-recover.full-drill.state.json \\\n    --lock-file /tmp/healtharchive-crawl-auto-recover.full-drill.lock \\\n    --textfile-out-dir /tmp \\\n    --textfile-out-file healtharchive_crawl_auto_recover.full-drill.prom'\n</code></pre> <p>Expected output includes:</p> <ul> <li><code>systemctl stop healtharchive-job7-phac-3way.service</code></li> <li><code>recover-stale-jobs ... --apply --source ...</code></li> <li><code>systemctl start healtharchive-job7-phac-3way.service</code></li> </ul>"},{"location":"operations/playbooks/crawl/crawl-auto-recover-drills/#notes","title":"Notes","text":"<ul> <li>In all drill cases above, the watchdog remains in dry-run mode and does not actually stop services.</li> <li>If you omit the <code>--simulate-stalled-job-runner ...</code> override, the watchdog will attempt best-effort runner detection   (worker vs transient unit) from the live system.</li> </ul> <pre><code>cd /opt/healtharchive-backend\nsudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; \\\n  /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-crawl-auto-recover.py \\\n    --skip-if-any-job-progress-within-seconds 0 \\\n    --simulate-stalled-job-id 7 \\\n    --state-file /tmp/healtharchive-crawl-auto-recover.full-drill.state.json \\\n    --lock-file /tmp/healtharchive-crawl-auto-recover.full-drill.lock \\\n    --textfile-out-dir /tmp \\\n    --textfile-out-file healtharchive_crawl_auto_recover.full-drill.prom'\n</code></pre> <p>Expected output includes:</p> <ul> <li><code>Planned actions (dry-run):</code></li> <li><code>systemctl stop ...</code> (either <code>healtharchive-worker.service</code> or a <code>healtharchive-job&lt;id&gt;-*.service</code> transient unit)</li> <li><code>recover-stale-jobs ... --apply --source ...</code></li> <li><code>systemctl start ...</code> (matching the stop target above)</li> </ul>"},{"location":"operations/playbooks/crawl/crawl-auto-recover-drills/#4-drill-queue-fill-auto-start-safe-on-production","title":"4) Drill: queue fill / auto-start (safe on production)","text":"<p>Goal: prove the watchdog would auto-start a queued/retryable annual job when the annual campaign is underfilled (fewer than N running jobs), without actually starting anything.</p> <p>Safety rules:</p> <ul> <li>Do not pass <code>--apply</code>.</li> <li>Always override:</li> <li><code>--state-file</code>, <code>--lock-file</code>, <code>--textfile-out-dir</code>, <code>--textfile-out-file</code> (use <code>/tmp</code> paths)</li> </ul> <p>This drill does not require <code>--simulate-stalled-job-id</code> \u2014 it exercises the \u201cno stalled jobs\u201d path.</p>"},{"location":"operations/playbooks/crawl/crawl-auto-recover-drills/#steps","title":"Steps","text":"<p>1) Confirm how many jobs are currently <code>status=running</code>:</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend list-jobs --status running --limit 10\n</code></pre> <p>2) Run a dry-run with <code>--ensure-min-running-jobs</code> set above the current count (example uses <code>3</code>):</p> <pre><code>cd /opt/healtharchive-backend\nsudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; \\\n  /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-crawl-auto-recover.py \\\n    --ensure-min-running-jobs 3 \\\n    --state-file /tmp/healtharchive-crawl-auto-recover.start-drill.state.json \\\n    --lock-file /tmp/healtharchive-crawl-auto-recover.start-drill.lock \\\n    --textfile-out-dir /tmp \\\n    --textfile-out-file healtharchive_crawl_auto_recover.start-drill.prom'\n</code></pre> <p>Expected output includes:</p> <ul> <li><code>DRY-RUN: would auto-start annual job_id=...</code></li> <li><code>Planned actions (dry-run):</code></li> <li><code>systemd-run ... ha-backend run-db-job --id ...</code></li> </ul> <p>Notes:</p> <ul> <li>Queue fill only targets annual jobs for the selected campaign year.</li> <li>For legacy annual jobs missing <code>campaign_kind</code> / <code>campaign_year</code>, the watchdog infers annual jobs from the canonical <code>-YYYY0101</code> suffix (for example <code>phac-20260101</code>).   In <code>--apply</code> mode, it will also backfill missing campaign metadata before starting the job.</li> </ul> <p>Confirm the drill metrics were written:</p> <pre><code>cat /tmp/healtharchive_crawl_auto_recover.start-drill.prom\n</code></pre>"},{"location":"operations/playbooks/crawl/crawl-auto-recover-drills/#5-cleanup","title":"5) Cleanup","text":"<p>Drill artifacts are safe to delete:</p> <pre><code>rm -f /tmp/healtharchive-crawl-auto-recover*.state.json\nrm -f /tmp/healtharchive-crawl-auto-recover*.lock\nrm -f /tmp/healtharchive_crawl_auto_recover*.prom\n</code></pre>"},{"location":"operations/playbooks/crawl/crawl-preflight/","title":"Crawl preflight audit playbook (production VPS)","text":"<p>Goal: catch obvious blockers before a large crawl (especially the Jan 01 UTC annual campaign).</p> <p>Canonical references:</p> <ul> <li>Production runbook: <code>../../../deployment/production-single-vps.md</code></li> <li>Annual scope/seeds (source of truth): <code>../../annual-campaign.md</code></li> <li>Growth/storage policy: <code>../../growth-constraints.md</code></li> <li>Baseline drift (policy vs reality): <code>../../baseline-drift.md</code></li> <li>Automation posture: <code>../../automation-implementation-plan.md</code>, <code>../../../deployment/systemd/README.md</code></li> </ul>"},{"location":"operations/playbooks/crawl/crawl-preflight/#preconditions","title":"Preconditions","text":"<ul> <li>You are on the VPS.</li> <li>Backend repo is present (default): <code>/opt/healtharchive-backend</code></li> <li>Venv exists at: <code>/opt/healtharchive-backend/.venv</code></li> <li>Backend env file exists: <code>/etc/healtharchive/backend.env</code></li> </ul>"},{"location":"operations/playbooks/crawl/crawl-preflight/#if-you-need-to-temporarily-defer-the-annual-crawl","title":"If you need to temporarily defer the annual crawl","text":"<p>If you aren\u2019t ready to run the annual campaign on Jan 01 UTC, disable the systemd timer and remove the automation sentinel:</p> <pre><code>sudo systemctl disable --now healtharchive-schedule-annual.timer\nsudo rm -f /etc/healtharchive/automation-enabled\n</code></pre> <p>Verify:</p> <pre><code>systemctl is-enabled healtharchive-schedule-annual.timer\nsystemctl status healtharchive-schedule-annual.timer\nls -la /etc/healtharchive/automation-enabled\n</code></pre> <p>Notes:</p> <ul> <li>This prevents automatic annual job enqueueing; it does not cancel any   already-queued jobs.</li> <li>The safe validation unit <code>healtharchive-schedule-annual-dry-run.service</code> can   still be run manually.</li> </ul>"},{"location":"operations/playbooks/crawl/crawl-preflight/#procedure-recommended","title":"Procedure (recommended)","text":"<ol> <li>Choose the annual campaign year:</li> <li>If it\u2019s before Jan 01 (UTC), use the upcoming year (e.g., Dec 2025 \u2192 <code>2026</code>).</li> <li>(Recommended) Run a rehearsal with caps (generates active-load evidence):</li> <li><code>cd /opt/healtharchive-backend</code></li> <li><code>./scripts/vps-smoke-crawl-rehearsal.sh --apply --source cihr --page-limit 25 --depth 1</code></li> <li>Run the preflight audit:</li> <li><code>cd /opt/healtharchive-backend</code></li> <li><code>YEAR=2026; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\"</code></li> </ol> <p>This writes a timestamped report directory under <code>/srv/healtharchive/ops/preflight/</code>.</p>"},{"location":"operations/playbooks/crawl/crawl-preflight/#if-it-fails-common-fixes","title":"If it fails (common fixes)","text":"<ul> <li>Campaign storage forecast fails (even if you\u2019re below 80% today): the annual campaign is projected to exceed disk headroom or the 80% review threshold. Follow the report output to free space or expand disk before Jan 01 UTC.</li> <li>Campaign storage forecast fails but you are using tiered storage (Storage Box): run preflight with the campaign tier root so the forecast uses the correct filesystem, e.g. <code>YEAR=2026; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\" --campaign-archive-root /srv/healtharchive/storagebox/jobs</code>.</li> <li>Rehearsal evidence (active crawl headroom) fails: you don\u2019t have a recent <code>--apply</code> rehearsal (or it recorded low MemAvailable / high swap). Run <code>./scripts/vps-smoke-crawl-rehearsal.sh --apply ...</code> to generate evidence, or upgrade the VPS / reduce crawl concurrency.</li> <li>CPU/RAM headroom fails: the VPS is already under sustained load / memory pressure (or swap usage). Stop other heavy work (indexing, other crawls), then re-run preflight; if it persists, reduce crawl concurrency or upgrade the VPS.</li> <li>Time sync (NTP) fails: fix time sync before crawling (TLS, scheduling, and log correlation all assume correct UTC).</li> <li>Docker daemon access fails: Docker is installed but not usable by the current user (or the daemon is down). Fix <code>systemctl status docker</code>, user group membership, and re-run.</li> <li>DB connectivity / Alembic-at-head fails: DB is down or the schema is behind the code version; apply migrations (<code>alembic upgrade head</code>) and re-run.</li> <li>Seed reachability fails: the annual seed URLs aren\u2019t reachable from the VPS right now; fix DNS/network/firewall issues (or investigate upstream downtime) before Jan 01 UTC.</li> <li>Disk usage is high (&gt;= 80%): pause, free space or expand disk before crawling.</li> <li>Backups are missing/stale: fix backups before crawling (don\u2019t run long jobs without recoverability).</li> <li><code>/api/health</code> fails on loopback: fix API/DB/service health first (check <code>systemctl status</code> + <code>journalctl</code>).</li> <li>Annual scheduler dry-run errors:</li> <li>Missing <code>Source</code> rows \u2192 run <code>ha-backend seed-sources</code>.</li> <li>Duplicated annual jobs for the year \u2192 resolve duplicates before scheduling.</li> <li>Active jobs blocking scheduling \u2192 finish/index them (or decide not to run annual yet).</li> <li>Temp cleanup candidates: the report lists indexed jobs that still have <code>.tmp*</code> dirs; use <code>ha-backend cleanup-job --mode temp-nonwarc</code> (safe) to reclaim space.</li> <li>Baseline drift failures: reconcile production with <code>docs/operations/production-baseline-policy.toml</code>, then re-run drift checks.</li> <li>Ops automation posture fails: enable required timers and sentinels (at minimum baseline drift), e.g. <code>sudo systemctl enable --now healtharchive-baseline-drift-check.timer &amp;&amp; sudo touch /etc/healtharchive/baseline-drift-enabled</code>.</li> <li>Admin/metrics auth check fails: ensure a real <code>HEALTHARCHIVE_ADMIN_TOKEN</code> is set in production and routing is correct.</li> </ul>"},{"location":"operations/playbooks/crawl/crawl-preflight/#optional-deep-checks","title":"Optional deep checks","text":"<ul> <li>Run a small crawl rehearsal (capped crawl + indexing, isolated sandbox DB). This is the best way to validate headroom under active crawl load, not just idle host metrics:</li> <li><code>cd /opt/healtharchive-backend</code></li> <li>Dry-run: <code>./scripts/vps-smoke-crawl-rehearsal.sh --source cihr</code></li> <li>Apply: <code>./scripts/vps-smoke-crawl-rehearsal.sh --apply --source cihr --page-limit 25 --depth 1</code></li> <li> <p>Evidence artifacts: <code>.../98-resource-monitor.jsonl</code> and <code>.../98-resource-summary.json</code></p> </li> <li> <p>Validate the systemd wrapper (safe dry-run):</p> </li> <li><code>sudo systemctl start healtharchive-schedule-annual-dry-run.service</code></li> <li><code>sudo journalctl -u healtharchive-schedule-annual-dry-run.service -n 200 --no-pager</code></li> <li>Capture a redacted \u201cbaseline inventory\u201d snapshot:</li> <li><code>OUT_DIR=\"/srv/healtharchive/ops/preflight/$(date -u +%Y%m%dT%H%M%SZ)\"; ./scripts/capture-baseline-inventory.sh --env-file /etc/healtharchive/backend.env --out \"$OUT_DIR/baseline-inventory.txt\"</code></li> </ul>"},{"location":"operations/playbooks/crawl/crawl-preflight/#optional-cleanup-disk-hygiene","title":"Optional cleanup (disk hygiene)","text":"<p>If you ran multiple rehearsals or preflight runs, keep only the most recent few directories as evidence and reclaim space.</p> <p>Keep the latest 3 rehearsal runs (removes older ones):</p> <pre><code>ls -1dt /srv/healtharchive/ops/rehearsal/* | tail -n +4 | sudo xargs -r rm -rf --\n</code></pre> <p>Keep the latest 10 preflight reports (removes older ones):</p> <pre><code>ls -1dt /srv/healtharchive/ops/preflight/* | tail -n +11 | sudo xargs -r rm -rf --\n</code></pre>"},{"location":"operations/playbooks/crawl/crawl-preflight/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li><code>YEAR=2026; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\"</code> exits <code>0</code>.</li> <li>The report directory exists and is retained as operator evidence for that crawl run.</li> </ul>"},{"location":"operations/playbooks/crawl/crawl-preflight/#during-the-crawl-ongoing-monitoring","title":"During the crawl (ongoing monitoring)","text":"<p>Once a large crawl is running, use the read-only status snapshot script for a quick \u201call the basics\u201d check:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-crawl-status.sh --year 2026\n</code></pre>"},{"location":"operations/playbooks/crawl/crawl-stalls/","title":"Crawl stalls (monitoring + recovery)","text":"<p>Use this playbook when a crawl job is running but appears stalled (no progress for an extended period), or when you receive the <code>HealthArchiveCrawlStalled</code> alert.</p> <p>Quick triage (recommended first):</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-crawl-status.sh --year 2026\n</code></pre> <p>Notes:</p> <ul> <li>The status script is read-only (no restarts, no DB writes); it\u2019s safe mid-crawl.</li> <li>If the combined log is very large and you only want recent timeout signals, use:</li> <li><code>./scripts/vps-crawl-status.sh --year 2026 --recent-lines 20000</code></li> </ul>"},{"location":"operations/playbooks/crawl/crawl-stalls/#1-identify-the-stalled-job","title":"1) Identify the stalled job","text":"<p>On the VPS:</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend list-jobs --status running --limit 10\n</code></pre> <p>Then inspect the specific job:</p> <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend show-job --id JOB_ID\n</code></pre>"},{"location":"operations/playbooks/crawl/crawl-stalls/#2-confirm-no-progress","title":"2) Confirm \u201cno progress\u201d","text":"<p>Find the newest combined log for the job\u2019s output directory:</p> <pre><code>JOBDIR=\"/srv/healtharchive/jobs/SOURCE/YYYYMMDDTHHMMSSZ__name\"\nls -lt \"${JOBDIR}\"/archive_*.combined.log | head -n 5\nLOG=\"$(ls -t \"${JOBDIR}\"/archive_*.combined.log | head -n 1)\"\n</code></pre> <p>Check the most recent crawlStatus line(s):</p> <pre><code>rg -n '\"context\":\"crawlStatus\"' \"${LOG}\" | tail -n 5\n</code></pre> <p>If <code>crawled</code> is not increasing for a long time (often with repeated <code>Navigation timeout</code> warnings), treat it as stalled.</p>"},{"location":"operations/playbooks/crawl/crawl-stalls/#3-recovery-safe-by-default","title":"3) Recovery (safe-by-default)","text":"<p>If you confirm the crawl is stalled and you want to restart it, do:</p> <pre><code># Stop the worker (interrupts the current crawl process).\nsudo systemctl stop healtharchive-worker.service\n\n# Mark the running job retryable so the worker can pick it up again.\nset -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs \\\n  --older-than-minutes 5 \\\n  --require-no-progress-seconds 3600 \\\n  --apply \\\n  --source SOURCE \\\n  --limit 5\n\n# Start the worker again.\nsudo systemctl start healtharchive-worker.service\n</code></pre> <p>Then confirm the worker picked the job up again and crawlStatus is moving:</p> <pre><code>sudo systemctl status healtharchive-worker.service --no-pager\nsudo journalctl -u healtharchive-worker.service -n 50 --no-pager\n</code></pre>"},{"location":"operations/playbooks/crawl/crawl-stalls/#notes","title":"Notes","text":"<ul> <li><code>archive_tool</code> has built-in monitoring/adaptation; most stalls should self-heal, but this recovery is the \u201cbreak glass\u201d operator workflow.</li> <li>Optional: you can enable the <code>healtharchive-crawl-auto-recover.timer</code> watchdog (sentinel: <code>/etc/healtharchive/crawl-auto-recover-enabled</code>) once you\u2019re confident in the thresholds/caps.</li> <li>To periodically validate the watchdog logic safely on production, run the drills in:</li> <li><code>crawl-auto-recover-drills.md</code></li> <li>The watchdog is designed to avoid interrupting a healthy crawl; when another job is actively making progress, it may \u201csoft recover\u201d zombie <code>status=running</code> jobs by marking them <code>retryable</code> without restarting the worker.</li> <li>If enabled via systemd, the watchdog can also auto-start underfilled annual jobs (<code>--ensure-min-running-jobs</code>) to maintain concurrency.</li> <li>See: <code>docs/operations/thresholds-and-tuning.md</code> and the \u201cqueue fill / auto-start\u201d drills in <code>crawl-auto-recover-drills.md</code>.</li> <li>If the watchdog is enabled but prints <code>SKIP ... max recoveries reached</code>, you can still do the manual recovery above, or (carefully) run the watchdog script once with a higher cap:   <pre><code>sudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-crawl-auto-recover.py --apply --max-recoveries-per-job-per-day 4'\n</code></pre></li> <li>If stalls repeat for the same URL(s), consider narrowing scope rules or adjusting crawler timeouts in the source\u2019s job configuration.</li> </ul>"},{"location":"operations/playbooks/external/adoption-signals/","title":"Adoption signals playbook (quarterly)","text":"<p>Goal: record lightweight, public-safe \u201cis anyone using this?\u201d signals without storing private contact details.</p> <p>Canonical references:</p> <ul> <li>Template: <code>../../../_templates/adoption-signals-log-template.md</code></li> <li>Ops roadmap (remaining external work): <code>../../healtharchive-ops-roadmap.md</code></li> </ul>"},{"location":"operations/playbooks/external/adoption-signals/#procedure-high-level","title":"Procedure (high level)","text":"<ol> <li>Create a new dated entry using <code>../../../_templates/adoption-signals-log-template.md</code>.</li> <li>Store it on the VPS under:</li> <li><code>/srv/healtharchive/ops/adoption/</code></li> </ol> <p>Rules:</p> <ul> <li>Links + aggregate counts only.</li> <li>No private emails, names, or identifying details unless permission is explicit and documented elsewhere.</li> </ul>"},{"location":"operations/playbooks/external/adoption-signals/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>A dated adoption signals entry exists under <code>/srv/healtharchive/ops/adoption/</code>.</li> </ul>"},{"location":"operations/playbooks/external/outreach-and-verification/","title":"Outreach + verification playbook (ongoing)","text":"<p>Goal: run external outreach and verification work without storing private contact details in git.</p> <p>Canonical references:</p> <ul> <li>Outreach email templates: <code>../../outreach-templates.md</code></li> <li>Partner kit (links + screenshot checklist): <code>../../partner-kit.md</code></li> <li>Verification packet outline: <code>../../verification-packet.md</code></li> <li>Mentions log (public-safe, link-only): <code>../../mentions-log.md</code></li> <li>Data handling &amp; retention rules: <code>../../data-handling-retention.md</code></li> <li>Adoption signals (quarterly, VPS-only): <code>adoption-signals.md</code></li> </ul>"},{"location":"operations/playbooks/external/outreach-and-verification/#rules-hard","title":"Rules (hard)","text":"<ul> <li>Never store emails, phone numbers, names, or private notes in git.</li> <li>Public logs must be link-only and permission-aware:</li> <li>If permission to name is unclear, use \u201cPending\u201d and keep the name out of public copy.</li> </ul>"},{"location":"operations/playbooks/external/outreach-and-verification/#procedure","title":"Procedure","text":""},{"location":"operations/playbooks/external/outreach-and-verification/#1-create-a-private-tracker-operator-only-not-in-git","title":"1) Create a private tracker (operator-only; not in git)","text":"<p>Pick one:</p> <ul> <li>A password manager note (preferred).</li> <li>A local spreadsheet in a folder outside the repo (e.g. <code>~/HealthArchive-private/outreach.xlsx</code>).</li> <li>A private doc in your personal notes system.</li> </ul> <p>Suggested fields:</p> <ul> <li><code>date_first_contacted_utc</code></li> <li><code>name</code> / <code>role</code> / <code>org</code> (private)</li> <li><code>contact_channel</code> (private)</li> <li><code>why_them</code></li> <li><code>template_used</code> (A/B/C)</li> <li><code>status</code> (no response / declined / interested / accepted)</li> <li><code>followup_1_sent_utc</code>, <code>followup_2_sent_utc</code></li> <li><code>public_link</code> (only if it exists)</li> <li><code>permission_to_name</code> (yes/no/pending) + date confirmed (private)</li> </ul>"},{"location":"operations/playbooks/external/outreach-and-verification/#2-prepare-partner-ready-assets-public","title":"2) Prepare partner-ready assets (public)","text":"<ul> <li>Confirm these pages are accurate and up-to-date:</li> <li><code>https://www.healtharchive.ca/brief</code></li> <li><code>https://www.healtharchive.ca/cite</code></li> <li>(Optional) Capture screenshots using <code>../../partner-kit.md</code> so you can attach them.</li> </ul>"},{"location":"operations/playbooks/external/outreach-and-verification/#3-build-a-target-list-operator-only","title":"3) Build a target list (operator-only)","text":"<p>Start with a small batch (e.g., 10\u201320), split between:</p> <ul> <li>Distribution partners (libraries / digital scholarship resource pages).</li> <li>Research / teaching partners.</li> <li>Journalism / communication partners.</li> <li>Verifier candidates (librarian / researcher / editor).</li> </ul>"},{"location":"operations/playbooks/external/outreach-and-verification/#4-send-outreach-operator-only","title":"4) Send outreach (operator-only)","text":"<ul> <li>Use <code>../../outreach-templates.md</code> and customize only what\u2019s needed:</li> <li>recipient name</li> <li>why this is relevant to them</li> <li>the single best link to include (usually <code>/digest</code> or <code>/changes</code>)</li> <li>Follow-up cadence:</li> <li>follow-up #1 at ~1 week</li> <li>follow-up #2 at ~2 weeks (final)</li> </ul>"},{"location":"operations/playbooks/external/outreach-and-verification/#5-update-the-public-safe-mentions-log-git-when-appropriate","title":"5) Update the public-safe mentions log (git) when appropriate","text":"<p>Only when there is a public link (and/or explicit permission to name), add an entry to:</p> <ul> <li><code>../../mentions-log.md</code></li> </ul>"},{"location":"operations/playbooks/external/outreach-and-verification/#6-run-the-verifier-workflow-operator-only","title":"6) Run the verifier workflow (operator-only)","text":"<ul> <li>Send <code>../../verification-packet.md</code> to the verifier.</li> <li>Ask explicitly:</li> <li>permission to name them publicly (yes/no)</li> <li>preferred wording (if any)</li> <li>If they grant permission and there is a public link (or they agree to be listed), record it in:</li> <li><code>../../mentions-log.md</code></li> </ul>"},{"location":"operations/playbooks/external/outreach-and-verification/#7-quarterly-adoption-signals-vps-only-public-safe","title":"7) Quarterly adoption signals (VPS-only; public-safe)","text":"<p>Run the adoption signals playbook and store the entry on the VPS:</p> <ul> <li><code>adoption-signals.md</code></li> </ul>"},{"location":"operations/playbooks/external/outreach-and-verification/#what-done-means-phase-4","title":"What \u201cdone\u201d means (Phase 4)","text":"<ul> <li>Private tracker exists outside git.</li> <li>At least one outreach batch is sent (with follow-ups scheduled).</li> <li>Mentions log exists and is updated only with public links and permission-aware entries.</li> </ul>"},{"location":"operations/playbooks/observability/monitoring-and-alerting/","title":"Monitoring + alerting playbook (operators)","text":"<p>Goal: detect user-visible outages and silent automation failures with low noise.</p> <p>Canonical reference:</p> <ul> <li><code>../../monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"operations/playbooks/observability/monitoring-and-alerting/#external-uptime-monitors-required","title":"External uptime monitors (required)","text":"<p>Ensure monitors exist for:</p> <ul> <li><code>https://api.healtharchive.ca/api/health</code></li> <li><code>https://www.healtharchive.ca/archive</code></li> <li><code>https://replay.healtharchive.ca/</code> (only if you rely on replay)</li> </ul> <p>After changes, you can smoke-test from any machine with internet:</p> <ul> <li><code>healtharchive-backend/scripts/smoke-external-monitors.sh</code></li> </ul>"},{"location":"operations/playbooks/observability/monitoring-and-alerting/#timer-ran-monitoring-optional-recommended","title":"\u201cTimer ran\u201d monitoring (optional, recommended)","text":"<p>If you want alerts when systemd timers stop running:</p> <ol> <li>Create checks in your Healthchecks provider.</li> <li>Store ping URLs only on the VPS:</li> <li><code>/etc/healtharchive/healthchecks.env</code> (root-owned)</li> <li>This file may be shared across multiple automations; it is OK to keep both:<ul> <li>legacy <code>HC_*</code> variables (DB backup + disk check)</li> <li>newer <code>HEALTHARCHIVE_HC_PING_*</code> variables (systemd unit templates)</li> </ul> </li> <li>Keep the unit templates installed/updated on the VPS:</li> <li><code>sudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker</code></li> </ol>"},{"location":"operations/playbooks/observability/monitoring-and-alerting/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>External monitors are green and alert routing is confirmed.</li> <li>If enabled, Healthchecks pings are configured without committing URLs to git.</li> <li>If you use internal Prometheus-based alerts, Alertmanager is configured and test alerts deliver:</li> <li>observability-guide.md#6-configure-alerting</li> <li>If you use WARC tiering to a Storage Box, tiering metrics are enabled so you get high-signal alerts:</li> <li><code>sudo systemctl enable --now healtharchive-tiering-metrics.timer</code></li> </ul>"},{"location":"operations/playbooks/observability/observability-guide/","title":"Observability Setup and Maintenance Guide","text":"<p>Scope: Complete setup of the private observability stack (Prometheus, Grafana, Alertmanager) on the production VPS.</p> <p>Canonical boundary doc (read first): observability-and-private-stats.md</p>"},{"location":"operations/playbooks/observability/observability-guide/#overview","title":"Overview","text":"<p>This guide covers the full observability stack installation in order:</p> <ol> <li>Bootstrap - Filesystem + secrets layout</li> <li>Install Exporters - node_exporter + postgres_exporter</li> <li>Configure Prometheus - Metrics collection</li> <li>Configure Grafana - Dashboards + tailnet access</li> <li>Provision Dashboards - Automated dashboard deployment</li> <li>Configure Alerting - Alertmanager + rules</li> <li>Ongoing Maintenance - Upgrades, rotation, troubleshooting</li> </ol> <p>Architecture: All services bind to loopback only. Access via Tailscale SSH port-forward.</p>"},{"location":"operations/playbooks/observability/observability-guide/#1-bootstrap-prerequisites","title":"1. Bootstrap (Prerequisites)","text":"<p>Goal: Prepare filesystem + secrets layout without installing services.</p>"},{"location":"operations/playbooks/observability/observability-guide/#preconditions","title":"Preconditions","text":"<ul> <li>On the production VPS with <code>sudo</code> access</li> <li><code>/srv/healtharchive/</code> exists</li> <li>Ops group exists (usually <code>healtharchive</code>)</li> </ul>"},{"location":"operations/playbooks/observability/observability-guide/#procedure","title":"Procedure","text":"<pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-bootstrap-observability-scaffold.sh\n</code></pre> <p>Populate secret files (do NOT store under <code>/srv/healtharchive/ops/</code>):</p> <pre><code>sudoedit /etc/healtharchive/observability/prometheus_backend_admin_token\nsudoedit /etc/healtharchive/observability/grafana_admin_password\nsudoedit /etc/healtharchive/observability/postgres_grafana_password\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#verify","title":"Verify","text":"<pre><code>stat -c '%U:%G %a %n' /srv/healtharchive/ops/observability /srv/healtharchive/ops/observability/*\nstat -c '%U:%G %a %n' /etc/healtharchive/observability/*\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#rollback","title":"Rollback","text":"<pre><code>sudo rm -rf /srv/healtharchive/ops/observability\nsudo rm -rf /etc/healtharchive/observability\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#2-install-exporters","title":"2. Install Exporters","text":"<p>Goal: Install node_exporter (host metrics) and postgres_exporter (DB health), loopback-only.</p>"},{"location":"operations/playbooks/observability/observability-guide/#preconditions_1","title":"Preconditions","text":"<ul> <li>Bootstrap complete (directories exist)</li> <li>Postgres running locally</li> </ul>"},{"location":"operations/playbooks/observability/observability-guide/#procedure_1","title":"Procedure","text":"<pre><code>cd /opt/healtharchive-backend\n./scripts/vps-install-observability-exporters.sh          # Dry-run\nsudo ./scripts/vps-install-observability-exporters.sh --apply\n</code></pre> <p>This installs packages, creates the <code>postgres_exporter</code> DB role with <code>pg_monitor</code>, and forces loopback binding (<code>127.0.0.1:9100</code>, <code>127.0.0.1:9187</code>).</p>"},{"location":"operations/playbooks/observability/observability-guide/#verify_1","title":"Verify","text":"<pre><code>curl -s http://127.0.0.1:9100/metrics | head\ncurl -s http://127.0.0.1:9187/metrics | head\nss -lntp | grep -E ':9100|:9187'  # Expect 127.0.0.1 only\nsystemctl --no-pager status prometheus-node-exporter prometheus-postgres-exporter\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#rollback_1","title":"Rollback","text":"<pre><code>sudo systemctl disable --now prometheus-node-exporter prometheus-postgres-exporter || true\nsudo rm -rf /etc/systemd/system/prometheus-node-exporter.service.d \\\n            /etc/systemd/system/prometheus-postgres-exporter.service.d\nsudo systemctl daemon-reload\nsudo apt-get remove -y prometheus-node-exporter prometheus-postgres-exporter\nsudo rm -f /etc/healtharchive/observability/postgres_exporter.env \\\n           /etc/healtharchive/observability/postgres_exporter_password\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#3-configure-prometheus","title":"3. Configure Prometheus","text":"<p>Goal: Install Prometheus and configure scraping of HealthArchive metrics.</p>"},{"location":"operations/playbooks/observability/observability-guide/#preconditions_2","title":"Preconditions","text":"<ul> <li>Exporters installed and loopback-only</li> <li><code>/etc/healtharchive/observability/prometheus_backend_admin_token</code> set to backend admin token</li> <li>Backend API reachable: <code>curl -s http://127.0.0.1:8001/api/health</code></li> </ul>"},{"location":"operations/playbooks/observability/observability-guide/#procedure_2","title":"Procedure","text":"<pre><code>cd /opt/healtharchive-backend\n./scripts/vps-install-observability-prometheus.sh          # Dry-run\nsudo ./scripts/vps-install-observability-prometheus.sh --apply\n</code></pre> <p>This installs Prometheus, writes config to <code>/etc/prometheus/prometheus.yml</code>, forces loopback binding (<code>127.0.0.1:9090</code>), and caps retention.</p>"},{"location":"operations/playbooks/observability/observability-guide/#verify_2","title":"Verify","text":"<pre><code>curl -s http://127.0.0.1:9090/-/ready\nss -lntp | grep -E ':9090\\b'  # Expect 127.0.0.1 only\ncurl -s http://127.0.0.1:9090/api/v1/targets | head\ncurl -s \"http://127.0.0.1:9090/api/v1/query?query=up%7Bjob%3D%22healtharchive_backend%22%7D\" | head\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#rollback_2","title":"Rollback","text":"<pre><code>sudo systemctl disable --now prometheus.service\nsudo rm -rf /etc/systemd/system/prometheus.service.d\nsudo systemctl daemon-reload\nsudo apt-get remove -y prometheus  # Optional\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#4-configure-grafana","title":"4. Configure Grafana","text":"<p>Goal: Install Grafana as operator-only dashboard, reachable via tailnet.</p>"},{"location":"operations/playbooks/observability/observability-guide/#preconditions_3","title":"Preconditions","text":"<ul> <li>Prometheus installed on <code>127.0.0.1:9090</code></li> <li>Tailscale connected to tailnet</li> <li>Secrets set: <code>grafana_admin_password</code>, <code>postgres_grafana_password</code></li> </ul>"},{"location":"operations/playbooks/observability/observability-guide/#procedure_3","title":"Procedure","text":"<pre><code>cd /opt/healtharchive-backend\n./scripts/vps-install-observability-grafana.sh          # Dry-run\nsudo ./scripts/vps-install-observability-grafana.sh --apply\n</code></pre> <p>This binds Grafana to <code>127.0.0.1:3000</code>, disables anonymous access, resets admin password, and creates the <code>grafana_readonly</code> Postgres role.</p>"},{"location":"operations/playbooks/observability/observability-guide/#access-options","title":"Access Options","text":"<p>Preferred - SSH port-forward (more private):</p> <pre><code>ssh -L 3000:127.0.0.1:3000 haadmin@&lt;vps-tailscale-ip&gt;\n# Then open http://127.0.0.1:3000\n</code></pre> <p>Optional - Tailscale Serve (requires HTTPS certs enabled):</p> <pre><code>./scripts/vps-enable-tailscale-serve-grafana.sh          # Dry-run\nsudo ./scripts/vps-enable-tailscale-serve-grafana.sh --apply\nsudo tailscale serve status  # Get HTTPS URL\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#configure-data-sources-grafana-ui","title":"Configure Data Sources (Grafana UI)","text":"<ol> <li>Prometheus: URL <code>http://127.0.0.1:9090</code></li> <li>Postgres: Host <code>127.0.0.1:5432</code>, DB <code>healtharchive</code>, User <code>grafana_readonly</code>, TLS disabled</li> </ol>"},{"location":"operations/playbooks/observability/observability-guide/#verify_3","title":"Verify","text":"<pre><code>ss -lntp | grep -E ':3000\\b'  # Expect 127.0.0.1 only\n# Test data sources in Grafana UI\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#rollback_3","title":"Rollback","text":"<pre><code>sudo tailscale serve reset  # If using Serve\nsudo systemctl disable --now grafana-server.service\nsudo rm -rf /etc/systemd/system/grafana-server.service.d\nsudo systemctl daemon-reload\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#5-provision-dashboards","title":"5. Provision Dashboards","text":"<p>Goal: Install ops and usage dashboards reproducibly.</p>"},{"location":"operations/playbooks/observability/observability-guide/#preconditions_4","title":"Preconditions","text":"<ul> <li>Prometheus and Grafana running</li> <li>Data sources configured in Grafana UI:</li> <li>Prometheus: named <code>prometheus</code></li> <li>Postgres: named <code>grafana-postgresql-datasource</code></li> </ul>"},{"location":"operations/playbooks/observability/observability-guide/#procedure_4","title":"Procedure","text":"<pre><code>cd /opt/healtharchive-backend\ngit pull\n./scripts/vps-install-observability-dashboards.sh          # Dry-run\nsudo ./scripts/vps-install-observability-dashboards.sh --apply\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#verify_4","title":"Verify","text":"<p>In Grafana, find the <code>HealthArchive</code> folder with these dashboards:</p> <ul> <li>HealthArchive - Ops Overview</li> <li>HealthArchive - Ops Console (Read-only)</li> <li>HealthArchive - Pipeline Health</li> <li>HealthArchive - Search Performance</li> <li>HealthArchive - Usage (Private, Aggregate)</li> <li>HealthArchive - Impact Summary (Private, Aggregate)</li> </ul>"},{"location":"operations/playbooks/observability/observability-guide/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Permission errors: Add Grafana to ops group: <code>sudo usermod -aG healtharchive grafana &amp;&amp; sudo systemctl restart grafana-server</code></li> <li>\"Data source not found\": Rename data sources to match expected names or edit dashboard JSON</li> </ul>"},{"location":"operations/playbooks/observability/observability-guide/#rollback_4","title":"Rollback","text":"<pre><code>sudo rm -f /etc/grafana/provisioning/dashboards/healtharchive.yaml\nsudo rm -rf /srv/healtharchive/ops/observability/dashboards/healtharchive\nsudo systemctl restart grafana-server\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#6-configure-alerting","title":"6. Configure Alerting","text":"<p>Goal: Get notified about real outages without pager fatigue.</p>"},{"location":"operations/playbooks/observability/observability-guide/#preconditions_5","title":"Preconditions","text":"<ul> <li>Prometheus running</li> <li>Node exporter installed (for disk metrics)</li> <li>If using WARC tiering: <code>sudo systemctl enable --now healtharchive-tiering-metrics.timer</code></li> </ul>"},{"location":"operations/playbooks/observability/observability-guide/#choose-operator-channel","title":"Choose Operator Channel","text":"<p>Create a webhook URL (Discord, Slack, or any HTTPS endpoint accepting Alertmanager JSON):</p> <pre><code>sudoedit /etc/healtharchive/observability/alertmanager_webhook_url\n</code></pre> <p>For Pushover:</p> <pre><code>sudoedit /etc/healtharchive/observability/pushover_app_token\nsudoedit /etc/healtharchive/observability/pushover_user_key\nsudo ./scripts/vps-install-observability-pushover-relay.sh --apply\n# Set webhook URL to: http://127.0.0.1:9911/alertmanager\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#procedure_5","title":"Procedure","text":"<pre><code>cd /opt/healtharchive-backend\ngit pull\n./scripts/vps-install-observability-alerting.sh          # Dry-run\nsudo ./scripts/vps-install-observability-alerting.sh --apply\n# Optional: --mountpoint / (if storage not on /)\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#alert-rules-high-signal-set","title":"Alert Rules (High-Signal Set)","text":"<ul> <li>Backend scrape down (&gt;5m)</li> <li>Disk usage &gt;80% warning, &gt;90% critical</li> <li>Sustained <code>/api/search</code> errors (traffic-gated)</li> <li>Job failures increased</li> <li>Storage Box mount down (if tiering enabled)</li> <li>WARC tiering bind-mount failed</li> <li>Tiering metrics stale (&gt;2 hours)</li> <li>Tiering hot path unreadable</li> <li>Annual campaign sentinel failed (Jan 01 UTC)</li> </ul>"},{"location":"operations/playbooks/observability/observability-guide/#verify_5","title":"Verify","text":"<pre><code>curl -s http://127.0.0.1:9093/-/ready\ncurl -s http://127.0.0.1:9090/api/v1/rules | head\nss -lntp | grep -E ':9093\\b|:9090\\b'\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#test-delivery","title":"Test Delivery","text":"<pre><code>amtool alert add HealthArchiveTestAlert severity=warning service=healtharchive\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#rollback_5","title":"Rollback","text":"<pre><code>sudo systemctl disable --now prometheus-alertmanager.service || \\\n  sudo systemctl disable --now alertmanager.service\nsudo rm -f /etc/prometheus/rules/healtharchive-alerts.yml\nsudo systemctl restart prometheus.service\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#7-ongoing-maintenance","title":"7. Ongoing Maintenance","text":""},{"location":"operations/playbooks/observability/observability-guide/#quick-verify-recommended","title":"Quick Verify (Recommended)","text":"<p>On VPS:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-verify-observability.sh\n</code></pre> <p>From laptop (tailnet SSH tunnel):</p> <pre><code>ssh -N \\\n  -L 3000:127.0.0.1:3000 \\\n  -L 9090:127.0.0.1:9090 \\\n  -L 8002:127.0.0.1:8002 \\\n  haadmin@&lt;vps-tailscale-ip&gt;\n</code></pre> <p>Then open Grafana (<code>http://127.0.0.1:3000/</code>) and check:</p> <ul> <li>HealthArchive - Ops Overview</li> <li>HealthArchive - Pipeline Health</li> <li>HealthArchive - Usage (Private, Aggregate)</li> </ul>"},{"location":"operations/playbooks/observability/observability-guide/#quarterly-upgrade-cadence","title":"Quarterly Upgrade Cadence","text":"<pre><code>sudo apt-get update &amp;&amp; sudo apt-get -y upgrade\nsudo systemctl restart prometheus prometheus-alertmanager \\\n  prometheus-node-exporter prometheus-postgres-exporter \\\n  grafana-server healtharchive-pushover-relay healtharchive-admin-proxy\n./scripts/vps-verify-observability.sh\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#dashboard-updates","title":"Dashboard Updates","text":"<pre><code>cd /opt/healtharchive-backend\ngit pull\nsudo ./scripts/vps-install-observability-dashboards.sh --apply\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#credential-rotation","title":"Credential Rotation","text":"<p>Backend admin token:</p> <pre><code># Update /etc/healtharchive/backend.env and /etc/healtharchive/observability/prometheus_backend_admin_token\nsudo systemctl restart healtharchive-api prometheus healtharchive-admin-proxy\n</code></pre> <p>Grafana admin password:</p> <pre><code># Update /etc/healtharchive/observability/grafana_admin_password\nsudo ./scripts/vps-install-observability-grafana.sh --apply --skip-apt --skip-db-role\n</code></pre> <p>Grafana Postgres password:</p> <pre><code># Update /etc/healtharchive/observability/postgres_grafana_password\nsudo ./scripts/vps-install-observability-grafana.sh --apply --skip-apt\n# Update data source in Grafana UI if needed\n</code></pre> <p>Alert webhook URL:</p> <pre><code># Update /etc/healtharchive/observability/alertmanager_webhook_url\nsudo ./scripts/vps-install-observability-alerting.sh --apply\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#prometheus-retention-tuning","title":"Prometheus Retention Tuning","text":"<pre><code>sudo ./scripts/vps-install-observability-prometheus.sh --apply --skip-apt \\\n  --retention-time 15d --retention-size 1GB\ncurl -s http://127.0.0.1:9090/-/ready\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#troubleshooting_1","title":"Troubleshooting","text":"<pre><code># Service status\nsystemctl status grafana-server prometheus prometheus-alertmanager --no-pager -l\n\n# Verify loopback-only binding\nss -lntp | grep -E ':3000|:8002|:9090|:9093|:9100|:9187|:9911'\n\n# Check Prometheus targets\ncurl -s http://127.0.0.1:9090/api/v1/targets | head\n</code></pre>"},{"location":"operations/playbooks/observability/observability-guide/#see-also","title":"See Also","text":"<ul> <li>monitoring-and-alerting.md - External monitors and Healthchecks setup</li> <li>observability-and-private-stats.md - Public/private boundary contract</li> </ul>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/","title":"Storage Box / <code>sshfs</code> recovery drills (safe on production)","text":"<p>Goal: periodically prove that:</p> <ul> <li>the watchdog logic would take the right recovery actions, without actually touching mounts, and</li> <li>the alert pipeline (Prometheus \u2192 Alertmanager) is wired correctly, without paging operators.</li> </ul> <p>These drills are designed to be safe on production even mid-crawl.</p> <p>Canonical background:</p> <ul> <li>Roadmap context: <code>../../../planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> <li>Real incident recovery procedure: <code>storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#0-safety-rules","title":"0) Safety rules","text":"<ul> <li>Never run recovery automation with <code>--apply</code> as part of a drill.</li> <li>For drills, always use:</li> <li>a temporary <code>--state-file</code> and <code>--lock-file</code> under <code>/tmp</code></li> <li>a temporary <code>--textfile-out-dir</code> under <code>/tmp</code>   so you don\u2019t affect production watchdog state or Prometheus metrics.</li> </ul>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#1-drill-watchdog-planned-actions-dry-run-simulation","title":"1) Drill: watchdog planned actions (dry-run simulation)","text":"<p>This validates the Phase 2 watchdog logic without breaking mounts.</p> <p>1) Pick a real \u201chot path\u201d to simulate as stale.</p> <p>Good candidates:</p> <ul> <li>an annual job output dir: <code>/srv/healtharchive/jobs/&lt;source&gt;/&lt;job_dir&gt;</code></li> <li>an imports hot path from tiering: <code>/srv/healtharchive/jobs/imports/...</code></li> </ul> <p>2) Run the watchdog in dry-run simulation mode (do not use <code>--apply</code>):</p> <pre><code>cd /opt/healtharchive-backend\nsudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; \\\n  /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-storage-hotpath-auto-recover.py \\\n    --confirm-runs 1 \\\n    --min-failure-age-seconds 0 \\\n    --state-file /tmp/healtharchive-storage-hotpath-drill.state.json \\\n    --lock-file /tmp/healtharchive-storage-hotpath-drill.lock \\\n    --textfile-out-dir /tmp \\\n    --textfile-out-file healtharchive_storage_hotpath_auto_recover.drill.prom \\\n    --simulate-broken-path /srv/healtharchive/jobs/hc/&lt;JOB_DIR&gt;'\n</code></pre> <p>3) Confirm output includes:</p> <ul> <li><code>DRILL: simulate-broken-path active</code></li> <li><code>Planned actions (dry-run):</code></li> <li>a sensible sequence (stop worker \u2192 unmount stale mountpoints \u2192 re-apply tiering \u2192 recover stale jobs \u2192 start worker)</li> </ul> <p>If this looks wrong, fix the watchdog logic before enabling the production timer.</p>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#15-drill-hot-path-staleness-evidence-correlation-phase-2-investigation-helper","title":"1.5) Drill: hot-path staleness evidence + correlation (Phase 2 investigation helper)","text":"<p>This drill is also safe on production (read-only bundles + optional dry-run simulation).</p> <p>It captures a pre and post evidence bundle and diffs them. It also appends a single TSV line you can use later for correlation across multiple drills/incidents.</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-hotpath-staleness-drill.sh \\\n  --simulate-broken-path /srv/healtharchive/jobs/hc/&lt;JOB_DIR&gt; \\\n  --note \"phase2 drill (dry-run)\"\n</code></pre> <p>Artifacts:</p> <ul> <li>Evidence bundles:</li> <li><code>/srv/healtharchive/ops/observability/hotpath-staleness/hotpath-staleness-&lt;ts&gt;-drill-pre/</code></li> <li><code>/srv/healtharchive/ops/observability/hotpath-staleness/hotpath-staleness-&lt;ts&gt;-drill-post/</code></li> <li>Correlation log:</li> <li><code>/srv/healtharchive/ops/observability/hotpath-staleness/investigation-log.tsv</code></li> </ul>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#2-drill-persistent-failed-apply-alert-condition-safe-no-paging","title":"2) Drill: persistent failed-apply alert condition (safe, no paging)","text":"<p>Goal: validate the Phase 2 alert condition logic for <code>HealthArchiveStorageHotpathApplyFailedPersistent</code> without writing anything to the live node_exporter collector path and without triggering notifications.</p> <p>1) Create a synthetic metrics file under <code>/tmp</code> (not the collector directory):</p> <pre><code>cat &gt;/tmp/healtharchive_storage_hotpath_auto_recover.alertcheck.prom &lt;&lt;'EOF'\nhealtharchive_storage_hotpath_auto_recover_enabled 1\nhealtharchive_storage_hotpath_auto_recover_apply_total 3\nhealtharchive_storage_hotpath_auto_recover_last_apply_ok 0\nhealtharchive_storage_hotpath_auto_recover_last_apply_timestamp_seconds 0\nEOF\n</code></pre> <p>2) Evaluate the alert predicate locally (safe/offline):</p> <pre><code>python3 - &lt;&lt;'PY'\nimport time\nfrom pathlib import Path\n\nmetrics = {}\nfor line in Path(\"/tmp/healtharchive_storage_hotpath_auto_recover.alertcheck.prom\").read_text().splitlines():\n    line = line.strip()\n    if not line or line.startswith(\"#\"):\n        continue\n    k, v = line.split(None, 1)\n    metrics[k] = float(v)\n\nok = (\n    metrics.get(\"healtharchive_storage_hotpath_auto_recover_enabled\", 0) == 1\n    and metrics.get(\"healtharchive_storage_hotpath_auto_recover_apply_total\", 0) &gt; 0\n    and metrics.get(\"healtharchive_storage_hotpath_auto_recover_last_apply_ok\", 1) == 0\n    and (time.time() - metrics.get(\"healtharchive_storage_hotpath_auto_recover_last_apply_timestamp_seconds\", time.time())) &gt; 86400\n)\nprint(\"ALERT_CONDITION_TRUE\" if ok else \"ALERT_CONDITION_FALSE\")\nPY\n</code></pre> <p>Expected output: <code>ALERT_CONDITION_TRUE</code>.</p> <p>3) Clean up:</p> <pre><code>rm -f /tmp/healtharchive_storage_hotpath_auto_recover.alertcheck.prom\n</code></pre> <p>Optional syntax check (safe):</p> <pre><code>promtool check rules /opt/healtharchive-backend/ops/observability/alerting/healtharchive-alerts.yml\n</code></pre>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#3-drill-alert-pipeline-no-paging","title":"3) Drill: alert pipeline (no paging)","text":"<p>This validates Prometheus rule loading + Alertmanager ingestion without sending notifications.</p> <p>Precondition:</p> <ul> <li>Alertmanager routes <code>severity=\"drill\"</code> to a null receiver. This is handled by the repo installer:</li> <li><code>scripts/vps-install-observability-alerting.sh</code></li> </ul>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#31-trigger-the-drill-alert-metric-auto-cleanup","title":"3.1 Trigger the drill alert metric (auto-cleanup)","text":"<pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-alert-pipeline-drill.sh --apply --duration-seconds 600\n</code></pre>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#32-confirm-prometheus-sees-the-alert","title":"3.2 Confirm Prometheus sees the alert","text":"<pre><code>curl -s http://127.0.0.1:9090/api/v1/alerts | rg 'HealthArchiveAlertPipelineDrill' || true\n</code></pre>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#33-confirm-alertmanager-received-the-alert-but-does-not-notify","title":"3.3 Confirm Alertmanager received the alert (but does not notify)","text":"<pre><code>curl -s http://127.0.0.1:9093/api/v2/alerts | rg 'HealthArchiveAlertPipelineDrill' || true\n</code></pre> <p>After ~10 minutes, the script removes the metric file and the alert should resolve.</p> <p>If you ever need to clean up manually:</p> <pre><code>sudo rm -f /var/lib/node_exporter/textfile_collector/healtharchive_alert_pipeline_drill.prom\n</code></pre>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#4-full-recovery-drill-staging-or-scheduled-maintenance-only","title":"4) Full recovery drill (staging or scheduled maintenance only)","text":"<p>Only do this on:</p> <ul> <li>a staging VPS (preferred), or</li> <li>a production maintenance window where crawl interruption is acceptable.</li> </ul> <p>High-level steps:</p> <p>1) Ensure you can tolerate crawl interruption (stop the worker first). 2) Intentionally create a stale mount condition (Errno 107) on a dedicated test hot path. 3) Confirm:    - alerts fire (<code>HealthArchiveTieringHotPathUnreadable</code> / <code>HealthArchiveStorageBoxMountDown</code>)    - watchdog recovers (tiering re-apply, stale job recovery)    - worker resumes and jobs make progress 4) Run post-incident integrity checks:    - <code>ha-backend verify-warcs --job-id &lt;ID&gt; --level 1 --since-minutes &lt;window&gt;</code>    - follow <code>warc-integrity-verification.md</code> if anything fails</p>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#5-phase-4-rollout-and-burn-in-evidence-capture","title":"5) Phase 4 rollout and burn-in evidence capture","text":"<p>Use this during the first week after shipping watchdog/alert updates.</p>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#51-daily-snapshot-safe","title":"5.1 Daily snapshot (safe)","text":"<pre><code>cd /opt/healtharchive-backend\npython3 scripts/vps-storage-watchdog-burnin-report.py --json &gt; /tmp/storage-watchdog-burnin-$(date -u +%Y%m%d).json\ncat /tmp/storage-watchdog-burnin-$(date -u +%Y%m%d).json\n</code></pre> <p>Optional (recommended): enable the daily snapshot timer so you don\u2019t rely on a human remembering.</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-bootstrap-ops-dirs.sh\nsudo install -m 0644 -o root -g root /dev/null /etc/healtharchive/storage-watchdog-burnin-enabled\nsudo systemctl enable --now healtharchive-storage-watchdog-burnin-snapshot.timer\n</code></pre> <p>Artifacts are written under:</p> <ul> <li><code>/srv/healtharchive/ops/burnin/storage-watchdog/latest.json</code></li> <li><code>/srv/healtharchive/ops/burnin/storage-watchdog/storage-watchdog-burnin-YYYYMMDD.json</code></li> </ul> <p>Expected:</p> <ul> <li><code>status</code> is usually <code>ok</code>.</li> <li><code>status=warn</code> means stale targets are currently detected (<code>detectedTargetsNow=true</code>) and should be triaged.</li> <li><code>status=fail</code> means persistent failed-apply or metrics writer failure and requires immediate triage.</li> </ul>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-drills/#52-end-of-week-clean-check-gate","title":"5.2 End-of-week clean check gate","text":"<pre><code>cd /opt/healtharchive-backend\npython3 scripts/vps-storage-watchdog-burnin-report.py --window-hours 168 --require-clean\n</code></pre> <p>Expected exit code:</p> <ul> <li><code>0</code>: <code>status=ok</code> (no persistent failed-apply signal and no currently detected targets).</li> <li><code>1</code>: <code>status=warn</code> or <code>status=fail</code>; investigate before escalation.</li> </ul>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/","title":"Storage Box / <code>sshfs</code> stale mount recovery (Errno 107)","text":"<p>Use this playbook when HealthArchive crawls/indexing/metrics start failing with:</p> <ul> <li><code>OSError: [Errno 107] Transport endpoint is not connected</code></li> </ul> <p>This typically indicates a stale FUSE mount (often <code>sshfs</code>) where the mountpoint still exists, but basic filesystem operations (<code>stat</code>, <code>ls</code>, <code>is_dir</code>) fail.</p> <p>Operational note:</p> <ul> <li>The worker skips jobs that recently failed with <code>crawler_status=infra_error</code> for a short cooldown window to prevent retry storms. This reduces alert noise but does not fix the underlying mount issue; use this playbook (or the hot-path auto-recover automation) to repair the stale mountpoint.</li> </ul> <p>For background and the full implementation plan (prevention + automation + integrity), see:</p> <ul> <li><code>../../../planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> <li>Drills (safe on production): <code>storagebox-sshfs-stale-mount-drills.md</code></li> </ul>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#quick-triage-60-seconds","title":"Quick triage (60 seconds)","text":"<p>On the VPS (<code>/opt/healtharchive-backend</code>):</p> <p>0) Capture an evidence bundle (recommended, read-only):</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-capture-hotpath-staleness-evidence.sh --tag pre-repair\n</code></pre> <p>Optional: if the affected crawl campaign year differs from the current UTC year, pass it explicitly:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-capture-hotpath-staleness-evidence.sh --tag pre-repair --year 2026\n</code></pre> <p>This writes a timestamped directory under:</p> <ul> <li><code>/srv/healtharchive/ops/observability/hotpath-staleness/</code></li> </ul> <p>Optional (recommended): after recovery actions complete, capture a second bundle for comparison:</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-capture-hotpath-staleness-evidence.sh --tag post-repair\n</code></pre> <p>Optional: diff the latest pre-repair vs post-repair bundles:</p> <pre><code>root=/srv/healtharchive/ops/observability/hotpath-staleness\nbefore=$(ls -1dt \"${root}\"/*pre-repair 2&gt;/dev/null | head -n 1)\nafter=$(ls -1dt \"${root}\"/*post-repair 2&gt;/dev/null | head -n 1)\n./scripts/vps-diff-hotpath-staleness-evidence.sh --before \"${before}\" --after \"${after}\"\n</code></pre> <p>1) Snapshot current crawl state:</p> <pre><code>./scripts/vps-crawl-status.sh --year \"$(date -u +%Y)\"\n</code></pre> <p>Optional: if Phase 2 automation has been enabled, check whether it is already attempting recovery (it is disabled-by-default unless the sentinel exists):</p> <pre><code>systemctl status healtharchive-storage-hotpath-auto-recover.timer --no-pager -l || true\nls -la /etc/healtharchive/storage-hotpath-auto-recover-enabled 2&gt;/dev/null || true\ncat /srv/healtharchive/ops/watchdog/storage-hotpath-auto-recover.json 2&gt;/dev/null || true\n</code></pre> <p>If the worker auto-start watchdog is enabled (optional), check it too:</p> <pre><code>systemctl status healtharchive-worker-auto-start.timer --no-pager -l || true\nls -la /etc/healtharchive/worker-auto-start-enabled 2&gt;/dev/null || true\ncat /srv/healtharchive/ops/watchdog/worker-auto-start.json 2&gt;/dev/null || true\n</code></pre>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#watchdog-failure-mode-matrix-deterministic-triage","title":"Watchdog failure-mode matrix (deterministic triage)","text":"<p>Use this matrix before manual repair so you can classify the watchdog state quickly.</p> <p>Primary evidence commands:</p> <pre><code>cat /srv/healtharchive/ops/watchdog/storage-hotpath-auto-recover.json 2&gt;/dev/null || true\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_storage_hotpath_auto_recover_(metrics_ok|detected_targets|deploy_lock_active|last_apply_ok|last_apply_timestamp_seconds|apply_total)'\n</code></pre> Failure class Typical signals Meaning Operator action Stale targets detected, no apply attempted yet <code>detected_targets &gt; 0</code>, <code>apply_total</code> unchanged, no fresh <code>last_apply_timestamp_seconds</code> Target not yet eligible (confirm-runs/min-age/rate-limit gate) Keep observing for a short window; if persistent, run dry-run drill and then manual recovery steps below Deploy-lock suppression <code>deploy_lock_active == 1</code>, state has <code>last_skip_reason=deploy_lock</code> Apply mode intentionally downgraded to safe dry-run during deploy Finish deploy first, then re-check watchdog state; do not force overlapping recovery Apply attempted and failed <code>apply_total &gt; 0</code>, <code>last_apply_ok == 0</code> (especially if older than 24h) Recovery ran but post-check failed (mount not restored/readable) Follow full recovery sequence in this playbook; inspect <code>last_apply_errors</code> and <code>last_apply_warnings</code> in watchdog state JSON Watchdog metrics stale/missing <code>metrics_ok == 0</code> or metric timestamp stale alert firing Timer or script is failing before/while writing metrics Check timer/service logs, fix watchdog execution first, then re-run dry-run drill <p>2) Confirm Storage Box base mount health:</p> <pre><code>mount | rg '/srv/healtharchive/storagebox'\nls -la /srv/healtharchive/storagebox &gt;/dev/null &amp;&amp; echo \"OK: storagebox readable\" || echo \"BAD: storagebox unreadable\"\n</code></pre> <p>3) Identify broken \u201chot paths\u201d (job output dirs):</p> <pre><code>./scripts/vps-crawl-status.sh --year \"$(date -u +%Y)\" | rg '^Output dir:'\nls -la /srv/healtharchive/jobs/hc/  # replace with source path(s) as needed\n</code></pre> <p>If you see <code>Transport endpoint is not connected</code> or <code>d?????????</code> for job output dirs, continue.</p>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#recovery-procedure-safe-ordering","title":"Recovery procedure (safe ordering)","text":""},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#1-stop-the-worker","title":"1) Stop the worker","text":"<p>Stop the worker first to prevent repeated filesystem touches while mounts are broken:</p> <pre><code>sudo systemctl stop healtharchive-worker.service\n</code></pre> <p>Note: if <code>healtharchive-worker-auto-start.timer</code> is enabled, it may restart the worker while you are mid-repair. Either:</p> <ul> <li>temporarily disable the timer, or</li> <li>temporarily remove <code>/etc/healtharchive/worker-auto-start-enabled</code>,</li> </ul> <p>then re-enable after recovery.</p> <p>Optional: if you suspect a crawler container is still running and stuck on IO, inspect it:</p> <pre><code>docker ps --format 'table {{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.Status}}' | rg 'zimit|openzim' || true\n</code></pre> <p>Only stop a container if you\u2019re sure it is part of the broken job and not making progress.</p>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#2-identify-stale-mountpoints-targeted","title":"2) Identify stale mountpoints (targeted)","text":"<p>This incident class often affects specific job output directories (not necessarily the whole Storage Box mount).</p> <p>For each affected job output dir (examples shown):</p> <pre><code>mount | rg '/srv/healtharchive/jobs/(hc|phac|cihr)/'\nsudo findmnt -T /srv/healtharchive/jobs/hc/&lt;JOB_DIR&gt; || true\n</code></pre> <p>If <code>ls</code> against a path returns <code>Transport endpoint is not connected</code>, treat it as stale.</p>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#3-unmount-stale-hot-paths-use-umount-first-then-l-only-if-needed","title":"3) Unmount stale hot paths (use <code>umount</code> first, then <code>-l</code> only if needed)","text":"<p>For each stale mountpoint, try:</p> <pre><code>sudo umount /srv/healtharchive/jobs/&lt;source&gt;/&lt;JOB_DIR&gt;\n</code></pre> <p>If it fails and the path is still broken/unstat\u2019able, use lazy unmount:</p> <pre><code>sudo umount -l /srv/healtharchive/jobs/&lt;source&gt;/&lt;JOB_DIR&gt;\n</code></pre> <p>Notes:</p> <ul> <li>Use targeted unmounts only (specific job dirs), not broad parent directories.</li> <li><code>umount -l</code> is an emergency tool; use it only for confirmed-stale mountpoints.</li> </ul>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#4-re-apply-tiering-mounts","title":"4) Re-apply tiering mounts","text":"<p>1) Re-apply WARC tiering bind mounts (manifest-driven):</p> <pre><code>sudo ./scripts/vps-warc-tiering-bind-mounts.sh --apply\n</code></pre> <p>If you have confirmed-stale mountpoints and want the script to attempt targeted repair automatically (still requires the worker to be stopped first):</p> <pre><code>sudo ./scripts/vps-warc-tiering-bind-mounts.sh --apply --repair-stale-mounts\n</code></pre> <p>If this fails with Errno 107 under <code>/srv/healtharchive/jobs/imports/...</code>, unmount those stale import mountpoints too and re-run.</p> <p>If the systemd unit is in a <code>failed</code> state, clear it and re-run (prevents repeated <code>WarcTieringFailed</code> alerts):</p> <pre><code>systemctl is-failed healtharchive-warc-tiering.service &amp;&amp; sudo systemctl reset-failed healtharchive-warc-tiering.service || true\nsudo systemctl start healtharchive-warc-tiering.service\n</code></pre> <p>2) Re-apply annual output tiering (campaign job output dirs \u2192 Storage Box):</p> <p>Preferred (avoids the systemd unit\u2019s internal worker stop/start):</p> <pre><code># Ensure we target the production DB (Postgres), not a local fallback (SQLite):\nset -a; source /etc/healtharchive/backend.env; set +a\nsystemctl is-active postgresql.service\n\nsudo /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py --apply --year \"$(date -u +%Y)\"\n</code></pre> <p>If you want the script to attempt targeted repair for stale mountpoints (Errno 107), pass:</p> <pre><code>sudo /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py --apply --repair-stale-mounts --allow-repair-running-jobs --year \"$(date -u +%Y)\"\n</code></pre> <p>Alternative (uses the systemd unit, which stops/starts the worker internally):</p> <pre><code>sudo systemctl start healtharchive-annual-output-tiering.service\n</code></pre>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#5-recover-job-state-stuck-running-retryable","title":"5) Recover job state (stuck <code>running</code> \u2192 <code>retryable</code>)","text":"<p>Load the backend env (production DB connection):</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n</code></pre> <p>Recover stale jobs:</p> <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend recover-stale-jobs --older-than-minutes 5 --apply --limit 25\n</code></pre> <p>If a job ended up <code>failed</code> due to the mount issue and you want it to run again:</p> <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend retry-job --id &lt;JOB_ID&gt;\n</code></pre>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#6-restart-the-worker","title":"6) Restart the worker","text":"<pre><code>sudo systemctl start healtharchive-worker.service\n</code></pre>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#replay-note-after-mount-repairs","title":"Replay note (after mount repairs)","text":"<p>If replay smoke tests start returning <code>503</code> for previously indexed jobs after a mount/tiering incident, restart replay to refresh its view of <code>/srv/healtharchive/jobs</code>:</p> <pre><code>sudo systemctl restart healtharchive-replay.service\nsudo systemctl start healtharchive-replay-smoke.service\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_replay_smoke_'\n</code></pre>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#validation-confirm-were-actually-healthy","title":"Validation (confirm we\u2019re actually healthy)","text":""},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#1-worker-is-running-and-picking-jobs","title":"1) Worker is running and picking jobs","text":"<pre><code>sudo systemctl status healtharchive-worker.service --no-pager -l\nsudo journalctl -u healtharchive-worker.service -n 80 --no-pager -l\n</code></pre>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#2-crawls-are-making-progress-not-just-running","title":"2) Crawls are making progress (not just \u201crunning\u201d)","text":"<p>Pick the active job ID and check progress:</p> <pre><code>./scripts/vps-crawl-status.sh --year \"$(date -u +%Y)\" --job-id &lt;JOB_ID&gt;\n</code></pre> <p>Look for:</p> <ul> <li><code>crawlStatus</code> counters increasing over time (<code>crawled</code> ticks up).</li> <li><code>healtharchive_crawl_running_job_stalled == 0</code></li> <li><code>last_progress_age_seconds</code> small (tens of seconds to a few minutes).</li> <li><code>healtharchive_crawl_running_job_state_parse_ok == 1</code> (state file readable; no sshfs weirdness)</li> <li><code>healtharchive_crawl_running_job_container_restarts_done</code> not climbing rapidly (avoid restart thrash)</li> <li>new <code>.warc.gz</code> files appearing under the job\u2019s active temp dir.</li> </ul>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#3-metrics-writers-are-healthy","title":"3) Metrics writers are healthy","text":"<pre><code>sudo systemctl start healtharchive-crawl-metrics.service\nsudo systemctl start healtharchive-tiering-metrics.service\nsudo systemctl status healtharchive-crawl-metrics.service healtharchive-tiering-metrics.service --no-pager -l\n</code></pre>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#if-recovery-fails","title":"If recovery fails","text":"<p>If hot paths are still unreadable after unmount + tiering reapply:</p> <p>1) Verify Storage Box base mount is readable:</p> <pre><code>ls -la /srv/healtharchive/storagebox &gt;/dev/null &amp;&amp; echo OK || echo BAD\nsudo systemctl status healtharchive-storagebox-sshfs.service --no-pager -l\n</code></pre> <p>2) Consider restarting the base mount:</p> <pre><code>sudo systemctl restart healtharchive-storagebox-sshfs.service\n</code></pre> <p>3) Re-run tiering reapply steps (WARC tiering + annual output tiering).</p> <p>If this becomes a recurring pattern, treat it as an infrastructure incident and follow:</p> <ul> <li><code>../core/incident-response.md</code></li> </ul> <p>If the persistent failed-apply alert is active (<code>HealthArchiveStorageHotpathApplyFailedPersistent</code>):</p> <ol> <li>Capture <code>last_apply_errors</code> / <code>last_apply_warnings</code> from:</li> <li><code>/srv/healtharchive/ops/watchdog/storage-hotpath-auto-recover.json</code></li> <li>Run this playbook\u2019s ordered recovery sequence (worker quiesce -&gt; targeted unmount -&gt; tiering re-apply -&gt; stale job recover -&gt; worker restart).</li> <li>Run the dry-run drill from <code>storagebox-sshfs-stale-mount-drills.md</code> to confirm planned actions are now sane.</li> </ol>"},{"location":"operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery/#sshfs-tuning-options","title":"sshfs tuning options","text":"<p>The <code>healtharchive-storagebox-sshfs.service</code> uses these sshfs options:</p> <pre><code>-o reconnect,ServerAliveInterval=15,ServerAliveCountMax=3,kernel_cache\n</code></pre> <p>These defaults are tuned for reliability:</p> <ul> <li><code>reconnect</code> - automatically reconnect when the SSH connection drops</li> <li><code>ServerAliveInterval=15</code> - send SSH keepalives every 15 seconds</li> <li><code>ServerAliveCountMax=3</code> - disconnect after 3 missed keepalives (~45s)</li> <li><code>kernel_cache</code> - use kernel caching for better performance</li> </ul> <p>If you experience frequent Errno 107 issues, consider these additional options in <code>/etc/healtharchive/storagebox.env</code> (requires service restart):</p> Option Description When to use <code>ServerAliveCountMax=5</code> Increase from 3 to tolerate more keepalive misses Unreliable network with brief dropouts <code>ConnectTimeout=30</code> Limit initial connection wait Slow network, avoids long hangs <code>max_write=65536</code> Smaller write chunks Large file writes cause timeouts <code>workaround=rename</code> Better rename handling If file moves fail intermittently <code>auto_cache</code> Smarter caching based on mtime If you see stale data <p>Note: Changing sshfs options can have unintended effects on performance and behavior. Test changes in a non-production environment first.</p>"},{"location":"operations/playbooks/storage/warc-integrity-verification/","title":"WARC integrity verification (post-incident + pre-index)","text":"<p>Use this playbook when you suspect WARC corruption or replay integrity risk, especially after:</p> <ul> <li>sshfs/FUSE mount instability (<code>Errno 107: Transport endpoint is not connected</code>)</li> <li>unexpected crawler/container termination during WARC writes</li> <li>manual intervention on job output directories</li> </ul> <p>This playbook is intentionally procedural; for background see:</p> <ul> <li>Roadmap/incident context: <code>docs/planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> <li>Storage infra recovery: <code>storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"operations/playbooks/storage/warc-integrity-verification/#0-safety-rules-do-not-skip","title":"0) Safety rules (do not skip)","text":"<ul> <li>Never quarantine while a job is <code>running</code>.</li> <li>Never quarantine after a job has been indexed (i.e., when <code>Snapshot</code> rows exist): moving WARCs breaks replay because <code>Snapshot.warc_path</code> must remain valid.</li> <li>If verification failures are <code>infra_error</code>, treat it as a storage incident first (recover mounts), not corruption.</li> </ul> <p>The CLI enforces the most important guards and will refuse unsafe operations.</p>"},{"location":"operations/playbooks/storage/warc-integrity-verification/#1-pick-a-verification-level-cost-vs-confidence","title":"1) Pick a verification level (cost vs confidence)","text":"<p>The <code>ha-backend verify-warcs</code> command supports three levels:</p> <ul> <li>Level 0 (cheap): file exists, is readable, size &gt; 0</li> <li>Level 1 (moderate, default): gzip stream integrity (detect truncation/CRC issues)</li> <li>Level 2 (heavier): WARC parseability (iterate records; streams bodies)</li> </ul> <p>Recommended posture on a single VPS:</p> <ul> <li>Post-incident window: Level 1 for WARCs touched during the incident window.</li> <li>\u201cAlways on\u201d before indexing: Level 0 (built into the indexing pipeline; optional deeper checks via env).</li> </ul>"},{"location":"operations/playbooks/storage/warc-integrity-verification/#2-verify-warcs-for-a-job-report-only","title":"2) Verify WARCs for a job (report-only)","text":"<p>Run a report-only verification:</p> <pre><code>cd /opt/healtharchive-backend\nsudo bash -lc 'set -a; source /etc/healtharchive/backend.env; set +a; ha-backend verify-warcs --job-id &lt;JOB_ID&gt; --level 1'\n</code></pre> <p>Bound the work if you\u2019re validating an incident window:</p> <pre><code>ha-backend verify-warcs --job-id &lt;JOB_ID&gt; --level 1 --since-minutes 180 --limit-warcs 50\n</code></pre> <p>Optional: write a Prometheus node_exporter textfile metric:</p> <pre><code>ha-backend verify-warcs --job-id &lt;JOB_ID&gt; --level 1 --metrics-file /var/lib/node_exporter/textfile_collector/healtharchive_warc_verify.prom\n</code></pre>"},{"location":"operations/playbooks/storage/warc-integrity-verification/#3-if-verification-fails-with-infra_error","title":"3) If verification fails with <code>infra_error</code>","text":"<p>This is usually mount instability, not corruption.</p> <ul> <li>Follow <code>storagebox-sshfs-stale-mount-recovery.md</code>.</li> <li>After recovery, re-run <code>verify-warcs</code>.</li> </ul>"},{"location":"operations/playbooks/storage/warc-integrity-verification/#4-if-verification-fails-with-corrupt_or_unreadable-pre-index-only","title":"4) If verification fails with <code>corrupt_or_unreadable</code> (pre-index only)","text":"<p>If the job has no Snapshot rows (not indexed), quarantine the corrupt WARCs:</p> <pre><code>ha-backend verify-warcs --job-id &lt;JOB_ID&gt; --level 1 --apply-quarantine\n</code></pre> <p>This will:</p> <ul> <li>move corrupt WARCs under <code>&lt;output_dir&gt;/warcs_quarantine/&lt;timestamp&gt;/...</code></li> <li>write <code>&lt;output_dir&gt;/WARCS_QUARANTINED.txt</code> with provenance + sha256</li> <li>set the job back to <code>retryable</code> and reset <code>retry_count</code> so the worker can re-run it</li> </ul> <p>Then let the worker pick it up (or restart the worker if it\u2019s not running).</p>"},{"location":"operations/playbooks/storage/warc-integrity-verification/#5-if-verification-fails-after-indexing-snapshots-exist","title":"5) If verification fails after indexing (snapshots exist)","text":"<p>Do not quarantine: this breaks replay.</p> <p>Treat it as a critical integrity incident:</p> <ul> <li>stop automated cleanup for the affected job</li> <li>preserve the job output directory as-is</li> <li>capture a verification report (<code>--json-out</code> recommended)</li> <li>decide whether to rebuild the dataset / replay from backups, or to re-crawl the affected source</li> </ul> <p>Escalate via <code>../core/incident-response.md</code> and record the outcome in <code>docs/operations/mentions-log.md</code>.</p>"},{"location":"operations/playbooks/storage/warc-storage-tiering/","title":"WARC storage tiering (SSD + Storage Box)","text":"<p>Goal: keep HealthArchive running on a small VPS SSD by tiering large WARC job directories onto a Hetzner Storage Box (\u201ccold\u201d storage), while still being able to replay pages from cold storage.</p> <p>This playbook assumes:</p> <ul> <li>Production-like host paths (<code>/srv/healtharchive/**</code>)</li> <li>Existing snapshots may already reference absolute paths under   <code>/srv/healtharchive/jobs/**</code></li> <li>You want to keep paths stable (so replay keeps working) while relocating   bytes to cheaper storage.</li> </ul>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#architecture-what-runs-where","title":"Architecture (what runs where)","text":"<ul> <li>VPS (hot / canonical paths)</li> <li>Canonical archive root: <code>/srv/healtharchive/jobs</code></li> <li>Backend services read WARCs from paths recorded in the DB (often absolute     paths under <code>/srv/healtharchive/jobs/**</code>).</li> <li> <p>For tiering, we keep these canonical paths intact and mount/bind cold data     into them.</p> </li> <li> <p>Storage Box (cold bytes)</p> </li> <li>Mounted on the VPS at: <code>/srv/healtharchive/storagebox</code></li> <li>Cold mirror root (suggested): <code>/srv/healtharchive/storagebox/jobs</code></li> <li>You store large job directories here and then bind-mount them into the     canonical paths under <code>/srv/healtharchive/jobs/**</code>.</li> </ul>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#create-the-storage-box-hetzner-console","title":"Create the Storage Box (Hetzner console)","text":"<p>Recommended choices for HealthArchive:</p> <ul> <li>Plan: <code>BX11</code> (1 TB) is a good starting tier for cold WARCs.</li> <li>Location: same region as the VPS.</li> <li>Access: SSH key auth (recommended).</li> <li>Additional settings</li> <li>Enable: <code>SSH Support</code></li> <li>Disable (not needed): <code>SMB Support</code>, <code>WebDAV Support</code></li> <li>External reachability: prefer disabled (you can access via the VPS; no     need to expose to the public internet).</li> <li>Labels: optional; if you use them, keep them simple:</li> <li><code>project=healtharchive</code></li> <li><code>role=warc-cold-storage</code></li> <li><code>env=prod</code></li> </ul> <p>Notes:</p> <ul> <li>\u201cSet as default key\u201d in Hetzner means \u201cpreselect this key for future Storage   Boxes by default\u201d (it doesn\u2019t change the key material).</li> <li>On the VPS, ensure private keys are locked down:</li> <li><code>chmod 700 ~/.ssh</code></li> <li><code>chmod 600 ~/.ssh/hetzner_storagebox</code></li> <li><code>chmod 644 ~/.ssh/hetzner_storagebox.pub</code></li> </ul>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#mount-the-storage-box-on-the-vps-sshfs","title":"Mount the Storage Box on the VPS (sshfs)","text":"<p>Run these on the VPS:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y sshfs\nsudo sed -i 's/^#user_allow_other/user_allow_other/' /etc/fuse.conf\nsudo mkdir -p /srv/healtharchive/storagebox\n</code></pre> <p>Mount (SSH runs on port <code>23</code> for Storage Boxes):</p> <pre><code>GID=\"$(getent group healtharchive | cut -d: -f3)\"\nsudo sshfs -p 23 \\\n  -o IdentityFile=/home/haadmin/.ssh/hetzner_storagebox \\\n  -o allow_other,default_permissions \\\n  -o uid=\"$(id -u haadmin)\",gid=\"${GID}\",umask=0027 \\\n  -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=3,kernel_cache \\\n  uNNNNNN@uNNNNNN.your-storagebox.de:/ \\\n  /srv/healtharchive/storagebox\n</code></pre> <p>Sanity check:</p> <pre><code>touch /srv/healtharchive/storagebox/_probe &amp;&amp; rm /srv/healtharchive/storagebox/_probe\ndf -h /srv/healtharchive/storagebox\n</code></pre> <p>Create the cold mirror root:</p> <pre><code>mkdir -p /srv/healtharchive/storagebox/jobs/imports\n</code></pre>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#make-the-mount-persistent-recommended","title":"Make the mount persistent (recommended)","text":"<p>If the Storage Box isn\u2019t mounted (e.g., after reboot), tiered paths may fall back to empty local directories and replay will break. Add a small systemd unit to mount it on boot.</p> <p>Use the repo templates under <code>docs/deployment/systemd/</code> (installed via <code>scripts/vps-install-systemd-units.sh</code>).</p> <p>1) Create <code>/etc/healtharchive/storagebox.env</code> (VPS):</p> <pre><code>sudo install -d -m 0755 /etc/healtharchive\nsudo tee /etc/healtharchive/storagebox.env &gt;/dev/null &lt;&lt;'EOF'\nSTORAGEBOX_HOST=uNNNNNN.your-storagebox.de\nSTORAGEBOX_USER=uNNNNNN\nSTORAGEBOX_IDENTITY=/home/haadmin/.ssh/hetzner_storagebox\nSTORAGEBOX_UID=1000\nSTORAGEBOX_GID=999\n# Optional (defaults are fine for most setups):\n# STORAGEBOX_REMOTE_PATH=\n# STORAGEBOX_PORT=23\n# STORAGEBOX_MOUNT=/srv/healtharchive/storagebox\nEOF\n</code></pre> <p>Replace:</p> <ul> <li><code>uNNNNNN</code> values with your Storage Box username/host.</li> <li><code>STORAGEBOX_UID</code> with <code>id -u haadmin</code>.</li> <li><code>STORAGEBOX_GID</code> with <code>getent group healtharchive | cut -d: -f3</code>.</li> </ul> <p>2) Install templates + enable the mount (VPS):</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-install-systemd-units.sh --apply\nsudo systemctl enable --now healtharchive-storagebox-sshfs.service\nsystemctl status healtharchive-storagebox-sshfs.service --no-pager\nmount | rg /srv/healtharchive/storagebox || true\n</code></pre> <p>If it fails due to host key prompts, prime root\u2019s known_hosts once:</p> <pre><code>sudo ssh -p 23 -i /home/haadmin/.ssh/hetzner_storagebox uNNNNNN@uNNNNNN.your-storagebox.de exit\n</code></pre>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#move-a-job-directory-from-ssd-storage-box-safe-swap","title":"Move a job directory from SSD \u2192 Storage Box (safe swap)","text":"<p>This procedure keeps the canonical path stable.</p> <p>1) Define paths (VPS):</p> <pre><code>HOT=/srv/healtharchive/jobs/imports/&lt;job_dir_name&gt;\nCOLD=/srv/healtharchive/storagebox/jobs/imports/&lt;job_dir_name&gt;\n</code></pre> <p>2) Copy to cold tier (VPS):</p> <pre><code>mkdir -p \"$(dirname \"$COLD\")\"\nrsync -rltH --info=progress2 --no-owner --no-group --no-perms \"$HOT/\" \"$COLD/\"\n</code></pre> <p>3) Stop services (VPS):</p> <pre><code>sudo systemctl stop healtharchive-worker.service\nsudo systemctl stop healtharchive-replay.service\nsudo systemctl stop healtharchive-api.service\n</code></pre> <p>4) Swap the canonical path to point at the cold copy (VPS):</p> <pre><code>sudo mv \"$HOT\" \"${HOT}.hot-backup\"\nsudo mkdir -p \"$HOT\"\nsudo mount --bind \"$COLD\" \"$HOT\"\n</code></pre> <p>5) Start services + verify (VPS):</p> <pre><code>sudo systemctl start healtharchive-api.service\nsudo systemctl start healtharchive-worker.service\nsudo systemctl start healtharchive-replay.service\n\ncurl -fsS http://127.0.0.1:8001/api/health &gt;/dev/null &amp;&amp; echo OK\n</code></pre> <p>6) If everything is OK, delete the backup (VPS):</p> <pre><code>sudo rm -rf \"${HOT}.hot-backup\"\n</code></pre> <p>Rollback (if needed):</p> <pre><code>sudo umount \"$HOT\"\nsudo rm -rf \"$HOT\"\nsudo mv \"${HOT}.hot-backup\" \"$HOT\"\n</code></pre>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#make-tiered-job-mounts-persistent-recommended","title":"Make tiered job mounts persistent (recommended)","text":"<p>Bind mounts created manually will not survive a reboot. Use the repo\u2019s unit template + manifest so tiered jobs come back automatically.</p> <p>1) Create the manifest <code>/etc/healtharchive/warc-tiering.binds</code> (VPS):</p> <pre><code>sudo tee /etc/healtharchive/warc-tiering.binds &gt;/dev/null &lt;&lt;'EOF'\n# cold_path hot_path\n/srv/healtharchive/storagebox/jobs/imports/legacy-hc-2025-04-21 /srv/healtharchive/jobs/imports/legacy-hc-2025-04-21\n/srv/healtharchive/storagebox/jobs/imports/legacy-cihr-2025-04 /srv/healtharchive/jobs/imports/legacy-cihr-2025-04\nEOF\n</code></pre> <p>2) Enable the service (VPS):</p> <pre><code>cd /opt/healtharchive-backend\nsudo ./scripts/vps-install-systemd-units.sh --apply\nsudo systemctl enable --now healtharchive-warc-tiering.service\nsystemctl status healtharchive-warc-tiering.service --no-pager\nmount | rg /srv/healtharchive/jobs/imports/legacy- || true\n</code></pre> <p>Note: the template service runs <code>vps-warc-tiering-bind-mounts.sh --apply --repair-stale-mounts</code> so it can automatically unmount stale Errno 107 mountpoints and re-apply bind mounts.</p>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#if-healtharchive-warc-tieringservice-is-failed-or-alert-is-firing","title":"If <code>healtharchive-warc-tiering.service</code> is failed (or alert is firing)","text":"<p>First, gather a read-only diagnostic report (safe while crawls are running):</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-diagnose-warc-tiering.sh\n</code></pre> <p>If storage hot-path auto-recover automation is enabled (<code>healtharchive-storage-hotpath-auto-recover.timer</code> + sentinel), it will also attempt to reconcile a stale failed tiering unit state (<code>reset-failed</code> + <code>start</code>) when no stale targets are currently eligible and the base Storage Box mount is readable.</p> <p>If the alert does not clear after a couple of timer cycles, run the manual reconcile steps below.</p> <p>Then, during a safe window (recommended: no active replay indexing and no ongoing maintenance), clear the failed state and re-apply tiering:</p> <pre><code>sudo systemctl reset-failed healtharchive-warc-tiering.service\nsudo systemctl start healtharchive-warc-tiering.service\nsystemctl status healtharchive-warc-tiering.service --no-pager -l\n</code></pre> <p>If it fails again, run the tiering script directly (shows the most actionable error output):</p> <pre><code>sudo /opt/healtharchive-backend/scripts/vps-warc-tiering-bind-mounts.sh --apply --repair-stale-mounts\n</code></pre> <p>If the unit is in a <code>failed</code> state from a prior incident, clear it before retrying:</p> <pre><code>systemctl is-failed healtharchive-warc-tiering.service &amp;&amp; sudo systemctl reset-failed healtharchive-warc-tiering.service || true\nsudo systemctl start healtharchive-warc-tiering.service\n</code></pre> <p>Manual validation (safe):</p> <pre><code>sudo /opt/healtharchive-backend/scripts/vps-warc-tiering-bind-mounts.sh\n</code></pre> <p>If <code>healtharchive-warc-tiering.service</code> repeatedly ends up in <code>failed</code> (e.g., after an sshfs disconnect), consider enabling the tiering health metrics timer so failures are visible quickly:</p> <pre><code>sudo systemctl enable --now healtharchive-tiering-metrics.timer\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_tiering_' || true\n</code></pre>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#replay-note-restart-after-tiering-changes","title":"Replay note (restart after tiering changes)","text":"<p>Replay runs in a long-lived Docker container and bind-mounts <code>/srv/healtharchive/jobs</code> into <code>/warcs</code>. After fixing stale mounts or changing tiering binds, restart replay so it sees a clean view of the mountpoints:</p> <pre><code>sudo systemctl restart healtharchive-replay.service\nsudo systemctl start healtharchive-replay-smoke.service\n</code></pre>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#annual-outputs-automatically-tier-to-storage-box","title":"Annual outputs: automatically tier to Storage Box","text":"<p>If you use the annual scheduler timer (<code>healtharchive-schedule-annual.timer</code>), the systemd template now triggers <code>healtharchive-annual-output-tiering.service</code> on success. This bind-mounts the newly enqueued annual job output directories onto the Storage Box tier and briefly stops the worker to reduce race conditions.</p> <p>To apply the updated template on the VPS:</p> <pre><code>cd /opt/healtharchive-backend\ngit pull\nsudo ./scripts/vps-install-systemd-units.sh --apply\nsudo systemctl daemon-reload\n</code></pre>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#rehearsal-before-jan-01-optional","title":"Rehearsal before Jan 01 (optional)","text":"<p>The tiering script selects annual jobs using the Jan 01\u2013Jan 03 UTC window by default. If you want to rehearse the end-to-end scheduling + tiering workflow before Jan 01, you can override the selection window:</p> <p>1) Stop the worker (prevents any queued jobs from running):</p> <pre><code>sudo systemctl stop healtharchive-worker.service\n</code></pre> <p>2) Enqueue annual jobs (this affects the production DB; delete them afterwards if you do not want them queued):</p> <pre><code>/opt/healtharchive-backend/.venv/bin/ha-backend schedule-annual --apply --year 2026 --sources hc phac cihr\n</code></pre> <p>3) Apply tiering for the jobs you just created (use a short window around \u201cnow\u201d):</p> <pre><code># Ensure we target the production DB (Postgres), not a local fallback (SQLite):\nset -a; source /etc/healtharchive/backend.env; set +a\nsystemctl is-active postgresql.service\n\nsudo /opt/healtharchive-backend/.venv/bin/python3 /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py \\\n  --apply \\\n  --repair-stale-mounts \\\n  --allow-repair-running-jobs \\\n  --year 2026 \\\n  --created-after \"$(date -u -d '2 hours ago' +%Y-%m-%dT%H:%M:%SZ)\"\n</code></pre> <p>4) Validate the expected output dirs are mounted (and that storagebox is still mounted):</p> <pre><code>mount | rg '/srv/healtharchive/storagebox|/srv/healtharchive/jobs/(hc|phac|cihr)/' || true\n</code></pre> <p>5) Decide what to do with the queued annual jobs:</p> <ul> <li>If you want to keep them queued for Jan 01, leave the worker stopped until you are ready.</li> <li>If you do not want them queued yet, delete them via the admin UI or CLI before restarting the worker.</li> </ul>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#alerting-for-tiering-recommended","title":"Alerting for tiering (recommended)","text":"<p>If you use Prometheus alerting, enable the tiering metrics writer:</p> <pre><code>sudo systemctl enable --now healtharchive-tiering-metrics.timer\n</code></pre> <p>This requires node_exporter to have the textfile collector enabled (the repo installer does this):</p> <pre><code>cd /opt/healtharchive-backend\ngit pull\nsudo ./scripts/vps-install-observability-exporters.sh --apply\n</code></pre>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#promote-cold-hot-later-optional","title":"Promote (cold \u2192 hot) later (optional)","text":"<p>If you decide a job should be \u201chot\u201d again:</p> <p>1) Stop services. 2) <code>umount</code> the canonical path. 3) <code>rsync</code> cold \u2192 hot (SSD). 4) Start services.</p> <p>This is the inverse of the \u201csafe swap\u201d above.</p>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#preflight-implications","title":"Preflight implications","text":"<ul> <li>If you intend the upcoming annual campaign outputs to land on the Storage Box,   run preflight with:</li> <li><code>YEAR=2026; ./scripts/vps-preflight-crawl.sh --year \"$YEAR\" --campaign-archive-root /srv/healtharchive/storagebox/jobs</code></li> <li>Ensure the Storage Box mount is active before preflight and before the annual   campaign runs.</li> </ul>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#annual-campaign-with-a-tiny-ssd-operational-pattern","title":"Annual campaign with a tiny SSD (operational pattern)","text":"<p>If the campaign won\u2019t fit on SSD:</p> <p>1) Keep the Storage Box mounted. 2) Schedule the annual jobs (dry-run first), then create a cold output directory    and bind-mount it into the canonical job output directory before the    worker runs the job.</p> <p>Sketch:</p> <pre><code># After jobs exist (queued), get the job output dir:\nJOB_ID=123\n/opt/healtharchive-backend/.venv/bin/ha-backend show-job --id \"$JOB_ID\" | rg output_dir\n\n# Suppose output_dir is:\nHOT=/srv/healtharchive/jobs/hc/20260101T000000Z__hc-2026\n\n# Create a matching cold location and mount it into place:\nCOLD=/srv/healtharchive/storagebox/jobs/hc/20260101T000000Z__hc-2026\nmkdir -p \"$COLD\"\nsudo mount --bind \"$COLD\" \"$HOT\"\n</code></pre> <p>This keeps DB WARC paths under <code>/srv/healtharchive/jobs/**</code> (stable), while the bytes live on Storage Box.</p>"},{"location":"operations/playbooks/storage/warc-storage-tiering/#common-pitfalls","title":"Common pitfalls","text":"<ul> <li><code>mkdir: Permission denied</code> under <code>/srv/healtharchive/storagebox/**</code>:</li> <li>Your sshfs mount is mapped to the wrong UID/GID; remount with     <code>uid=$(id -u haadmin)</code>, <code>gid=$(getent group healtharchive | cut -d: -f3)</code>,     and a restrictive <code>umask</code>.</li> <li><code>rsync ... chgrp failed: Permission denied</code>:</li> <li>Use <code>--no-owner --no-group --no-perms</code> for cross-filesystem copies to     Storage Box.</li> <li>Storage Box not mounted (but directories still exist):</li> <li>The filesystem checks will silently target the SSD unless you validate the     mount; keep the mount persistent and verify with <code>mount | grep storagebox</code>.</li> </ul>"},{"location":"operations/playbooks/validation/automation-maintenance/","title":"Automation maintenance playbook (systemd timers)","text":"<p>Goal: keep automation boring, observable, and explicitly controlled.</p> <p>Canonical references:</p> <ul> <li>systemd unit templates + enable/rollback: <code>../../../deployment/systemd/README.md</code></li> <li>Verification ritual: <code>../../automation-verification-rituals.md</code></li> </ul>"},{"location":"operations/playbooks/validation/automation-maintenance/#installupdate-templates-after-repo-updates","title":"Install/update templates (after repo updates)","text":"<p>On the VPS:</p> <ul> <li><code>cd /opt/healtharchive-backend</code></li> <li><code>sudo ./scripts/vps-install-systemd-units.sh --apply --restart-worker</code></li> </ul>"},{"location":"operations/playbooks/validation/automation-maintenance/#bootstrap-ops-directories-one-time","title":"Bootstrap ops directories (one-time)","text":"<p>If <code>/srv/healtharchive/ops/</code> is not prepared:</p> <ul> <li><code>sudo ./scripts/vps-bootstrap-ops-dirs.sh</code></li> </ul>"},{"location":"operations/playbooks/validation/automation-maintenance/#enablement-controls-sentinel-files","title":"Enablement controls (sentinel files)","text":"<p>Automation is intentionally gated by sentinel files under <code>/etc/healtharchive/</code>.</p> <p>Follow the enable/rollback steps in <code>../../../deployment/systemd/README.md</code>.</p>"},{"location":"operations/playbooks/validation/automation-maintenance/#verify-posture","title":"Verify posture","text":"<ul> <li><code>./scripts/verify_ops_automation.sh</code></li> <li>Spot-check logs:</li> <li><code>journalctl -u &lt;service&gt; -n 200</code></li> </ul>"},{"location":"operations/playbooks/validation/automation-maintenance/#storage-watchdog-cadence-monthly","title":"Storage watchdog cadence (monthly)","text":"<p>For stale-mount watchdog reliability, include this in the periodic automation review:</p> <ol> <li>Re-run a safe dry-run watchdog drill:</li> <li><code>../storage/storagebox-sshfs-stale-mount-drills.md</code> (Section 1)</li> <li>Re-run the safe persistent failed-apply alert-condition drill:</li> <li><code>../storage/storagebox-sshfs-stale-mount-drills.md</code> (Section 2)</li> <li>Review watchdog state + key metrics:</li> <li><code>/srv/healtharchive/ops/watchdog/storage-hotpath-auto-recover.json</code></li> <li><code>healtharchive_storage_hotpath_auto_recover_last_apply_ok</code></li> <li><code>healtharchive_storage_hotpath_auto_recover_apply_total</code></li> <li>If <code>HealthArchiveStorageHotpathApplyFailedPersistent</code> fired recently, follow:</li> <li><code>../storage/storagebox-sshfs-stale-mount-recovery.md</code></li> </ol> <p>Burn-in helper command (safe, read-only summary):</p> <ul> <li><code>python3 scripts/vps-storage-watchdog-burnin-report.py --window-hours 168 --json</code></li> </ul>"},{"location":"operations/playbooks/validation/coverage-guardrails/","title":"Coverage guardrails (annual regression checks)","text":"<p>Goal: detect large year-over-year coverage drops after annual jobs are indexed.</p> <p>Canonical refs:</p> <ul> <li>systemd unit templates: <code>../../../deployment/systemd/README.md</code></li> <li>monitoring checklist: <code>../../monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"operations/playbooks/validation/coverage-guardrails/#what-this-does","title":"What this does","text":"<ul> <li>Compares the latest indexed annual job for each source to the prior year.</li> <li>Emits node_exporter textfile metrics:</li> <li><code>healtharchive_coverage_ratio{source=\"hc\",year=\"2026\"}</code></li> <li><code>healtharchive_coverage_regression{source=\"hc\",year=\"2026\"}</code></li> <li><code>healtharchive_coverage_warning{source=\"hc\",year=\"2026\"}</code></li> </ul>"},{"location":"operations/playbooks/validation/coverage-guardrails/#enablement-vps","title":"Enablement (VPS)","text":"<pre><code>sudo touch /etc/healtharchive/coverage-guardrails-enabled\nsudo systemctl enable --now healtharchive-coverage-guardrails.timer\n</code></pre>"},{"location":"operations/playbooks/validation/coverage-guardrails/#manual-check","title":"Manual check","text":"<pre><code>sudo systemctl start healtharchive-coverage-guardrails.service\nsudo journalctl -u healtharchive-coverage-guardrails.service -n 200 --no-pager\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_coverage_'\n</code></pre>"},{"location":"operations/playbooks/validation/coverage-guardrails/#if-an-alert-fires","title":"If an alert fires","text":"<ol> <li>Identify the affected source and year from the metric labels.</li> <li>Confirm current and prior annual jobs:    <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend list-jobs --source hc --status indexed --limit 10\n/opt/healtharchive-backend/.venv/bin/ha-backend show-job --id &lt;JOB_ID&gt;\n</code></pre></li> <li>If the drop is real, inspect crawl logs for stalls/timeouts and consider:</li> <li>re-running the crawl (retryable),</li> <li>adjusting scope rules for that source,</li> <li>or filing a follow-up for annual tuning.</li> </ol>"},{"location":"operations/playbooks/validation/coverage-guardrails/#config","title":"Config","text":"<p>Edit <code>ops/automation/coverage-guardrails.toml</code> to change thresholds.</p>"},{"location":"operations/playbooks/validation/dataset-release/","title":"Dataset release integrity playbook (quarterly)","text":"<p>Goal: confirm a dataset release exists and its checksums verify cleanly.</p> <p>Canonical reference:</p> <ul> <li><code>../../dataset-release-runbook.md</code></li> </ul>"},{"location":"operations/playbooks/validation/dataset-release/#procedure-high-level","title":"Procedure (high level)","text":"<ol> <li>Identify the latest dataset release (GitHub Releases, datasets repo).</li> <li>Download the release assets for the quarter/date you expect.</li> <li>Verify integrity:</li> <li><code>sha256sum -c SHA256SUMS</code></li> </ol>"},{"location":"operations/playbooks/validation/dataset-release/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li><code>sha256sum -c SHA256SUMS</code> completes without errors for the latest release.</li> </ul>"},{"location":"operations/playbooks/validation/healthchecks-parity/","title":"Healthchecks.io parity (env \u2194 systemd \u2194 Healthchecks)","text":"<p>Do not enable or change production automations until the annual crawl/scrape is finished and the campaign jobs are indexed.</p> <p>Goal: ensure the Healthchecks.io dashboard is a faithful reflection of what the VPS actually runs (and only that).</p> <p>This playbook focuses on three sources of truth:</p> <p>1) systemd timers on the VPS (what actually runs) 2) <code>/etc/healtharchive/healthchecks.env</code> (which pings are wired on the VPS) 3) Healthchecks.io checks (what the dashboard expects to hear from)</p> <p>Key rule:</p> <ul> <li>A Healthchecks.io check should exist iff there is a corresponding ping URL in <code>/etc/healtharchive/healthchecks.env</code> (or a legacy <code>HC_*</code> URL used by the disk/backup scripts).</li> </ul> <p>If you follow that rule, the dashboard cannot drift into \u201cchecks we don\u2019t use\u201d or \u201cmissing checks for enabled jobs\u201d.</p>"},{"location":"operations/playbooks/validation/healthchecks-parity/#current-state-as-of-2026-01-03","title":"Current state (as of 2026-01-03)","text":"<p>These pings are configured in <code>/etc/healtharchive/healthchecks.env</code>:</p> <ul> <li><code>HEALTHARCHIVE_HC_PING_REPLAY_RECONCILE</code> \u2192 <code>healtharchive-replay-reconcile.timer</code> (daily)</li> <li><code>HEALTHARCHIVE_HC_PING_SCHEDULE_ANNUAL</code> \u2192 <code>healtharchive-schedule-annual.timer</code> (yearly)</li> <li><code>HEALTHARCHIVE_HC_PING_ANNUAL_SENTINEL</code> \u2192 <code>healtharchive-annual-campaign-sentinel.timer</code> (yearly)</li> <li><code>HEALTHARCHIVE_HC_PING_BASELINE_DRIFT</code> \u2192 <code>healtharchive-baseline-drift-check.timer</code> (weekly)</li> <li><code>HEALTHARCHIVE_HC_PING_PUBLIC_VERIFY</code> \u2192 <code>healtharchive-public-surface-verify.timer</code> (daily)</li> <li><code>HEALTHARCHIVE_HC_PING_ANNUAL_SEARCH_VERIFY</code> \u2192 <code>healtharchive-annual-search-verify.timer</code> (daily)</li> <li><code>HEALTHARCHIVE_HC_PING_CHANGE_TRACKING</code> \u2192 <code>healtharchive-change-tracking.timer</code> (daily)</li> <li><code>HEALTHARCHIVE_HC_PING_COVERAGE_GUARDRAILS</code> \u2192 <code>healtharchive-coverage-guardrails.timer</code> (daily)</li> <li><code>HEALTHARCHIVE_HC_PING_REPLAY_SMOKE</code> \u2192 <code>healtharchive-replay-smoke.timer</code> (daily)</li> </ul> <p>Legacy script checks (separate from the systemd wrapper):</p> <ul> <li><code>HC_DB_BACKUP_URL</code> \u2192 <code>healtharchive-db-backup.timer</code> (daily)</li> <li><code>HC_DISK_URL</code> + <code>HC_DISK_THRESHOLD</code> \u2192 <code>healtharchive-disk-check.timer</code> (hourly)</li> </ul> <p>Known \u201cnot wired (by design) right now\u201d:</p> <ul> <li><code>HEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION</code> exists as a ping var in the installed unit, but:</li> <li><code>healtharchive-cleanup-automation.timer</code> is disabled, and</li> <li><code>/etc/healtharchive/cleanup-automation-enabled</code> sentinel is missing.</li> <li>Result: do not create a Healthchecks.io check or env var until cleanup automation is intentionally enabled.</li> </ul>"},{"location":"operations/playbooks/validation/healthchecks-parity/#audit-checklist-safe-no-restarts","title":"Audit checklist (safe; no restarts)","text":""},{"location":"operations/playbooks/validation/healthchecks-parity/#1-list-the-ping-vars-currently-configured-vps-env-file","title":"1) List the ping vars currently configured (VPS env file)","text":"<p>This prints only variable names (not URLs):</p> <pre><code>sudo awk -F= '$1 ~ /^(HEALTHARCHIVE_HC_PING_|HC_)/ {print $1}' /etc/healtharchive/healthchecks.env | sort -u\n</code></pre>"},{"location":"operations/playbooks/validation/healthchecks-parity/#2-list-what-timers-are-actually-enabled-what-will-run","title":"2) List what timers are actually enabled (what will run)","text":"<pre><code>systemctl list-timers --all | grep healtharchive-\n</code></pre>"},{"location":"operations/playbooks/validation/healthchecks-parity/#3-confirm-healthchecksio-check-schedules-match-reality","title":"3) Confirm Healthchecks.io check schedules match reality","text":"<p>Use the <code>NEXT</code> column from <code>systemctl list-timers</code> to configure Healthchecks schedules:</p> <ul> <li>Hourly timers (disk): Healthchecks \u201c1 hour\u201d period + ~2 hours grace.</li> <li>Daily timers: Healthchecks \u201c1 day\u201d period + ~6 hours grace.</li> <li>Yearly timers (schedule annual + annual sentinel): Healthchecks cron in UTC + large grace (7\u201314 days).</li> </ul> <p>If a yearly check is configured with a small grace (hours), it will look \u201cdown\u201d most of the year.</p>"},{"location":"operations/playbooks/validation/healthchecks-parity/#reconcile-achieve-11-parity-what-exists-vs-what-should-exist","title":"Reconcile: achieve 1:1 parity (what exists vs what should exist)","text":""},{"location":"operations/playbooks/validation/healthchecks-parity/#rule-a-if-a-timer-is-enabled-and-important-it-should-have-a-healthchecks-ping","title":"Rule A \u2014 If a timer is enabled and important, it should have a Healthchecks ping","text":"<p>For each enabled \u201cimportant outcome\u201d timer, ensure:</p> <p>1) A Healthchecks.io check exists 2) Its ping URL is stored in <code>/etc/healtharchive/healthchecks.env</code></p> <p>Important outcome timers (recommended to monitor):</p> <ul> <li><code>healtharchive-replay-reconcile.timer</code></li> <li><code>healtharchive-public-surface-verify.timer</code></li> <li><code>healtharchive-change-tracking.timer</code></li> <li><code>healtharchive-coverage-guardrails.timer</code></li> <li><code>healtharchive-replay-smoke.timer</code></li> <li><code>healtharchive-annual-search-verify.timer</code></li> <li><code>healtharchive-baseline-drift-check.timer</code></li> <li><code>healtharchive-schedule-annual.timer</code> (yearly)</li> <li><code>healtharchive-annual-campaign-sentinel.timer</code> (yearly)</li> <li>legacy: <code>healtharchive-db-backup.timer</code>, <code>healtharchive-disk-check.timer</code></li> </ul> <p>High-frequency timers (recommended NOT to monitor in Healthchecks; too noisy):</p> <ul> <li><code>healtharchive-crawl-metrics.timer</code></li> <li><code>healtharchive-tiering-metrics.timer</code></li> <li><code>healtharchive-crawl-auto-recover.timer</code></li> <li><code>healtharchive-storage-hotpath-auto-recover.timer</code></li> </ul>"},{"location":"operations/playbooks/validation/healthchecks-parity/#rule-b-if-a-healthchecksio-check-exists-it-must-correspond-to-a-real-job-you-run","title":"Rule B \u2014 If a Healthchecks.io check exists, it must correspond to a real job you run","text":"<p>If a Healthchecks.io check exists but:</p> <ul> <li>there is no enabled timer for it, and</li> <li>it is not one of the legacy script checks,</li> </ul> <p>then delete it in Healthchecks.io and remove its env var from <code>/etc/healtharchive/healthchecks.env</code>.</p>"},{"location":"operations/playbooks/validation/healthchecks-parity/#cleanup-automation-what-remains-to-do-deferred-until-after-crawl","title":"Cleanup automation: what remains to do (deferred until after crawl)","text":"<p>Cleanup automation is currently installed but intentionally disabled:</p> <ul> <li>Timer: <code>healtharchive-cleanup-automation.timer</code> (disabled)</li> <li>Sentinel: <code>/etc/healtharchive/cleanup-automation-enabled</code> (missing)</li> <li>Ping var supported by the unit: <code>HEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION</code></li> </ul>"},{"location":"operations/playbooks/validation/healthchecks-parity/#why-we-are-waiting","title":"Why we are waiting","text":"<p>Enabling cleanup changes production behavior (even if intended to be safe). Defer until after crawl so:</p> <ul> <li>we avoid adding churn during the annual campaign,</li> <li>we can review retention expectations and confirm cleanup boundaries.</li> </ul>"},{"location":"operations/playbooks/validation/healthchecks-parity/#post-crawl-enablement-checklist-when-you-decide-yes-enable-cleanup","title":"Post-crawl enablement checklist (when you decide \u201cyes, enable cleanup\u201d)","text":"<p>1) Review the cleanup behavior and config:    - Playbook: <code>../crawl/cleanup-automation.md</code>    - Config: <code>/opt/healtharchive-backend/ops/automation/cleanup-automation.toml</code></p> <p>2) Decide Healthchecks schedule (from the timer):</p> <pre><code>systemctl cat healtharchive-cleanup-automation.timer\n</code></pre> <p>Current schedule (template): weekly Sunday 04:45 UTC.</p> <p>Recommended Healthchecks schedule for that timer:</p> <ul> <li>Cron (UTC): <code>45 4 * * 0</code></li> <li>Grace: 2 days</li> </ul> <p>3) Create the Healthchecks.io check: - Name: <code>healtharchive-cleanup-automation</code> - Schedule: cron above (UTC) - Grace: 2 days</p> <p>4) Add the ping URL to <code>/etc/healtharchive/healthchecks.env</code>:</p> <pre><code>sudoedit /etc/healtharchive/healthchecks.env\n</code></pre> <p>Add:</p> <pre><code>HEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION=https://hc-ping.com/&lt;uuid&gt;\n</code></pre> <p>5) Enable cleanup automation (two gates):</p> <pre><code>sudo install -d -m 0755 /etc/healtharchive\nsudo touch /etc/healtharchive/cleanup-automation-enabled\nsudo systemctl enable --now healtharchive-cleanup-automation.timer\n</code></pre> <p>6) Verify the ping wiring (safe; does not run cleanup):</p> <pre><code>sudo bash -lc 'set -a; source /etc/healtharchive/healthchecks.env; set +a; /opt/healtharchive-backend/scripts/systemd-healthchecks-wrapper.sh --ping-var HEALTHARCHIVE_HC_PING_CLEANUP_AUTOMATION -- echo ok'\n</code></pre> <p>7) Verify real runs on the next scheduled window:</p> <pre><code>sudo journalctl -u healtharchive-cleanup-automation.service -n 200 --no-pager\n</code></pre> <p>If you decide \u201cno, don\u2019t enable cleanup\u201d, keep it disabled and do not create the Healthchecks check or env var.</p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/","title":"Post-Reboot Annual Job Tiering Verification","text":"<p>Type: Validation Runbook Category: Operations / Storage Tiering Last updated: 2026-02-06</p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#purpose","title":"Purpose","text":"<p>After a VPS reboot, verify that: 1. Storage Box is mounted correctly 2. Bind mounts for annual jobs are healthy 3. Postgres is running and can connect 4. The tiering script can find jobs and verify their state</p> <p>When to use this: After any VPS reboot, especially during annual campaign season (December-February).</p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#prerequisites","title":"Prerequisites","text":"<ul> <li>SSH access to VPS</li> <li>Worker should be stopped during validation (or use <code>--allow-repair-running-jobs</code> cautiously)</li> </ul>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#verification-checklist","title":"Verification Checklist","text":""},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#1-verify-storage-box-mount","title":"1. Verify Storage Box Mount","text":"<pre><code># Check mount is present and accessible\nfindmnt /srv/healtharchive/storagebox\nls -ld /srv/healtharchive/storagebox\n\n# Should NOT show Errno 107 (stale mount)\nls /srv/healtharchive/storagebox/jobs\n</code></pre> <p>Expected: Directory listing works without \"Transport endpoint not connected\" error.</p> <p>If mount is missing: Follow storage box mounting procedure in main deployment docs.</p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#2-verify-bind-mounts","title":"2. Verify Bind Mounts","text":"<pre><code># List all bind mounts for annual jobs\nfindmnt | grep /srv/healtharchive/jobs | grep storagebox\n</code></pre> <p>Expected: See entries like: <pre><code>/srv/healtharchive/jobs/2025_annual_hc [...] /srv/healtharchive/storagebox/jobs/2025_annual_hc\n</code></pre></p> <p>If bind mounts are missing: Re-run tiering script (see step 5).</p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#3-verify-postgres-is-running","title":"3. Verify Postgres is Running","text":"<pre><code># Check postgres status\nsystemctl status postgresql\n\n# Test database connection\nexport $(cat /etc/healtharchive/env.production | xargs)\npsql \"${HEALTHARCHIVE_DATABASE_URL}\" -c \"SELECT COUNT(*) FROM archive_jobs WHERE status='running';\"\n</code></pre> <p>Expected: Postgres is active, query returns successfully.</p> <p>If connection fails: - Ensure Postgres is started: <code>sudo systemctl start postgresql</code> - Verify env file exists and is readable - Check <code>DATABASE_URL</code> format in env file</p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#4-verify-job-discovery","title":"4. Verify Job Discovery","text":"<p>For each active annual job, verify the output directory is accessible:</p> <pre><code># Example for 2025 annual jobs\nha-backend show-job --id &lt;annual_job_id&gt; --warc-details\n</code></pre> <p>Expected: - <code>WARC source: stable</code> (if job is indexed) - No <code>Errno 107</code> errors - File sizes and counts are reasonable</p> <p>If errors occur: Note job ID and continue to step 5.</p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#5-run-tiering-script-in-dry-run-mode","title":"5. Run Tiering Script in Dry-Run Mode","text":"<pre><code># This will detect and report any issues\n/opt/healtharchive-backend/.venv/bin/python3 \\\n  /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py \\\n  --year 2025\n</code></pre> <p>Expected output: - No database connection errors - All annual jobs show <code>OK (already mounted)</code> or are correctly identified for tiering - If you see <code>WARN ... reason=unexpected_mount_type</code>, the output dir is mounted but not as a bind mount (higher staleness risk).   - Plan a maintenance window to convert it (stop the worker first).</p> <p>If tiering script fails with database error: <pre><code># Load environment and retry\nexport $(cat /etc/healtharchive/env.production | xargs)\n/opt/healtharchive-backend/.venv/bin/python3 \\\n  /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py \\\n  --year 2025\n</code></pre></p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#6-repair-stale-mounts-if-needed","title":"6. Repair Stale Mounts (If Needed)","text":"<p>If step 5 shows <code>STALE (Errno 107)</code> entries:</p> <pre><code># Stop worker first!\nsudo systemctl stop healtharchive-worker\n\n# Run tiering script with repair flag\nsudo /opt/healtharchive-backend/.venv/bin/python3 \\\n  /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py \\\n  --year 2025 \\\n  --apply \\\n  --repair-stale-mounts \\\n  --allow-repair-running-jobs\n\n# If step 5 shows `WARN ... reason=unexpected_mount_type` entries:\n# (maintenance only; converts direct sshfs mounts into bind mounts)\nsudo /opt/healtharchive-backend/.venv/bin/python3 \\\n  /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py \\\n  --year 2025 \\\n  --apply \\\n  --repair-unexpected-mounts \\\n  --allow-repair-running-jobs\n</code></pre> <p>After repair: Re-run step 4 to verify jobs are now accessible.</p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#7-restart-worker","title":"7. Restart Worker","text":"<pre><code># Restart worker (will now use healthy mounts)\nsudo systemctl start healtharchive-worker\n\n# Verify worker can pick up jobs\nsudo journalctl -u healtharchive-worker -f\n</code></pre> <p>Expected: Worker logs show normal job selection, no <code>RuntimeError</code> about root device.</p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#common-issues-and-fixes","title":"Common Issues and Fixes","text":""},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#issue-cannot-connect-to-database","title":"Issue: \"Cannot connect to database\"","text":"<p>Symptom: Tiering script or <code>ha-backend</code> commands fail with Postgres connection error.</p> <p>Fix: <pre><code>export $(cat /etc/healtharchive/env.production | xargs)\n# Retry command\n</code></pre></p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#issue-annual-job-output-directory-still-on-devsda1","title":"Issue: \"Annual job output directory still on /dev/sda1\"","text":"<p>Symptom: Worker refuses to start annual job crawl.</p> <p>Fix: 1. Check Storage Box mount (step 1) 2. Re-run tiering script with <code>--apply --repair-stale-mounts</code> (step 6) 3. Verify bind mounts are created (step 2) 4. Restart worker (step 7)</p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#issue-transport-endpoint-not-connected-errno-107","title":"Issue: \"Transport endpoint not connected (Errno 107)\"","text":"<p>Symptom: Cannot access annual job directories.</p> <p>Fix: <pre><code># Unmount stale mount\nsudo umount -l /srv/healtharchive/jobs/2025_annual_hc\n\n# Re-run tiering script\nsudo /opt/healtharchive-backend/.venv/bin/python3 \\\n  /opt/healtharchive-backend/scripts/vps-annual-output-tiering.py \\\n  --year 2025 \\\n  --apply\n</code></pre></p>"},{"location":"operations/playbooks/validation/post-reboot-tiering-verify/#see-also","title":"See Also","text":"<ul> <li>Production Single VPS Deployment - Main VPS runbook</li> <li>Annual Output Tiering Design - Technical details</li> </ul>"},{"location":"operations/playbooks/validation/replay-smoke-tests/","title":"Replay smoke tests (daily replay validation)","text":"<p>Goal: confirm replay is serving real content for the latest indexed jobs.</p> <p>Canonical refs:</p> <ul> <li>replay runbook: <code>../../../deployment/replay-service-pywb.md</code></li> <li>systemd unit templates: <code>../../../deployment/systemd/README.md</code></li> </ul>"},{"location":"operations/playbooks/validation/replay-smoke-tests/#what-this-does","title":"What this does","text":"<ul> <li>Picks the latest indexed job per source (including canary).</li> <li>Uses the first seed URL as a replay target (or falls back to the source registry defaults for legacy jobs that lack seeds in <code>ArchiveJob.config</code>):</li> <li><code>https://replay.healtharchive.ca/job-&lt;id&gt;/&lt;seed&gt;</code></li> <li>Emits node_exporter textfile metrics:</li> <li><code>healtharchive_replay_smoke_target_present{source=\"hc\"}</code></li> <li><code>healtharchive_replay_smoke_ok{source=\"hc\",job_id=\"123\"}</code></li> <li><code>healtharchive_replay_smoke_canary_ok</code> (1 if canary passes, 0 otherwise)</li> </ul>"},{"location":"operations/playbooks/validation/replay-smoke-tests/#canary-job","title":"Canary job","text":"<p>The <code>hc_canary</code> source is a small, local-only job (2 pages, ~5 MB) that: - Never gets tiered to Storage Box (always stays on root disk) - Baselines pywb health vs storage tiering health</p> <p>Metric interpretation:</p> <ul> <li><code>canary_ok=1, prod_ok=0</code> \u2192 Storage tiering issue (annual job WARCs inaccessible)</li> <li><code>canary_ok=0, prod_ok=0</code> \u2192 pywb service issue (replay service down or broken)</li> <li><code>canary_ok=1, prod_ok=1</code> \u2192 All healthy</li> </ul> <p>Creating the canary:</p> <pre><code>ha-backend create-canary-job\n</code></pre> <p>This is idempotent and safe to re-run.</p>"},{"location":"operations/playbooks/validation/replay-smoke-tests/#enablement-vps","title":"Enablement (VPS)","text":"<pre><code>sudo touch /etc/healtharchive/replay-smoke-enabled\nsudo systemctl enable --now healtharchive-replay-smoke.timer\n</code></pre>"},{"location":"operations/playbooks/validation/replay-smoke-tests/#manual-check","title":"Manual check","text":"<pre><code>sudo systemctl start healtharchive-replay-smoke.service\nsudo journalctl -u healtharchive-replay-smoke.service -n 200 --no-pager\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_replay_smoke_'\n</code></pre>"},{"location":"operations/playbooks/validation/replay-smoke-tests/#if-an-alert-fires","title":"If an alert fires","text":"<ol> <li>Confirm replay is enabled:    <pre><code>rg -n 'HEALTHARCHIVE_REPLAY_BASE_URL' /etc/healtharchive/backend.env\n</code></pre></li> <li>Confirm replay service health:    <pre><code>sudo systemctl status healtharchive-replay.service --no-pager -l\ncurl -I https://replay.healtharchive.ca/ | head\n</code></pre></li> <li>If replay is up (<code>/</code> is <code>200</code>) but smoke requests return <code>503</code>, suspect WARC/mount access (often after <code>sshfs</code>/tiering incidents).</li> </ol> <p>1) Ensure the WARC tiering unit is not stuck in a failed state:    <pre><code>systemctl is-failed healtharchive-warc-tiering.service &amp;&amp; sudo systemctl reset-failed healtharchive-warc-tiering.service || true\nsudo systemctl start healtharchive-warc-tiering.service\nsudo systemctl status healtharchive-warc-tiering.service --no-pager -l | sed -n '1,120p'\n</code></pre></p> <p>2) Restart replay to refresh its view of <code>/srv/healtharchive/jobs</code>:    <pre><code>sudo systemctl restart healtharchive-replay.service\n</code></pre></p> <p>3) Re-run smoke:    <pre><code>sudo systemctl start healtharchive-replay-smoke.service\ncurl -s http://127.0.0.1:9100/metrics | rg '^healtharchive_replay_smoke_'\n</code></pre></p> <ol> <li>If replay is up but a source still fails, re-run replay reconcile:    <pre><code>sudo systemctl start healtharchive-replay-reconcile.service\n</code></pre></li> </ol>"},{"location":"operations/playbooks/validation/replay-smoke-tests/#config","title":"Config","text":"<p>Edit <code>ops/automation/replay-smoke.toml</code> to adjust timeouts or sources.</p>"},{"location":"operations/playbooks/validation/restore-test/","title":"Restore test playbook (quarterly)","text":"<p>Goal: prove backups are usable by performing a restore and minimal API checks.</p> <p>Canonical reference:</p> <ul> <li><code>../../restore-test-procedure.md</code></li> </ul>"},{"location":"operations/playbooks/validation/restore-test/#procedure-high-level","title":"Procedure (high level)","text":"<ol> <li>Follow <code>../../restore-test-procedure.md</code>.</li> <li>Record results using the template:</li> <li><code>../../../_templates/restore-test-log-template.md</code></li> <li>Store the public-safe log on the VPS:</li> <li><code>/srv/healtharchive/ops/restore-tests/</code></li> </ol>"},{"location":"operations/playbooks/validation/restore-test/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>A dated restore-test log exists under <code>/srv/healtharchive/ops/restore-tests/</code>.</li> <li>Core API checks against the restored DB succeed (health, stats, sources).</li> </ul>"},{"location":"operations/playbooks/validation/security-posture/","title":"Security posture playbook (operators)","text":"<p>Goal: keep the public surface safe-by-default and avoid accidental exposure.</p> <p>Canonical references:</p> <ul> <li>Production runbook: <code>../../../deployment/production-single-vps.md</code></li> <li>Hosting checklist (TLS/HSTS): <code>../../../deployment/hosting-and-live-server-to-dos.md</code></li> <li>Env wiring + CORS: <code>../../../deployment/environments-and-configuration.md</code></li> <li>Admin verification: <code>./scripts/verify-security-and-admin.sh</code></li> </ul>"},{"location":"operations/playbooks/validation/security-posture/#secrets-discipline-always","title":"Secrets discipline (always)","text":"<ul> <li>Store secrets only in VPS/Vercel env (or a secret manager), never in git.</li> <li><code>HEALTHARCHIVE_ADMIN_TOKEN</code></li> <li>DB URL/password</li> <li>Healthchecks ping URLs</li> </ul>"},{"location":"operations/playbooks/validation/security-posture/#https-hsts-api","title":"HTTPS + HSTS (API)","text":"<ul> <li>Maintain HSTS at the reverse proxy (Caddy) for <code>api.healtharchive.ca</code>.</li> <li>After changes, verify HSTS is present:</li> <li><code>./scripts/verify-security-and-admin.sh --api-base https://api.healtharchive.ca --require-hsts</code></li> </ul>"},{"location":"operations/playbooks/validation/security-posture/#strict-cors-allowlist-api","title":"Strict CORS allowlist (API)","text":"<ul> <li>Keep <code>HEALTHARCHIVE_CORS_ORIGINS</code> narrow.</li> <li>Treat widening CORS as a deliberate decision (and re-verify headers).</li> <li>Verify real headers from production (example):</li> <li><code>curl -sS -D- -o /dev/null -H 'Origin: https://healtharchive.ca' https://api.healtharchive.ca/api/health | rg -i '^access-control-allow-origin:'</code></li> </ul>"},{"location":"operations/playbooks/validation/security-posture/#what-done-means","title":"What \u201cdone\u201d means","text":"<ul> <li>Admin endpoints are not publicly accessible.</li> <li>HSTS is present on <code>https://api.healtharchive.ca/api/health</code>.</li> <li>CORS behavior matches the allowlist policy.</li> </ul>"},{"location":"operations/reports/2026-01-19-deployment-log/","title":"Deployment Log: Annual Crawl Hardening 2026","text":"<p>Date: 2026-01-19 Operator: Auto-Deployment Agent Scope: VPS Production Environment (Job 6, 7, 8)</p>"},{"location":"operations/reports/2026-01-19-deployment-log/#objectives","title":"Objectives","text":"<ul> <li>Deploy strict timeout handling (180s) to prevent stalls.</li> <li>Enable auto-recovery for SSHFS mounts and worker processes.</li> <li>Implement deep operational monitoring (metrics &amp; alerts).</li> </ul>"},{"location":"operations/reports/2026-01-19-deployment-log/#execution-log","title":"Execution Log","text":"Time (EST) Phase Action Result 10:45 Phase 1 Pre-deployment state capture Baseline recorded. Job 6 running. 10:50 Phase 2 Codebase Update Pulled <code>main</code> (commit <code>18a8818</code>). 10:55 Phase 3 Service Restart Worker restarted. <code>daemon-reload</code> applied. 11:00 Phase 4 Verification Job detected. Metrics confirmed flowing. 11:15 Phase 5 Investigation Confirmed <code>indexed_pages=0</code> is expected behavior. 11:30 Phase 6 Alerting 9 Alert rules verified with <code>promtool</code>."},{"location":"operations/reports/2026-01-19-deployment-log/#final-status-verified","title":"Final Status Verified","text":"<ul> <li>Job 6 Status: Running (Active).</li> <li>Progress: 359/2908 pages scanned. 56 WARCs generated.</li> <li>Monitoring: Active. <code>node_exporter</code> scraping <code>healtharchive_crawl.prom</code>.</li> <li>Alerts: 9 Rules active. \"Zero Rules Firing\" (Green state).</li> </ul>"},{"location":"operations/reports/2026-01-19-deployment-log/#handoff-notes","title":"Handoff Notes","text":"<ul> <li>New Ops Docs: See <code>docs/operations/monitoring-and-alerting.md</code> and <code>docs/operations/runbooks/</code>.</li> <li>Next scheduled action: None. System is in auto-pilot.</li> </ul>"},{"location":"operations/reports/2026-01-19-indexing-investigation/","title":"Investigation Report: Indexing Delay / Zero Indexed Pages","text":"<p>Date: 2026-01-19 Subject: Job 6 \"indexed_pages\" count remaining at 0 despite WARC generation. Status: RESOLVED (Expected Behavior)</p>"},{"location":"operations/reports/2026-01-19-indexing-investigation/#issue-description","title":"Issue Description","text":"<p>During the deployment of the 2026 Annual Crawl Hardening, it was observed that Job 6 (Health Canada) had generated 56 WARC files but the <code>indexed_pages</code> metric in the database remained at <code>0</code>. This raised concerns that the indexing pipeline was broken or stalled.</p>"},{"location":"operations/reports/2026-01-19-indexing-investigation/#investigation-steps-phase-5","title":"Investigation Steps (Phase 5)","text":"<ol> <li>Static Analysis: Searched for <code>index_job</code> calls in the worker source code.</li> <li>Runtime Analysis: Verified <code>healtharchive-worker</code> logs.</li> <li>State Verification: Checked filesystem for WARCs vs DB status.</li> </ol>"},{"location":"operations/reports/2026-01-19-indexing-investigation/#findings","title":"Findings","text":"<ol> <li> <p>Indexing is Terminal:    Code analysis of <code>src/ha_backend/worker/main.py</code> confirmed that <code>index_job(job_id)</code> is only called after the crawl loop exits successfully.    Unlike some crawlers that index incrementally, HealthArchive currently indexes in batches after the crawl completes.</p> </li> <li> <p>Crawl is Active:    Job 6 is still in <code>running</code> state.</p> </li> <li>56 WARC files exist on disk.</li> <li> <p><code>last_progress</code> timestamps are updating.</p> </li> <li> <p>Conclusion:    The <code>indexed_pages=0</code> metric is correct for a running job. It will update to the full count once the job finishes and the indexing phase begins.</p> </li> </ol>"},{"location":"operations/reports/2026-01-19-indexing-investigation/#hardening-actions-taken","title":"Hardening Actions Taken","text":"<p>To prevent future confusion and catch actual indexing failures:</p> <ol> <li>New Alert: <code>IndexingNotStartedAfterCrawl</code> (in <code>prometheus-alerts-crawl.yml</code>).</li> <li>Fires if <code>status='completed'</code> AND <code>indexed_pages=0</code> for &gt; 1 hour.</li> <li>Runbook: <code>docs/operations/runbooks/indexing-not-started.md</code>.</li> </ol>"},{"location":"operations/reports/2026-01-19-indexing-investigation/#resolution","title":"Resolution","text":"<p>No fix required. The system is functioning as designed. Monitoring will alert if the post-crawl indexing fails.</p>"},{"location":"operations/runbooks/","title":"Runbooks (internal)","text":"<p>Runbooks are short, goal-oriented procedures intended for live incident response.</p> <p>If you are unsure where to start, use the main incident response playbook:</p> <ul> <li>Incident response playbook</li> </ul>"},{"location":"operations/runbooks/#runbooks","title":"Runbooks","text":"<ul> <li>Crawl restart budget low</li> <li>Crawl state file probe failure</li> <li>Indexing not started</li> </ul>"},{"location":"operations/runbooks/crawl-restart-budget-low/","title":"Runbook: CrawlRestartBudgetLow","text":"<p>Alert Name: <code>CrawlRestartBudgetLow</code> Severity: Warning Trigger: <code>healtharchive_crawl_running_job_container_restarts_done &gt; 15</code> (limit is 20).</p>"},{"location":"operations/runbooks/crawl-restart-budget-low/#description","title":"Description","text":"<p>The annual crawl job is restarting its zimit container frequently. Annual jobs are configured with <code>max_container_restarts=20</code> to survive occasional timeouts or memory leaks. Reaching 15 restarts means the job is consuming its budget faster than expected and risks failing completely.</p>"},{"location":"operations/runbooks/crawl-restart-budget-low/#impact","title":"Impact","text":"<ul> <li>If restarts hit 20, the job enters <code>failed</code> state and crawling stops.</li> <li>This protects the infrastructure from infinite loops but might leave the crawl incomplete.</li> </ul>"},{"location":"operations/runbooks/crawl-restart-budget-low/#diagnosis","title":"Diagnosis","text":"<ol> <li>Check Restart Rate:    Is the job restarting every few minutes (thrashing) or once every few hours?</li> </ol> <pre><code># Check restart timestamps\n/opt/healtharchive-backend/scripts/vps-crawl-status.sh\n</code></pre> <ol> <li>Review Crash Reasons:    Check the combined log for the reason before the restart.</li> </ol> <pre><code>tail -n 500 /srv/healtharchive/jobs/&lt;source&gt;/archive_*.combined.log\n</code></pre> <ul> <li>TimeoutErrors: Site is too slow.</li> <li>HTTP 5xx: Site is overloaded.</li> <li>OOM / Killed: Zimit is running out of RAM.</li> </ul>"},{"location":"operations/runbooks/crawl-restart-budget-low/#mitigation","title":"Mitigation","text":"<ol> <li>Increase Budget (If progress is good):    If the job is making good progress (thousands of pages) and just hitting occasional glitches, you can manually increase the budget in the database to keep it going.</li> </ol> <pre><code>ha-backend db-shell\n# UPDATE archive_jobs SET tool_options = jsonb_set(tool_options, '{max_container_restarts}', '30') WHERE id=6;\n</code></pre> <p>Then restart the worker to pick up the new config:</p> <pre><code>sudo systemctl restart healtharchive-worker.service\n</code></pre> <ol> <li>Pause Job (If thrashing):    If restarts are happening rapidly with no progress, pause the job to save resources.</li> </ol> <pre><code>ha-backend db-shell\n# UPDATE archive_jobs SET status='paused' WHERE id=6;\n</code></pre>"},{"location":"operations/runbooks/crawl-state-file-probe-failure/","title":"Runbook: CrawlStateFileProbeFailure","text":"<p>Alert Name: <code>CrawlStateFileProbeFailure</code> Severity: Warning Trigger: <code>healtharchive_crawl_running_job_state_file_ok == 0</code> for 5m.</p>"},{"location":"operations/runbooks/crawl-state-file-probe-failure/#description","title":"Description","text":"<p>The monitoring script on the VPS cannot read the <code>.archive_state.json</code> file for a running job. This almost always indicates that the SSHFS mount to the Hetzner StorageBox has dropped or disconnected.</p>"},{"location":"operations/runbooks/crawl-state-file-probe-failure/#impact","title":"Impact","text":"<ul> <li>Metrics will flatline.</li> <li>Adaptive Worker scaling will pause (cannot read state).</li> <li>The crawl itself might still be running (container has its own mount namespace), but new output writes might eventually fail if the host mount is totally dead.</li> </ul>"},{"location":"operations/runbooks/crawl-state-file-probe-failure/#diagnosis","title":"Diagnosis","text":"<ol> <li>Check Mounts:    ssh to VPS and run:</li> </ol> <pre><code>findmnt -T /srv/healtharchive/jobs\n</code></pre> <p>If it returns nothing or shows \"unreachable\", the mount is gone.</p> <ol> <li>Check StorageBox Connectivity:</li> </ol> <pre><code>ping -c 3 u524803.your-storagebox.de\n</code></pre> <ol> <li>Check Permissions:    If mount is up, check if the file exists and is readable:</li> </ol> <pre><code>ls -la /srv/healtharchive/jobs/&lt;source&gt;/&lt;job_timestamp&gt;/.archive_state.json\n</code></pre>"},{"location":"operations/runbooks/crawl-state-file-probe-failure/#mitigation","title":"Mitigation","text":"<ol> <li>Remount SSHFS:</li> </ol> <pre><code>sudo systemctl restart healtharchive-storagebox-sshfs.service\n</code></pre> <p>Verify mount is back:</p> <pre><code>df -h | grep storagebox\n</code></pre> <ol> <li>Restart Workers (If simple remount fails):    If the mount was stale, the worker process might be hung on I/O.</li> </ol> <pre><code>sudo systemctl restart healtharchive-worker.service\n</code></pre>"},{"location":"operations/runbooks/indexing-not-started/","title":"Runbook: IndexingNotStartedAfterCrawl","text":"<p>Alert Name: <code>IndexingNotStartedAfterCrawl</code> Severity: Critical Trigger: <code>status=\"completed\"</code> AND <code>indexed_pages == 0</code> for &gt; 1 hour.</p>"},{"location":"operations/runbooks/indexing-not-started/#description","title":"Description","text":"<p>A crawl job has successfully completed (reached the \"completed\" state with RC 0), but the <code>indexed_pages</code> count remains at 0 for more than an hour. This suggests the indexing pipeline\u2014which should run immediately after crawl completion\u2014failed to start or crash-looped silently.</p>"},{"location":"operations/runbooks/indexing-not-started/#background","title":"Background","text":"<p>The <code>healtharchive-worker</code> process runs jobs in two phases:</p> <ol> <li>Crawl Phase: Runs <code>archive-tool</code> container.</li> <li>Index Phase: Runs <code>index_job()</code> Python function.</li> </ol> <p>If phase 1 finishes but phase 2 crashes or fails to commit, this alert fires.</p>"},{"location":"operations/runbooks/indexing-not-started/#diagnosis","title":"Diagnosis","text":"<ol> <li>Check Indexing Logs:    Search for the transition from crawl to index in the worker logs.</li> </ol> <pre><code>sudo journalctl -u healtharchive-worker.service --since \"4 hours ago\" | grep -i \"indexing\"\n</code></pre> <p>Look for:    - <code>Starting indexing for job &lt;ID&gt;</code> (Good)    - <code>Indexing for job &lt;ID&gt; failed: ...</code> (Bad)</p> <ol> <li>Verify WARC Existence:    Confirm the WARCs physically exist.</li> </ol> <pre><code>/opt/healtharchive-backend/scripts/vps-crawl-status.sh --job-id &lt;ID&gt;\n</code></pre> <ol> <li>Check Job Status:    Is the job status actually <code>completed</code> or <code>index_failed</code>?</li> </ol> <pre><code>ha-backend show-job &lt;ID&gt;\n</code></pre>"},{"location":"operations/runbooks/indexing-not-started/#mitigation","title":"Mitigation","text":"<ol> <li>Manual Re-indexing:    If the pipeline failed transiently (e.g. DB lock), you can reset the job status to trigger re-indexing.    Warning: This restarts the logic loop. Ensure the crawl is truly done.</li> </ol> <pre><code>ha-backend reindex-job &lt;ID&gt;\n</code></pre> <p>(Note: Check if <code>reindex-job</code> CLI command exists, otherwise use python shell):</p> <pre><code>from ha_backend.indexing import index_job\nindex_job(&lt;ID&gt;)\n</code></pre>"},{"location":"planning/","title":"Roadmaps","text":""},{"location":"planning/#current-backlog","title":"Current backlog","text":"<ul> <li>Future roadmap (what is not implemented yet): <code>roadmap.md</code></li> </ul>"},{"location":"planning/#implementation-plans-active","title":"Implementation plans (active)","text":"<p>Implementation plans live directly under <code>docs/planning/</code> while they are active. When complete, move them to <code>docs/planning/implemented/</code> and date them.</p> <p>Active plans:</p> <ul> <li>Crawl operability (locks, writability, retry controls): <code>2026-02-06-crawl-operability-locks-and-retry-controls.md</code></li> <li>Hot-path staleness root-cause investigation: <code>2026-02-06-hotpath-staleness-root-cause-investigation.md</code></li> </ul>"},{"location":"planning/#operator-follow-through-maintenance-window","title":"Operator Follow-Through (Maintenance Window)","text":"<p>Some plans are \"implemented in repo\" but still require a short, operator-run maintenance step on the VPS.</p> <p>Current known items:</p> <ul> <li>Job lock-dir cutover: restart services that read <code>/etc/healtharchive/backend.env</code> after crawls are idle.</li> <li>Plan: <code>2026-02-06-crawl-operability-locks-and-retry-controls.md</code> (Phase 4)</li> <li>Hard requirement: do not restart the worker mid-crawl unless you explicitly accept interrupting crawls.</li> <li>Annual output-dir mount topology conversion (direct <code>sshfs</code> mounts \u2192 bind mounts):</li> <li>Current state: the active 2026 annual job output dirs are mounted directly via <code>sshfs</code> (higher Errno 107/staleness risk).</li> <li>Why maintenance-only: converting requires unmount/remount of job output dirs and can interrupt active crawls.</li> <li>Status tracking: <code>../operations/healtharchive-ops-roadmap.md</code></li> </ul>"},{"location":"planning/#implemented-plans-history","title":"Implemented plans (history)","text":"<ul> <li>Implemented plans archive: <code>implemented/README.md</code></li> <li>Operational resilience improvements: <code>implemented/2026-02-01-operational-resilience-improvements.md</code></li> <li>Deploy workflow hardening (single VPS): <code>implemented/2026-02-07-deploy-workflow-hardening.md</code></li> <li>CI schema + governance guardrails: <code>implemented/2026-02-06-ci-schema-and-governance-guardrails.md</code></li> <li>Storage watchdog observability hardening: <code>implemented/2026-02-06-storage-watchdog-observability-hardening.md</code></li> <li>Disk usage investigation (48GB discrepancy): <code>implemented/2026-02-01-disk-usage-investigation.md</code></li> <li>WARC discovery consistency improvements (partial): <code>implemented/2026-01-29-warc-discovery-consistency.md</code></li> <li>WARC manifest verification: <code>implemented/2026-01-29-warc-manifest-verification.md</code></li> <li>Patch-job-config CLI + integration tests: <code>implemented/2026-01-28-patch-job-config-and-integration-tests.md</code></li> <li>archive_tool hardening + ops improvements: <code>implemented/2026-01-27-archive-tool-hardening-and-ops-improvements.md</code></li> <li>Annual crawl throughput and WARC-first artifacts: <code>implemented/2026-01-23-annual-crawl-throughput-and-artifacts.md</code></li> <li>Infra-error retry storms + Storage Box hot-path resilience: <code>implemented/2026-01-24-infra-error-and-storage-hotpath-hardening.md</code></li> <li>SLA and service commitments (v1): <code>implemented/2026-01-17-sla-and-service-commitments.md</code></li> <li>Test coverage: critical business logic: <code>implemented/2026-01-17-test-coverage-critical-business-logic.md</code></li> <li>Disaster recovery and escalation procedures: <code>implemented/2026-01-17-disaster-recovery-and-escalation-procedures.md</code></li> <li>Operational hardening: tiering alerting + incident follow-ups: <code>implemented/2026-01-17-ops-tiering-alerting-and-incident-followups.md</code></li> <li>Search ranking + snippet quality iteration (v3): <code>implemented/2026-01-03-search-ranking-and-snippets-v3.md</code></li> <li>Storage Box / sshfs stale mount recovery + integrity: <code>implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> </ul>"},{"location":"planning/#historical-context","title":"Historical context","text":"<ul> <li>HealthArchive 6-Phase Upgrade Roadmap (2025; archived): <code>implemented/2025-12-24-6-phase-upgrade-roadmap-2025.md</code></li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/","title":"2026-02-06: Crawl Operability - Locks, Writability, and Retry Controls","text":"<p>Plan Version: v1.5 Status: Implemented in Repo (Phases 1-4 complete; Phase 4 requires operator execution on VPS during a safe window) Scope: Improve crawl operability and safety around job locking, output-dir health visibility, and retry budget recovery UX. Batched items: #7, #8, #9</p>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#maintenance-window-reminder-operator","title":"Maintenance Window Reminder (Operator)","text":"<p>This plan is implemented in repo, but the production lock-dir cutover is only complete after you restart the services that read <code>/etc/healtharchive/backend.env</code> during a safe window.</p> <ul> <li>Safe anytime (even mid-crawl): set <code>export HEALTHARCHIVE_JOB_LOCK_DIR=/srv/healtharchive/ops/locks/jobs</code> and create <code>/srv/healtharchive/ops/locks/jobs</code>.</li> <li>Maintenance window only: restart <code>healtharchive-worker.service</code> (and API) to pick up the env change.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#operator-status-as-of-2026-02-07","title":"Operator status (as of 2026-02-07)","text":"<ul> <li>\u2705 Non-disruptive staging completed on the VPS:</li> <li><code>/etc/healtharchive/backend.env</code> updated to set <code>HEALTHARCHIVE_JOB_LOCK_DIR=/srv/healtharchive/ops/locks/jobs</code></li> <li><code>/srv/healtharchive/ops/locks/jobs</code> created with intended perms</li> <li>\u23f3 Cutover still pending (maintenance window):</li> <li>restart <code>healtharchive-worker.service</code> and <code>healtharchive-api.service</code> once crawls are idle.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#implementation-progress","title":"Implementation Progress","text":"<ul> <li>Phase 1: Implemented in repository (migration-ready lock directory behavior).</li> <li>Stop forcing <code>1777</code> permissions for non-<code>/tmp</code> lock dirs (preserves group/setgid semantics):<ul> <li><code>src/ha_backend/jobs.py</code></li> <li><code>tests/test_jobs_persistent.py</code></li> </ul> </li> <li>Bootstrapped a dedicated ops lock directory path:<ul> <li><code>/srv/healtharchive/ops/locks/jobs</code> via <code>scripts/vps-bootstrap-ops-dirs.sh</code></li> </ul> </li> <li> <p>Added operator guidance for production lock-dir cutover:</p> <ul> <li><code>docs/deployment/systemd/README.md</code></li> </ul> </li> <li> <p>Phase 2: Implemented in repository (annual queued/retryable output-dir writability probes).</p> </li> <li>Added per-annual-job output-dir writability metrics (worker-user permission drift detection):<ul> <li><code>scripts/vps-crawl-metrics-textfile.py</code></li> <li><code>tests/test_ops_metrics_textfile_scripts.py</code></li> </ul> </li> <li>Added an alert for sustained non-writable annual output dirs:<ul> <li><code>ops/observability/alerting/healtharchive-alerts.yml</code> (<code>HealthArchiveAnnualOutputDirNotWritable</code>)</li> <li><code>tests/test_ops_alert_rules.py</code></li> </ul> </li> <li> <p>Documented the metric + alert:</p> <ul> <li><code>docs/operations/monitoring-and-alerting.md</code></li> <li><code>docs/operations/thresholds-and-tuning.md</code></li> </ul> </li> <li> <p>Phase 3: Implemented in repository (operator retry-budget reset CLI).</p> </li> <li>Added an operator-safe command to reset <code>retry_count</code> (dry-run by default, <code>--apply</code> required):<ul> <li><code>src/ha_backend/cli.py</code> (<code>reset-retry-count</code>)</li> <li><code>tests/test_cli_reset_retry_count.py</code></li> </ul> </li> <li> <p>Documented command usage:</p> <ul> <li><code>docs/reference/cli-commands.md</code></li> </ul> </li> <li> <p>Phase 4: Implemented in repository (operator cutover helper + checklist).</p> </li> <li>Added VPS helper to print an idempotent cutover plan and rollback steps:<ul> <li><code>scripts/vps-job-lock-dir-cutover.sh</code></li> </ul> </li> <li>Linked in systemd deployment docs:<ul> <li><code>docs/deployment/systemd/README.md</code></li> </ul> </li> </ul> <p>All phases (1-4) are implemented in-repo; production cutover remains an operator-run task.</p>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#current-state-summary","title":"Current State Summary","text":"<p>The backend already has resilient job execution and infra-error classification (<code>src/ha_backend/jobs.py</code>), crawl metrics (<code>scripts/vps-crawl-metrics-textfile.py</code>), and stale-job recovery CLI (<code>ha-backend recover-stale-jobs</code>). However, three operability gaps remain:</p> <ol> <li>Job locks currently live under <code>/tmp/healtharchive-job-locks</code> by default, which contributed to cross-user permission edge cases during incident response.</li> <li>Output-dir readability is monitored for running jobs, but proactive visibility for queued/retryable annual jobs is limited.</li> <li>Operators currently rely on ad hoc DB snippets for retry-count reset in edge cases; there is no dedicated audited CLI command.</li> </ol> <p>Relevant current assets:</p> <ul> <li>Job lock implementation:</li> <li><code>src/ha_backend/jobs.py</code> (<code>_job_lock</code>, <code>DEFAULT_JOB_LOCK_DIR</code>)</li> <li><code>scripts/vps-crawl-auto-recover.py</code> (<code>DEFAULT_JOB_LOCK_DIR</code>, lock probes)</li> <li>Crawl metrics exporter:</li> <li><code>scripts/vps-crawl-metrics-textfile.py</code></li> <li>Existing CLI flows:</li> <li><code>src/ha_backend/cli.py</code> (<code>retry-job</code>, <code>recover-stale-jobs</code>)</li> <li>Existing tests:</li> <li><code>tests/test_jobs_persistent.py</code></li> <li><code>tests/test_cli_recover_stale_jobs.py</code></li> <li><code>tests/test_ops_metrics_textfile_scripts.py</code></li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#key-unknowns","title":"Key Unknowns","text":"<ul> <li>Target lock directory choice for production (<code>/run</code>, <code>/srv/healtharchive/ops</code>, or another dedicated path).</li> <li>Whether dual-read lock probing is required during migration to avoid transient blind spots.</li> <li>Desired policy boundaries for resetting retry counts (allowed statuses, required reason field, audit detail level).</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#assumptions","title":"Assumptions","text":"<ul> <li>Service restarts to pick up lock-dir env updates can be scheduled during a safe window.</li> <li>Additional metrics and alerts can be added without overloading signal quality.</li> <li>A dedicated retry-reset CLI can remain operator-only and intentionally explicit (<code>--apply</code>).</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#goals","title":"Goals","text":"<ul> <li>Remove lock-file fragility caused by <code>/tmp</code> semantics and cross-user edge cases.</li> <li>Surface output-dir writability risks before jobs consume retries.</li> <li>Provide a safe, auditable retry-budget reset path that avoids manual DB mutation.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#non-goals","title":"Non-Goals","text":"<ul> <li>Reworking entire job scheduler semantics.</li> <li>Changing archive_tool crawl logic.</li> <li>Fully automating all retry policy decisions.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#constraints","title":"Constraints","text":"<ul> <li>Must not interrupt active crawl during implementation.</li> <li>Migration path must avoid lock-state ambiguity while workers/watchdogs are live.</li> <li>New alerts must be low-noise and scoped to actionable cases.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#phased-implementation-plan","title":"Phased Implementation Plan","text":""},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#phase-0-design-and-migration-strategy","title":"Phase 0: Design and Migration Strategy","text":"<p>Goal: Finalize lock-dir migration and CLI policy contract before code changes.</p> <p>Tasks:</p> <ol> <li>Select production lock-dir target and permissions model.</li> <li>Define transition strategy:</li> <li>dual-read lock detection vs single-cutover,</li> <li>environment variable rollout sequence,</li> <li>restart order for worker and watchdog services.</li> <li>Define <code>reset-retry-count</code> command policy:</li> <li>required flags,</li> <li>supported statuses,</li> <li>audit output format.</li> </ol> <p>Deliverables:</p> <ul> <li>Approved lock migration design.</li> <li>Approved CLI contract for retry reset.</li> </ul> <p>Validation:</p> <ul> <li>Maintainer sign-off on migration sequence and failure handling.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#phase-1-lock-directory-migration-safe-and-backward-compatible","title":"Phase 1: Lock Directory Migration (Safe and Backward Compatible)","text":"<p>Goal: Move lock operations away from <code>/tmp</code> with minimal runtime risk.</p> <p>Tasks:</p> <ol> <li>Update lock-dir resolution in backend and watchdog scripts to support controlled migration.</li> <li>Introduce backward-compatible probing during migration window (if selected in Phase 0).</li> <li>Update deployment/systemd docs for lock-dir environment variable configuration.</li> <li>Add/adjust tests covering:</li> <li>lock acquisition in new directory,</li> <li>lock detection by watchdog under mixed/legacy conditions.</li> </ol> <p>Deliverables:</p> <ul> <li>Lock-dir migration-capable code.</li> <li>Updated tests and operator docs.</li> </ul> <p>Validation:</p> <ul> <li>Existing lock-related tests pass.</li> <li>New migration tests pass in CI.</li> <li>Dry-run verification confirms runner detection still works.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#phase-2-output-dir-writability-probes-for-queuedretryable-annual-jobs","title":"Phase 2: Output-Dir Writability Probes for Queued/Retryable Annual Jobs","text":"<p>Goal: Detect non-writable annual job output dirs before crawl attempts consume retries.</p> <p>Tasks:</p> <ol> <li>Extend crawl metrics exporter to probe queued/retryable annual job output dirs (bounded cardinality).</li> <li>Emit metrics for probe status and errno with source/job labels.</li> <li>Add alert rule(s) for sustained non-writable annual output dirs.</li> <li>Add/extend tests in metrics and alert suites.</li> </ol> <p>Deliverables:</p> <ul> <li>New proactive writability metrics.</li> <li>New alert rule and docs updates in:</li> <li><code>docs/operations/monitoring-and-alerting.md</code></li> <li><code>docs/operations/thresholds-and-tuning.md</code></li> </ul> <p>Validation:</p> <ul> <li>Script tests pass with simulated permission and Errno 107 failures.</li> <li>Alert rule parses and is covered by test(s).</li> <li>Drill run shows expected metric output without touching running crawl.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#phase-3-add-reset-retry-count-operator-cli","title":"Phase 3: Add <code>reset-retry-count</code> Operator CLI","text":"<p>Goal: Replace ad hoc DB snippets with a safe, explicit admin command.</p> <p>Tasks:</p> <ol> <li>Add new CLI command in <code>src/ha_backend/cli.py</code> (dry-run default, explicit <code>--apply</code>).</li> <li>Enforce policy guardrails:</li> <li>disallow running jobs,</li> <li>optional source/status filters,</li> <li>optional reason note for auditability.</li> <li>Add tests for happy path and refusal paths.</li> <li>Document command usage in CLI reference and relevant playbooks/incidents.</li> </ol> <p>Deliverables:</p> <ul> <li><code>ha-backend reset-retry-count</code> command.</li> <li>Tests and documentation.</li> </ul> <p>Validation:</p> <ul> <li>CLI tests pass for dry-run/apply and invalid-state behavior.</li> <li>Command output provides clear audit trail for before/after counts.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#phase-4-controlled-rollout-and-cutover","title":"Phase 4: Controlled Rollout and Cutover","text":"<p>Goal: Transition production safely with no crawl disruption.</p> <p>Operator checklist (VPS):</p> <p>This phase has two parts:</p> <ol> <li>Non-disruptive staging (safe even while crawls are running):</li> <li>back up <code>/etc/healtharchive/backend.env</code></li> <li>set <code>HEALTHARCHIVE_JOB_LOCK_DIR</code></li> <li>ensure <code>/srv/healtharchive/ops/locks/jobs</code> exists with correct perms</li> <li>Disruptive cutover (maintenance window only):</li> <li>restart services that read <code>backend.env</code> (worker, API, and any watchdog units that use lock probes)</li> </ol> <p>Hard requirement: do not restart the worker while a crawl you care about is running. Wait until:</p> <ul> <li><code>/opt/healtharchive-backend/.venv/bin/ha-backend list-jobs --status running --limit 5</code> shows no running jobs, or</li> <li>you have explicitly decided it is OK to interrupt crawls.</li> </ul> <p>Important note: if your VPS checkout at <code>/opt/healtharchive-backend</code> is behind the repo, it may not include the helper script <code>scripts/vps-job-lock-dir-cutover.sh</code> or the latest <code>scripts/vps-bootstrap-ops-dirs.sh</code> that creates the lock dir. In that case, either deploy/pull first, or follow the manual commands below.</p> <p>Non-disruptive staging (VPS):</p> <pre><code>sudo cp -av /etc/healtharchive/backend.env /etc/healtharchive/backend.env.bak.$(date -u +%Y%m%dT%H%M%SZ)\n\nsudo rg -n '^export HEALTHARCHIVE_JOB_LOCK_DIR=' /etc/healtharchive/backend.env &gt;/dev/null \\\n  &amp;&amp; sudo sed -i 's|^export HEALTHARCHIVE_JOB_LOCK_DIR=.*$|export HEALTHARCHIVE_JOB_LOCK_DIR=/srv/healtharchive/ops/locks/jobs|g' /etc/healtharchive/backend.env \\\n  || echo 'export HEALTHARCHIVE_JOB_LOCK_DIR=/srv/healtharchive/ops/locks/jobs' | sudo tee -a /etc/healtharchive/backend.env &gt;/dev/null\n\nrg -n '^export HEALTHARCHIVE_JOB_LOCK_DIR=' /etc/healtharchive/backend.env | tail -n 2\n\n# If the lock dir does not exist yet:\nsudo install -d -m 2770 -o root -g healtharchive /srv/healtharchive/ops/locks\nsudo install -d -m 2770 -o root -g healtharchive /srv/healtharchive/ops/locks/jobs\nls -ld /srv/healtharchive/ops/locks /srv/healtharchive/ops/locks/jobs\n</code></pre> <p>Maintenance-window cutover (VPS):</p> <pre><code>set -a; source /etc/healtharchive/backend.env; set +a\n/opt/healtharchive-backend/.venv/bin/ha-backend list-jobs --status running --limit 5\n\n# Only proceed if you are OK restarting services (recommended: no running jobs).\nsudo systemctl restart healtharchive-worker.service\nsudo systemctl restart healtharchive-api.service\nsudo systemctl is-active healtharchive-worker.service healtharchive-api.service\ncurl -fsS http://127.0.0.1:8001/api/health &gt;/dev/null &amp;&amp; echo OK\n</code></pre> <p>Rollback (VPS):</p> <pre><code>sudo ls -1 /etc/healtharchive/backend.env.bak.* | tail -n 1\nsudo cp -av \"$(sudo ls -1 /etc/healtharchive/backend.env.bak.* | tail -n 1)\" /etc/healtharchive/backend.env\nsudo systemctl restart healtharchive-worker.service\nsudo systemctl restart healtharchive-api.service\n</code></pre> <p>Tasks:</p> <ol> <li>Deploy code with migration-safe lock handling.</li> <li>During maintenance window:</li> <li>set lock-dir env in service environment,</li> <li>restart affected services in safe order,</li> <li>verify lock probes and watchdog visibility.</li> <li>Enable new writability alert after confirming metric quality.</li> <li>Publish operator-facing migration note.</li> </ol> <p>Deliverables:</p> <ul> <li>Production lock-dir cutover complete.</li> <li>Writability metrics and alerts active.</li> <li>Retry-reset CLI available for operators.</li> </ul> <p>Validation:</p> <ul> <li>Post-cutover status checks show no false runner detection.</li> <li>New metrics appear and alert remains quiet under healthy conditions.</li> <li>Operator can execute retry reset without manual DB access.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#dependencies","title":"Dependencies","text":"<ul> <li>Access to update systemd environment config on VPS.</li> <li>Alert rule deployment process.</li> <li>Maintainer/operator alignment on retry-reset governance.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#risks-and-mitigations","title":"Risks and Mitigations","text":"<ul> <li>Risk: Lock-dir cutover causes temporary lock visibility mismatch.</li> <li>Mitigation: migration-safe dual-read/probe strategy and controlled restart order.</li> <li>Risk: Writability probes create noisy alerts.</li> <li>Mitigation: scope to annual queued/retryable jobs and add sustained-duration thresholds.</li> <li>Risk: Retry-reset command is overused.</li> <li>Mitigation: explicit <code>--apply</code>, state guardrails, and required audit output.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#progress-validation-framework","title":"Progress Validation Framework","text":"<ul> <li>Phase complete only when code/docs are merged and verification artifacts exist (tests, dry-run outputs, or operator checklist completion).</li> <li>Production cutover complete only when lock detection and crawl safety checks pass in post-deploy verification.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#timeline-and-milestones","title":"Timeline and Milestones","text":"<p>Expected timeline (single maintainer + one maintenance window):</p> <ul> <li>Milestone A (Days 1-2): Phase 0 complete (design and policy decisions finalized).</li> <li>Milestone B (Days 3-6): Phase 1 complete (lock migration code + tests).</li> <li>Milestone C (Days 7-9): Phase 2 complete (writability metrics/alerts + docs).</li> <li>Milestone D (Days 10-11): Phase 3 complete (retry-reset CLI + docs/tests).</li> <li>Milestone E (Day 12+maintenance window): Phase 4 rollout complete.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#rollout-approach","title":"Rollout Approach","text":"<ol> <li>Merge and validate code in CI.</li> <li>Deploy migration-safe lock handling first.</li> <li>Perform lock-dir env cutover in maintenance window.</li> <li>Enable/observe new metrics and alerts.</li> <li>Announce and document new retry-reset operational flow.</li> </ol>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#rollback-approach","title":"Rollback Approach","text":"<ul> <li>Lock migration rollback:</li> <li>revert env var to legacy lock dir,</li> <li>restart services,</li> <li>keep migration code while investigating.</li> <li>Metrics/alert rollback:</li> <li>disable new alert rule and keep metrics for diagnostics.</li> <li>CLI rollback:</li> <li>remove command if policy misuse is observed, fallback to existing controlled runbook.</li> </ul> <p>Rollback steps are operational and do not require schema changes.</p>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#exit-criteria","title":"Exit Criteria","text":"<ul> <li>Incident follow-up for lock-directory migration is closed.</li> <li>Proactive output-dir writability monitoring exists for annual queued/retryable jobs.</li> <li>Operators no longer require manual DB snippets to reset retry budget.</li> <li>No observed regression in lock-based stale-job safety checks across one campaign cycle.</li> </ul>"},{"location":"planning/2026-02-06-crawl-operability-locks-and-retry-controls/#related-sources","title":"Related Sources","text":"<ul> <li><code>docs/operations/incidents/2026-02-06-auto-recover-stall-detection-bugs.md</code></li> <li><code>docs/operations/incidents/2026-01-09-annual-crawl-phac-output-dir-permission-denied.md</code></li> <li><code>src/ha_backend/jobs.py</code></li> <li><code>scripts/vps-crawl-auto-recover.py</code></li> <li><code>scripts/vps-crawl-metrics-textfile.py</code></li> <li><code>src/ha_backend/cli.py</code></li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/","title":"2026-02-06: Hot-Path Staleness Root-Cause Investigation","text":"<p>Plan Version: v1.4 Status: In Progress (Phases 0-2 implemented in repo; evidence capture + drills require operator execution on VPS) Scope: Determine and mitigate underlying causes of recurring hot-path stale mount events (Errno 107). Batched items: #6</p>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#current-operator-decision-as-of-2026-02-07","title":"Current Operator Decision (as of 2026-02-07)","text":"<p>During the active 2026 annual crawl, we are not converting existing annual job output dirs from direct <code>sshfs</code> mounts into bind mounts yet, even though this is likely a contributor to hot-path staleness risk.</p> <p>Why we are holding off:</p> <ul> <li>Converting mount topology requires an unmount + re-mount of the job output dir.</li> <li>If a crawler container is actively writing to that output dir, this can interrupt the crawl and/or create confusing partial   failure modes.</li> <li>The watchdog + playbooks already provide bounded recovery for Errno 107; the incremental benefit of a topology conversion is   real, but not worth intentionally interrupting an in-progress annual crawl.</li> </ul> <p>What we will do instead (crawl-safe):</p> <ul> <li>Keep capturing pre/post evidence bundles on any Errno 107 event.</li> <li>Run Phase 2 dry-run drills (simulation only) to ensure planned recovery remains sensible.</li> <li>Schedule the mount-topology conversion for a maintenance window after the campaign is idle.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#current-observations-as-of-2026-02-07","title":"Current observations (as of 2026-02-07)","text":"<ul> <li>The active 2026 annual job output dirs are mounted directly as <code>sshfs</code> mountpoints (not bind mounts):</li> <li>This was confirmed via <code>findmnt</code> and via <code>scripts/vps-annual-output-tiering.py --year 2026</code> warnings     (<code>reason=unexpected_mount_type</code>).</li> <li>Benefit of fixing: reduce Errno 107 blast radius and make hot-path recovery simpler/more deterministic.</li> <li>Why deferred: fixing requires unmount/re-mount of job output dirs and risks interrupting active crawls.</li> <li>Deploy-lock suppression was observed and cleared:</li> <li>A stale <code>/tmp/healtharchive-backend-deploy.lock</code> existed and caused apply-mode watchdogs to skip.</li> <li>The lock file was removed; metrics now show deploy lock inactive.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#implementation-progress","title":"Implementation Progress","text":"<ul> <li>Phase 0: Implemented in repository (hypothesis matrix + evidence criteria).</li> <li>Phase 1: Implemented in repository (operator evidence capture script + playbook integration).</li> <li>Evidence capture helper:<ul> <li><code>scripts/vps-capture-hotpath-staleness-evidence.sh</code></li> </ul> </li> <li>Evidence diff helper:<ul> <li><code>scripts/vps-diff-hotpath-staleness-evidence.sh</code></li> </ul> </li> <li>Recovery playbook now recommends capturing a bundle before state changes:<ul> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul> </li> <li>The playbook also recommends a post-repair evidence bundle (<code>--tag post-repair</code>) so you can diff pre/post state.</li> <li>Phase 2: Implemented in repository (Phase 2 drill helper + investigation log format; still requires operator-run execution on the VPS).</li> <li>Phase 2 drill helper:<ul> <li><code>scripts/vps-hotpath-staleness-drill.sh</code></li> </ul> </li> <li>Evidence bundles now include a crawl-status snapshot (<code>vps-crawl-status.txt</code>) to correlate mount issues with live jobs.</li> <li>Phase 2.5: Implemented in repository (tiering UX improvement: detect + optionally repair \"unexpected mount type\" for annual output dirs).</li> <li><code>scripts/vps-annual-output-tiering.py</code> now warns when an annual output dir is mounted but not as a bind mount, and can repair it     during a maintenance window (<code>--apply --repair-unexpected-mounts</code>).</li> <li>Phase 3-5: Pending.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#current-state-summary","title":"Current State Summary","text":"<p>HealthArchive has strong reactive handling for stale mount incidents:</p> <ul> <li>Detection and bounded recovery automation for stale hot paths:</li> <li><code>scripts/vps-storage-hotpath-auto-recover.py</code></li> <li>Alerting for stale/unrecovered paths:</li> <li><code>ops/observability/alerting/healtharchive-alerts.yml</code></li> <li>Operational playbooks and drills:</li> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-drills.md</code></li> </ul> <p>However, incident follow-ups still list a root-cause gap: why hot-path mounts go stale while base mount health can appear normal (<code>docs/operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop.md</code>, <code>docs/operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount.md</code>). Current automation reduces impact but does not yet explain or eliminate recurrence.</p>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#key-unknowns","title":"Key Unknowns","text":"<ul> <li>Primary failure domain for stale events:</li> <li>SSH transport instability,</li> <li>sshfs/FUSE behavior under load,</li> <li>bind-mount propagation edge case,</li> <li>kernel-level interaction,</li> <li>Storage Box endpoint behavior.</li> <li>Whether stale events correlate with specific operations (tiering, replay, high crawl I/O, reconnect cycles).</li> <li>Which sshfs option changes reduce stale events without harming throughput or recovery.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#assumptions","title":"Assumptions","text":"<ul> <li>Production VPS access remains operator-only; investigation execution requires operator-run commands.</li> <li>Stale events are infrequent enough that controlled drills and passive telemetry both are required.</li> <li>Crawl-safe posture remains mandatory during data collection (no disruptive experiments on active crawl unless scheduled).</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#goals","title":"Goals","text":"<ul> <li>Identify likely root cause(s) of hot-path staleness with evidence.</li> <li>Produce a bounded mitigation plan (config/process changes) with measurable success criteria.</li> <li>Update canonical docs and incident backlog to close long-standing unknowns.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#non-goals","title":"Non-Goals","text":"<ul> <li>Replacing Storage Box architecture entirely in this plan.</li> <li>Building a full new storage subsystem.</li> <li>Turning all investigation logic into always-on heavy telemetry.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#constraints","title":"Constraints","text":"<ul> <li>Active crawl must not be disturbed by investigation work.</li> <li>Must operate within single-VPS resource and ops bandwidth constraints.</li> <li>Any production configuration changes require staged rollout and rollback path.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#phased-implementation-plan","title":"Phased Implementation Plan","text":""},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#phase-0-investigation-charter-and-hypothesis-matrix","title":"Phase 0: Investigation Charter and Hypothesis Matrix","text":"<p>Goal: Define what evidence is needed to prove/disprove each hypothesis.</p> <p>Tasks:</p> <ol> <li>Build hypothesis matrix with observable signals:</li> <li>network/transport instability,</li> <li>sshfs reconnect behavior,</li> <li>bind mount repair race,</li> <li>service restart sequencing.</li> <li>Define evidence requirements for closure:</li> <li>minimum number of incidents/drills observed,</li> <li>required signal correlation quality.</li> <li>Define crawl-safe and maintenance-window boundaries for experiments.</li> </ol> <p>Deliverables:</p> <ul> <li>Hypothesis matrix and evidence criteria (in this plan).</li> </ul> <p>Validation:</p> <ul> <li>Maintainer agrees criteria are sufficient to close open incident action items.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#hypothesis-matrix-initial","title":"Hypothesis matrix (initial)","text":"<p>This matrix is intentionally pragmatic: it lists what we can actually observe on a single VPS without adding heavy telemetry.</p> Hypothesis Observable signals How to confirm / rule out Transport instability (SSH/TCP) storagebox sshfs logs show reconnects; kernel logs show TCP resets/timeouts; hot-path staleness coincides with network blips Evidence bundle contains correlated <code>journal-storagebox.txt</code> + <code>dmesg-tail.txt</code> + timestamps from watchdog state/metrics sshfs/FUSE stale state under load base mount stays readable but specific hot paths become stale; <code>fuse.sshfs</code> mounts persist in <code>findmnt</code> while ops hang Evidence bundle contains <code>tiering-hotpath-probes.txt</code> showing per-path staleness while <code>findmnt-storagebox.txt</code> remains healthy Bind-mount propagation / tiering inconsistency hot paths appear as direct <code>fuse.sshfs</code> mounts instead of bind mounts (unexpected layout); <code>findmnt -T &lt;hot&gt;</code> shows <code>SOURCE</code> like <code>user@host:/path</code> and mount <code>OPTIONS</code> missing <code>bind</code> Evidence bundle captures <code>mount.txt</code> / <code>findmnt</code> outputs that show whether hot paths are bind mounts or direct sshfs Recovery sequencing race with worker activity staleness correlates with worker touching paths during tiering repair; repeated re-picks / infra-error thrash Capture <code>journal-worker.txt</code> alongside <code>journal-hotpath-watchdog.txt</code>; look for tight loops at the same timestamps Deploy overlap / lock suppression hides apply attempts watchdog detects targets but apply is suppressed by deploy lock; issue persists beyond expected window Evidence bundle captures watchdog metrics + deploy lock metrics from <code>watchdog-metrics.prom</code> and node_exporter metrics <p>Evidence closure criteria:</p> <ul> <li>Capture at least 2 real-world staleness events with bundles (or 1 real + 1 maintenance-window reproduction).</li> <li>At least one event must clearly show whether the base mount was healthy while a hot path was stale.</li> <li>At least one event must clearly show the mount topology at the time (bind mount vs direct sshfs).</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#phase-1-low-risk-instrumentation-and-evidence-capture","title":"Phase 1: Low-Risk Instrumentation and Evidence Capture","text":"<p>Goal: Improve event-level forensic context with minimal runtime risk.</p> <p>Tasks:</p> <ol> <li>Extend watchdog event logging/state capture (if needed) to include:</li> <li>base mount readability,</li> <li>mount metadata consistency,</li> <li>operation context (running job, next job, tiering path),</li> <li>command outcomes and durations.</li> <li>Add a lightweight operator script for event snapshot capture (journal excerpts + mount/network state) to standardize incident evidence.</li> <li>Document where evidence artifacts are stored and retention expectations.</li> </ol> <p>Deliverables:</p> <ul> <li>Enhanced event capture in watchdog state/logs.</li> <li>Repeatable evidence collection script/workflow.</li> </ul> <p>Validation:</p> <ul> <li>Synthetic drill produces complete evidence bundle.</li> <li>Evidence bundle is usable for post-event analysis without ad hoc shell history.</li> </ul> <p>Operator how-to (VPS):</p> <p>When you see Errno 107 alerts or symptoms (before unmounting/repairing):</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-capture-hotpath-staleness-evidence.sh --tag pre-repair\n</code></pre> <p>Then proceed with state-changing recovery steps in:</p> <ul> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#phase-2-controlled-drill-and-correlation-runs","title":"Phase 2: Controlled Drill and Correlation Runs","text":"<p>Goal: Reproduce stale-like conditions safely enough to test hypotheses.</p> <p>Tasks:</p> <ol> <li>Execute structured dry-run drills and limited maintenance-window drills.</li> <li>Capture and compare:</li> <li>pre-failure state,</li> <li>failure detection state,</li> <li>post-recovery state.</li> <li>Correlate stale events with:</li> <li>system logs,</li> <li>sshfs service behavior,</li> <li>crawl/tiering/replay activity.</li> </ol> <p>Deliverables:</p> <ul> <li>Investigation log with timestamped drill/event records.</li> <li>Preliminary hypothesis ranking by evidence strength.</li> </ul> <p>Validation:</p> <ul> <li>At least one complete event lifecycle captured with full telemetry.</li> <li>At least one hypothesis downgraded or eliminated by evidence.</li> </ul> <p>Operator how-to (VPS) (safe, dry-run):</p> <p>Use this to capture pre/post bundles and optionally run the watchdog in dry-run simulation mode (no service changes, no unmounts).</p> <pre><code>cd /opt/healtharchive-backend\n./scripts/vps-hotpath-staleness-drill.sh \\\n  --simulate-broken-path /srv/healtharchive/jobs/hc/&lt;JOB_DIR&gt; \\\n  --note \"phase2 drill (dry-run)\"\n</code></pre> <p>Outputs:</p> <ul> <li><code>drill-pre</code> and <code>drill-post</code> evidence bundles under:</li> <li><code>/srv/healtharchive/ops/observability/hotpath-staleness/</code></li> <li>A small correlation log line appended to:</li> <li><code>/srv/healtharchive/ops/observability/hotpath-staleness/investigation-log.tsv</code></li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#phase-3-mitigation-candidate-definition-and-risk-assessment","title":"Phase 3: Mitigation Candidate Definition and Risk Assessment","text":"<p>Goal: Turn evidence into actionable, bounded changes.</p> <p>Tasks:</p> <ol> <li>Define mitigation candidates (for example sshfs option changes, restart policy adjustments, sequencing refinements).</li> <li>For each candidate, document:</li> <li>expected effect,</li> <li>failure modes,</li> <li>rollout risk,</li> <li>rollback command path.</li> <li>Select one primary and one fallback mitigation for staged rollout.</li> </ol> <p>Deliverables:</p> <ul> <li>Candidate mitigation matrix.</li> <li>Selected rollout candidates with rationale.</li> </ul> <p>Validation:</p> <ul> <li>Candidate choice is evidence-backed and has explicit rollback.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#phase-4-staged-rollout-and-measurement","title":"Phase 4: Staged Rollout and Measurement","text":"<p>Goal: Validate mitigation effectiveness under real workload.</p> <p>Tasks:</p> <ol> <li>Apply chosen mitigation in controlled maintenance window.</li> <li>Monitor key indicators over defined observation period:</li> <li>stale event frequency,</li> <li>watchdog recovery count,</li> <li>crawl interruption count,</li> <li>replay/tiering related errors.</li> <li>Compare against pre-change baseline.</li> </ol> <p>Deliverables:</p> <ul> <li>Rollout report with before/after metrics.</li> </ul> <p>Validation:</p> <ul> <li>Predefined success threshold met (for example sustained reduction in stale incidents over observation window).</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#phase-5-documentation-closure-and-decision-record","title":"Phase 5: Documentation Closure and Decision Record","text":"<p>Goal: Institutionalize findings and close open backlog actions.</p> <p>Tasks:</p> <ol> <li>Update storage playbooks with validated root-cause findings and mitigations.</li> <li>Create/update decision record if operational baseline changes materially.</li> <li>Close incident follow-up TODOs with links to evidence and decisions.</li> </ol> <p>Deliverables:</p> <ul> <li>Updated canonical docs.</li> <li>Decision record (if required).</li> <li>Closed follow-up action items.</li> </ul> <p>Validation:</p> <ul> <li>Incident TODOs for root-cause investigation marked complete with references.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#dependencies","title":"Dependencies","text":"<ul> <li>Operator access/time for drill execution on VPS.</li> <li>Existing observability stack for metric and log retrieval.</li> <li>Coordination with crawl schedule for maintenance-window experiments.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#risks-and-mitigations","title":"Risks and Mitigations","text":"<ul> <li>Risk: Rare events delay conclusive evidence.</li> <li>Mitigation: combine passive event capture with controlled drills.</li> <li>Risk: Investigative changes increase operational complexity.</li> <li>Mitigation: keep instrumentation lightweight and reversible.</li> <li>Risk: Candidate mitigation regresses throughput.</li> <li>Mitigation: staged rollout with explicit rollback and performance checks.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#progress-validation-framework","title":"Progress Validation Framework","text":"<p>Progress is validated per phase by concrete artifacts:</p> <ul> <li>hypothesis matrix,</li> <li>evidence bundles,</li> <li>drill logs,</li> <li>mitigation matrix,</li> <li>rollout report,</li> <li>docs/decision updates.</li> </ul> <p>No phase is complete without both artifact creation and verification evidence.</p>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#timeline-and-milestones","title":"Timeline and Milestones","text":"<p>Because event frequency is variable, timeline is milestone-based with target windows:</p> <ul> <li>Milestone A (Week 1): Phase 0 complete (charter and hypotheses).</li> <li>Milestone B (Weeks 1-2): Phase 1 complete (instrumentation and evidence workflow).</li> <li>Milestone C (Weeks 2-4): Phase 2 complete (drills/correlation evidence).</li> <li>Milestone D (Week 4): Phase 3 complete (mitigation selection).</li> <li>Milestone E (Weeks 5-6): Phase 4 complete (staged rollout + observation).</li> <li>Milestone F (Week 6): Phase 5 complete (docs closure and follow-up completion).</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#rollout-approach","title":"Rollout Approach","text":"<ul> <li>Investigation work starts with non-disruptive telemetry and drills.</li> <li>Configuration changes occur only in planned maintenance windows.</li> <li>One mitigation change at a time to preserve attribution.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#rollback-approach","title":"Rollback Approach","text":"<ul> <li>For each mitigation change, predefine rollback commands before apply.</li> <li>If stale frequency or crawl stability worsens, immediately revert to prior known-good settings and continue evidence collection.</li> <li>Keep watchdog automation in place during rollback to preserve resilience.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#exit-criteria","title":"Exit Criteria","text":"<ul> <li>Root-cause hypothesis is narrowed to a defensible primary explanation (or clearly bounded set of explanations).</li> <li>At least one mitigation has been validated in production without degrading crawl stability.</li> <li>Canonical storage playbooks and, if needed, a decision record reflect the new baseline.</li> <li>Open root-cause TODOs in related incidents are closed with evidence links.</li> </ul>"},{"location":"planning/2026-02-06-hotpath-staleness-root-cause-investigation/#related-sources","title":"Related Sources","text":"<ul> <li><code>docs/operations/incidents/2026-01-24-infra-error-107-hotpath-thrash-and-worker-stop.md</code></li> <li><code>docs/operations/incidents/2026-01-08-storage-hotpath-sshfs-stale-mount.md</code></li> <li><code>scripts/vps-storage-hotpath-auto-recover.py</code></li> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"planning/roadmap/","title":"Future roadmap (backlog)","text":"<p>This file tracks not-yet-implemented work and planned upgrades.</p> <p>It is intentionally not an implementation plan.</p>"},{"location":"planning/roadmap/#how-to-use-this-file-workflow","title":"How to use this file (workflow)","text":"<ol> <li>Pick a reasonable amount of work from the items in this backlog.</li> <li>Create a focused implementation plan in <code>docs/planning/</code> (example name: <code>YYYY-MM-&lt;topic&gt;.md</code>).</li> <li>Implement the work.</li> <li>Update canonical documentation so operators/users can run and maintain the result.</li> <li>Move the completed implementation plan to <code>docs/planning/implemented/</code> and date it.</li> </ol>"},{"location":"planning/roadmap/#external-irl-work-not-implementable-in-git","title":"External / IRL work (not implementable in git)","text":"<p>These items are intentionally \u201cexternal\u201d and require ongoing human follow-through.</p> <ul> <li>External outreach + verification execution (operator-only):</li> <li>Playbook: <code>../operations/playbooks/external/outreach-and-verification.md</code></li> <li>Secure at least 1 distribution partner (permission to name them publicly).</li> <li>Secure at least 1 verifier (permission to name them publicly).</li> <li>Maintain a public-safe mentions/citations log with real entries:</li> <li><code>../operations/mentions-log.md</code> (links only; no private contact data)</li> <li>Healthchecks.io alignment: keep systemd timers, <code>/etc/healtharchive/healthchecks.env</code>, and the Healthchecks UI in sync.</li> <li>See: <code>../operations/playbooks/validation/healthchecks-parity.md</code> and <code>../deployment/production-single-vps.md</code></li> </ul> <p>Track the current status and next actions in:</p> <ul> <li><code>../operations/healtharchive-ops-roadmap.md</code></li> </ul> <p>Supporting materials:</p> <ul> <li><code>../operations/outreach-templates.md</code></li> <li><code>../operations/partner-kit.md</code></li> <li><code>../operations/verification-packet.md</code></li> </ul>"},{"location":"planning/roadmap/#transparency-public-reporting-policy-posture","title":"Transparency &amp; public reporting (policy posture)","text":"<ul> <li>Incident disclosure posture (current default: Option B):</li> <li>Publish public-safe notes only when an incident changes user expectations (outage/degradation, integrity risk, security posture, policy change).</li> <li>Decision record: <code>../decisions/2026-01-09-public-incident-disclosure-posture.md</code></li> <li>Revisit later: consider moving to \u201cOption A\u201d (always publish public-safe notes for sev0/sev1) once operations are demonstrably stable over multiple full campaign cycles.</li> </ul>"},{"location":"planning/roadmap/#technical-backlog-candidates","title":"Technical backlog (candidates)","text":"<p>Keep this list short; prefer linking to the canonical doc that explains the item.</p>"},{"location":"planning/roadmap/#storage-retention-backend","title":"Storage &amp; retention (backend)","text":"<ul> <li>Storage/retention upgrades (only with a designed replay retention policy).</li> <li>See: <code>../operations/growth-constraints.md</code>, <code>../deployment/replay-service-pywb.md</code></li> </ul>"},{"location":"planning/roadmap/#crawling-indexing-reliability-backend","title":"Crawling &amp; indexing reliability (backend)","text":"<ul> <li>WARC discovery consistency follow-through (deferred Phase 2-4 work; keep behavior coherent across status, indexing, and cleanup).</li> <li>Historical context: <code>implemented/2026-01-29-warc-discovery-consistency.md</code></li> <li>Already implemented: <code>implemented/2026-01-29-warc-manifest-verification.md</code></li> <li>Consider whether a separate staging backend is worth it (increases ops surface; only do if it buys real safety).</li> <li>See: <code>../deployment/environments-and-configuration.md</code></li> </ul>"},{"location":"planning/roadmap/#repo-governance-future","title":"Repo governance (future)","text":"<ul> <li>Tighten GitHub merge discipline when there are multiple committers (PR-only + required checks).</li> <li>See: <code>../operations/monitoring-and-ci-checklist.md</code></li> </ul>"},{"location":"planning/roadmap/#autonomous-ai-agent-work-quality-governance-improvements","title":"Autonomous AI Agent Work (Quality &amp; Governance Improvements)","text":"<p>The following items can be completed nearly autonomously by AI coding agents with minimal human intervention. These items emerged from a comprehensive 2026-02-11 audit covering governance, code quality, security, documentation, and professionalism gaps across all three repos.</p> <p>Priority order reflects admissions/portfolio value, not implementation effort.</p> <p>NOTE: Items #8-22 (excluding #15, #23-24) were implemented as part of the \"Governance, SEO, and Security Foundations\" implementation plan (2026-02-12), now archived in <code>docs/planning/implemented/2026-02-12-governance-seo-and-security-foundations.md</code>.</p>"},{"location":"planning/roadmap/#governance-open-source-standards-immediate-priority","title":"Governance &amp; Open Source Standards (Immediate Priority)","text":"<p>STATUS: Items #1-6 deferred due to AI content filtering constraints (2026-02-12) - CITATION.cff, SECURITY.md, and CODE_OF_CONDUCT.md files trigger content policy blocks - These items remain high-value but require manual implementation or alternative tooling - Other governance items (.mailmap, issue templates, LICENSE) may be implementable separately</p> <ol> <li>Add CITATION.cff to all repos (S: 1-2h) [COMPLETED 2026-02-12 - Frontend]</li> <li>\u2705 Frontend: CITATION.cff created with project metadata, authors, license, keywords</li> <li>\u23f3 Backend: Pending</li> <li>\u23f3 Datasets: Pending</li> <li>Makes project citable in academic work; GitHub renders \"Cite this repository\" button</li> <li>Evidence: Scholar contribution, research-grade project</li> <li>Category: Scholarship/Evaluation</li> <li> <p>Commit: 9818c0c (frontend)</p> </li> <li> <p>Add SECURITY.md with vulnerability disclosure policy (S: 1-2h) [COMPLETED 2026-02-12 - Frontend]</p> </li> <li>\u2705 Frontend: SECURITY.md created with supported versions, reporting process, response timeline, scope, safe harbor</li> <li>\u23f3 Backend: Pending</li> <li>\u23f3 Datasets: Pending</li> <li>Evidence: Professional security posture, visible on GitHub Security tab</li> <li>Category: Privacy/Security/Ethics</li> <li> <p>Commit: 9818c0c (frontend)</p> </li> <li> <p>Add CODE_OF_CONDUCT.md (S: 1h) [DEFERRED]</p> </li> <li>Contributor Covenant (standard) to all 3 repos</li> <li>Customize contact method (project email)</li> <li>Evidence: Inclusive governance, professional community standards</li> <li> <p>Category: Professionalism/Governance</p> </li> <li> <p>Add LICENSE to datasets repo (S: 30min) [DEFERRED]</p> </li> <li>Currently MISSING from datasets repo</li> <li>Match backend/frontend or use CC-BY-4.0 for metadata exports</li> <li>Evidence: Basic open-source requirement</li> <li> <p>Category: Professionalism/Governance</p> </li> <li> <p>Add GitHub issue and PR templates (S: 2-3h) [DEFERRED]</p> </li> <li><code>.github/ISSUE_TEMPLATE/</code> with bug report, feature request, data quality issue</li> <li><code>.github/PULL_REQUEST_TEMPLATE.md</code> with checklist</li> <li>All 3 repos</li> <li>Evidence: Professional project management, structured intake</li> <li> <p>Category: Professionalism/Governance</p> </li> <li> <p>Normalize git identities with .mailmap (S: 30min) [DEFERRED]</p> </li> <li>Currently 3 identities (Jeremy Dawson, Jer, jerdaw) across 809 commits</li> <li>Create <code>.mailmap</code> in each repo to consolidate to single canonical identity</li> <li>Evidence: Clean contributor attribution, professional presentation</li> <li> <p>Category: Professionalism/Governance</p> </li> <li> <p>Add changelog/release tags to backend and frontend (M: 1 day)</p> </li> <li>Only datasets has tags currently</li> <li>Create semantic version tags (v1.0.0 or v0.1.0)</li> <li>Generate CHANGELOG.md from git history</li> <li>Set up GitHub Releases with release notes</li> <li>Evidence: Versioned releases show maturity</li> <li>Category: Professionalism/Governance</li> </ol>"},{"location":"planning/roadmap/#frontend-seo-discoverability-high-value","title":"Frontend SEO &amp; Discoverability (High Value)","text":"<ol> <li>Add Open Graph + SEO meta tags to frontend (M: 1 day) [COMPLETED 2026-02-12]</li> <li>\u2705 Added OpenGraph metadata (og:title, og:description, og:url, og:siteName, og:locale, og:type)</li> <li>\u2705 Added Twitter Card metadata (card, title, description)</li> <li>\u2705 Added JSON-LD structured data for Organization (src/components/seo/JsonLd.tsx)</li> <li>\u2705 Added JSON-LD structured data for Dataset (src/components/seo/DatasetJsonLd.tsx)</li> <li>\u2705 Enhanced buildPageMetadata() in src/lib/metadata.ts with full OG/Twitter support</li> <li>Evidence: Rich previews on social media, professional sharing</li> <li>Category: Communication/Documentation</li> <li> <p>Commit: 9818c0c</p> </li> <li> <p>Add sitemap.xml and robots.txt (S: 2-3h) [COMPLETED 2026-02-12]</p> </li> <li>\u2705 Implemented dynamic sitemap.xml in src/app/sitemap.ts</li> <li>\u2705 Bilingual entries with EN/FR alternates for all static pages</li> <li>\u2705 Updated robots.txt to reference sitemap.xml</li> <li>\u23f3 Submit to Google Search Console (manual step pending)</li> <li>Evidence: Search engine discoverability, indexing</li> <li>Category: Communication/Documentation</li> <li> <p>Commit: 9818c0c</p> </li> <li> <p>Add JSON-LD structured data for datasets (S: 2-3h) [COMPLETED 2026-02-12]</p> <ul> <li>\u2705 Created DatasetJsonLd component with Schema.org Dataset markup</li> <li>\u2705 Includes: name, description, license (CC-BY-4.0), distribution formats (JSONL/CSV)</li> <li>\u2705 Temporal coverage (2024/..), spatial coverage (Canada), keywords</li> <li>\u2705 Integrated on /exports page</li> <li>\u2705 Test coverage added (tests/datasetJsonLd.test.tsx)</li> <li>Evidence: Dataset discoverable in Google Dataset Search</li> <li>Category: Scholarship/Evaluation</li> <li>Commit: 9818c0c</li> </ul> </li> <li> <p>Add RSS/Atom feed discovery meta tags (S: 1h) [COMPLETED 2026-02-12]</p> <ul> <li>\u2705 Added RSS feed auto-discovery link in src/app/[locale]/layout.tsx</li> <li>\u2705 Points to /api/changes/rss endpoint</li> <li>\u2705 Uses Next.js metadata.alternates.types API</li> <li>\u23f3 Validate with W3C Feed Validator (manual step pending)</li> <li>Evidence: RSS auto-discovery in browsers</li> <li>Category: Communication/Documentation</li> <li>Commit: 9818c0c</li> </ul> </li> </ol>"},{"location":"planning/roadmap/#backend-quality-testing-engineering-excellence","title":"Backend Quality &amp; Testing (Engineering Excellence)","text":"<ol> <li> <p>Enforce test coverage thresholds in CI (M: 1 day) [COMPLETED 2026-02-12]</p> <ul> <li>Backend: Added <code>--cov-fail-under=75</code> to pytest for critical modules (api, indexing, worker)</li> <li>Current coverage: 76.96% (target: 75% enforced, 80% goal)</li> <li>Added Makefile targets: coverage, coverage-critical, coverage-target</li> <li>Integrated into check-full target (pre-deploy gate)</li> <li>Created comprehensive coverage documentation in docs/development/test-coverage.md</li> <li>Frontend coverage: deferred (needs vitest configuration)</li> <li>Evidence: Concrete quality baseline (76.96%), prevents regressions</li> <li>Category: Reliability/Quality</li> </ul> </li> <li> <p>Expand backend test coverage for untested areas (M: 1-2 days) [COMPLETED 2026-02-12]</p> <ul> <li>Added CORS header validation tests (7 tests)</li> <li>Added search query edge case tests (14 tests): SQL injection, XSS, Unicode, special chars, path traversal, command injection</li> <li>Added concurrent request tests (8 tests): health checks, stats, sources, search, mixed requests, unique request IDs</li> <li>Added health check error scenario tests (15 tests): empty DB, response format, security headers, CORS, method restrictions, query params, concurrent writes</li> <li>Total: 44 comprehensive edge case and reliability tests</li> <li>Evidence: More robust test suite, broader API coverage, security vulnerability testing</li> <li>Category: Reliability/Quality</li> </ul> </li> <li> <p>Add request ID / correlation logging (S: 2-3h) [COMPLETED 2026-02-12]</p> <ul> <li>Backend: Add middleware to generate unique request IDs (UUID) for every API request</li> <li>Include in all log messages</li> <li>Return as <code>X-Request-Id</code> response header</li> <li>Update logging format in <code>logging_config.py</code></li> <li>Evidence: Professional observability, easier debugging</li> <li>Category: Reliability/Quality</li> </ul> </li> <li> <p>Add API health integration tests to PR CI (M: 1 day)</p> <ul> <li>Currently E2E tests only run on main branch post-merge</li> <li>Promote E2E smoke tests to all PRs</li> <li>Add lightweight API contract tests (verify response schemas match Pydantic models)</li> <li>Evidence: Faster feedback loop, catch issues earlier</li> <li>Category: Reliability/Quality</li> </ul> </li> <li> <p>Normalize pre-commit hooks across repos (S: 2-3h) [COMPLETED 2026-02-12]</p> <ul> <li>Backend: Added ruff format + ruff lint + mypy hooks to .pre-commit-config.yaml</li> <li>Frontend: Added eslint + prettier hooks to .pre-commit-config.yaml</li> <li>Datasets: Added ruff format + ruff lint + mypy hooks to .pre-commit-config.yaml</li> <li>All repos: Consistent base hooks (trailing-whitespace, end-of-file-fixer, check-yaml, check-toml, check-added-large-files, detect-private-key)</li> <li>Mypy excludes: scripts/, alembic/, archive_tool/ in backend; scripts/ in datasets</li> <li>All hooks passing on all repos</li> <li>Evidence: Consistent quality gates across all repos, disciplined engineering</li> <li>Category: Reliability/Quality</li> </ul> </li> </ol>"},{"location":"planning/roadmap/#security-hardening-critical-for-health-data","title":"Security Hardening (Critical for Health Data)","text":"<ol> <li> <p>Add rate limiting middleware to backend API (M: 1 day) [COMPLETED 2026-02-12]</p> <ul> <li>Added slowapi for IP-based rate limiting</li> <li>Per-endpoint limits implemented: search 60/min, exports 10/min, reports 5/min</li> <li>Default limit: 120/min for all other endpoints</li> <li>Returns 429 with Retry-After header when exceeded</li> <li>Rate limit headers (X-RateLimit-Limit, X-RateLimit-Remaining) on limited endpoints</li> <li>Configurable via HEALTHARCHIVE_RATE_LIMITING_ENABLED environment variable</li> <li>Evidence: Production-grade availability, abuse prevention</li> <li>Category: Reliability/Quality</li> </ul> </li> <li> <p>Add pip-audit and npm-audit to CI as blocking checks (S: 2-3h) [PARTIALLY COMPLETED 2026-02-12]</p> <ul> <li>CI workflow updated, Makefile updated, but requires fixing existing vulnerabilities first</li> <li>Backend: pillow 11.3.0 \u2192 12.1.1, pip 25.3 \u2192 26.0</li> <li>Frontend: next 16.1.1 \u2192 16.1.6</li> <li>See manual steps below before enabling blocking behavior</li> <li>Backend: Add <code>pip-audit</code> to <code>backend-ci.yml</code> (blocking, not advisory)</li> <li>Frontend: Add <code>npm audit --audit-level=high</code> to <code>frontend-ci.yml</code></li> <li>Add Dependabot config to backend/datasets (frontend already has it)</li> <li>Evidence: Proactive dependency vulnerability scanning</li> <li>Category: Privacy/Security/Ethics</li> </ul> </li> <li> <p>Add Content Security Policy (CSP) headers to backend (S: 2-3h) [COMPLETED 2026-02-12]</p> <ul> <li>Added CSP middleware to FastAPI with restrictive default policy</li> <li>JSON endpoints: <code>default-src 'none'; frame-ancestors 'none'</code></li> <li>Raw snapshot endpoint: permissive policy for archived HTML (inline scripts/styles, external resources)</li> <li>Added Strict-Transport-Security (HSTS) header (max-age=1 year, includeSubDomains)</li> <li>Configurable via HEALTHARCHIVE_CSP_ENABLED and HEALTHARCHIVE_HSTS_ENABLED</li> <li>CSP policy documented in API consumer guide</li> <li>Evidence: XSS/injection attack prevention, HTTPS enforcement</li> <li>Category: Privacy/Security/Ethics</li> </ul> </li> <li> <p>Add request size limits to backend API (S: 1-2h) [COMPLETED 2026-02-12]</p> <ul> <li>Add request body size limit middleware (e.g., 1MB max for POST /api/reports)</li> <li>Add query parameter length limits</li> <li>Return proper 413 Payload Too Large responses</li> <li>Evidence: Prevents abuse via oversized payloads</li> <li>Category: Privacy/Security/Ethics</li> </ul> </li> <li> <p>Add automated dependency update policy (S: 1-2h) [COMPLETED 2026-02-12]</p> <ul> <li>\u2705 Frontend: .github/dependabot.yml added with weekly NPM + GitHub Actions updates, 5 PR limit</li> <li>\u2705 Backend: Dependabot configured</li> <li>\u2705 Datasets: Dependabot configured</li> <li>\u23f3 Document dependency update policy in CONTRIBUTING.md (pending)</li> <li>Evidence: Proactive security management</li> <li>Category: Privacy/Security/Ethics</li> <li>Commit: 9818c0c (frontend)</li> </ul> </li> </ol>"},{"location":"planning/roadmap/#accessibility-inclusive-design-health-advocate","title":"Accessibility &amp; Inclusive Design (Health Advocate)","text":"<ol> <li> <p>Add accessibility (a11y) testing to CI (M: 1-2 days) [COMPLETED 2026-02-13]</p> <ul> <li>\u2705 Frontend: Installed and configured vitest-axe for automated a11y testing</li> <li>\u2705 Added eslint-plugin-jsx-a11y with WCAG 2.1 AA rules to eslint config</li> <li>\u2705 Created a11y test suite for home page (EN/FR) and static pages (about, methods, contact, researchers)</li> <li>\u2705 Created docs/accessibility.md with WCAG 2.1 Level AA conformance statement</li> <li>\u2705 Configured TypeScript types for vitest-axe matchers</li> <li>\u2705 Documented testing methodology, known limitations, and roadmap</li> <li>\u2705 All 84 tests passing including 12 a11y tests</li> <li>Evidence: Automated a11y testing with axe-core, ESLint a11y linting, formal documentation</li> <li>Category: Privacy/Security/Ethics</li> <li>Commit: 2bb1f7b (frontend), 37820b0 (backend roadmap update)</li> </ul> </li> <li> <p>Create formal accessibility audit document (M: 1-2 days)</p> <ul> <li>Run axe-core or Lighthouse accessibility audit on all public pages</li> <li>Document results in <code>docs/accessibility-audit.md</code></li> <li>List WCAG 2.1 AA conformance status per criterion</li> <li>Create remediation plan for failures</li> <li>Evidence: Tangible accessibility commitment, health advocacy</li> <li>Category: Privacy/Security/Ethics</li> </ul> </li> <li> <p>Add frontend error boundary components (M: 1 day)</p> <ul> <li>Currently NO error.tsx files exist</li> <li>Verify error.tsx for key route segments</li> <li>Add global error boundary with bilingual messaging</li> <li>Add loading.tsx skeletons for data-heavy pages</li> <li>Test error states in CI</li> <li>Evidence: Graceful error handling, user-centered design</li> <li>Category: Reliability/Quality</li> </ul> </li> </ol>"},{"location":"planning/roadmap/#documentation-transparency-communication","title":"Documentation &amp; Transparency (Communication)","text":"<ol> <li> <p>Generate and publish OpenAPI spec (M: 1 day)</p> <ul> <li>FastAPI generates spec automatically - ensure complete and accurate</li> <li>Publish to GitHub Pages alongside MkDocs site</li> <li>Add Swagger UI or Redoc endpoint to public API</li> <li>Link from README and API consumer guide</li> <li>Evidence: Professional API documentation, machine-readable</li> <li>Category: Communication/Documentation</li> </ul> </li> <li> <p>Create data retention schedule table (S: 2h)</p> <ul> <li>Expand <code>data-handling-retention.md</code> with explicit retention windows:</li> <li>Server logs: X days</li> <li>Issue reports: until resolved + X days</li> <li>Usage metrics: aggregated daily, raw dropped after X days</li> <li>Database backups: 14 days</li> <li>WARCs: permanent (archival)</li> <li>Add to governance page as public summary</li> <li>Evidence: Formalized data governance</li> <li>Category: Privacy/Security/Ethics</li> </ul> </li> <li> <p>Add disaster recovery SLO (RTO/RPO) (S: 1-2h)</p> <ul> <li>Define Recovery Time Objective (RTO) and Recovery Point Objective (RPO) in <code>service-levels.md</code></li> <li>Document last tested recovery time (from incident notes)</li> <li>Add to disaster recovery playbook</li> <li>Evidence: Quantified operational maturity</li> <li>Category: Reliability/Quality</li> </ul> </li> <li> <p>Write first-responder / on-call runbook (S: 2-3h)</p> <ul> <li>Create <code>docs/operations/playbooks/first-responder-runbook.md</code></li> <li>Cover: site down check, backend health, common failures, escalation</li> <li>Link from ops cadence checklist</li> <li>Evidence: Operational readiness for team growth</li> <li>Category: Reliability/Quality</li> </ul> </li> <li> <p>Create change management runbook (S: 2-3h)</p> <ul> <li>Create <code>docs/operations/playbooks/change-management.md</code></li> <li>Cover: PR review, CI requirements, staging verification, deployment, rollback</li> <li>Reference existing staging/production checklists</li> <li>Evidence: Process maturity, governance</li> <li>Category: Professionalism/Governance</li> </ul> </li> <li> <p>Formalize ethics/research exemption statement (S: 1-2h)</p> <ul> <li>Add section to governance page: archives public government content, no personal data, not human subjects research</li> <li>Reference institutional guidelines if applicable</li> <li>Note archived content is not medical advice (already in terms)</li> <li>Evidence: Ethical awareness, health advocate role</li> <li>Category: Privacy/Security/Ethics</li> </ul> </li> </ol>"},{"location":"planning/roadmap/#observability-operations-professional","title":"Observability &amp; Operations (Professional)","text":"<ol> <li> <p>Add error tracking integration (Sentry setup) (M: 1 day)</p> <ul> <li>Add Sentry SDK to backend (FastAPI) and frontend (Next.js)</li> <li>Configure source maps for frontend</li> <li>Set up error alerts for critical paths (search failures, indexing failures)</li> <li>Document in ops monitoring guide</li> <li>Evidence: Production-grade error monitoring, dashboard for portfolio</li> <li>Category: Reliability/Quality</li> <li>Prereqs: Sentry account (free tier available)</li> </ul> </li> <li> <p>Add automated uptime monitoring badge (S: 1-2h)</p> <ul> <li>Set up UptimeRobot or similar for healtharchive.ca and API</li> <li>Add uptime badge to README</li> <li>Display uptime on status page</li> <li>Evidence: Public uptime percentage, operational reliability</li> <li>Category: Reliability/Quality</li> <li>Prereqs: External service account (free tier)</li> </ul> </li> <li> <p>Add public status page content (M: 1 day)</p> <ul> <li><code>/status</code> page exists but needs real uptime data</li> <li>Wire to <code>/api/health</code> endpoint for live status</li> <li>Display historical uptime percentage (from usage metrics)</li> <li>Show last crawl timestamp and next scheduled crawl</li> <li>Evidence: Production-grade status page</li> <li>Category: Communication/Documentation</li> </ul> </li> <li> <p>Add API versioning headers (S: 1-2h) [COMPLETED 2026-02-12]</p> <ul> <li>Add <code>X-API-Version: 1</code> response header to all API endpoints</li> <li>Document versioning strategy in API consumer guide</li> <li>Add deprecation policy to docs</li> <li>Evidence: Forward-thinking API design</li> <li>Category: Reliability/Quality</li> </ul> </li> </ol>"},{"location":"planning/roadmap/#frontend-quality-performance-professional","title":"Frontend Quality &amp; Performance (Professional)","text":"<ol> <li> <p>Consolidate bilingual strings (remove inline ternaries) (L: 1-2 weeks)</p> <ul> <li>Currently ~80+ inline <code>locale === \"fr\"</code> ternaries scattered</li> <li>Move all into <code>src/lib/siteCopy.ts</code> or locale dictionaries</li> <li>Use <code>pickLocalized()</code> helper consistently</li> <li>Add lint rule to prevent new inline ternaries</li> <li>Evidence: Proper i18n architecture, maintainability</li> <li>Category: Reliability/Quality</li> </ul> </li> <li> <p>Add automated performance/Lighthouse testing (M: 1 day)</p> <ul> <li>Add Lighthouse CI to frontend CI pipeline</li> <li>Set performance budgets (LCP &lt; 2.5s, FID &lt; 100ms, CLS &lt; 0.1)</li> <li>Add bundle size tracking</li> <li>Generate performance reports on PRs</li> <li>Evidence: Performance metrics for portfolio</li> <li>Category: Reliability/Quality</li> </ul> </li> <li> <p>Add automated link checking to frontend CI (S: 1-2h)</p> <ul> <li>Backend docs already have Lychee link checking</li> <li>Add link check step to <code>frontend-ci.yml</code></li> <li>Check all internal links in built site</li> <li>Run as advisory initially, then promote to blocking</li> <li>Evidence: No broken links on production site</li> <li>Category: Reliability/Quality</li> </ul> </li> <li> <p>Add coverage badges to READMEs (S: 1-2h)</p> <ul> <li>Generate coverage badges from CI (Codecov or shields.io)</li> <li>Add CI status badges to all 3 READMEs</li> <li>Add license badge</li> <li>Evidence: Visual quality indicators</li> <li>Category: Communication/Documentation</li> <li>Prereqs: Coverage reporting in CI (#12)</li> </ul> </li> </ol>"},{"location":"planning/roadmap/#portfolio-impact-documentation-communication","title":"Portfolio &amp; Impact Documentation (Communication)","text":"<ol> <li> <p>Create portfolio-ready project summary page (M: 1 day)</p> <ul> <li>Create <code>docs/project-summary.md</code> or <code>/project</code> frontend page</li> <li>Include: mission, architecture diagram, tech stack, metrics (snapshots, pages, uptime), timeline, governance</li> <li>Keep factual and concise</li> <li>Evidence: One-page summary for ABS/portfolio</li> <li>Category: Communication/Documentation</li> </ul> </li> <li> <p>Generate architecture diagrams (Mermaid/D2) (M: 1 day)</p> <ul> <li>Create diagrams for: system architecture (frontend/backend/VPS/datasets), data flow (crawl \u2192 WARC \u2192 index \u2192 API \u2192 UI), job lifecycle state machine</li> <li>Embed in <code>docs/architecture.md</code> and project summary</li> <li>Evidence: Visual architecture for portfolio/presentations</li> <li>Category: Communication/Documentation</li> </ul> </li> <li> <p>Create public changelog page on frontend (M: 1 day)</p> <ul> <li>Create <code>/changelog</code> page on frontend (changelog process doc exists)</li> <li>Pull from release notes or manually curated entries</li> <li>Include dates, categories (feature, fix, ops), brief descriptions</li> <li>Evidence: Visible changelog shows active development</li> <li>Category: Communication/Documentation</li> </ul> </li> <li> <p>Create automated WARC/data integrity report (M: 1 day)</p> <ul> <li>CI job or scheduled script generates report: total snapshots, WARC count, checksum verification, last successful crawl per source</li> <li>Publish to docs site or CI artifact</li> <li>Evidence: Research-grade data quality metrics</li> <li>Category: Scholarship/Evaluation</li> </ul> </li> </ol> <p>Total: 42 autonomous work items</p> <p>Effort breakdown: - S (Small: 1-3h): 18 items - M (Medium: 1-2 days): 22 items - L (Large: 1-4 weeks): 2 items</p> <p>Category breakdown: - Privacy/Security/Ethics: 9 items - Reliability/Quality: 12 items - Professionalism/Governance: 6 items - Communication/Documentation: 8 items - Scholarship/Evaluation: 4 items - Leadership/Collaboration: 3 items</p> <p>Quickest high-value wins (S effort): 1. CITATION.cff (#1) 2. SECURITY.md (#2) 3. CODE_OF_CONDUCT.md (#3) 4. LICENSE for datasets (#4) 5. .mailmap normalization (#6) 6. RSS feed discovery (#11) 7. Request ID logging (#14) 8. pip-audit/npm-audit blocking (#18)</p> <p>Items requiring external accounts (free tier): - Error tracking (#31): Sentry account - Uptime monitoring (#32): UptimeRobot or similar</p> <p>Items excluded (require human intervention): - Partner outreach, verifier letters - Conference presentations, user testimonials - IRB/ethics board consultation - Financial/hosting decisions - VPS production access</p>"},{"location":"planning/roadmap/#adjacent-optional-in-this-monorepo-not-core-ha","title":"Adjacent / optional (in this monorepo, not core HA)","text":"<ul> <li><code>rcdc/CDC_zim_mirror</code>: add startup DB sanity checks and clearer failure modes (empty/invalid LevelDB, missing prefixes, etc.).</li> </ul>"},{"location":"planning/implemented/","title":"Implemented plans (archive)","text":"<p>This folder contains historical implementation plans that have already been executed.</p> <p>Implemented plans:</p> <ul> <li><code>2025-12-24-6-phase-upgrade-roadmap-2025.md</code></li> <li><code>2025-12-24-sequential-implementation-plan.md</code></li> <li><code>2025-12-24-ops-observability-and-private-usage.md</code></li> <li><code>2025-12-25-compare-live.md</code></li> <li><code>2026-01-03-ci-e2e-smoke.md</code></li> <li><code>2026-01-03-ci-e2e-smoke-hardening.md</code></li> <li><code>2026-01-03-crawl-safe-roadmap-batch.md</code></li> <li><code>2026-01-03-ops-automation-verification-json.md</code></li> <li><code>2026-01-03-ops-automation-verifier-improvements.md</code></li> <li><code>2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity.md</code></li> <li><code>2026-01-17-documentation-architecture-improvements.md</code></li> <li><code>2026-01-17-ops-tiering-alerting-and-incident-followups.md</code></li> <li><code>2026-01-17-disaster-recovery-and-escalation-procedures.md</code></li> <li><code>2026-01-03-search-ranking-and-snippets-v3.md</code></li> <li><code>2026-01-19-annual-crawl-resiliency-hardening.md</code></li> <li><code>2026-01-23-annual-crawl-throughput-and-artifacts.md</code></li> <li><code>2026-01-24-infra-error-and-storage-hotpath-hardening.md</code></li> <li><code>2026-01-27-archive-tool-hardening-and-ops-improvements.md</code></li> <li><code>2026-01-28-patch-job-config-and-integration-tests.md</code></li> <li><code>2026-01-29-warc-discovery-consistency.md</code></li> <li><code>2026-01-29-warc-manifest-verification.md</code></li> <li><code>2026-02-01-disk-usage-investigation.md</code></li> <li><code>2026-02-01-operational-resilience-improvements.md</code></li> <li><code>2026-02-06-ci-schema-and-governance-guardrails.md</code></li> <li><code>2026-02-06-roadmap-backlog-items.md</code></li> <li><code>2026-02-06-storage-watchdog-observability-hardening.md</code></li> <li><code>2026-02-07-deploy-workflow-hardening.md</code></li> </ul>"},{"location":"planning/implemented/#guidelines","title":"Guidelines","text":"<ul> <li>Compressed format: Plans &gt;200 lines should be compressed to summary format (~40-80 lines) after implementation. See <code>docs/documentation-guidelines.md</code> for the template.</li> <li>Dated filenames: Use <code>YYYY-MM-DD-&lt;short-slug&gt;.md</code>.</li> <li>Canonical docs first: Ensure outcomes are captured in canonical docs under <code>docs/deployment/</code>, <code>docs/operations/</code>, or <code>docs/development/</code>. The plan here serves as supporting history.</li> <li>Historical detail: Detailed phase-by-phase narratives are preserved in git history.</li> <li>Minimal edits: Treat these as a record; avoid ongoing edits except for small errata.</li> </ul>"},{"location":"planning/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/","title":"HealthArchive 6-Phase Upgrade Roadmap (Implemented 2025-12-24)","text":"<p>Status: Implemented | Scope: Comprehensive upgrade program (Phases 0\u20136) covering narrative tightening, governance, change tracking, and researcher tools.</p> <p>For current operations, see <code>docs/operations/README.md</code> and <code>docs/operations/healtharchive-ops-roadmap.md</code>.</p>"},{"location":"planning/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#outcomes","title":"Outcomes","text":"<ul> <li>Phase 0 (Narrative): Standardized mission statement; \"archived, not current\" messaging on all workflow pages</li> <li>Phase 1 (Governance): Published Governance, Terms, Privacy, Changelog, and Report-an-issue flow</li> <li>Phase 2 (Stats): <code>/status</code> and <code>/impact</code> pages with real archive metrics</li> <li>Phase 3 (Change Tracking): Change detection, compare views, RSS feed for changes</li> <li>Phase 4 (Researcher Tools): Timeline views, citation helpers, export endpoints</li> <li>Phase 5 (Monitoring): Prometheus/Grafana observability stack via Tailscale</li> <li>Phase 6 (Refinement): Ongoing iteration on search, snippets, and ops automation</li> </ul>"},{"location":"planning/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li>Architecture: architecture.md</li> <li>Production runbook: deployment/production-single-vps.md</li> <li>Annual campaign: operations/annual-campaign.md</li> <li>Replay service: deployment/replay-service-pywb.md</li> <li>Cross-repo config: deployment/environments-and-configuration.md</li> <li>Observability: operations/observability-and-private-stats.md</li> </ul>"},{"location":"planning/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Single VPS model: Postgres + API + worker + storage on one Hetzner server</li> <li>Tailscale-only SSH: No public SSH access; private observability via tailnet</li> <li>CSP security headers: Restrictive Content-Security-Policy in report-only mode</li> <li>No AI summaries: Preserve provenance; avoid medical interpretation features</li> <li>Aggregate-only usage: Daily counters only; no per-user tracking</li> </ul>"},{"location":"planning/implemented/2025-12-24-6-phase-upgrade-roadmap-2025/#historical-context","title":"Historical Context","text":"<p>This was the foundational multi-phase upgrade that established the current architecture. The detailed phase-by-phase narrative (2,400+ lines) is preserved in git history. Key appendices covered:</p> <ul> <li>Appendix A: Stats pages structure</li> <li>Appendix B: /status page metrics</li> <li>Appendix C: /impact page design</li> <li>Appendix D: Changelog template</li> <li>Appendix E: Report-an-issue categories</li> <li>Appendix F: Governance page outline</li> <li>Appendix G: Change tracking decisions</li> <li>Appendix H: Digest/RSS structure</li> </ul>"},{"location":"planning/implemented/2025-12-24-ops-observability-and-private-usage/","title":"Ops Observability + Private Usage Dashboards (Implemented 2025-12-24)","text":"<p>Status: Implemented | Scope: Prometheus/Grafana observability stack with Tailscale-only access for ops health and aggregate usage metrics.</p>"},{"location":"planning/implemented/2025-12-24-ops-observability-and-private-usage/#outcomes","title":"Outcomes","text":"<ul> <li>Prometheus: Scrapes backend <code>/metrics</code>, node exporter, and postgres exporter on loopback</li> <li>Grafana: Private stats surface accessible only via Tailscale (SSH port-forward or Tailscale Serve)</li> <li>Dashboards: Ops overview, pipeline health, search performance, private usage, impact summary</li> <li>Alerting: Minimal high-signal alerts via Prometheus rules + Alertmanager webhook routing</li> <li>Admin Proxy: Loopback-only proxy for <code>/api/admin/**</code> without manual token copying</li> <li>Usage Metrics: Expanded event set (<code>changes_list</code>, <code>compare_view</code>, <code>timeline_view</code>, exports) stored as daily aggregates</li> </ul>"},{"location":"planning/implemented/2025-12-24-ops-observability-and-private-usage/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li>Contract: operations/observability-and-private-stats.md</li> <li>Production runbook: deployment/production-single-vps.md</li> <li>Playbook: playbooks/observability/observability-guide.md (consolidated)</li> </ul>"},{"location":"planning/implemented/2025-12-24-ops-observability-and-private-usage/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Tailscale-only access: No new public DNS or ports; keeps public attack surface unchanged</li> <li>Aggregate-only usage: Daily counters in <code>usage_metrics</code> table; no IPs, query strings, or user IDs</li> <li>Grafana as ops UI: Dashboards serve read-only console needs; bespoke admin UI deferred</li> <li>Least-privilege Postgres role: <code>grafana_readonly</code> with SELECT on specific tables + redacted views for sensitive data</li> </ul>"},{"location":"planning/implemented/2025-12-24-ops-observability-and-private-usage/#scripts-added","title":"Scripts Added","text":"<ul> <li><code>scripts/vps-bootstrap-observability-scaffold.sh</code></li> <li><code>scripts/vps-install-observability-exporters.sh</code></li> <li><code>scripts/vps-install-observability-prometheus.sh</code></li> <li><code>scripts/vps-install-observability-grafana.sh</code></li> <li><code>scripts/vps-install-observability-dashboards.sh</code></li> <li><code>scripts/vps-install-observability-alerting.sh</code></li> </ul>"},{"location":"planning/implemented/2025-12-24-ops-observability-and-private-usage/#historical-context","title":"Historical Context","text":"<p>10-phase sequential implementation (800+ lines) with detailed acceptance criteria, rollback plans, and configuration skeletons. Appendices covered Prometheus config, Grafana data sources, Postgres roles, and usage metrics expansion. Preserved in git history.</p>"},{"location":"planning/implemented/2025-12-24-sequential-implementation-plan/","title":"HealthArchive sequential implementation plan (Implemented 2025-12-24)","text":"<p>Status: Implemented | Scope: Historical, step-by-step execution plan used to bring HealthArchive to a stable single-VPS production baseline (backend + operations) with supporting CI and verification workflows.</p>"},{"location":"planning/implemented/2025-12-24-sequential-implementation-plan/#outcomes","title":"Outcomes","text":"<ul> <li>Established a production baseline discipline:</li> <li>desired-state policy in git + observed snapshots generated on the VPS + drift reports.</li> <li>Documented and operationalized:</li> <li>security/access controls,</li> <li>environment wiring and deployment posture,</li> <li>monitoring/alerting and verification rituals,</li> <li>replay service operations,</li> <li>dataset/export release workflows,</li> <li>automation enablement via systemd templates and sentinel files.</li> </ul>"},{"location":"planning/implemented/2025-12-24-sequential-implementation-plan/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li><code>docs/deployment/production-single-vps.md</code></li> <li><code>docs/deployment/systemd/README.md</code></li> <li><code>docs/operations/baseline-drift.md</code></li> <li><code>docs/operations/monitoring-and-ci-checklist.md</code></li> <li><code>docs/operations/playbooks/core/deploy-and-verify.md</code></li> <li><code>docs/operations/growth-constraints.md</code></li> </ul>"},{"location":"planning/implemented/2025-12-24-sequential-implementation-plan/#decisions-created-if-any","title":"Decisions Created (if any)","text":"<ul> <li>None (this plan consolidated existing posture into canonical docs rather than introducing new decisions).</li> </ul>"},{"location":"planning/implemented/2025-12-24-sequential-implementation-plan/#historical-context","title":"Historical Context","text":"<p>This file originally contained a long, sequential \u201cjournal style\u201d execution checklist. It has been compressed to keep the implemented-plan archive scannable; the full history is preserved in git.</p>"},{"location":"planning/implemented/2025-12-25-compare-live/","title":"Compare-live (snapshot vs live) - implementation plan","text":"<p>Status: implemented (2025-12-25).</p>"},{"location":"planning/implemented/2025-12-25-compare-live/#1-goal","title":"1) Goal","text":"<p>Provide a public compare-to-live workflow that diffs an archived snapshot against the current live page, without caching or persisting live content.</p>"},{"location":"planning/implemented/2025-12-25-compare-live/#2-decisions-locked","title":"2) Decisions (locked)","text":"<ul> <li>Public endpoint (no admin token gating).</li> <li>Always fresh fetch (no caching of live results).</li> <li>Descriptive-only copy; no interpretation or medical advice.</li> <li>Safety controls: timeouts, byte limits, redirect limits, SSRF blocking, and per-process concurrency caps.</li> </ul>"},{"location":"planning/implemented/2025-12-25-compare-live/#3-implementation-summary","title":"3) Implementation summary","text":""},{"location":"planning/implemented/2025-12-25-compare-live/#31-backend","title":"3.1 Backend","text":"<ul> <li>New endpoint: <code>GET /api/snapshots/{snapshot_id}/compare-live</code>.</li> <li>Uses existing diffing pipeline (<code>normalize_html_for_diff</code> + <code>compute_diff</code>).</li> <li>Computes section/line stats and high-noise flag using the same heuristic as change tracking.</li> <li>Response headers set <code>Cache-Control: no-store</code> and <code>X-Robots-Tag: noindex, nofollow</code>.</li> <li>Usage event: <code>compare_live_view</code> recorded into aggregate metrics.</li> </ul>"},{"location":"planning/implemented/2025-12-25-compare-live/#32-frontend","title":"3.2 Frontend","text":"<ul> <li>New route: <code>/compare-live?to=&lt;snapshotId&gt;</code>.</li> <li>Two-step UX: initial landing + explicit \"Fetch live diff\" button (<code>run=1</code>) to prevent prefetch-triggered fetches.</li> <li>Entry points added on snapshot page and archived compare page (prefetch disabled).</li> <li>Compare-live copy warns that the live page is not archived and should not be cited.</li> </ul>"},{"location":"planning/implemented/2025-12-25-compare-live/#33-documentation","title":"3.3 Documentation","text":"<ul> <li>Citation guidance updated to clarify that compare-live is not an archival record.</li> <li>Deployment config docs include compare-live env toggles.</li> </ul>"},{"location":"planning/implemented/2025-12-25-compare-live/#4-config-env","title":"4) Config (env)","text":"<ul> <li><code>HEALTHARCHIVE_COMPARE_LIVE_ENABLED</code> (default <code>1</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_TIMEOUT_SECONDS</code> (default <code>8</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_REDIRECTS</code> (default <code>4</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_BYTES</code> (default <code>2000000</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_ARCHIVE_BYTES</code> (default <code>2000000</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_MAX_CONCURRENCY</code> (default <code>4</code>)</li> <li><code>HEALTHARCHIVE_COMPARE_LIVE_USER_AGENT</code> (default identifies HealthArchive)</li> </ul>"},{"location":"planning/implemented/2025-12-25-compare-live/#5-safety-guardrails","title":"5) Safety guardrails","text":"<ul> <li>Only <code>http</code>/<code>https</code> URLs allowed.</li> <li>Blocks private, loopback, link-local, and reserved IP ranges.</li> <li>Disallows non-80/443 ports and embedded credentials.</li> <li>Redirects are capped and re-validated on every hop.</li> <li>Response body size is capped to avoid memory pressure.</li> </ul>"},{"location":"planning/implemented/2025-12-25-compare-live/#6-testing","title":"6) Testing","text":"<ul> <li>Manual test from a known HTML snapshot:</li> <li><code>/compare-live?to=&lt;id&gt;</code> then \"Fetch live diff\".</li> <li>Verify headers (<code>Cache-Control: no-store</code>), diff output, and live metadata.</li> <li>Non-HTML snapshot should return 422 with a clear error message.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke-hardening/","title":"CI e2e smoke hardening (implementation plan)","text":"<p>Status: implemented (2026-01-03)</p>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke-hardening/#goal","title":"Goal","text":"<p>Improve the end-to-end CI smoke coverage so it is:</p> <ul> <li>Higher-signal (catches \u201c200 but broken/miswired\u201d regressions),</li> <li>Less flaky (no fixed ports, better readiness, robust teardown),</li> <li>Easier to debug (logs uploaded as CI artifacts on failure),</li> <li>Faster (avoid duplicate frontend builds in CI where possible),</li> <li>Bilingual-aware (checks both EN unprefixed routes and <code>/fr/...</code> routes).</li> </ul> <p>This work builds on the existing smoke harness:</p> <ul> <li><code>scripts/ci-e2e-seed.py</code></li> <li><code>scripts/ci-e2e-smoke.sh</code></li> <li><code>scripts/verify_public_surface.py</code></li> <li>CI jobs in both repos.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke-hardening/#scope","title":"Scope","text":""},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke-hardening/#reliability-improvements","title":"Reliability improvements","text":"<ul> <li>Remove fixed ports in the smoke runner; select free ports dynamically.</li> <li>Improve readiness checks:</li> <li>Backend: <code>GET /api/health</code></li> <li>Frontend: <code>GET /archive</code> and <code>GET /fr/archive</code></li> <li>Ensure teardown kills process groups and always prints useful logs on failure.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke-hardening/#higher-signal-assertions","title":"Higher-signal assertions","text":"<p>In <code>scripts/verify_public_surface.py</code>:</p> <ul> <li>Add minimal API contract checks (JSON shape and key invariants).</li> <li>Add minimal frontend HTML assertions:</li> <li>For <code>/archive</code> and <code>/fr/archive</code>, verify a stable <code>&lt;title&gt;</code> marker.</li> <li>For <code>/snapshot/{id}</code> and <code>/fr/snapshot/{id}</code>, verify the page contains the snapshot title     returned by the backend API (seeded deterministically in CI).</li> </ul>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke-hardening/#ci-diagnostics-and-runtime","title":"CI diagnostics and runtime","text":"<ul> <li>Upload smoke logs (and optionally the tiny seeded artifacts) as GitHub Actions   artifacts on failure.</li> <li>Reduce duplicated frontend builds:</li> <li>Frontend CI should build once in the main job and reuse that build for the     e2e smoke job (artifact download), rather than building twice.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke-hardening/#guardrails-tests","title":"Guardrails (tests)","text":"<ul> <li>Add a small backend test that prevents accidental regression of EN+FR frontend   page coverage in the verifier.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke-hardening/#non-goals","title":"Non-goals","text":"<ul> <li>Browser automation (Playwright/Cypress).</li> <li>Replay (pywb) service validation in CI (smoke uses <code>--skip-replay</code>).</li> <li>Search relevance quality evaluation (golden-query harness is separate).</li> </ul>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke-hardening/#implementation-steps","title":"Implementation steps","text":"<ol> <li>Update <code>scripts/ci-e2e-smoke.sh</code>:</li> <li>Add dynamic port selection.</li> <li>Add <code>--skip-frontend-build</code>.</li> <li>Improve readiness, teardown, and failure log reporting.</li> <li>Update <code>scripts/verify_public_surface.py</code>:</li> <li>Strengthen API contract checks.</li> <li>Add minimal HTML assertions for the public pages.</li> <li>CI workflows:</li> <li>Upload smoke logs as artifacts on failure (backend + frontend repos).</li> <li>Frontend repo: reuse the build output for e2e smoke instead of rebuilding.</li> <li>Add tests:</li> <li>Add a test ensuring verifier includes both EN and FR public pages.</li> <li>Update docs:</li> <li>Backend: <code>docs/operations/monitoring-and-ci-checklist.md</code></li> <li>Backend: <code>docs/development/testing-guidelines.md</code></li> <li>Frontend: <code>healtharchive-frontend/docs/development/bilingual-dev-guide.md</code> (if needed)</li> <li>Run:</li> <li><code>healtharchive-backend: make check</code></li> <li><code>healtharchive-frontend: npm run check</code></li> <li><code>healtharchive-backend: ./scripts/ci-e2e-smoke.sh --frontend-dir ../healtharchive-frontend</code></li> <li>Archive this plan under <code>docs/planning/implemented/</code> and update indices.</li> </ol>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke-hardening/#acceptance-criteria","title":"Acceptance criteria","text":"<ul> <li>The smoke script is robust to port collisions (no fixed ports).</li> <li>The verifier fails on \u201cwrong API / empty snapshot page\u201d even if the HTTP status is 200.</li> <li>CI uploads logs on smoke failures.</li> <li>Frontend CI does not rebuild Next twice just to run e2e smoke.</li> <li>Tests protect EN+FR surface coverage.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke/","title":"CI end-to-end smoke coverage (implementation plan)","text":"<p>Status: implemented (2026-01-03)</p>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke/#goal","title":"Goal","text":"<p>Add a fast, local end-to-end smoke check to CI that exercises the public\u2011critical surfaces:</p> <ul> <li>Frontend: <code>GET /archive</code>, <code>GET /snapshot/{id}</code></li> <li>Backend: <code>GET /api/search</code>, <code>GET /api/sources</code>, <code>GET /api/snapshot/{id}</code></li> </ul> <p>The intent is to catch \u201capp starts but user-critical paths fail\u201d regressions early (miswired env, breaking API contract changes, runtime errors, etc.).</p>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke/#scope","title":"Scope","text":"<ul> <li>Reuse the existing <code>healtharchive-backend/scripts/verify_public_surface.py</code>   verifier by running it against locally started backend + frontend.</li> <li>Seed a tiny SQLite database + tiny WARC so that:</li> <li><code>/api/sources</code> is non-empty,</li> <li><code>/api/search</code> returns at least one snapshot,</li> <li><code>/api/snapshots/raw/{id}</code> can serve real HTML.</li> <li>Add GitHub Actions jobs to run the smoke check in:</li> <li><code>healtharchive-backend</code> CI (checks backend changes against latest frontend <code>main</code>),</li> <li><code>healtharchive-frontend</code> CI (checks frontend changes against latest backend <code>main</code>).</li> <li>Update canonical docs to describe the smoke check and local reproduction steps.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke/#non-goals","title":"Non-goals","text":"<ul> <li>Full browser automation (Playwright/Cypress).</li> <li>Replay (pywb) service validation; the CI smoke should not depend on replay being present.</li> <li>Performance benchmarking or search quality evaluation.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke/#implementation-steps","title":"Implementation steps","text":"<ol> <li>Add a backend script to seed an e2e dataset:</li> <li>Create schema (SQLite).</li> <li>Insert seed sources.</li> <li>Write a tiny gzipped WARC with one HTML record.</li> <li>Insert a <code>Snapshot</code> pointing at that WARC record.</li> <li>Add a backend script to run the e2e smoke:</li> <li>Start backend via <code>uvicorn</code> on a fixed local port.</li> <li>Start frontend via <code>next build</code> + <code>next start</code> on a fixed local port.</li> <li>Run <code>verify_public_surface.py</code> with <code>--api-base</code> and <code>--frontend-base</code>      pointing at the local servers, and <code>--skip-replay</code>.</li> <li>Ensure clean shutdown on success/failure.</li> <li>Wire the CI workflows (backend + frontend) to run the smoke script as a    separate job alongside the existing unit/lint checks.</li> <li>Update docs:</li> <li>Add a short section to <code>docs/operations/monitoring-and-ci-checklist.md</code>      describing what the smoke covers and how to run it locally.</li> <li>Remove this item from <code>docs/planning/roadmap.md</code>.</li> </ol>"},{"location":"planning/implemented/2026-01-03-ci-e2e-smoke/#acceptance-criteria","title":"Acceptance criteria","text":"<ul> <li>Running the smoke locally succeeds:</li> <li><code>healtharchive-backend/scripts/ci-e2e-smoke.sh</code></li> <li>Backend CI includes an \u201ce2e smoke\u201d job that is green on <code>main</code>.</li> <li>Frontend CI includes an \u201ce2e smoke\u201d job that is green on <code>main</code>.</li> <li>Canonical docs mention the smoke check and how to reproduce failures locally.</li> </ul>"},{"location":"planning/implemented/2026-01-03-crawl-safe-roadmap-batch/","title":"Crawl-Safe Roadmap Batch (Implemented 2026-01-03)","text":"<p>Status: Implemented | Scope: Four roadmap items implementable without touching the running annual crawl: frontend power controls, dataset pipeline hardening, repo governance, and external outreach scaffolding.</p>"},{"location":"planning/implemented/2026-01-03-crawl-safe-roadmap-batch/#outcomes","title":"Outcomes","text":""},{"location":"planning/implemented/2026-01-03-crawl-safe-roadmap-batch/#frontend-archive-power-controls","title":"Frontend Archive Power Controls","text":"<ul> <li>URL params <code>view=pages|snapshots</code>, <code>includeNon2xx=true</code>, <code>includeDuplicates=true</code> fully wired and documented</li> <li>URL canonicalization removes ineffective flags (e.g., <code>includeDuplicates</code> when <code>view=pages</code>)</li> <li>Regression tests for URL semantics and filter round-trips added</li> </ul>"},{"location":"planning/implemented/2026-01-03-crawl-safe-roadmap-batch/#dataset-release-pipeline-hardening","title":"Dataset Release Pipeline Hardening","text":"<ul> <li>Release bundle validation script: <code>healtharchive-datasets/scripts/validate_release_bundle.py</code></li> <li>Enforces manifest fields, <code>truncated=false</code>, SHA-256 verification, gzip integrity</li> <li>Workflow hardening: concurrency control, timeouts, artifact uploads on failure, immutable tags by default</li> </ul>"},{"location":"planning/implemented/2026-01-03-crawl-safe-roadmap-batch/#repo-governance","title":"Repo Governance","text":"<ul> <li>Documented Mode A (solo-fast) vs Mode B (multi-committer) governance</li> <li>CI check name inventory for all repos (backend, frontend, datasets)</li> <li>Added CODEOWNERS, PR template, and pre-push hook to datasets repo</li> </ul>"},{"location":"planning/implemented/2026-01-03-crawl-safe-roadmap-batch/#external-outreach-scaffolding","title":"External Outreach Scaffolding","text":"<ul> <li>Operator playbook: <code>docs/operations/playbooks/external/outreach-and-verification.md</code></li> <li>Public-safe mentions log: <code>docs/operations/mentions-log.md</code></li> <li>Healthchecks.io alignment completed for all enabled timers</li> </ul>"},{"location":"planning/implemented/2026-01-03-crawl-safe-roadmap-batch/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li>Frontend implementation guide: <code>healtharchive-frontend/docs/implementation-guide.md</code> (query param contract)</li> <li>Dataset release runbook: operations/dataset-release-runbook.md</li> <li>Datasets README: <code>healtharchive-datasets/README.md</code></li> <li>Monitoring/CI: operations/monitoring-and-ci-checklist.md</li> <li>Ops indexes: operations/README.md, playbooks/README.md</li> </ul>"},{"location":"planning/implemented/2026-01-03-crawl-safe-roadmap-batch/#historical-context","title":"Historical Context","text":"<p>This plan was explicitly designed to be \"crawl-safe\" \u2014 implementable without production VPS restarts or DB migrations while the annual scrape was running. Detailed phase notes (800+ lines) preserved in git history.</p>"},{"location":"planning/implemented/2026-01-03-ops-automation-verification-json/","title":"Ops automation verification JSON output (implementation plan)","text":"<p>Status: implemented (2026-01-03)</p>"},{"location":"planning/implemented/2026-01-03-ops-automation-verification-json/#goal","title":"Goal","text":"<p>Make the production posture check <code>./scripts/verify_ops_automation.sh</code> easier to diff and reason about by adding a JSON output mode that summarizes:</p> <ul> <li>which expected timer units exist,</li> <li>which are enabled/active,</li> <li>sentinel file presence (when applicable),</li> <li>worker override presence,</li> <li>expected ops directories presence,</li> <li>and a top-level pass/fail.</li> </ul> <p>This should preserve the current human-readable default output.</p>"},{"location":"planning/implemented/2026-01-03-ops-automation-verification-json/#scope","title":"Scope","text":"<ul> <li>Add a <code>--json</code> option to <code>scripts/verify_ops_automation.sh</code>.</li> <li>Ensure JSON mode writes JSON to stdout only (human logs go to stderr, or are suppressed).</li> <li>Update ops documentation to mention JSON mode and a suggested diff workflow.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ops-automation-verification-json/#non-goals","title":"Non-goals","text":"<ul> <li>Changing which timers are considered required by default.</li> <li>Adding new timers or altering systemd unit files.</li> <li>Building a separate CI job to run this (it is intended for the VPS).</li> </ul>"},{"location":"planning/implemented/2026-01-03-ops-automation-verification-json/#implementation-steps","title":"Implementation steps","text":"<ol> <li>Extend <code>scripts/verify_ops_automation.sh</code>:</li> <li>Parse <code>--json</code>.</li> <li>Collect per-check results into an in-memory summary.</li> <li>Emit a stable JSON object at the end of the run.</li> <li>On hosts without <code>systemctl</code>, in JSON mode emit a \u201cskipped\u201d JSON payload and exit <code>0</code>.</li> <li>Update canonical ops docs:</li> <li><code>docs/operations/automation-verification-rituals.md</code></li> <li><code>docs/operations/ops-cadence-checklist.md</code></li> <li>Update <code>docs/planning/README.md</code> to list this plan as active.</li> </ol>"},{"location":"planning/implemented/2026-01-03-ops-automation-verification-json/#acceptance-criteria","title":"Acceptance criteria","text":"<ul> <li><code>./scripts/verify_ops_automation.sh</code> (default) behaves as before.</li> <li><code>./scripts/verify_ops_automation.sh --json</code> prints a single valid JSON object to stdout.</li> <li>Exit codes remain compatible:</li> <li><code>0</code> when all required checks pass,</li> <li><code>1</code> when required checks fail.</li> <li>JSON includes at least:</li> <li><code>timers[]</code> (name, required, unit_present, enabled_state, active_state, sentinel_path, sentinel_present, meets_required),</li> <li><code>worker_override</code> (path, present, required),</li> <li><code>ops_dirs[]</code> (path, present),</li> <li><code>failures</code>, <code>warnings</code>, <code>ok</code>.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ops-automation-verification-json/#operator-usage","title":"Operator usage","text":"<ul> <li>Human mode: <code>./scripts/verify_ops_automation.sh</code></li> <li>JSON mode (diff-friendly): <code>./scripts/verify_ops_automation.sh --json &gt; /srv/healtharchive/ops/automation/posture.json</code></li> <li>Pretty-print (optional): <code>./scripts/verify_ops_automation.sh --json | python3 -m json.tool</code></li> </ul>"},{"location":"planning/implemented/2026-01-03-ops-automation-verifier-improvements/","title":"Ops automation verifier improvements (implementation plan)","text":"<p>Status: implemented (2026-01-03)</p>"},{"location":"planning/implemented/2026-01-03-ops-automation-verifier-improvements/#roadmap-item-single-prioritized","title":"Roadmap item (single, prioritized)","text":"<p>Harden <code>./scripts/verify_ops_automation.sh</code> so it is:</p> <ul> <li>easier to run strictly (one flag instead of many),</li> <li>easier to consume by automation (clean JSON-only mode + stable schema),</li> <li>easier to maintain (single \u201cexpected timers\u201d inventory),</li> <li>and harder to regress (basic CI test for JSON output invariants).</li> </ul>"},{"location":"planning/implemented/2026-01-03-ops-automation-verifier-improvements/#goals","title":"Goals","text":"<ol> <li>Keep human output useful while keeping JSON output truly machine-friendly.</li> <li>Reduce drift by defining expected timers/dirs in one place.</li> <li>Make \u201cstrict posture\u201d checks easy for operators.</li> <li>Add minimal regression coverage for JSON output.</li> </ol>"},{"location":"planning/implemented/2026-01-03-ops-automation-verifier-improvements/#scope","title":"Scope","text":""},{"location":"planning/implemented/2026-01-03-ops-automation-verifier-improvements/#script-improvements-scriptsverify_ops_automationsh","title":"Script improvements (<code>scripts/verify_ops_automation.sh</code>)","text":"<ul> <li>Add <code>--quiet</code> (suppress all human logs) and <code>--json-only</code> (implies <code>--json --quiet</code>).</li> <li>Add convenience flags:</li> <li><code>--require-all-present</code> (fail if any expected timer unit is missing)</li> <li><code>--require-all-enabled</code> (fail if any expected timer is not enabled; implies <code>--require-all-present</code>)</li> <li>Centralize the expected timers list into a single inventory structure and drive checks from it.</li> <li>Emit a concise end-of-run summary in human mode:</li> <li><code>failures</code>, <code>warnings</code>, <code>missing_optional</code>, <code>disabled_optional</code> (best-effort)</li> <li>Extend JSON with a <code>summary</code> object and (best-effort) <code>unexpected_timers[]</code>.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ops-automation-verifier-improvements/#docs-improvements-ops","title":"Docs improvements (ops)","text":"<ul> <li>Document <code>--json-only</code> and posture snapshot/diff conventions in:</li> <li><code>docs/operations/automation-verification-rituals.md</code></li> <li><code>docs/operations/ops-cadence-checklist.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-03-ops-automation-verifier-improvements/#tests-backend","title":"Tests (backend)","text":"<ul> <li>Add one pytest test that asserts JSON mode invariants:</li> <li>stdout is a single JSON object line</li> <li>parses successfully</li> <li>includes required top-level keys (<code>schema_version</code>, <code>skipped</code>, <code>ok</code>, <code>failures</code>, <code>warnings</code>)</li> </ul>"},{"location":"planning/implemented/2026-01-03-ops-automation-verifier-improvements/#non-goals","title":"Non-goals","text":"<ul> <li>Changing systemd unit file behavior or timer schedules.</li> <li>Expanding the set of production automation units.</li> <li>Adding complex script configuration that would weaken posture checks.</li> </ul>"},{"location":"planning/implemented/2026-01-03-ops-automation-verifier-improvements/#implementation-steps","title":"Implementation steps","text":"<ol> <li>Update <code>scripts/verify_ops_automation.sh</code>:</li> <li>add flags and usage text</li> <li>refactor timers into a single expected inventory</li> <li>implement strict flags + summary + JSON-only output</li> <li>Update ops docs to reflect the new flags and recommended posture snapshot workflow.</li> <li>Add the JSON invariants test under <code>tests/</code>.</li> <li>Run <code>make check</code>.</li> <li>Move this plan to <code>docs/planning/implemented/</code> and update the implemented index.</li> </ol>"},{"location":"planning/implemented/2026-01-03-ops-automation-verifier-improvements/#acceptance-criteria","title":"Acceptance criteria","text":"<ul> <li>Default human output remains readable and exit-code behavior remains compatible.</li> <li><code>--json</code> continues to emit JSON to stdout (logs to stderr).</li> <li><code>--json-only</code> emits only JSON to stdout and nothing else.</li> <li>Strict flags work as intended:</li> <li><code>--require-all-present</code> fails on any missing timer unit</li> <li><code>--require-all-enabled</code> fails on any expected timer not enabled</li> <li>Tests pass in CI: <code>make check</code>.</li> </ul>"},{"location":"planning/implemented/2026-01-03-search-ranking-and-snippets-v3/","title":"Search Ranking + Snippet Quality v3 (Implemented 2026-01-18)","text":"<p>Status: Implemented | Scope: Improve search relevance for broad queries and snippet quality by reducing boilerplate, using Postgres FTS with lightweight heuristics.</p>"},{"location":"planning/implemented/2026-01-03-search-ranking-and-snippets-v3/#outcomes","title":"Outcomes","text":"<ul> <li>Ranking v3: New scoring version with improved hub detection for broad queries like <code>covid</code></li> <li>Snippet extraction: DOM pruning removes nav/header/footer/ARIA boilerplate; content-root selection prefers <code>&lt;main&gt;</code> or <code>&lt;article&gt;</code></li> <li><code>Snapshot.is_archived</code> column: Tri-state flag (NULL/true/false) computed at indexing time from title + banner signals</li> <li>FTS vector input: ~4KB of cleaned main content (up from ~2KB) improves recall</li> <li>Golden query evaluation: Repeatable capture + diff workflow with artifacts stored on VPS</li> </ul>"},{"location":"planning/implemented/2026-01-03-search-ranking-and-snippets-v3/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li>Search quality: operations/search-quality.md</li> <li>Golden queries: operations/search-golden-queries.md</li> <li>Search rollout: deployment/search-rollout.md (v2 \u2192 v3)</li> <li>Architecture: architecture.md (search section)</li> </ul>"},{"location":"planning/implemented/2026-01-03-search-ranking-and-snippets-v3/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>No new search infrastructure: Postgres FTS + heuristics; no Elasticsearch/Meilisearch</li> <li>Generic-first snippet heuristics: Bilingual boilerplate phrase list; Canada.ca-specific rules only if needed</li> <li>Reversible rollout: <code>HA_SEARCH_RANKING_VERSION=v3</code> env var; rollback is single-var flip</li> <li>Archived detection in DB: Cheaper and more stable than query-time snippet heuristics</li> </ul>"},{"location":"planning/implemented/2026-01-03-search-ranking-and-snippets-v3/#scripts-updated","title":"Scripts Updated","text":"<ul> <li><code>scripts/search-eval-capture.sh</code> \u2014 Supports <code>--ranking v3</code></li> <li><code>scripts/search-eval-run.sh</code> \u2014 v2 vs v3 comparisons</li> </ul>"},{"location":"planning/implemented/2026-01-03-search-ranking-and-snippets-v3/#backfill-commands","title":"Backfill Commands","text":"<pre><code># Refresh snippets for recent jobs\nha-backend refresh-snapshot-metadata --job-id &lt;ID&gt;\n\n# Backfill search vectors (off-peak)\nha-backend backfill-search-vector --force\n</code></pre>"},{"location":"planning/implemented/2026-01-03-search-ranking-and-snippets-v3/#historical-context","title":"Historical Context","text":"<p>7-phase sequential implementation (680+ lines) with baseline captures, snippet failure taxonomy, v3 scoring spec, evaluation loop, and rollout procedure. Preserved in git history.</p>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/","title":"Storage Box / <code>sshfs</code> stale mount incident \u2014 prevention, auto-recovery, and data integrity (2026-01-08) \u2014 implementation plan","text":"<p>Status: implemented (completed 2026-01-16; created 2026-01-08)</p> <p>This plan documents a real production incident and turns it into a concrete, sequenced set of repo changes to:</p> <p>1) prevent recurrence (or at least detect it quickly), 2) automate safe recovery, and 3) preserve data integrity and crawl completeness when it happens anyway.</p> <p>It is intentionally very detailed so future operators can use it as:</p> <ul> <li>an incident postmortem reference,</li> <li>a runbook for manual recovery,</li> <li>and the canonical implementation plan for the code changes.</li> </ul>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#executive-summary","title":"Executive summary","text":"<p>What happened: Several job output directories under <code>/srv/healtharchive/jobs/**</code> became unreadable with:</p> <ul> <li><code>OSError: [Errno 107] Transport endpoint is not connected</code></li> </ul> <p>This is a classic FUSE failure mode (commonly seen with <code>sshfs</code>) where a mountpoint stays present but the underlying connection is gone, so basic filesystem operations (<code>stat</code>, <code>is_dir</code>, <code>ls</code>, etc.) fail.</p> <p>Impact: The crawler/worker and related monitoring scripts attempted to <code>stat()</code> files under these mountpoints, threw exceptions, and the system degraded into a \u201clooks alive but isn\u2019t making progress\u201d state:</p> <ul> <li><code>archive_tool</code> crashed on <code>Path.is_file()</code> against a combined log path.</li> <li>The worker loop hit unexpected exceptions while preparing or updating jobs, leaving jobs in confusing states.</li> <li>Crawl metrics timer failed repeatedly because the metrics writer script crashed while probing a job output dir.</li> <li>The annual campaign was blocked: jobs appeared <code>running</code> or ended up <code>failed</code>, with <code>indexed_pages=0</code>.</li> </ul> <p>Recovery: We stopped the worker, identified the stale mountpoints, lazily unmounted them, re-applied the tiering mounts, recovered stale jobs to <code>retryable</code>, re-queued jobs for retry, restarted the worker, and confirmed crawl progress resumed (crawl counters increased; new <code>warc.gz</code> produced; stalled metric stayed 0).</p> <p>Root cause (proximate): job output paths were backed by <code>sshfs</code>/FUSE mounts that became disconnected. \u201cStorage Box mount service active\u201d did not imply these per-job hot paths were healthy. Monitoring and error handling were not robust to this failure mode.</p>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#incident-details-high-fidelity-narrative","title":"Incident details (high-fidelity narrative)","text":""},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#environment-and-components-involved","title":"Environment and components involved","text":"<p>This incident concerns the \u201cproduction single VPS\u201d deployment described in:</p> <ul> <li><code>docs/deployment/production-single-vps.md</code></li> </ul> <p>Key components in play:</p> <ul> <li><code>healtharchive-worker.service</code> (runs the backend worker loop and launches crawls via Docker).</li> <li><code>archive_tool</code> (in-tree crawler CLI under <code>src/archive_tool/</code>).</li> <li><code>healtharchive-crawl-metrics.timer</code> / <code>healtharchive-crawl-metrics.service</code> (writes node_exporter textfile metrics via <code>scripts/vps-crawl-metrics-textfile.py</code>).</li> <li>Storage tiering / mounts (scripts under <code>scripts/</code> + systemd units documented under <code>docs/deployment/systemd/</code>).</li> <li><code>scripts/vps-crawl-status.sh</code> (operator snapshot script used throughout this incident).</li> </ul> <p>The job output directory pattern affected:</p> <ul> <li><code>/srv/healtharchive/jobs/&lt;source&gt;/&lt;job_id_timestamp&gt;__&lt;job_name&gt;</code></li> </ul>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#observable-symptoms-what-we-saw","title":"Observable symptoms (what we saw)","text":"<p>At the filesystem layer:</p> <ul> <li><code>ls</code> against a job output dir failed with: <code>Transport endpoint is not connected</code></li> <li>directories showed as <code>d?????????</code> when listed (unstat\u2019able)</li> </ul> <p>At the service/script layer:</p> <ul> <li>The crawl metrics writer failed on a path probe:</li> <li><code>scripts/vps-crawl-metrics-textfile.py</code> crashed when calling <code>Path.is_dir()</code> because <code>Path.stat()</code> raised <code>OSError: [Errno 107] Transport endpoint is not connected</code>.</li> <li><code>archive_tool</code> crashed on a combined log probe:</li> <li>stack trace showed <code>src/archive_tool/main.py</code> calling <code>Path.is_file()</code> on the stage combined log path, which raised the same Errno 107.</li> <li>The worker loop logged \u201cUnexpected error in worker iteration\u201d with Errno 107 against per-job output paths.</li> </ul> <p>At the job/state layer (as surfaced by <code>scripts/vps-crawl-status.sh</code> and the worker logs):</p> <ul> <li>campaign jobs became blocked in <code>running</code>/<code>queued</code>/<code>failed</code> states that did not reflect real crawl progress.</li> <li>retries were consumed by infrastructure errors (mount failures), not by true crawl failures.</li> <li>indexing remained at 0 because crawls were not completing successfully.</li> </ul>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#timeline-derived-from-the-operator-session-and-systemd-journal","title":"Timeline (derived from the operator session and systemd journal)","text":"<p>This timeline focuses on the causal chain; it is deliberately explicit about each observed step.</p> <p>1) Crawl had been progressing previously    - Earlier snapshots showed job 6 (hc) <code>running</code> with steadily increasing <code>crawled</code> counts and new <code>warc.gz</code> files appearing over time.</p> <p>2) Mountpoints became stale/unreadable    - <code>ls -la</code> under <code>/srv/healtharchive/jobs/hc/...</code> failed with <code>Transport endpoint is not connected</code>.    - The failing directories showed \u201cunknown\u201d metadata (<code>d?????????</code>) indicating <code>stat()</code> failed.</p> <p>3) Metrics writer began failing repeatedly    - <code>healtharchive-crawl-metrics.service</code> exited non-zero because it could not probe a job output dir without crashing.</p> <p>4) Crawl runs began failing in ways that looked like \u201cstalls\u201d    - <code>archive_tool</code> and the worker loop hit hard exceptions, leaving jobs stuck in <code>status=running</code> or <code>status=failed</code> without meaningful progress.</p> <p>5) Operator intervention recovered state    - Worker stopped, stale mountpoints unmounted, tiering re-applied, stale jobs marked retryable, worker restarted.</p>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#root-cause-analysis-rca","title":"Root cause analysis (RCA)","text":""},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#proximate-cause","title":"Proximate cause","text":"<p>One or more <code>sshfs</code>-backed mountpoints under <code>/srv/healtharchive/jobs/**</code> entered a stale/disconnected state where <code>stat()</code> calls failed with:</p> <ul> <li><code>OSError: [Errno 107] Transport endpoint is not connected</code></li> </ul>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#contributing-factors","title":"Contributing factors","text":"<p>C1) \u201cBase mount healthy\u201d did not imply \u201chot paths healthy\u201d.</p> <ul> <li>The Storage Box mount service could remain \u201cactive\u201d while specific mounted subpaths used by the crawler were broken.</li> <li>Monitoring primarily checked:</li> <li><code>/srv/healtharchive/storagebox</code> reachability, and</li> <li>certain systemd unit health,   but did not validate every hot path we depend on.</li> </ul> <p>C2) Scripts and critical paths were not robust to Errno 107.</p> <ul> <li><code>scripts/vps-crawl-metrics-textfile.py</code> crashed instead of emitting \u201cunhealthy\u201d metrics.</li> <li><code>archive_tool</code> crashed instead of classifying the error as \u201cstorage unavailable\u201d and making it recoverable by automation.</li> </ul> <p>C3) Job lifecycle semantics did not separate infra failures from crawl failures.</p> <ul> <li>Infra errors consumed retry budgets and produced confusing states (<code>running</code> + <code>finished_at</code> inconsistencies; <code>crawl_rc</code>/<code>crawl_status</code> not clearly tied to a specific attempt).</li> </ul> <p>C4) Recovery steps existed but were not packaged as a single safe operation.</p> <ul> <li>Operators could fix it (stop worker \u2192 unmount stale \u2192 reapply mounts \u2192 recover jobs), but the system did not do this automatically and safely.</li> </ul>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#impact-assessment","title":"Impact assessment","text":"<p>Primary impact:</p> <ul> <li>Annual campaign blocked (no jobs completed or indexed during the failure window).</li> </ul> <p>Secondary impact:</p> <ul> <li>Monitoring degraded (metrics writer failing), increasing time-to-detection.</li> </ul> <p>Data risk:</p> <ul> <li>Partial writes or truncated <code>warc.gz</code> files are plausible if a mount disconnect occurred mid-write.</li> <li>\u201cCompleteness risk\u201d: if resume state/config is lost and the crawler restarts \u201cfresh\u201d, crawl may recrawl already-covered pages and still miss some queued pages unless we persist resumption state reliably.</li> </ul>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#resolution-what-we-did-step-by-step","title":"Resolution (what we did, step-by-step)","text":"<p>This section is both a record and a proto-runbook.</p>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#1-stabilize-services","title":"1) Stabilize services","text":"<ul> <li>Stop worker to prevent repeated failures while repairing storage:</li> <li><code>sudo systemctl stop healtharchive-worker.service</code></li> </ul>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#2-identify-stale-mountpoints","title":"2) Identify stale mountpoints","text":"<p>Use the playbook:</p> <ul> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#3-repair-mountpoints-re-apply-tiering","title":"3) Repair mountpoints + re-apply tiering","text":"<p>Preferred:</p> <ul> <li><code>sudo systemctl restart healtharchive-storagebox-sshfs.service</code></li> <li><code>sudo systemctl reset-failed healtharchive-warc-tiering.service</code></li> <li><code>sudo systemctl start healtharchive-warc-tiering.service</code></li> </ul>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#4-recover-job-state","title":"4) Recover job state","text":"<p>Safest recovery for \u201cstale running\u201d jobs:</p> <ul> <li><code>ha-backend recover-stale-jobs --older-than-minutes 10 --require-no-progress-seconds 3600 --apply</code></li> </ul> <p>Then restart the worker.</p>"},{"location":"planning/implemented/2026-01-08-storagebox-sshfs-stale-mount-recovery-and-integrity/#implemented-outputs-what-exists-now","title":"Implemented outputs (what exists now)","text":"<p>As of 2026-01-16, this plan is considered implemented; the operational \u201csurface area\u201d is:</p> <ul> <li>Storage hot-path watchdog:</li> <li><code>scripts/vps-storage-hotpath-auto-recover.py</code></li> <li><code>docs/deployment/systemd/healtharchive-storage-hotpath-auto-recover.timer</code></li> <li>sentinel: <code>/etc/healtharchive/storage-hotpath-auto-recover-enabled</code></li> <li>Tiering bind-mount helper:</li> <li><code>scripts/vps-warc-tiering-bind-mounts.sh</code> (supports <code>--repair-stale-mounts</code>)</li> <li><code>docs/deployment/systemd/healtharchive-warc-tiering.service</code> uses <code>--repair-stale-mounts</code></li> <li>Crawl stall recovery:</li> <li><code>scripts/vps-crawl-auto-recover.py</code> (safe-by-default; caps recoveries)</li> <li><code>ha-backend recover-stale-jobs</code> supports <code>--require-no-progress-seconds</code></li> <li>Replay resilience:</li> <li>replay systemd/runbook recommends <code>-v /srv/healtharchive/jobs:/warcs:ro,rshared</code></li> <li>replay smoke tests: <code>healtharchive-replay-smoke.timer</code></li> </ul> <p>Remaining follow-up (not in this plan) is alerting/visibility on the new metrics.</p>"},{"location":"planning/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/","title":"Disaster Recovery and Escalation Procedures (Implemented 2026-01-18)","text":"<p>Status: Implemented | Scope: Comprehensive DR runbook and escalation procedures for single-VPS production environment.</p>"},{"location":"planning/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#outcomes","title":"Outcomes","text":""},{"location":"planning/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#disaster-recovery-runbook","title":"Disaster Recovery Runbook","text":"<ul> <li>RTO/RPO targets: RPO 24 hours, RTO 8 hours, MTTR 4 hours (appropriate for single-VPS)</li> <li>VPS restoration procedure: Complete steps from NAS backup to verified services</li> <li>Database restoration: <code>pg_restore</code> procedure with integrity verification</li> <li>Archive root reconstruction: WARC recovery scenarios and integrity checks</li> <li>Service startup sequence: PostgreSQL \u2192 API \u2192 Worker \u2192 pywb \u2192 Caddy</li> <li>DR drill schedule: Quarterly tabletop/partial, annual full drill</li> </ul>"},{"location":"planning/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#escalation-procedures","title":"Escalation Procedures","text":"<ul> <li>Severity levels: Sev0-Sev3 with response times and actions</li> <li>Escalation path: Primary operator \u2192 break-glass procedures \u2192 documented backups</li> <li>DRI assignments: All areas assigned to operator (single-operator reality)</li> <li>Break-glass procedures: API unresponsive, database unreachable, VPS unreachable</li> <li>Contact storage: Secure location (<code>/etc/healtharchive/contacts.env</code>), not in git</li> </ul>"},{"location":"planning/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#canonical-docs-created","title":"Canonical Docs Created","text":"<ul> <li>deployment/disaster-recovery.md</li> <li>operations/escalation-procedures.md</li> </ul>"},{"location":"planning/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#docs-updated","title":"Docs Updated","text":"<ul> <li>operations/playbooks/core/incident-response.md \u2014 links to escalation</li> <li>deployment/production-single-vps.md \u2014 links to DR</li> <li>operations/risk-register.md \u2014 references DR mitigation</li> <li><code>mkdocs.yml</code> \u2014 navigation updated</li> </ul>"},{"location":"planning/implemented/2026-01-17-disaster-recovery-and-escalation-procedures/#historical-context","title":"Historical Context","text":"<p>7-phase documentation plan (630+ lines) with detailed procedures for each DR scenario, break-glass commands, and drill templates. Preserved in git history.</p>"},{"location":"planning/implemented/2026-01-17-documentation-architecture-improvements/","title":"Documentation Architecture Improvements (Implemented 2026-01-17)","text":"<p>Status: Implemented | Scope: Improve documentation discoverability, navigation, and organization following Di\u00e1taxis framework principles.</p>"},{"location":"planning/implemented/2026-01-17-documentation-architecture-improvements/#outcomes","title":"Outcomes","text":"<ul> <li>Template isolation: Moved 8 templates to <code>docs/_templates/</code> directory</li> <li>Expanded navigation: Critical docs (production runbook, live testing, monitoring) directly accessible in sidebar</li> <li>Playbook categorization: Grouped 32 playbooks into logical categories (Core, Observability, Crawl, Storage, Validation, External)</li> <li>Cross-repo linking: Standardized on GitHub URLs for cross-repo references</li> <li>Audience-based entry points: Added role-based quick start sections to README index pages</li> <li>Navigation coverage: Increased from ~19% to ~60% of docs in mkdocs.yml</li> </ul>"},{"location":"planning/implemented/2026-01-17-documentation-architecture-improvements/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li>documentation-guidelines.md \u2014 Navigation policy, template usage, cross-repo linking conventions</li> <li>operations/README.md \u2014 Audience-based quick start</li> <li>operations/playbooks/README.md \u2014 Category groupings</li> <li>README.md \u2014 Role-based quick start</li> <li><code>mkdocs.yml</code> \u2014 Expanded navigation structure</li> </ul>"},{"location":"planning/implemented/2026-01-17-documentation-architecture-improvements/#key-decisions","title":"Key Decisions","text":"<ul> <li>Preserve README index pattern: Keep READMEs as comprehensive indices; navigation adds direct access to critical docs</li> <li>7\u00b12 rule for top-level: Keep top-level sections manageable (8 items)</li> <li>Progressive disclosure: Start with overview, drill down to details via navigation or README links</li> <li>Templates excluded from nav: <code>_templates/</code> for authoring reference, not published content</li> </ul>"},{"location":"planning/implemented/2026-01-17-documentation-architecture-improvements/#files-moved","title":"Files Moved","text":"<p>Templates were consolidated under <code>docs/_templates/</code> (historical file locations are preserved in git history).</p> Template Canonical path Runbook template <code>docs/_templates/runbook-template.md</code> Playbook template <code>docs/_templates/playbook-template.md</code> Incident template <code>docs/_templates/incident-template.md</code> Decision template <code>docs/_templates/decision-template.md</code> Log templates <code>docs/_templates/*.md</code>"},{"location":"planning/implemented/2026-01-17-documentation-architecture-improvements/#historical-context","title":"Historical Context","text":"<p>7-phase implementation (690+ lines) with detailed navigation proposals, Di\u00e1taxis framework reference, and MkDocs Material feature analysis. Preserved in git history.</p>"},{"location":"planning/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/","title":"Tiering alerting + incident follow-ups (Implemented 2026-01-18)","text":"<p>Status: Implemented | Scope: Close high-leverage operational gaps by enabling tiering health metrics/alerting and completing follow-ups from early 2026 incidents.</p>"},{"location":"planning/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#outcomes","title":"Outcomes","text":"<ul> <li>Tiering health observability:</li> <li>Enabled tiering health metrics written to node_exporter textfile collector.</li> <li>Added alerting for tiering failures and for stale metrics (no updates) to avoid silent breakage.</li> <li>Incident follow-ups closed:</li> <li>Documented the operational failure modes and the recovery procedures discovered during incidents.</li> <li>Captured deferred follow-ups as backlog items where implementation was not yet appropriate.</li> <li>Replay resilience posture:</li> <li>Documented the \u201ccanary replay job\u201d idea as a future option to distinguish replay failures from storage-tiering failures.</li> </ul>"},{"location":"planning/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li><code>docs/operations/monitoring-and-alerting.md</code></li> <li><code>docs/operations/incidents/2026-01-16-replay-smoke-503-and-warctieringfailed.md</code></li> <li><code>docs/operations/incidents/2026-01-09-annual-crawl-hc-job-stalled.md</code></li> <li><code>docs/operations/healtharchive-ops-roadmap.md</code></li> <li><code>docs/deployment/systemd/README.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#decisions-created-if-any","title":"Decisions Created (if any)","text":"<ul> <li>None (follow-ups were captured in canonical incident notes and roadmaps).</li> </ul>"},{"location":"planning/implemented/2026-01-17-ops-tiering-alerting-and-incident-followups/#historical-context","title":"Historical Context","text":"<p>This plan was executed to move existing monitoring/automation from \u201cimplemented but unwired\u201d to \u201cenabled and actionable\u201d on the single-VPS deployment. Detailed implementation history is preserved in git.</p>"},{"location":"planning/implemented/2026-01-17-sla-and-service-commitments/","title":"SLA and Service Commitments (Implemented 2026-01-18)","text":"<p>Status: Implemented | Scope: Explicit service level documentation defining availability, response time, data freshness, and communication commitments.</p>"},{"location":"planning/implemented/2026-01-17-sla-and-service-commitments/#outcomes","title":"Outcomes","text":""},{"location":"planning/implemented/2026-01-17-sla-and-service-commitments/#service-level-objectives","title":"Service Level Objectives","text":"<ul> <li>Availability: 99.5% monthly (allows ~3.6 hours downtime; realistic for single-VPS)</li> <li>Response times: p95 targets per endpoint (/api/health &lt;100ms, /api/search &lt;2s)</li> <li>Data freshness: Primary sources crawled monthly; new content searchable within 48 hours</li> </ul>"},{"location":"planning/implemented/2026-01-17-sla-and-service-commitments/#maintenance-windows","title":"Maintenance Windows","text":"<ul> <li>Routine: 24 hours notice, &lt;30 minutes</li> <li>Major: 72 hours notice, &lt;4 hours</li> <li>Emergency: As needed, documented afterward</li> </ul>"},{"location":"planning/implemented/2026-01-17-sla-and-service-commitments/#communication-commitments","title":"Communication Commitments","text":"<ul> <li>Planned changes in changelog</li> <li>Incidents: public-safe summary when user expectations affected</li> <li>No dedicated status page (future consideration)</li> </ul>"},{"location":"planning/implemented/2026-01-17-sla-and-service-commitments/#performance-baselines","title":"Performance Baselines","text":"<ul> <li>API response time baselines documented</li> <li>Crawl/indexing throughput baselines documented</li> <li>Semi-annual review cadence</li> </ul>"},{"location":"planning/implemented/2026-01-17-sla-and-service-commitments/#canonical-doc-created","title":"Canonical Doc Created","text":"<ul> <li>operations/service-levels.md</li> </ul>"},{"location":"planning/implemented/2026-01-17-sla-and-service-commitments/#docs-updated","title":"Docs Updated","text":"<ul> <li>operations/monitoring-and-ci-checklist.md \u2014 references SLOs</li> <li>operations/playbooks/core/incident-response.md \u2014 references communication commitments</li> <li><code>mkdocs.yml</code> \u2014 navigation updated</li> </ul>"},{"location":"planning/implemented/2026-01-17-sla-and-service-commitments/#key-decisions","title":"Key Decisions","text":"<ul> <li>Best-effort, not contractual: Clear language that these are targets, not SLAs</li> <li>Single-VPS reality: Conservative targets that don't require HA infrastructure</li> <li>Solo operator: Response times reflect operator availability constraints</li> </ul>"},{"location":"planning/implemented/2026-01-17-sla-and-service-commitments/#historical-context","title":"Historical Context","text":"<p>7-phase documentation plan (547+ lines) with detailed target rationale, measurement approaches, and baseline templates. Preserved in git history.</p>"},{"location":"planning/implemented/2026-01-17-test-coverage-critical-business-logic/","title":"Test Coverage: Critical Business Logic (Implemented 2026-01-18)","text":"<p>Status: Implemented | Scope: Comprehensive unit test coverage for 5 critical modules that lacked direct testing.</p>"},{"location":"planning/implemented/2026-01-17-test-coverage-critical-business-logic/#outcomes","title":"Outcomes","text":"<ul> <li><code>diffing.py</code> \u2014 Tests for HTML normalization, heading extraction, banner/noise detection, diff algorithm edge cases</li> <li><code>changes.py</code> \u2014 Tests for backfill computation, incremental change detection, edge cases (first snapshot, gaps, duplicates)</li> <li><code>archive_storage.py</code> \u2014 Tests for WARC consolidation, manifest generation/parsing, storage statistics, deduplication</li> <li><code>pages.py</code> \u2014 Tests for PostgreSQL/SQLite SQL generation, URL normalization, page grouping logic</li> <li><code>archive_tool/strategies.py</code> \u2014 Tests for adaptive crawl strategy selection, restart logic, fallback mechanisms, stall detection</li> </ul>"},{"location":"planning/implemented/2026-01-17-test-coverage-critical-business-logic/#test-files-created","title":"Test Files Created","text":"<ul> <li><code>tests/test_diffing.py</code> \u2014 15-25 tests, HTML fixtures</li> <li><code>tests/test_changes.py</code> \u2014 12-18 tests, database fixtures</li> <li><code>tests/test_archive_storage.py</code> \u2014 12-16 tests, temp file fixtures</li> <li><code>tests/test_pages.py</code> \u2014 12-15 tests, both SQL dialects</li> <li><code>tests/test_archive_tool_strategies.py</code> \u2014 12-18 tests, mocked dependencies</li> </ul>"},{"location":"planning/implemented/2026-01-17-test-coverage-critical-business-logic/#coverage-targets","title":"Coverage Targets","text":"<ul> <li>All modules &gt;80% line coverage</li> <li>All public functions have at least one test</li> <li>Tests are deterministic (no flaky failures)</li> <li>Test execution time &lt;30 seconds per file</li> </ul>"},{"location":"planning/implemented/2026-01-17-test-coverage-critical-business-logic/#test-commands","title":"Test Commands","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test file\npytest tests/test_diffing.py -v\n\n# Run with coverage\npytest --cov=src/ha_backend/diffing tests/test_diffing.py\n</code></pre>"},{"location":"planning/implemented/2026-01-17-test-coverage-critical-business-logic/#historical-context","title":"Historical Context","text":"<p>7-phase sequential implementation (640+ lines) with detailed test categories, fixture design, and coverage verification. Preserved in git history.</p>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/","title":"Annual crawl resiliency hardening (implemented) \u2014 2026-01-19","text":"<p>This is a historical implementation note capturing a production incident follow-up for the 2026 annual campaign.</p>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#problem-statement","title":"Problem statement","text":"<p>During the 2026 annual campaign, we observed:</p> <ul> <li>Retry churn / starvation: jobs could remain <code>retryable</code> and not make forward progress due to queue tie-breaking and long backoff delays.</li> <li>Restart thrash on long annual crawls (notably <code>canada.ca</code>): low timeout thresholds triggered repeated adaptive restarts + long backoffs, reducing throughput.</li> <li>Config error failures (e.g., invalid Zimit args) that consumed time and retries without a clear \u201cthis is a config bug\u201d signal.</li> <li>Limited observability into restart/worker-throttle state without manually inspecting <code>.archive_state.json</code>.</li> </ul> <p>HealthArchive policy is completeness-first archival, so \u201cpage caps\u201d and other early-stop controls are not acceptable for annual crawls.</p>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#goals","title":"Goals","text":"<ul> <li>Make annual crawl defaults more resilient to long-tail timeouts and transient network/protocol noise.</li> <li>Make the annual job pick order deterministic in a single-worker environment.</li> <li>Prevent retry churn on invalid CLI/config failures by classifying them as configuration errors.</li> <li>Improve monitoring to detect restart thrash and state-file health issues.</li> </ul>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#work-implemented","title":"Work implemented","text":""},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#safer-annual-defaults","title":"Safer annual defaults","text":"<ul> <li>Annual job tool options now default to:</li> <li><code>max_container_restarts = 20</code></li> <li><code>error_threshold_timeout = 50</code></li> <li><code>error_threshold_http = 50</code></li> <li><code>backoff_delay_minutes = 2</code></li> <li>Files:</li> <li><code>src/ha_backend/job_registry.py</code></li> </ul>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#deterministic-annual-queue-order","title":"Deterministic annual queue order","text":"<ul> <li><code>ha-backend schedule-annual</code> staggers <code>queued_at</code> timestamps (hc \u2192 phac \u2192 cihr).</li> <li>Files:</li> <li><code>src/ha_backend/cli.py</code></li> <li>Doc note: <code>docs/operations/annual-campaign.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#configuration-error-classification","title":"Configuration error classification","text":"<ul> <li>Persist combined log path when available and classify \u201cinvalid CLI args\u201d failures as <code>infra_error_config</code>.</li> <li>Files:</li> <li><code>src/ha_backend/jobs.py</code></li> </ul>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#auto-recover-guardrails","title":"Auto-recover guardrails","text":"<ul> <li>Auto-recover now enforces annual minimums for restart budget and thresholds when recovering jobs.</li> <li>Files:</li> <li><code>scripts/vps-crawl-auto-recover.py</code></li> </ul>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#monitoring-improvements","title":"Monitoring improvements","text":"<ul> <li><code>vps-crawl-metrics-textfile.py</code> now exports <code>.archive_state.json</code> health + counters:</li> <li>state file probe OK/errno</li> <li>parse OK</li> <li>mtime age seconds</li> <li>current workers, reductions, container restarts, VPN rotations, temp dir count</li> <li>Files:</li> <li><code>scripts/vps-crawl-metrics-textfile.py</code></li> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> <li><code>docs/reference/archive-tool.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#documentation-corrections","title":"Documentation corrections","text":"<ul> <li>Corrected misleading mention of unsupported Zimit page-cap args and clarified that annual crawls should not use caps.</li> <li>Files:</li> <li><code>src/archive_tool/docs/documentation.md</code></li> <li><code>docs/tutorials/debug-crawl.md</code></li> <li><code>docs/architecture.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#tests","title":"Tests","text":"<ul> <li>Added/updated tests for ordering, auto-recover tool option enforcement, config error classification, and state-file metrics.</li> <li>Files:</li> <li><code>tests/test_cli_schedule_annual.py</code></li> <li><code>tests/test_ops_crawl_auto_recover_tool_options.py</code></li> <li><code>tests/test_jobs_persistent.py</code></li> <li><code>tests/test_ops_crawl_metrics_textfile_state.py</code></li> </ul>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#definition-of-done-checked","title":"Definition of done (checked)","text":"<ul> <li> Annual defaults updated in code.</li> <li> Annual queue ordering made deterministic.</li> <li> Invalid CLI/config failures classified as <code>infra_error_config</code>.</li> <li> Auto-recover enforces annual minimum guardrails.</li> <li> Metrics exporter includes state-file health + restart counters.</li> <li> Docs updated to reflect reality and policy (no annual caps).</li> <li> Tests updated/added and pass in CI gate (<code>make ci</code>).</li> </ul>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#follow-ups-not-part-of-this-change","title":"Follow-ups (not part of this change)","text":"<ul> <li>Confirm production deployment and validate dashboards/alerts incorporate the new state-file metrics.</li> <li>Ensure indexing runs after crawl completion and that WARC discovery/counting is consistent between CLI/status output and the indexing pipeline.</li> </ul>"},{"location":"planning/implemented/2026-01-19-annual-crawl-resiliency-hardening/#references","title":"References","text":"<ul> <li>Decision record:</li> <li><code>docs/decisions/2026-01-19-annual-crawl-resiliency-and-queue-order.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/","title":"Annual crawl throughput and WARC-first artifacts (implemented, 2026-01-23)","text":"<p>Status: implemented</p>"},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#goal","title":"Goal","text":"<p>Increase annual crawl throughput on the single production VPS while staying aligned with campaign values:</p> <ul> <li>completeness-first within explicit scope boundaries</li> <li>accuracy and reproducibility</li> <li>search-first readiness (WARCs indexed ASAP)</li> </ul>"},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#constraints","title":"Constraints","text":"<ul> <li>Production: Hetzner <code>cx33</code> (4 vCPU / 8GB RAM / 80GB SSD)</li> <li>Optional StorageBox is for cold storage/tiering, not crawl hot-path I/O.</li> </ul>"},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#changes-implemented","title":"Changes implemented","text":""},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#1-warc-first-annual-pipeline-skip-optional-zim-build","title":"1) WARC-first annual pipeline (skip optional ZIM build)","text":"<ul> <li>Added <code>archive_tool</code> support for skipping the final <code>--warcs</code> ZIM stage:</li> <li><code>archive-tool --skip-final-build</code></li> <li>Wired through DB job config:</li> <li><code>tool_options.skip_final_build = true</code></li> <li>Annual source defaults now set <code>skip_final_build=true</code> so the crawl exits successfully once WARCs are produced, enabling indexing to start sooner.</li> </ul> <p>Rationale:</p> <ul> <li>The backend indexes WARCs; <code>.zim</code> is an optional artifact and is not required for annual \u201cdone\u201d.</li> </ul>"},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#2-container-devshm-tuning-for-stability","title":"2) Container <code>/dev/shm</code> tuning for stability","text":"<ul> <li>Added <code>archive-tool --docker-shm-size &lt;value&gt;</code> and pass through to <code>docker run --shm-size</code>.</li> <li>Annual defaults set <code>docker_shm_size=\"1g\"</code> for browser-driven crawl stability.</li> </ul>"},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#3-modest-parallelism-on-the-single-vps","title":"3) Modest parallelism on the single VPS","text":"<ul> <li>Annual defaults increased to <code>initial_workers=2</code> for all three v1 sources.</li> <li><code>canada.ca</code> sources default <code>stall_timeout_minutes=60</code> to avoid false-stall recoveries on long-tail pages.</li> </ul>"},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#4-reduce-duplicatetrap-like-url-expansion-on-shared-host-canadaca","title":"4) Reduce duplicate/trap-like URL expansion on shared-host canada.ca","text":"<ul> <li>Hardened the allowlist regexes for <code>hc</code> and <code>phac</code> content paths to exclude querystring/fragment variants (assets remain permissive).</li> </ul>"},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#docs-updated","title":"Docs updated","text":"<ul> <li>Archive tool reference and internals docs for new flags.</li> <li>Annual campaign doc clarified WARC-first/search-first posture and PDF indexing non-goal for v1.</li> <li>Production runbook includes swap recommendation and \u201clocal SSD hot-path\u201d guidance.</li> <li>Decision record captured in <code>docs/decisions/2026-01-23-annual-crawl-throughput-and-artifacts.md</code>.</li> </ul>"},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#verification","title":"Verification","text":"<ul> <li>Repo checks: <code>make ci</code> (ruff, mypy, pytest).</li> <li>Tests updated for:</li> <li>new default tool options</li> <li>canada.ca scope regex expectations</li> <li>auto-recover tool option behavior</li> </ul>"},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#follow-ups-ops","title":"Follow-ups (ops)","text":"<ul> <li>Deploy the updated backend and restart the worker on production.</li> <li>For already-created annual jobs, ensure their <code>tool_options</code> reflect the desired values if you want them to take effect on the next retry/recovery cycle.</li> </ul>"},{"location":"planning/implemented/2026-01-23-annual-crawl-throughput-and-artifacts/#references","title":"References","text":"<ul> <li>Decision record: <code>../../decisions/2026-01-23-annual-crawl-throughput-and-artifacts.md</code></li> <li>Annual campaign scope: <code>../../operations/annual-campaign.md</code></li> <li>Production runbook: <code>../../deployment/production-single-vps.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/","title":"Infra-error retry storms + storage hot-path resilience (Implemented 2026-01-24)","text":"<p>Status: Implemented | Scope: Prevent single-VPS retry storms and \u201ceverything stopped\u201d states caused by infrastructure failures (notably Errno 107 stale <code>sshfs</code>/FUSE mountpoints) during the 2026 annual campaign.</p>"},{"location":"planning/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#outcomes","title":"Outcomes","text":"<ul> <li>Worker resilience:</li> <li>Added an infra-error cooldown so jobs that end in <code>crawler_status=infra_error</code> are not immediately re-selected in a tight loop.</li> <li>Improved logging/operator signal around infra errors vs crawl failures.</li> <li>Storage hot-path auto-recover hardening:</li> <li>Detect stale/unreadable mountpoints (Errno 107) not just for running jobs, but also for \u201cnext jobs\u201d (queued/retryable) to prevent retry storms.</li> <li>Conservative recovery sequence with caps/cooldowns and deploy-lock avoidance.</li> <li>Tiering helpers support stale-mount repair flags for safer recovery.</li> <li>Worker auto-start safety:</li> <li>Added a conservative watchdog to start the worker only when it should be running (jobs pending + storage OK), sentinel-gated.</li> <li>Observability:</li> <li>Exported watchdog metrics to node_exporter textfile collector (and documented enablement).</li> </ul>"},{"location":"planning/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li><code>docs/deployment/systemd/README.md</code></li> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-drills.md</code></li> <li><code>docs/operations/playbooks/validation/automation-maintenance.md</code></li> <li><code>docs/operations/thresholds-and-tuning.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#decisions-created-if-any","title":"Decisions Created (if any)","text":"<ul> <li><code>docs/decisions/2026-01-24-single-vps-ops-automation-guardrails-for-crawl-and-storage.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-24-infra-error-and-storage-hotpath-hardening/#historical-context","title":"Historical Context","text":"<p>This plan was triggered by a 2026-01-24 production incident involving Errno 107 hot-path failures. Detailed implementation history and verification steps are preserved in git.</p>"},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/","title":"Archive Tool Hardening and Ops Improvements (2026-01-27)","text":"<p>Implementation plan completed 2026-01-27. This work addressed 35+ improvements across 5 phases, hardening the archive_tool crawler and ops automation.</p>"},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#summary","title":"Summary","text":"<p>This plan improved reliability, observability, and code quality across: - <code>archive_tool</code> subpackage (crawler orchestration) - VPS automation scripts (crawl recovery, metrics, storage hotpath) - Prometheus alerting rules - Backend job management</p>"},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phases","title":"Phases","text":""},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phase-1-pre-crawl-critical-fixes","title":"Phase 1: Pre-Crawl Critical Fixes","text":"<ul> <li>Docker memory/CPU limits (configurable via environment)</li> <li>CIHR stall_timeout override in job registry</li> <li>Thread lock for state file operations</li> <li>OSError handling for stale mounts</li> <li>Pre-crawl output directory writability check</li> <li>Exception handling hardening in main.py</li> </ul>"},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phase-2-operational-automation-improvements","title":"Phase 2: Operational Automation Improvements","text":"<ul> <li>Deploy lock check in crawl-auto-recover.py</li> <li>Prometheus textfile metrics for crawl recovery</li> <li>Lock file to prevent concurrent recovery runs</li> <li>fsync for durable state writes</li> <li>OSError handling in log file discovery</li> </ul>"},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phase-3-monitoring-and-observability","title":"Phase 3: Monitoring and Observability","text":"<ul> <li>Per-job error type counters in crawl metrics</li> <li>Search error type breakdown in runtime metrics</li> <li>Alert for slow crawl rate (&lt;5 ppm for 30m)</li> <li>Alert for high infra_error rate (&gt;=3 in 10m)</li> <li>Alert for stale crawl metrics (&gt;10m old)</li> </ul>"},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phase-4-architecture-and-code-quality","title":"Phase 4: Architecture and Code Quality","text":"<ul> <li>Extracted timeout magic numbers to constants.py</li> <li>Extracted CLI defaults to constants.py</li> <li>Moved late imports to module top in utils.py</li> <li>Consolidated stats regex pattern to constants.py</li> <li>Docker resource limits configurable via environment variables:</li> <li><code>HEALTHARCHIVE_DOCKER_MEMORY_LIMIT</code> (default: \"4g\")</li> <li><code>HEALTHARCHIVE_DOCKER_CPU_LIMIT</code> (default: \"1.5\")</li> <li><code>HEALTHARCHIVE_ZIMIT_DOCKER_IMAGE</code> (default: \"ghcr.io/openzim/zimit\")</li> </ul>"},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#phase-5-documentation","title":"Phase 5: Documentation","text":"<ul> <li>Comprehensive docstrings for monitor.py key functions</li> <li>Comprehensive docstrings for docker_runner.py key functions</li> <li>Comprehensive docstrings for state.py key functions</li> <li>Inline comments for main.py complex logic</li> <li>Inline comments for state.py persistence logic</li> </ul>"},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#key-files-changed","title":"Key Files Changed","text":"<p>archive_tool subpackage: - <code>src/archive_tool/constants.py</code> - Centralized timeout/CLI/resource constants - <code>src/archive_tool/docker_runner.py</code> - Docker orchestration with resource limits - <code>src/archive_tool/main.py</code> - Main loop with improved comments - <code>src/archive_tool/monitor.py</code> - Log monitoring with docstrings - <code>src/archive_tool/state.py</code> - State persistence with thread safety - <code>src/archive_tool/cli.py</code> - CLI using named constants</p> <p>Backend: - <code>src/ha_backend/jobs.py</code> - Retry cap enforcement - <code>src/ha_backend/job_registry.py</code> - CIHR stall timeout - <code>src/ha_backend/runtime_metrics.py</code> - Error type breakdown - <code>src/ha_backend/infra_errors.py</code> - Network errno handling - <code>src/ha_backend/crawl_stats.py</code> - Error count metrics</p> <p>Automation scripts: - <code>scripts/vps-crawl-auto-recover.py</code> - Deploy lock, metrics, lock file - <code>scripts/vps-crawl-metrics-textfile.py</code> - Error type metrics - <code>scripts/vps-storage-hotpath-auto-recover.py</code> - fsync - <code>scripts/vps-worker-auto-start.py</code> - fsync helper</p> <p>Alerting: - <code>ops/observability/alerting/healtharchive-alerts.yml</code> - New alerts</p>"},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#testing","title":"Testing","text":"<p>All existing tests continue to pass. The improvements focus on runtime resilience rather than new testable features. Future work (deferred): add integration tests for main.py stage loop.</p>"},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li><code>docs/reference/archive-tool.md</code> - Points to constants for resource limits</li> <li>Environment variables documented in implementation plan (this file)</li> </ul>"},{"location":"planning/implemented/2026-01-27-archive-tool-hardening-and-ops-improvements/#related","title":"Related","text":"<ul> <li>Previous hardening: <code>2026-01-24-infra-error-and-storage-hotpath-hardening.md</code></li> <li>Previous crawl throughput: <code>2026-01-23-annual-crawl-throughput-and-artifacts.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-28-patch-job-config-and-integration-tests/","title":"2026-01-28: patch-job-config Command and Integration Tests","text":"<p>Status: Implemented 2026-01-28 Scope: Phase 2 implementation from hardening backlog</p>"},{"location":"planning/implemented/2026-01-28-patch-job-config-and-integration-tests/#overview","title":"Overview","text":"<p>Implemented two improvements from the post-hardening backlog: 1. <code>patch-job-config</code> CLI command for modifying job tool_options without recreating jobs 2. Integration tests for <code>archive_tool/main.py</code> stage loop orchestration</p>"},{"location":"planning/implemented/2026-01-28-patch-job-config-and-integration-tests/#implementation-details","title":"Implementation Details","text":""},{"location":"planning/implemented/2026-01-28-patch-job-config-and-integration-tests/#1-patch-job-config-command","title":"1. patch-job-config Command","text":"<p>Location: <code>src/ha_backend/cli.py</code></p> <p>Purpose: Enable patching existing annual jobs with new config options (e.g., <code>skip_final_build</code>, <code>docker_shm_size</code>, <code>stall_timeout_minutes</code>) without recreating them.</p> <p>Features: - Type coercion: <code>true</code>/<code>false</code> \u2192 bool, numeric strings \u2192 int, others \u2192 str - Dry-run by default (shows diff), <code>--apply</code> to save changes - Status restrictions: Only <code>queued</code>, <code>retryable</code>, or <code>failed</code> jobs can be patched - Validates against <code>ArchiveToolOptions</code> dataclass fields - Validates tool_options dependencies (e.g., adaptive_workers requires monitoring)</p> <p>Usage: <pre><code># Dry-run (shows changes without applying)\nha-backend patch-job-config --id 42 \\\n  --set-tool-option skip_final_build=true \\\n  --set-tool-option docker_shm_size=2g\n\n# Apply changes\nha-backend patch-job-config --id 42 \\\n  --set-tool-option skip_final_build=true \\\n  --set-tool-option stall_timeout_minutes=60 \\\n  --apply\n</code></pre></p> <p>Tests: <code>tests/test_cli_patch_job.py</code> (15 tests) - Dry-run and apply modes - Type coercion (bool, int, str) - Status validation - Tool options validation - Error handling</p>"},{"location":"planning/implemented/2026-01-28-patch-job-config-and-integration-tests/#2-integration-tests-for-archive_tool-mainpy","title":"2. Integration Tests for archive_tool main.py","text":"<p>Location: <code>tests/test_archive_tool_main_integration.py</code></p> <p>Purpose: Test main.py orchestration logic without requiring real Docker containers.</p> <p>Coverage (13 tests): - Existing ZIM handling: Exit behavior with/without <code>--overwrite</code> - Docker start failures: Exception handling, None returns - Docker availability: Failure when Docker is unavailable - Dry-run mode: No container starts in dry-run - Output directory: Auto-creation of missing dirs - CrawlState: State file creation, adaptation count persistence, temp dir tracking - Temp directory discovery: <code>discover_temp_dirs()</code> utility - Worker count parsing: Passthrough <code>--workers</code> arg parsing</p> <p>Testing approach: - Mocked Docker operations (no real containers needed) - Tests focus on early exit conditions and state management - Full stage loop tests with threading are complex (log drain thread) - deferred</p>"},{"location":"planning/implemented/2026-01-28-patch-job-config-and-integration-tests/#files-changed","title":"Files Changed","text":"<p>Modified: - <code>src/ha_backend/cli.py</code> - Added <code>cmd_patch_job_config</code> and helpers - <code>AGENTS.md</code> - Added patch-job-config to examples - <code>docs/planning/roadmap.md</code> - Removed completed integration tests item</p> <p>Added: - <code>tests/test_cli_patch_job.py</code> - 15 tests for patch-job-config - <code>tests/test_archive_tool_main_integration.py</code> - 13 integration tests</p>"},{"location":"planning/implemented/2026-01-28-patch-job-config-and-integration-tests/#verification","title":"Verification","text":"<p>CI Status: \u2705 All checks passing - Format check: \u2705 - Lint: \u2705 - Type check: \u2705 - Tests: 183 passed (added 28 new tests)</p> <p>Manual verification: <pre><code># Test patch-job-config help\nha-backend patch-job-config --help\n\n# Test on a dev job (dry-run)\nha-backend patch-job-config --id 1 --set-tool-option skip_final_build=true\n</code></pre></p>"},{"location":"planning/implemented/2026-01-28-patch-job-config-and-integration-tests/#deferred-items-phase-3","title":"Deferred Items (Phase 3)","text":"<p>These items remain in the backlog per the original plan: - Same-day dedupe with provenance preservation - WARC discovery consistency improvements (manifest verification) - Canary replay job (local-only) - Search authority signals tuning (requires measurement first)</p>"},{"location":"planning/implemented/2026-01-28-patch-job-config-and-integration-tests/#related-documentation","title":"Related Documentation","text":"<ul> <li>Implementation plan: This file</li> <li>Roadmap: <code>docs/planning/roadmap.md</code> (updated)</li> <li>Agent/CLI reference: <code>AGENTS.md</code> (updated)</li> <li>Archive tool internals: <code>src/archive_tool/docs/documentation.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-29-warc-discovery-consistency/","title":"WARC Discovery Consistency Improvements (Partial, Updated 2026-01-29)","text":"<p>Status: Partially Implemented | Scope: Keep WARC discovery and WARC counts coherent across status output, indexing, and cleanup.</p>"},{"location":"planning/implemented/2026-01-29-warc-discovery-consistency/#outcomes-implemented","title":"Outcomes (Implemented)","text":"<ul> <li>Added an operator-facing manifest verification command (and associated tests):</li> <li>Plan: <code>2026-01-29-warc-manifest-verification.md</code></li> </ul>"},{"location":"planning/implemented/2026-01-29-warc-discovery-consistency/#deferred-follow-through","title":"Deferred / Follow-Through","text":"<p>Remaining follow-through work stayed as backlog items (not actively implemented in this plan):</p> <ul> <li>Unify and formalize discovery return semantics (e.g., a <code>WarcDiscoveryResult</code> summary type).</li> <li>Improve manifest error handling and reporting.</li> <li>Align <code>scripts/vps-crawl-status.sh</code> with the canonical Python discovery logic.</li> </ul> <p>Backlog tracker:</p> <ul> <li><code>../roadmap.md</code> (WARC discovery consistency follow-through)</li> </ul>"},{"location":"planning/implemented/2026-01-29-warc-discovery-consistency/#historical-context","title":"Historical Context","text":"<p>Detailed analysis and proposed changes are preserved in git history.</p>"},{"location":"planning/implemented/2026-01-29-warc-manifest-verification/","title":"2026-01-29: WARC Manifest Verification Command","text":"<p>Status: Implemented 2026-01-29 Scope: Crawling &amp; indexing reliability (Phase 1 of WARC discovery consistency)</p>"},{"location":"planning/implemented/2026-01-29-warc-manifest-verification/#overview","title":"Overview","text":"<p>Added a <code>verify-warc-manifest</code> CLI command to allow operators to verify that the WARC consolidation manifest accurately reflects files on disk. This addresses the identified gap where operators had no way to verify manifest integrity post-consolidation.</p>"},{"location":"planning/implemented/2026-01-29-warc-manifest-verification/#implementation-details","title":"Implementation Details","text":""},{"location":"planning/implemented/2026-01-29-warc-manifest-verification/#1-verify_warc_manifest-function","title":"1. verify_warc_manifest() Function","text":"<p>Location: <code>src/ha_backend/archive_storage.py</code></p> <p>Purpose: Verify the WARC consolidation manifest against actual files on disk.</p> <p>Checks performed: 1. Manifest exists and is valid JSON 2. All entries in manifest have corresponding files on disk 3. Size matches (if check_size=True) 4. SHA256 matches (if check_hash=True) 5. Detects orphaned WARCs in <code>warcs/</code> not in manifest (warning only)</p> <p>New dataclass: <pre><code>@dataclass\nclass ManifestVerificationResult:\n    valid: bool\n    manifest_path: Path\n    entries_total: int\n    entries_verified: int\n    missing: list[str]\n    size_mismatches: list[tuple[str, int, int]]  # (name, expected, actual)\n    hash_mismatches: list[tuple[str, str, str]]  # (name, expected, actual)\n    orphaned: list[str]\n    errors: list[str]\n</code></pre></p>"},{"location":"planning/implemented/2026-01-29-warc-manifest-verification/#2-verify-warc-manifest-cli-command","title":"2. verify-warc-manifest CLI Command","text":"<p>Location: <code>src/ha_backend/cli.py</code></p> <p>Usage: <pre><code># Verify all WARCs match manifest (size check)\nha-backend verify-warc-manifest --id 42\n\n# Check file presence only (fast)\nha-backend verify-warc-manifest --id 42 --level presence\n\n# Full SHA256 verification (slow)\nha-backend verify-warc-manifest --id 42 --level hash\n\n# JSON output format\nha-backend verify-warc-manifest --id 42 --json\n</code></pre></p> <p>Output example: <pre><code>Job 42: Verifying /path/to/job/warcs/manifest.json\nLevel: size\nManifest entries: 15\nFiles verified: 15/15\nOrphaned files: 0\nStatus: OK\n</code></pre></p> <p>Exit codes: - 0: All checks passed - 1: One or more checks failed (missing files, mismatches, errors)</p>"},{"location":"planning/implemented/2026-01-29-warc-manifest-verification/#3-tests","title":"3. Tests","text":"<p>New test files: - <code>tests/test_cli_verify_manifest.py</code> (14 tests) - CLI command tests - <code>tests/test_warc_discovery.py</code> (13 tests) - WARC discovery edge case tests</p> <p>Test coverage: - Valid manifest verification - Missing manifest handling - Missing WARC file detection - Size mismatch detection - Hash mismatch detection - Orphaned WARC detection - JSON output format - All three verification levels (presence, size, hash) - Edge cases: empty files, symlinks, subdirectories</p>"},{"location":"planning/implemented/2026-01-29-warc-manifest-verification/#files-changed","title":"Files Changed","text":"<p>Modified: - <code>src/ha_backend/archive_storage.py</code> - Added ManifestVerificationResult dataclass and verify_warc_manifest() - <code>src/ha_backend/cli.py</code> - Added cmd_verify_warc_manifest() and argparse configuration</p> <p>Added: - <code>tests/test_cli_verify_manifest.py</code> - 14 tests for manifest verification CLI - <code>tests/test_warc_discovery.py</code> - 13 tests for WARC discovery edge cases</p>"},{"location":"planning/implemented/2026-01-29-warc-manifest-verification/#verification","title":"Verification","text":"<p>CI Status: \u2705 All checks passing - Format check: \u2705 - Lint: \u2705 - Type check: \u2705 - Tests: 197 passed (added 27 new tests)</p>"},{"location":"planning/implemented/2026-01-29-warc-manifest-verification/#remaining-work-deferred","title":"Remaining Work (Deferred)","text":"<p>The following items from the original WARC discovery consistency plan remain in the backlog:</p> <ol> <li>Enhanced WarcDiscoveryResult dataclass - Add structured result with source tracking</li> <li>Improve show-job WARC reporting - Show discovery source (stable/temp)</li> <li>Update vps-crawl-status.sh - Use CLI instead of inline Python</li> <li>Unified discovery method - Reduce fragmentation between discovery entry points</li> </ol> <p>These items are tracked in the parent plan: - <code>docs/planning/implemented/2026-01-29-warc-discovery-consistency.md</code></p>"},{"location":"planning/implemented/2026-01-29-warc-manifest-verification/#related-documentation","title":"Related Documentation","text":"<ul> <li>Existing WARC verification: <code>docs/operations/playbooks/storage/warc-integrity-verification.md</code></li> <li>Archive storage: <code>src/ha_backend/archive_storage.py</code></li> <li>Agent/CLI reference: <code>AGENTS.md</code></li> </ul>"},{"location":"planning/implemented/2026-02-01-disk-usage-investigation/","title":"VPS Disk Usage Investigation (Resolved 2026-02-04)","text":"<p>Status: Resolved | Scope: Explain and fix a large <code>df</code> vs <code>du</code> discrepancy that threatened crawl continuity.</p>"},{"location":"planning/implemented/2026-02-01-disk-usage-investigation/#outcome","title":"Outcome","text":"<p>This was not an ext4 accounting bug. Root cause was annual crawl output directories ending up on the VPS root filesystem instead of being tiered/mounted onto the Storage Box, pushing <code>/</code> into the worker safety threshold.</p>"},{"location":"planning/implemented/2026-02-01-disk-usage-investigation/#canonical-incident-note","title":"Canonical Incident Note","text":"<ul> <li><code>../../operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk.md</code></li> </ul>"},{"location":"planning/implemented/2026-02-01-disk-usage-investigation/#mitigation-summary","title":"Mitigation Summary","text":"<ul> <li>Pause crawls during recovery.</li> <li>Copy affected job output dirs to the Storage Box.</li> <li>Re-apply annual output tiering mounts.</li> <li>Remove local root-disk copies to restore headroom.</li> </ul>"},{"location":"planning/implemented/2026-02-01-disk-usage-investigation/#historical-context","title":"Historical Context","text":"<p>Full investigative commands and intermediate hypotheses are preserved in git history.</p>"},{"location":"planning/implemented/2026-02-01-operational-resilience-improvements/","title":"Operational resilience improvements (Implemented 2026-02-01)","text":"<p>Status: Implemented | Scope: Single-VPS operational hardening for the 2026 annual campaign: reduce disk-pressure wedges, improve crawl recovery safety, and make watchdog behavior observable and drillable.</p>"},{"location":"planning/implemented/2026-02-01-operational-resilience-improvements/#outcomes","title":"Outcomes","text":"<ul> <li>Disk pressure safeguards:</li> <li>Worker pre-crawl headroom gate (prevents starting new jobs when disk is too full).</li> <li>Safe cleanup automation for indexed jobs (<code>temp-nonwarc</code>) plus a disk-threshold safety net.</li> <li>Crawl recovery hardening:</li> <li>Crawl auto-recover watchdog uses a 60m stall threshold with a guard window to avoid disrupting healthy crawls.</li> <li>Safe dry-run drills for crawl auto-recover (simulate stalled jobs; verify planned actions end-to-end).</li> <li>Automated \u201csoft recover\u201d option: mark a stalled job retryable without stopping the worker when another job is progressing.</li> <li>Storage hot-path hardening (Errno 107):</li> <li>Storage hot-path auto-recover watchdog detects stale/unreadable mountpoints and repairs them with bounded, rate-limited actions.</li> <li>Tiering helpers run with stale-mount repair flags to reduce manual intervention during campaigns.</li> <li>Operator UX:</li> <li>Clearer operator workflows for recovering stalled jobs and validating watchdog behavior via drills and metrics.</li> <li>Follow-up captured:</li> <li>Ongoing <code>df</code> vs <code>du</code> disk-usage discrepancy investigation kept as an active plan for a future maintenance window.</li> </ul>"},{"location":"planning/implemented/2026-02-01-operational-resilience-improvements/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li><code>docs/operations/thresholds-and-tuning.md</code></li> <li><code>docs/deployment/systemd/README.md</code></li> <li><code>docs/operations/playbooks/crawl/crawl-stalls.md</code></li> <li><code>docs/operations/playbooks/crawl/crawl-auto-recover-drills.md</code></li> <li><code>docs/operations/playbooks/crawl/cleanup-automation.md</code></li> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> </ul>"},{"location":"planning/implemented/2026-02-01-operational-resilience-improvements/#decisions-created-if-any","title":"Decisions Created (if any)","text":"<ul> <li><code>docs/decisions/2026-02-03-crawl-job-db-state-reconciliation.md</code></li> </ul>"},{"location":"planning/implemented/2026-02-01-operational-resilience-improvements/#historical-context","title":"Historical Context","text":"<p>This plan was executed during the 2026 annual campaign while prioritizing safe-by-default automation (sentinel-gated, capped, observable). Detailed implementation history is preserved in git.</p>"},{"location":"planning/implemented/2026-02-06-ci-schema-and-governance-guardrails/","title":"CI Schema and Governance Guardrails (Implemented 2026-02-06)","text":"<p>Status: Implemented | Scope: Prevent schema-related API regressions and keep merges low-risk with lightweight enforcement.</p>"},{"location":"planning/implemented/2026-02-06-ci-schema-and-governance-guardrails/#outcomes","title":"Outcomes","text":"<ul> <li>Added schema parity tests that exercise real query paths:</li> <li><code>tests/test_ci_schema_parity.py</code></li> <li>Added a migration-required guard for PRs (diff-driven; hard-fail with an exceptions file + expiry):</li> <li><code>scripts/ci_migration_guard.py</code></li> <li><code>.github/migration-guard-exceptions.txt</code></li> <li><code>tests/test_ci_migration_guard.py</code></li> <li>Wired these into the fast CI gate without making it burdensome:</li> <li><code>Makefile</code> targets used by <code>make ci</code></li> <li><code>.github/workflows/backend-ci.yml</code> PR behavior</li> <li>Documented solo-dev branch protection expectations and required checks:</li> <li><code>docs/operations/monitoring-and-ci-checklist.md</code></li> <li><code>docs/deployment/production-rollout-checklist.md</code></li> </ul>"},{"location":"planning/implemented/2026-02-06-ci-schema-and-governance-guardrails/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li><code>docs/development/playbooks/database-migrations.md</code></li> <li><code>docs/operations/monitoring-and-ci-checklist.md</code></li> <li><code>docs/deployment/production-rollout-checklist.md</code></li> </ul>"},{"location":"planning/implemented/2026-02-06-ci-schema-and-governance-guardrails/#validation","title":"Validation","text":"<ul> <li><code>make ci</code> remains green and includes the guardrails.</li> </ul>"},{"location":"planning/implemented/2026-02-06-ci-schema-and-governance-guardrails/#historical-context","title":"Historical Context","text":"<p>Detailed implementation narrative is preserved in git history.</p>"},{"location":"planning/implemented/2026-02-06-roadmap-backlog-items/","title":"Roadmap Backlog Items Implementation","text":"<p>Date: 2026-02-06 Status: Completed Implementation time: ~20 hours (estimated)</p>"},{"location":"planning/implemented/2026-02-06-roadmap-backlog-items/#summary","title":"Summary","text":"<p>Implemented four roadmap items that improved crawl/storage reliability and monitoring without compromising data integrity:</p> <ol> <li>WARC discovery consistency (single shared implementation across scripts + CLI + indexing)</li> <li>Annual crawl guardrails (refuse to crawl if annual outputs are still on root disk)</li> <li>Canary replay job (separates replay failures from tiering/storage failures)</li> <li>Same-day snapshot deduplication (dry-run by default; reversible)</li> </ol>"},{"location":"planning/implemented/2026-02-06-roadmap-backlog-items/#shipped-changes-high-level","title":"Shipped changes (high level)","text":"<ul> <li>WARC discovery: unified WARC discovery logic and removed script drift risk.</li> <li>Guardrails: worker safety checks prevent annual crawls from filling <code>/</code> if tiering fails.</li> <li>Canary replay: a tiny non-tiered job improves replay smoke signal quality.</li> <li>Deduplication: adds a reversible dedupe flag + audit trail; search excludes deduped by default (opt-in to include).</li> </ul>"},{"location":"planning/implemented/2026-02-06-roadmap-backlog-items/#operator-notes","title":"Operator notes","text":"<ul> <li>Canary job setup (optional): <code>ha-backend create-canary-job</code></li> <li>Deduplication is dry-run by default:</li> <li>Dry-run: <code>ha-backend dedupe-snapshots --id &lt;job_id&gt;</code></li> <li>Apply: <code>ha-backend dedupe-snapshots --id &lt;job_id&gt; --apply</code></li> <li>Restore: <code>ha-backend restore-deduped-snapshots --id &lt;job_id&gt; --apply</code></li> </ul>"},{"location":"planning/implemented/2026-02-06-roadmap-backlog-items/#migration","title":"Migration","text":"<ul> <li>The deduplication schema is shipped via Alembic and is required for production:</li> <li><code>alembic/versions/0014_snapshot_deduplication.py</code></li> </ul>"},{"location":"planning/implemented/2026-02-06-roadmap-backlog-items/#verification","title":"Verification","text":"<ul> <li>Local: <code>make ci</code> passes.</li> <li>VPS: deploy script runs Alembic upgrades and the public surface verifier exercises <code>/api/search</code> and <code>/api/changes</code>.</li> </ul>"},{"location":"planning/implemented/2026-02-06-roadmap-backlog-items/#related-documents","title":"Related Documents","text":"<ul> <li>Roadmap: <code>../roadmap.md</code> (updated to remove completed items)</li> <li>WARC discovery plan (partial): <code>2026-01-29-warc-discovery-consistency.md</code></li> <li>Annual disk incident: <code>../operations/incidents/2026-02-04-annual-crawl-output-dirs-on-root-disk.md</code></li> <li>Search quality: <code>../operations/search-quality.md</code></li> <li>Growth constraints: <code>../operations/growth-constraints.md</code></li> </ul>"},{"location":"planning/implemented/2026-02-06-storage-watchdog-observability-hardening/","title":"Storage Watchdog Observability Hardening (Implemented 2026-02-06)","text":"<p>Status: Implemented | Scope: Improve confidence and signal quality for Storage Box hot-path auto-recovery (Errno 107).</p>"},{"location":"planning/implemented/2026-02-06-storage-watchdog-observability-hardening/#outcomes","title":"Outcomes","text":"<ul> <li>Added explicit tests for hot-path auto-recover behavior and failure modes:</li> <li><code>tests/test_ops_storage_hotpath_auto_recover.py</code></li> <li>Added persistent failed-apply alerting (startup-safe, low-noise default severity):</li> <li><code>ops/observability/alerting/healtharchive-alerts.yml</code> (<code>HealthArchiveStorageHotpathApplyFailedPersistent</code>)</li> <li><code>tests/test_ops_alert_rules.py</code></li> <li>Added burn-in tooling for evidence capture and a clean gate:</li> <li><code>scripts/vps-storage-watchdog-burnin-report.py</code></li> <li><code>tests/test_ops_storage_watchdog_burnin_report.py</code></li> <li>Added optional daily snapshot scheduling for burn-in (read-only):</li> <li><code>docs/deployment/systemd/healtharchive-storage-watchdog-burnin-snapshot.service</code></li> <li><code>docs/deployment/systemd/healtharchive-storage-watchdog-burnin-snapshot.timer</code></li> <li><code>scripts/vps-storage-watchdog-burnin-snapshot.sh</code></li> </ul>"},{"location":"planning/implemented/2026-02-06-storage-watchdog-observability-hardening/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li><code>docs/operations/monitoring-and-alerting.md</code></li> <li><code>docs/operations/thresholds-and-tuning.md</code></li> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-recovery.md</code></li> <li><code>docs/operations/playbooks/storage/storagebox-sshfs-stale-mount-drills.md</code></li> <li><code>docs/operations/playbooks/validation/automation-maintenance.md</code></li> </ul>"},{"location":"planning/implemented/2026-02-06-storage-watchdog-observability-hardening/#validation","title":"Validation","text":"<ul> <li><code>make ci</code> remains green.</li> <li>Burn-in gate command returns exit 0 when no unresolved issues exist:</li> <li><code>python3 scripts/vps-storage-watchdog-burnin-report.py --window-hours 168 --require-clean</code></li> </ul>"},{"location":"planning/implemented/2026-02-06-storage-watchdog-observability-hardening/#historical-context","title":"Historical Context","text":"<p>Detailed implementation narrative and tuning is preserved in git history.</p>"},{"location":"planning/implemented/2026-02-07-deploy-workflow-hardening/","title":"Deploy Workflow Hardening (Implemented 2026-02-07)","text":"<p>Status: Implemented | Scope: Make routine deploys boring, repeatable, and crawl-safe on the single VPS.</p>"},{"location":"planning/implemented/2026-02-07-deploy-workflow-hardening/#outcomes","title":"Outcomes","text":"<ul> <li>Added a safe deploy wrapper with an explicit backend-only mode for external frontend outages:</li> <li><code>scripts/vps-hetzdeploy.sh</code> (<code>--mode full|backend-only</code>)</li> <li>Added an installer so operators use a real command instead of fragile shell aliases:</li> <li><code>scripts/vps-install-hetzdeploy.sh</code> (installs <code>hetzdeploy</code> under <code>/usr/local/bin/</code>)</li> <li>Preserved deploy safety properties:</li> <li>refuses dirty trees by default</li> <li>keeps worker restart crawl-safe (skips restart while jobs are running)</li> <li>keeps baseline drift checks enabled by default</li> </ul>"},{"location":"planning/implemented/2026-02-07-deploy-workflow-hardening/#canonical-docs-updated","title":"Canonical Docs Updated","text":"<ul> <li><code>docs/operations/playbooks/core/deploy-and-verify.md</code></li> <li><code>docs/deployment/production-single-vps.md</code></li> <li><code>docs/deployment/production-rollout-checklist.md</code></li> </ul>"},{"location":"planning/implemented/2026-02-07-deploy-workflow-hardening/#validation","title":"Validation","text":"<ul> <li><code>make ci</code> remains green.</li> <li>On the VPS, <code>type hetzdeploy</code> reports <code>/usr/local/bin/hetzdeploy</code> (not an alias).</li> </ul>"},{"location":"planning/implemented/2026-02-07-deploy-workflow-hardening/#historical-context","title":"Historical Context","text":"<p>Detailed iteration is preserved in git history and associated incident notes.</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/","title":"Implementation Plan: Governance, SEO, and Security Foundations","text":"<p>Date: 2026-02-12 Status: Mostly Completed (pending manual vulnerability fixes) Roadmap Items: #1-6 (deferred), #8-11, #14, #18 (partial), #20, #21, #34</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#overview","title":"Overview","text":"<p>This plan implemented the first batch of autonomous work items from the comprehensive 2026-02-11 audit, focusing on open-source governance standards, frontend SEO/discoverability, and CI security foundations. These items provide high admissions/portfolio value with minimal code risk and no VPS access requirements.</p> <p>Items Completed: #8, #9, #10, #11, #14, #20, #21, #34 Items Partially Completed: #18 (requires manual vulnerability fixes) Items Deferred: #1-6 (AI content filtering constraints)</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#implementation-summary","title":"Implementation Summary","text":""},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#phase-1-open-source-governance-deferred","title":"Phase 1: Open Source Governance (Deferred)","text":"<p>Status: Deferred due to AI content filtering constraints</p> <p>Items #1-6 from the roadmap (CITATION.cff, SECURITY.md, CODE_OF_CONDUCT.md, .mailmap, GitHub issue templates, LICENSE for datasets) triggered content policy blocks when attempting automated implementation. These files require manual creation or alternative tooling.</p> <p>Files that were successfully created before filtering: - <code>CITATION.cff</code> in all 3 repos \u2713 - <code>SECURITY.md</code> in all 3 repos \u2713</p> <p>Action Required: Review these files to ensure they meet project requirements. The remaining governance files (CODE_OF_CONDUCT.md, .mailmap, issue templates) need manual implementation.</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#phase-2-frontend-seo-discoverability","title":"Phase 2: Frontend SEO &amp; Discoverability","text":"<p>Status: \u2705 Completed Roadmap Items: #8, #9, #11</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#implemented-features","title":"Implemented Features","text":"<ol> <li>Open Graph + Twitter Card Meta Tags (#8)</li> <li>Modified <code>src/lib/metadata.ts</code> to include OpenGraph and Twitter Card metadata</li> <li>All pages now generate rich social media previews</li> <li>Includes: og:title, og:description, og:url, og:siteName, og:locale, og:type</li> <li> <p>Twitter card: summary format</p> </li> <li> <p>JSON-LD Structured Data (#8)</p> </li> <li>Created <code>src/components/seo/JsonLd.tsx</code> with Schema.org Organization markup</li> <li>Added to root layout for automatic inclusion on all pages</li> <li> <p>Includes organization info and GitHub repository links</p> </li> <li> <p>Sitemap Generation (#9)</p> </li> <li>Created <code>src/app/sitemap.ts</code> generating XML sitemap for all static pages</li> <li>Includes EN/FR alternates for bilingual support</li> <li>Excludes <code>/compare-live</code> (matches robots.txt)</li> <li> <p>Modified <code>src/app/robots.ts</code> to reference sitemap</p> </li> <li> <p>RSS Feed Discovery (#11)</p> </li> <li>Added RSS alternate link to root layout metadata</li> <li>Points to <code>/api/changes/rss</code> for change feed auto-discovery</li> </ol>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#files-modifiedcreated","title":"Files Modified/Created","text":"<p>Modified: - <code>src/lib/metadata.ts</code> - Added OG/Twitter meta generation - <code>src/app/[locale]/layout.tsx</code> - Added JsonLd component and RSS alternate link - <code>src/app/robots.ts</code> - Added sitemap directive</p> <p>Created: - <code>src/components/seo/JsonLd.tsx</code> - JSON-LD structured data component - <code>src/app/sitemap.ts</code> - Sitemap generation</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#validation","title":"Validation","text":"<p>All frontend checks passed: - \u2705 71 tests passed - \u2705 TypeScript compilation successful - \u2705 Build generates sitemap.xml - \u2705 ESLint, Prettier, type-check all passed</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#phase-2-extension-dataset-json-ld-structured-data","title":"Phase 2 Extension: Dataset JSON-LD Structured Data","text":"<p>Status: \u2705 Completed Roadmap Item: #10</p> <p>Added Schema.org Dataset markup to the exports page for academic/research discoverability in Google Dataset Search and similar services.</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#files-created","title":"Files Created","text":"<ul> <li><code>src/components/seo/DatasetJsonLd.tsx</code> - Dataset structured data component</li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#files-modified","title":"Files Modified","text":"<ul> <li><code>src/app/[locale]/exports/page.tsx</code> - Added DatasetJsonLd component</li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#structured-data-includes","title":"Structured Data Includes","text":"<ul> <li>Dataset name, description, license (CC-BY-4.0)</li> <li>Distribution formats (JSON, JSONL, CSV for snapshots and changes)</li> <li>All export endpoints with proper <code>encodingFormat</code> and descriptions</li> <li>Temporal coverage (2024/..)</li> <li>Spatial coverage (Canada)</li> <li>Keywords (public health, web archiving, government websites, metadata, etc.)</li> <li>Data catalog reference (GitHub releases)</li> <li><code>isAccessibleForFree: true</code></li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#validation_1","title":"Validation","text":"<ul> <li>\u2705 All frontend tests pass (71/71)</li> <li>\u2705 Build successful</li> <li>\u2705 JSON-LD structured data will be in page source at <code>/exports</code></li> <li>\u2705 Discoverable by Google Dataset Search and other academic search engines</li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#phase-3-ci-security-quick-wins","title":"Phase 3: CI &amp; Security Quick Wins","text":"<p>Status: Partially Completed Roadmap Items: #14, #18, #21</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#1-request-id-correlation-logging-14","title":"1. Request ID / Correlation Logging (#14)","text":"<p>Status: \u2705 Completed</p> <p>Implemented comprehensive request ID tracking for observability and debugging.</p> <p>Files Created: - <code>src/ha_backend/request_context.py</code> - Context variable management for request IDs - <code>tests/test_request_id.py</code> - Test suite (3 tests, all passing)</p> <p>Files Modified: - <code>src/ha_backend/api/__init__.py</code> - Added request_id_middleware - <code>src/ha_backend/logging_config.py</code> - Added RequestIdFilter and updated log format</p> <p>Features: - Auto-generates UUIDv4 request IDs for every API request - Honors incoming <code>X-Request-Id</code> headers (pass-through) - Returns <code>X-Request-Id</code> in all responses - Injects request ID into all log records via custom filter - Log format: <code>%(asctime)s [%(levelname)s] %(name)s [%(request_id)s]: %(message)s</code></p> <p>Validation: - \u2705 All backend CI checks passed (271 tests) - \u2705 Request ID tests pass (3/3) - \u2705 Format, lint, typecheck all passed</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#2-blocking-dependency-audits-18","title":"2. Blocking Dependency Audits (#18)","text":"<p>Status: \u26a0\ufe0f Partially Completed - Manual Steps Required</p> <p>Infrastructure is in place but requires fixing existing vulnerabilities before enabling blocking behavior.</p> <p>Files Modified: - <code>healtharchive-backend/Makefile</code> - Removed <code>|| true</code> from audit target - <code>healtharchive-backend/.github/workflows/backend-ci.yml</code> - Added pip-audit step - <code>healtharchive-frontend/.github/workflows/frontend-ci.yml</code> - Removed <code>|| true</code> from npm audit</p> <p>Current Vulnerabilities (must fix before enabling):</p> <p>Backend: <pre><code>pillow 11.3.0 \u2192 CVE-2026-25990 \u2192 fix: 12.1.1\npip 25.3 \u2192 CVE-2026-1703 \u2192 fix: 26.0\n</code></pre></p> <p>Frontend: <pre><code>next 16.1.1 \u2192 GHSA-9g9p-9gw9-jx7f, GHSA-h25m-26qc-wcjf, GHSA-5f7q-jpqc-wp7h \u2192 fix: 16.1.6\n</code></pre></p> <p>Manual Steps Required:</p> <ol> <li> <p>Backend (run from <code>healtharchive-backend/</code>):    <pre><code>source .venv/bin/activate\n# Update pillow\npip install --upgrade 'pillow&gt;=12.1.1'\n# Update pip\npip install --upgrade 'pip&gt;=26.0'\n# Verify\npip-audit\n# If passes, commit pyproject.toml changes\n</code></pre></p> </li> <li> <p>Frontend (run from <code>healtharchive-frontend/</code>):    <pre><code>npm audit fix --force\n# Or manually update next in package.json to 16.1.6\nnpm install\nnpm audit --audit-level=high\n# If passes, commit package.json and package-lock.json\n</code></pre></p> </li> <li> <p>Test CI:</p> </li> <li>Backend: <code>make ci &amp;&amp; pip-audit</code></li> <li> <p>Frontend: <code>npm run check &amp;&amp; npm audit --audit-level=high</code></p> </li> <li> <p>Commit and push - CI will now block on audit failures</p> </li> </ol>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#3-dependabot-configuration-21","title":"3. Dependabot Configuration (#21)","text":"<p>Status: \u2705 Completed</p> <p>Added Dependabot configs to all 3 repos for automated dependency updates.</p> <p>Files Created: - <code>healtharchive-backend/.github/dependabot.yml</code> - pip + github-actions, weekly, limit 5 PRs - <code>healtharchive-frontend/.github/dependabot.yml</code> - npm + github-actions, weekly, limit 5 PRs - <code>healtharchive-datasets/.github/dependabot.yml</code> - pip + github-actions, weekly, limit 3 PRs</p> <p>Configuration: - Weekly updates on Mondays - Open PR limits: 5 (backend/frontend), 3 (datasets) - Automatic labels: <code>dependencies</code>, package ecosystem label</p> <p>Validation: - Configs use valid YAML syntax (checked by pre-commit hooks) - Will activate automatically on next push to GitHub</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#phase-4-api-quality-professional-touches","title":"Phase 4: API Quality &amp; Professional Touches","text":"<p>Status: \u2705 Completed Roadmap Item: #34</p> <p>Added API versioning headers for forward compatibility and professional API design.</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#api-versioning-headers-34","title":"API Versioning Headers (#34)","text":"<p>Status: \u2705 Completed</p> <p>Implemented header-based API versioning to support future evolution and compatibility management.</p> <p>Files Modified: - <code>src/ha_backend/api/__init__.py</code> - Added API_VERSION constant and version middleware - <code>tests/test_api_health_and_sources.py</code> - Added version header assertions - <code>tests/test_request_id.py</code> - Added version header assertions - <code>docs/api-consumer-guide.md</code> - Documented versioning strategy and standard headers</p> <p>Features: - Returns <code>X-API-Version: 1</code> on all API responses - Middleware runs after request ID, before security headers - Major version only (semantic: breaking changes increment version) - Documented versioning policy and deprecation strategy</p> <p>Versioning Policy: - Major version changes (1 \u2192 2): Breaking changes - Minor updates (within v1): Additive only - 6-month deprecation notice for breaking changes - Parallel version support during transitions</p> <p>Validation: - \u2705 Health endpoint test passes with version header check - \u2705 Request ID tests pass with version header check - \u2705 All backend CI checks passed (271 tests) - \u2705 Documentation updated with versioning strategy</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#phase-5-security-hardening","title":"Phase 5: Security Hardening","text":"<p>Status: \u2705 Completed Roadmap Item: #20</p> <p>Added request size limits to prevent abuse via oversized payloads.</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#request-size-limits-20","title":"Request Size Limits (#20)","text":"<p>Status: \u2705 Completed</p> <p>Implemented middleware to enforce request body and query string size limits.</p> <p>Files Created: - <code>tests/test_request_size_limits.py</code> - Test suite (5 tests, all passing)</p> <p>Files Modified: - <code>src/ha_backend/config.py</code> - Added size limit configuration functions - <code>src/ha_backend/api/__init__.py</code> - Added request size limit middleware - <code>docs/api-consumer-guide.md</code> - Documented size limits and error responses</p> <p>Size Limits (configurable via environment variables): - Request body: 1MB default (configurable: 1KB - 10MB) - Query string: 8KB default (configurable: 1KB - 64KB)</p> <p>Error Responses: - <code>413 Payload Too Large</code>: Request body exceeds limit - <code>414 URI Too Long</code>: Query string exceeds limit - JSON responses with clear error messages</p> <p>Middleware Order: 1. Request ID generation 2. Request size limits (NEW) 3. API version injection 4. Security headers</p> <p>Validation: - \u2705 All 5 new tests pass - \u2705 All backend CI checks passed (271 tests) - \u2705 Limits are enforced before request processing - \u2705 Error messages are clear and actionable - \u2705 Documentation updated with limits and best practices</p> <p>Security Benefits: - Prevents DoS via large payloads - Prevents memory exhaustion - Enforces reasonable API usage patterns - Configurable limits for different environments</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#phase-6-rate-limiting-middleware","title":"Phase 6: Rate Limiting Middleware","text":"<p>Status: \u2705 Completed Roadmap Item: #17</p> <p>Added IP-based rate limiting middleware to prevent API abuse and ensure fair resource allocation.</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#rate-limiting-middleware-17","title":"Rate Limiting Middleware (#17)","text":"<p>Status: \u2705 Completed</p> <p>Implemented slowapi-based rate limiting with per-endpoint limits and IP-based tracking.</p> <p>Dependencies Added: - <code>slowapi&gt;=0.1.9</code> to pyproject.toml</p> <p>Files Created: - <code>src/ha_backend/rate_limiting.py</code> - Limiter configuration and rate limit constants - <code>tests/test_rate_limiting.py</code> - Test suite (7 tests, all passing)</p> <p>Files Modified: - <code>src/ha_backend/api/__init__.py</code> - Registered limiter and exception handler - <code>src/ha_backend/api/routes_public.py</code> - Added rate limit decorators to search, exports, reports - <code>src/ha_backend/config.py</code> - Added get_rate_limiting_enabled() configuration function - <code>docs/api-consumer-guide.md</code> - Documented rate limits and best practices</p> <p>Rate Limits (per client IP, per minute): - <code>POST /api/reports</code>: 5 requests/minute (spam prevention) - <code>GET /api/exports/*</code>: 10 requests/minute (large payloads) - <code>GET /api/search</code>: 60 requests/minute (CPU-intensive queries) - All other endpoints: 120 requests/minute (default)</p> <p>Features: - IP-based rate limiting using slowapi + in-memory storage - Per-endpoint rate limit decorators - Returns 429 with Retry-After header when exceeded - Includes X-RateLimit-Limit and X-RateLimit-Remaining headers on limited endpoints - Configurable via HEALTHARCHIVE_RATE_LIMITING_ENABLED environment variable - Custom exception handler for proper JSON error responses</p> <p>Error Responses: - <code>429 Too Many Requests</code>: Rate limit exceeded - JSON response with error details and Retry-After header - Example: <code>{\"error\": \"Rate limit exceeded\", \"detail\": \"60 per 1 minute\"}</code></p> <p>Validation: - \u2705 All 7 new tests pass - \u2705 All 271 existing backend tests still pass (278 total) - \u2705 Format, lint, typecheck all passed - \u2705 Rate limits enforced correctly per endpoint - \u2705 Rate limiting can be disabled via environment variable - \u2705 429 responses include proper headers and error format</p> <p>Security Benefits: - Prevents DoS via excessive requests - Fair resource allocation across clients - Per-endpoint limits match resource intensity - IP-based tracking prevents single-client abuse - Configurable for different environments</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#phase-7-content-security-policy-csp-and-hsts-headers","title":"Phase 7: Content Security Policy (CSP) and HSTS Headers","text":"<p>Status: \u2705 Completed Roadmap Item: #19</p> <p>Added Content Security Policy (CSP) and HTTP Strict Transport Security (HSTS) headers to prevent XSS/injection attacks and enforce HTTPS.</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#csp-and-hsts-headers-19","title":"CSP and HSTS Headers (#19)","text":"<p>Status: \u2705 Completed</p> <p>Implemented comprehensive security headers middleware with CSP and HSTS support.</p> <p>Files Modified: - <code>src/ha_backend/config.py</code> - Added CSP and HSTS configuration functions - <code>src/ha_backend/api/__init__.py</code> - Enhanced security_headers_middleware with CSP and HSTS - <code>docs/api-consumer-guide.md</code> - Documented CSP policies and security headers</p> <p>Files Created: - <code>tests/test_security_headers.py</code> - Comprehensive test suite (9 tests, all passing)</p> <p>CSP Policies:</p> <p>For JSON endpoints (most of the API): <pre><code>Content-Security-Policy: default-src 'none'; frame-ancestors 'none'\n</code></pre> - Very restrictive: blocks all resource loading - Prevents XSS and code injection - Prevents the API from being framed</p> <p>For HTML replay endpoints (<code>/api/snapshots/raw/*</code>): <pre><code>Content-Security-Policy: default-src 'none'; script-src 'unsafe-inline' 'unsafe-eval';\n  style-src 'unsafe-inline' *; img-src * data: blob:; font-src * data:;\n  connect-src *; media-src *; object-src 'none'; frame-src *;\n  base-uri 'self'; form-action 'self'\n</code></pre> - Permissive policy for archived HTML replay - Allows inline scripts/styles (required for archived pages) - Allows external resources (images, fonts, media) - Still blocks dangerous features (object/embed tags)</p> <p>HSTS Configuration: <pre><code>Strict-Transport-Security: max-age=31536000; includeSubDomains\n</code></pre> - Enforces HTTPS for 1 year - Includes all subdomains - Configurable max-age via HEALTHARCHIVE_HSTS_MAX_AGE</p> <p>Configuration Options: - <code>HEALTHARCHIVE_CSP_ENABLED</code> (default: true) - Enable/disable CSP headers - <code>HEALTHARCHIVE_HSTS_ENABLED</code> (default: true) - Enable/disable HSTS headers - <code>HEALTHARCHIVE_HSTS_MAX_AGE</code> (default: 31536000) - HSTS max-age in seconds</p> <p>Validation: - \u2705 All 9 new tests pass - \u2705 All 271 existing backend tests still pass (292 total) - \u2705 Format, lint, typecheck all passed - \u2705 CSP correctly applied to JSON and HTML endpoints - \u2705 HSTS header present when enabled - \u2705 CSP and HSTS can be disabled via environment variables - \u2705 All security headers consistently applied across endpoints</p> <p>Security Benefits: - Prevents XSS (Cross-Site Scripting) attacks - Prevents code injection attacks - Prevents clickjacking via frame-ancestors - Enforces HTTPS for 1 year after first visit - Disables sensitive browser features (geolocation, microphone, camera) - Prevents MIME type confusion attacks - Controls referrer information leakage</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#phase-8-test-coverage-thresholds","title":"Phase 8: Test Coverage Thresholds","text":"<p>Status: \u2705 Completed Roadmap Item: #12</p> <p>Added test coverage enforcement to prevent quality regressions and provide concrete portfolio metrics.</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#test-coverage-thresholds-12","title":"Test Coverage Thresholds (#12)","text":"<p>Status: \u2705 Completed</p> <p>Implemented test coverage thresholds with CI enforcement for critical backend modules.</p> <p>Files Created: - <code>docs/development/test-coverage.md</code> - Comprehensive coverage requirements documentation</p> <p>Files Modified: - <code>pyproject.toml</code> - Added pytest and coverage configuration - <code>Makefile</code> - Added coverage, coverage-critical, coverage-target, coverage-report targets - <code>Makefile</code> - Updated check-full to include coverage-critical - <code>.gitignore</code> - Added htmlcov/, htmlcov-critical/, .coverage.* patterns</p> <p>Coverage Results:</p> <p>Critical Modules (API, worker, indexing): - <code>ha_backend/api</code>: 95.81% (routes_admin) / 77.29% (routes_public) - <code>ha_backend/worker</code>: 81.76% - <code>ha_backend/indexing</code>: Mixed (22.55% pipeline - 93.33% deduplication) - Overall: 76.96% coverage (threshold: 75%, goal: 80%)</p> <p>Makefile Targets Added: <pre><code>make coverage           # Full coverage report (all modules)\nmake coverage-critical  # Critical modules only (enforced in check-full)\nmake coverage-target    # Show current threshold and improvement path\nmake coverage-report    # Display HTML report paths\n</code></pre></p> <p>CI Integration: - Enforced in <code>make check-full</code> (pre-deploy quality gate) - NOT enforced in <code>make ci</code> (keeps PR checks fast) - HTML reports: <code>htmlcov/</code> (full) and <code>htmlcov-critical/</code> (critical only)</p> <p>Configuration (<code>pyproject.toml</code>): <pre><code>[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = \"test_*.py\"\n\n[tool.coverage.run]\nsource = [\"src\"]\nomit = [\"*/tests/*\", \"*/test_*.py\", \"*/__pycache__/*\"]\n\n[tool.coverage.report]\nprecision = 2\nshow_missing = true\nexclude_lines = [\"pragma: no cover\", \"if TYPE_CHECKING:\", ...]\n</code></pre></p> <p>Why 75% threshold (not 80%)? - Current coverage: 76.96% on critical modules - Bottleneck: <code>indexing/pipeline.py</code> at 22.55% (background processing, complex file I/O) - 75% is realistic baseline that prevents regressions - 80% is achievable goal with ~100 more test lines - Provides concrete portfolio metric: \"Maintains &gt;75% test coverage with CI enforcement\"</p> <p>Path to 80%: 1. Add integration tests for indexing/pipeline.py 2. Test error paths in WARC processing 3. Test edge cases in text extraction 4. Current bottleneck identified and documented</p> <p>Validation: - \u2705 Coverage-critical passes at 76.96% (above 75% threshold) - \u2705 HTML reports generated successfully - \u2705 Integration into check-full working - \u2705 Comprehensive documentation created - \u2705 .gitignore updated for coverage artifacts - \u2705 All existing tests still pass</p> <p>Quality Benefits: - Prevents coverage regressions on critical modules - Concrete quality metric for admissions/portfolio - HTML reports identify untested code paths - Baseline for incremental improvements - Evidence: \"76.96% test coverage on critical paths, CI-enforced\"</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#phase-9-test-coverage-expansion","title":"Phase 9: Test Coverage Expansion","text":"<p>Status: \u2705 Completed Roadmap Item: #13</p> <p>Expanded backend test coverage with comprehensive edge case, security, and reliability tests.</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#tests-added","title":"Tests Added","text":"<p>1. CORS Header Validation (<code>tests/test_cors_headers.py</code> - 7 tests): - Test CORS middleware configuration and behavior - Test origin handling (wildcard, multiple origins, configured origins) - Test safe HTTP methods (GET, HEAD, OPTIONS) - Test credentials disabled - Test consistent application across endpoints</p> <p>2. Search Query Edge Cases (<code>tests/test_search_edge_cases.py</code> - 14 tests): - SQL injection attempts (8 payloads): <code>' OR '1'='1</code>, <code>'; DROP TABLE</code>, <code>UNION SELECT</code>, etc. - XSS attempts (7 payloads): <code>&lt;script&gt;alert()</code>, <code>&lt;img onerror&gt;</code>, <code>&lt;svg/onload&gt;</code>, etc. - Empty query handling - Very long queries (1000+ chars) - Unicode characters (French, Chinese, Cyrillic, emoji) - Special characters (&amp; @ $ % ? !) - Invalid page numbers and page sizes - Null byte injection - Path traversal attempts (<code>../../../etc/passwd</code>) - Command injection attempts (<code>;ls</code>, <code>|cat</code>, <code>$(whoami)</code>) - NoSQL injection attempts (<code>{\"$gt\": \"\"}</code>) - Invalid source parameter handling</p> <p>3. Concurrent Request Tests (<code>tests/test_concurrent_requests.py</code> - 8 tests): - Concurrent health checks (10 parallel requests) - Concurrent stats requests (10 parallel) - Concurrent source requests (10 parallel) - Concurrent search requests (10 parallel with different queries) - Mixed concurrent requests (20 parallel, 5 of each type) - Same session concurrent requests - Unique request IDs under concurrency (20 parallel) - Load testing (50 parallel requests, 95% success rate)</p> <p>4. Health Check Error Scenarios (<code>tests/test_health_error_scenarios.py</code> - 15 tests): - Empty database handling - Missing optional fields - Response format validation - Security headers presence - CORS headers presence - Supported HTTP methods (GET, HEAD) - Reject unsafe methods (POST, PUT, DELETE) - Stats endpoint with empty database - Query parameter handling - Database consistency checks - Response time requirements (&lt;1s) - Concurrent writes during health checks - Content-Type header validation - Cache control headers</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#files-created_1","title":"Files Created","text":"<ul> <li><code>tests/test_cors_headers.py</code> - 7 tests for CORS validation</li> <li><code>tests/test_search_edge_cases.py</code> - 14 tests for search security and edge cases</li> <li><code>tests/test_concurrent_requests.py</code> - 8 tests for concurrent request handling</li> <li><code>tests/test_health_error_scenarios.py</code> - 15 tests for health endpoint reliability</li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#validation_2","title":"Validation","text":"<ul> <li>\u2705 All 44 new tests pass</li> <li>\u2705 Fast CI test suite still passes (271 tests)</li> <li>\u2705 Formatting and linting passed (ruff)</li> <li>\u2705 No existing tests broken</li> <li>\u2705 Tests cover SQL injection, XSS, path traversal, command injection</li> <li>\u2705 Tests verify concurrent request safety</li> <li>\u2705 Tests validate error handling and edge cases</li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#security-coverage","title":"Security Coverage","text":"<p>Attack vectors tested: - SQL injection (8 payloads) - XSS (7 payloads) - Path traversal (3 payloads) - Command injection (5 payloads) - NoSQL injection (3 payloads) - Null byte injection - Invalid input handling - Request method abuse - Concurrent request races</p> <p>Benefits: - Verifies parameterized queries prevent SQL injection - Confirms JSON encoding prevents XSS - Validates input sanitization - Tests API resilience under concurrent load - Ensures proper HTTP method restrictions - Verifies security headers on all endpoints - Confirms rate limiting works correctly (observed 429 responses)</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#phase-10-normalize-pre-commit-hooks-across-repos","title":"Phase 10: Normalize Pre-commit Hooks Across Repos","text":"<p>Status: \u2705 Completed Roadmap Item: #16</p> <p>Implemented consistent pre-commit quality gates across all three repositories (backend, frontend, datasets).</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#implementation","title":"Implementation","text":"<p>Backend (<code>.pre-commit-config.yaml</code>): - Upgraded pre-commit-hooks from v5.0.0 to v6.0.0 - Added ruff-format hook (v0.9.3) for code formatting - Added ruff lint hook (v0.9.3) with <code>--fix</code> and <code>--exit-non-zero-on-fix</code> - Added mypy hook (v1.17.1) with types-requests, types-python-dateutil - Mypy exclusions: <code>^(tests/|scripts/|alembic/|src/archive_tool/)</code> - Removed 5 unused <code># type: ignore[arg-type]</code> comments (no longer needed with updated mypy)</p> <p>Frontend (<code>.pre-commit-config.yaml</code>): - Upgraded pre-commit-hooks from v6.0.0 (already current) - Added ESLint hook (v9.19.0) with <code>--fix</code> and <code>--max-warnings=0</code> - Added Prettier hook (v4.0.0-alpha.8) with <code>--write</code> and <code>--ignore-unknown</code> - ESLint additional dependencies: eslint@^9.19.0, eslint-config-next@16.1.1, typescript@^5.7.3 - File patterns: <code>\\.(js|jsx|ts|tsx)$</code> for ESLint, <code>\\.(js|jsx|ts|tsx|json|css|md|yaml|yml)$</code> for Prettier</p> <p>Datasets (<code>.pre-commit-config.yaml</code>): - Upgraded pre-commit-hooks from v5.0.0 to v6.0.0 - Added ruff-format hook (v0.9.3) for code formatting - Added ruff lint hook (v0.9.3) with <code>--fix</code> and <code>--exit-non-zero-on-fix</code> - Added mypy hook (v1.17.1) - Mypy exclusions: <code>^scripts/</code> - Auto-fixed 6 files with missing end-of-file newlines</p> <p>Consistent Base Hooks (all repos): - <code>trailing-whitespace</code> - Remove trailing whitespace - <code>end-of-file-fixer</code> - Ensure files end with newline - <code>check-yaml</code> - Validate YAML syntax (backend excludes mkdocs.yml) - <code>check-toml</code> - Validate TOML syntax - <code>check-added-large-files</code> - Prevent files &gt;500KB - <code>detect-private-key</code> - Prevent accidental key commits</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#files-modified_1","title":"Files Modified","text":"<ul> <li><code>healtharchive-backend/.pre-commit-config.yaml</code> - Added ruff, mypy hooks</li> <li><code>healtharchive-frontend/.pre-commit-config.yaml</code> - Added eslint, prettier hooks</li> <li><code>healtharchive-datasets/.pre-commit-config.yaml</code> - Added ruff, mypy hooks</li> <li><code>healtharchive-backend/src/ha_backend/authority.py</code> - Removed unused type: ignore</li> <li><code>healtharchive-backend/src/ha_backend/cli.py</code> - Removed 4 unused type: ignore comments</li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#validation_3","title":"Validation","text":"<ul> <li>\u2705 Backend: All hooks pass (9 checks)</li> <li>\u2705 Frontend: All hooks pass (8 checks)</li> <li>\u2705 Datasets: All hooks pass (9 checks, mypy skipped - no Python source files)</li> <li>\u2705 Pre-commit installed in all three repos</li> <li>\u2705 Hooks run automatically on git commit</li> <li>\u2705 Formatting, linting, type checking enforced pre-commit</li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#benefits","title":"Benefits","text":"<p>Code Quality: - Automatic code formatting (ruff for Python, prettier for TypeScript/JavaScript) - Linting enforcement (ruff for Python, eslint for TypeScript/JavaScript) - Type checking (mypy for Python source code) - Prevents common issues (trailing whitespace, missing newlines, large files)</p> <p>Consistency: - Same base hooks across all repos - Unified Python tooling (ruff + mypy) - Unified frontend tooling (eslint + prettier) - Standardized hook versions</p> <p>Developer Experience: - Fast feedback (catches issues before CI) - Auto-fixing where possible (ruff --fix, eslint --fix, prettier --write) - Clear error messages at commit time - Prevents CI failures from formatting/linting issues</p> <p>Security: - Detects private keys before commit - Prevents large binary files - Validates configuration file syntax</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#rollback","title":"Rollback","text":"<p>All changes are additive and can be rolled back by reverting the relevant commits: - No schema changes - No data migrations - No VPS operations - No breaking API changes</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#post-implementation-checklist","title":"Post-Implementation Checklist","text":""},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#completed","title":"Completed \u2705","text":"<ul> <li> Frontend SEO enhancements deployed (OG tags, sitemap, RSS discovery, Organization JSON-LD)</li> <li> Dataset JSON-LD structured data on exports page</li> <li> Request ID middleware and logging implemented</li> <li> API versioning headers implemented with documented policy</li> <li> Request size limits implemented (body + query string)</li> <li> Rate limiting middleware implemented (per-endpoint limits)</li> <li> Content Security Policy (CSP) and HSTS headers implemented</li> <li> Test coverage thresholds enforced (75% on critical modules)</li> <li> Test coverage expansion completed (44 new tests: CORS, security, concurrent, edge cases)</li> <li> Pre-commit hooks normalized across all repos (ruff, mypy, eslint, prettier)</li> <li> Backend CI tests pass (271 tests in CI-fast target)</li> <li> Backend full test suite: 339 tests (271 CI + 24 middleware + 44 coverage expansion)</li> <li> New middleware tests: 24 total (3 request ID + 5 size limits + 7 rate limiting + 9 security headers)</li> <li> New coverage expansion tests: 44 total (7 CORS + 14 search edge cases + 8 concurrent + 15 health scenarios)</li> <li> Backend coverage: 76.96% on critical modules (api, worker, indexing)</li> <li> Frontend tests pass (72 tests)</li> <li> Dependabot configs committed to all repos</li> <li> Pre-commit hooks installed and passing in all repos</li> <li> Roadmap updated with completion status</li> <li> API consumer guide updated with versioning strategy, size limits, rate limits, and CSP/HSTS</li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#manual-steps-required","title":"Manual Steps Required \u26a0\ufe0f","text":"<ul> <li> Fix backend vulnerabilities (pillow, pip) before pip-audit becomes blocking</li> <li> Fix frontend vulnerabilities (next.js) before npm audit becomes blocking</li> <li> Review and keep/remove CITATION.cff and SECURITY.md files created in Phase 1</li> <li> Manually implement remaining governance files (CODE_OF_CONDUCT.md, .mailmap, issue templates, datasets LICENSE)</li> <li> Test Dependabot PRs when they arrive (verify CI passes, merge if appropriate)</li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#verification-after-manual-steps","title":"Verification After Manual Steps","text":"<p>After completing the manual vulnerability fixes:</p> <ol> <li> <p>Backend:    <pre><code>cd healtharchive-backend\nsource .venv/bin/activate\nmake ci &amp;&amp; pip-audit  # Should pass with no vulnerabilities\n</code></pre></p> </li> <li> <p>Frontend:    <pre><code>cd healtharchive-frontend\nnpm run check &amp;&amp; npm audit --audit-level=high  # Should pass\n</code></pre></p> </li> <li> <p>Verify in production (after push):</p> </li> <li>View page source: check for OG tags, Twitter Card, JSON-LD script</li> <li>Visit <code>/sitemap.xml</code> - should return valid XML</li> <li>Visit <code>/robots.txt</code> - should reference sitemap</li> <li>Check RSS discovery: <code>&lt;link rel=\"alternate\" type=\"application/rss+xml\"&gt;</code></li> <li>Make API request: verify <code>X-Request-Id</code> and <code>X-API-Version</code> headers in response</li> <li>Check backend logs: verify request IDs in log format</li> <li>Test headers with curl:      <pre><code>curl -I https://api.healtharchive.ca/api/health\n# Should see: X-Request-Id: &lt;uuid&gt;\n# Should see: X-API-Version: 1\n</code></pre></li> <li>Test request size limits:      <pre><code># Test oversized query string (should return 414)\ncurl -I \"https://api.healtharchive.ca/api/search?q=$(python3 -c 'print(\"x\"*10000)')\"\n\n# Test large body (should return 413)\ncurl -X POST https://api.healtharchive.ca/api/reports \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"data\":\"'\"$(python3 -c 'print(\"x\"*2000000)')\"'\"}'\n</code></pre></li> <li>Test rate limiting:      <pre><code># Make multiple search requests and check for rate limit headers\ncurl -I \"https://api.healtharchive.ca/api/search?q=test\"\n# Should see: X-RateLimit-Limit: 60\n# Should see: X-RateLimit-Remaining: 59\n\n# Exceed rate limit (run 65+ times rapidly)\nfor i in {1..65}; do curl -s -o /dev/null -w \"%{http_code}\\n\" \\\n  \"https://api.healtharchive.ca/api/search?q=test$i\"; done\n# Should eventually return 429 (Too Many Requests)\n</code></pre></li> <li>Test security headers (CSP and HSTS):      <pre><code># Check CSP on JSON endpoint\ncurl -I \"https://api.healtharchive.ca/api/health\"\n# Should see: Content-Security-Policy: default-src 'none'; frame-ancestors 'none'\n# Should see: Strict-Transport-Security: max-age=31536000; includeSubDomains\n\n# Check CSP on HTML replay endpoint\ncurl -I \"https://api.healtharchive.ca/api/snapshots/raw/1\"\n# Should see: Content-Security-Policy with script-src 'unsafe-inline' and img-src *\n# Should NOT see: X-Frame-Options (allows frontend iframe)\n\n# Verify all security headers are present\ncurl -I \"https://api.healtharchive.ca/api/stats\"\n# Should see: X-Content-Type-Options: nosniff\n# Should see: Referrer-Policy: strict-origin-when-cross-origin\n# Should see: X-Frame-Options: SAMEORIGIN\n# Should see: Permissions-Policy: geolocation=(), microphone=(), camera=()\n</code></pre></li> </ol>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#success-criteria","title":"Success Criteria","text":""},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#completed_1","title":"Completed \u2705","text":"<ul> <li> Frontend: OG + Twitter Card tags in page source</li> <li> Frontend: sitemap.xml served and valid</li> <li> Frontend: robots.txt has Sitemap directive</li> <li> Frontend: RSS discovery link present</li> <li> Frontend: JSON-LD Organization in page source (layout)</li> <li> Frontend: JSON-LD Dataset in page source (exports page)</li> <li> Backend: X-Request-Id header on all API responses</li> <li> Backend: X-API-Version header on all API responses</li> <li> Backend: Request IDs in log output</li> <li> Backend: Request size limits enforced (413/414 responses)</li> <li> Backend: Rate limiting enforced (429 responses with Retry-After)</li> <li> Backend: Rate limit headers present on limited endpoints (X-RateLimit-Limit, X-RateLimit-Remaining)</li> <li> Backend: CSP headers present on all responses (restrictive for JSON, permissive for HTML replay)</li> <li> Backend: HSTS header present (max-age=1 year, includeSubDomains)</li> <li> Backend: All security headers documented (CSP, HSTS, X-Frame-Options, etc.)</li> <li> Backend: Versioning strategy documented in API consumer guide</li> <li> Backend: Size limits documented in API consumer guide</li> <li> Backend: Rate limits documented in API consumer guide</li> <li> Backend: CSP policies documented in API consumer guide</li> <li> Backend: Test coverage thresholds enforced (75% on critical modules)</li> <li> Backend: Coverage documentation created (docs/development/test-coverage.md)</li> <li> Backend: Coverage at 76.96% on critical modules (api, worker, indexing)</li> <li> Backend: 44 new edge case and security tests (SQL injection, XSS, concurrent requests, etc.)</li> <li> All 3 repos: .github/dependabot.yml present</li> <li> All 3 repos: .pre-commit-config.yaml with normalized hooks</li> <li> Backend: Pre-commit with ruff-format, ruff lint, mypy (9 hooks passing)</li> <li> Frontend: Pre-commit with eslint, prettier (8 hooks passing)</li> <li> Datasets: Pre-commit with ruff-format, ruff lint, mypy (9 hooks passing)</li> <li> CI: All checks passing (backend + frontend)</li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#pending-manual-completion","title":"Pending Manual Completion \u26a0\ufe0f","text":"<ul> <li> CI: pip-audit blocking in backend (after vulnerability fixes)</li> <li> CI: npm audit blocking in frontend (after vulnerability fixes)</li> <li> Governance files complete (CODE_OF_CONDUCT.md, .mailmap, issue templates, datasets LICENSE)</li> </ul>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#metrics-impact","title":"Metrics &amp; Impact","text":"<p>Code Changes: - Files created: 19 (11 original + rate_limiting.py + test_rate_limiting.py + test_security_headers.py + test-coverage.md + 4 new test files) - Files modified: 28 (15 original + 3 for rate limiting + 2 for CSP/HSTS + 2 for coverage config + 1 roadmap update + 3 pre-commit configs + 2 Python files for unused type: ignore) - Lines added: ~3300 (original ~1600 + ~1500 for new tests + ~200 for pre-commit hooks) - Lines removed: ~10 (5 unused type: ignore comments + 5 misc) - Tests added: 68 total:   - 24 middleware tests (3 request ID + 5 size limits + 7 rate limiting + 9 security headers)   - 44 edge case/security tests (7 CORS + 14 search edge cases + 8 concurrent + 15 health scenarios) - Pre-commit hooks: 26 total (9 backend + 8 frontend + 9 datasets)</p> <p>Coverage: - All new backend code is tested (request ID, version, size limit, rate limiting, CSP/HSTS middleware, logging filter) - Frontend SEO changes validated via build + existing tests - Request size limits thoroughly tested with boundary conditions - Rate limiting thoroughly tested with per-endpoint limits and disable flag - Security headers thoroughly tested for JSON and HTML endpoints - Security vulnerability testing: SQL injection, XSS, path traversal, command injection, NoSQL injection - Concurrent request safety: tested with 10-50 parallel requests across endpoints - Edge case coverage: empty queries, Unicode, special chars, invalid parameters, null bytes - Health check reliability: tested with empty DB, missing fields, concurrent writes, query params - CORS configuration: tested origin handling, method restrictions, credentials disabled - Test coverage enforced: 76.96% on critical modules (75% threshold, CI-enforced)</p> <p>Admissions/Portfolio Value: - \u2705 Professional SEO implementation (OG tags, sitemap, Organization + Dataset structured data) - \u2705 Dataset discoverability in Google Dataset Search - \u2705 Observable API with correlation logging + request IDs - \u2705 API versioning with documented deprecation policy - \u2705 Request size limits prevent abuse/DoS - \u2705 Rate limiting prevents API abuse with per-endpoint limits - \u2705 CSP and HSTS prevent XSS/injection and enforce HTTPS - \u2705 Complete security hardening stack (size limits + rate limits + CSP/HSTS) - \u2705 Production-grade abuse prevention and security posture - \u2705 Test coverage: 76.96% on critical modules (CI-enforced 75% threshold) - \u2705 68 comprehensive tests: 24 middleware + 44 edge case/security tests - \u2705 Security testing: SQL injection, XSS, path traversal, command injection, NoSQL injection - \u2705 Concurrent request safety: tested with 10-50 parallel requests - \u2705 Edge case coverage: Unicode, special chars, invalid input, null bytes - \u2705 API reliability: health checks, error scenarios, CORS validation - \u2705 Pre-commit quality gates: Consistent hooks across all repos (ruff, mypy, eslint, prettier) - \u2705 Automated code quality: Formatting, linting, type checking enforced pre-commit - \u2705 Concrete quality metrics for portfolio/admissions - \u2705 Automated dependency management (Dependabot) - \u26a0\ufe0f Security audit discipline (partial - requires manual vuln fixes) - \u26a0\ufe0f Open-source governance (deferred - requires manual implementation)</p>"},{"location":"planning/implemented/2026-02-12-governance-seo-and-security-foundations/#next-steps","title":"Next Steps","text":"<p>Immediate (high priority): 1. Fix vulnerability warnings (backend: pillow/pip, frontend: next.js) 2. Enable blocking audit behavior in CI after fixes 3. Complete governance file implementation manually 4. Test Dependabot PRs when they arrive</p> <p>Future (roadmap items): - #15: Add API health integration tests to PR CI (M: 1 day) - #22: Add accessibility (a11y) testing to CI (M: 1-2 days) - #23: Create formal accessibility audit document (M: 1-2 days) - #24: Add frontend error boundary components (M: 1 day) - #25: Generate and publish OpenAPI spec (M: 1 day)</p> <p>Operational: - Monitor Dependabot PRs (weekly on Mondays) - Watch for request ID usage in production logs - Monitor rate limiting metrics in production (429 response rates) - Verify SEO improvements in Google Search Console (if configured) - Monitor sitemap indexing status</p>"},{"location":"reference/archive-tool/","title":"Archive Tool Reference","text":"<p>The archive_tool is HealthArchive's internal crawler and orchestrator subpackage.</p>"},{"location":"reference/archive-tool/#quick-overview","title":"Quick Overview","text":"<p>archive_tool is a Docker-based web crawler that: - Wraps the <code>zimit</code> crawler (from OpenZIM) - Manages crawl state and resumption - Monitors crawl health (stall detection, error thresholds) - Supports adaptive worker scaling - Optionally rotates VPN connections</p> <p>Location: <code>src/archive_tool/</code></p> <p>Technology: - Python 3.11+ - Docker (runs <code>ghcr.io/openzim/zimit</code> container) - State persistence (<code>.archive_state.json</code>)</p> <p>Environment Variables: - <code>HEALTHARCHIVE_ZIMIT_DOCKER_IMAGE</code>: Override Docker image (default: <code>ghcr.io/openzim/zimit</code>) - <code>HEALTHARCHIVE_DOCKER_MEMORY_LIMIT</code>: Container memory limit (default: <code>4g</code>) - <code>HEALTHARCHIVE_DOCKER_CPU_LIMIT</code>: Container CPU limit (default: <code>1.5</code>)</p> <p>Note: the HealthArchive backend indexes WARCs into <code>Snapshot</code> rows; it does not read <code>.zim</code> files. ZIM output is an optional artifact and can be skipped with <code>--skip-final-build</code>.</p>"},{"location":"reference/archive-tool/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 HealthArchive Backend                           \u2502\n\u2502                                                 \u2502\n\u2502  ha_backend.jobs.run_persistent_job()           \u2502\n\u2502         \u2502                                       \u2502\n\u2502         \u251c\u2500\u2500&gt; Builds CLI args from job config   \u2502\n\u2502         \u2502                                       \u2502\n\u2502         \u2514\u2500\u2500&gt; subprocess.run()                   \u2502\n\u2502                     \u2502                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n                      \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502   archive-tool CLI         \u2502\n         \u2502   (archive_tool/cli.py)    \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502\n                      \u251c\u2500\u2500&gt; Validates Docker\n                      \u251c\u2500\u2500&gt; Determines run mode\n                      \u251c\u2500\u2500&gt; Spawns zimit in Docker\n                      \u251c\u2500\u2500&gt; Monitors progress\n                      \u251c\u2500\u2500&gt; Writes WARCs to .tmp_N/\n                      \u2514\u2500\u2500&gt; Builds ZIM (optional)\n</code></pre>"},{"location":"reference/archive-tool/#canonical-documentation","title":"Canonical Documentation","text":"<p>Full technical reference: <code>src/archive_tool/docs/documentation.md</code></p> <p>1,508 lines covering: - CLI interface and all flags - Run modes (Fresh, Resume, New-with-Consolidation, Overwrite) - State management (<code>.archive_state.json</code>) - Docker orchestration details - Monitoring and adaptive workers - VPN rotation mechanism - WARC discovery and consolidation - Error handling and recovery - Testing and development</p> <p>Read the full docs for: - Detailed CLI flag reference - State machine diagrams - Docker volume mapping - Log parsing internals - Adding new features to archive_tool</p>"},{"location":"reference/archive-tool/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/archive-tool/#cli-usage","title":"CLI Usage","text":"<pre><code>archive-tool \\\n  --name CRAWL_NAME \\\n  --output-dir /path/to/output \\\n  --initial-workers N \\\n  [--enable-monitoring] \\\n  [--enable-adaptive-workers] \\\n  [--enable-vpn-rotation --vpn-connect-command \"...\"] \\\n  SEED_URL [SEED_URL...]\n</code></pre>"},{"location":"reference/archive-tool/#common-flags","title":"Common Flags","text":"Flag Purpose <code>--name</code> Crawl name (used in output naming; ZIM is optional) <code>--output-dir</code> Output directory path <code>--initial-workers</code> Number of parallel workers (default: 1) <code>--enable-monitoring</code> Enable stall/error detection <code>--stall-timeout-minutes</code> Abort if no progress (requires monitoring) <code>--enable-adaptive-workers</code> Reduce workers on errors (requires monitoring) <code>--enable-vpn-rotation</code> Rotate VPN on stalls (requires monitoring) <code>--docker-shm-size</code> Increase container <code>/dev/shm</code> (can improve stability) <code>--skip-final-build</code> Skip the final <code>.zim</code> build stage (WARCs still produced) <code>--cleanup</code> Delete temp dirs after successful crawl <code>--overwrite</code> Delete existing output before starting"},{"location":"reference/archive-tool/#run-modes","title":"Run Modes","text":"<p>archive-tool automatically determines the run mode based on state:</p> <ol> <li>Fresh - No prior state, start new crawl</li> <li>Resume - State exists and incomplete, resume from checkpoint</li> <li>New-with-Consolidation - State complete, start new crawl but consolidate WARCs</li> <li>Overwrite - <code>--overwrite</code> flag set, delete everything and start fresh</li> </ol> <p>See: <code>src/archive_tool/docs/documentation.md</code> (Run Modes) for decision tree</p>"},{"location":"reference/archive-tool/#output-structure","title":"Output Structure","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 .archive_state.json              # Persistent state\n\u251c\u2500\u2500 .tmp_1/                          # First crawl attempt\n\u2502   \u2514\u2500\u2500 collections/\n\u2502       \u2514\u2500\u2500 crawl-YYYYMMDD.../\n\u2502           \u251c\u2500\u2500 archive/             # WARCs here\n\u2502           \u2502   \u251c\u2500\u2500 rec-00000-....warc.gz\n\u2502           \u2502   \u2514\u2500\u2500 rec-00001-....warc.gz\n\u2502           \u2514\u2500\u2500 logs/\n\u251c\u2500\u2500 .tmp_2/                          # Second attempt (if restarted)\n\u251c\u2500\u2500 archive_STAGE_TIMESTAMP.log      # Individual stage logs\n\u251c\u2500\u2500 archive_STAGE_TIMESTAMP.combined.log  # Aggregated logs\n\u2514\u2500\u2500 zim/\n    \u2514\u2500\u2500 NAME_DATE.zim                # Optional ZIM file\n</code></pre>"},{"location":"reference/archive-tool/#state-file-format","title":"State File Format","text":"<p><code>.archive_state.json</code>: <pre><code>{\n  \"current_workers\": 4,\n  \"initial_workers\": 4,\n  \"temp_dirs_host_paths\": [\"/some/output/.tmp123\", \"...\"],\n  \"vpn_rotations_done\": 1,\n  \"worker_reductions_done\": 1,\n  \"container_restarts_done\": 1\n}\n</code></pre></p>"},{"location":"reference/archive-tool/#backend-integration","title":"Backend Integration","text":"<p>The backend calls archive-tool via subprocess. Key files:</p>"},{"location":"reference/archive-tool/#job-execution","title":"Job Execution","text":"<p><code>ha_backend/jobs.py:run_persistent_job()</code> (lines 439-560): - Loads <code>ArchiveJob.config</code> from database - Translates <code>tool_options</code> to CLI flags - Builds command: <code>archive-tool --flag1 val1 --flag2 val2 ... SEEDS</code> - Executes with <code>subprocess.run()</code> - Updates job status based on exit code</p> <p>Config \u2192 CLI Mapping: <pre><code>config[\"tool_options\"][\"enable_monitoring\"] \u2192 --enable-monitoring\nconfig[\"tool_options\"][\"initial_workers\"] \u2192 --initial-workers N\nconfig[\"tool_options\"][\"stall_timeout_minutes\"] \u2192 --stall-timeout-minutes N\n</code></pre></p>"},{"location":"reference/archive-tool/#warc-discovery","title":"WARC Discovery","text":"<p><code>ha_backend/indexing/warc_discovery.py</code>: - Uses <code>archive_tool.state.CrawlState</code> to load <code>.archive_state.json</code> - Uses <code>archive_tool.utils.find_all_warc_files()</code> to locate WARCs - Ensures backend and archive-tool use identical logic</p>"},{"location":"reference/archive-tool/#cleanup","title":"Cleanup","text":"<p><code>ha_backend/cli/cmd_cleanup_job.py</code>: - Uses <code>archive_tool.utils.cleanup_temp_dirs()</code> to remove <code>.tmp*</code> directories - Deletes <code>.archive_state.json</code> - Updates <code>ArchiveJob.cleanup_status</code></p>"},{"location":"reference/archive-tool/#monitoring-features","title":"Monitoring Features","text":""},{"location":"reference/archive-tool/#stall-detection","title":"Stall Detection","text":"<p>When <code>--enable-monitoring</code> is set: - Monitors log output every <code>--monitor-interval-seconds</code> (default: 30) - Parses \"Crawl statistics\" JSON from logs - Detects stalls: no new pages for <code>--stall-timeout-minutes</code> - Action: Abort crawl with non-zero exit code</p>"},{"location":"reference/archive-tool/#error-thresholds","title":"Error Thresholds","text":"<ul> <li><code>--error-threshold-timeout N</code>: Abort if N timeout errors</li> <li><code>--error-threshold-http N</code>: Abort if N HTTP errors</li> <li>Prevents runaway crawls that repeatedly fail</li> </ul>"},{"location":"reference/archive-tool/#adaptive-workers","title":"Adaptive Workers","text":"<p>When <code>--enable-adaptive-workers</code> is set: - Reduces worker count on sustained errors - Min workers: <code>--min-workers</code> (default: 1) - Max reductions: <code>--max-worker-reductions</code> (default: 2) - Strategy: Reduce by 1 each time threshold exceeded</p>"},{"location":"reference/archive-tool/#vpn-rotation","title":"VPN Rotation","text":"<p>When <code>--enable-vpn-rotation</code> is set: - Rotates VPN connection on stalls or errors - Command: <code>--vpn-connect-command \"vpn connect server\"</code> - Frequency: Every <code>--vpn-rotation-frequency-minutes</code> - Max rotations: <code>--max-vpn-rotations</code></p> <p>Use case: Avoid IP bans during large crawls</p>"},{"location":"reference/archive-tool/#development","title":"Development","text":""},{"location":"reference/archive-tool/#running-locally","title":"Running Locally","text":"<pre><code># Direct execution\ncd src/archive_tool\npython -m archive_tool.cli \\\n  --name test \\\n  --output-dir /tmp/test-crawl \\\n  https://example.com\n\n# Via installed command\narchive-tool --name test --output-dir /tmp/test https://example.com\n</code></pre>"},{"location":"reference/archive-tool/#testing","title":"Testing","text":"<pre><code># Run archive_tool tests\npytest tests/test_archive_tool*.py\n\n# Test state management\npytest tests/test_archive_state.py\n\n# Test WARC discovery\npytest tests/test_warc_discovery.py\n</code></pre>"},{"location":"reference/archive-tool/#adding-new-features","title":"Adding New Features","text":"<ol> <li>Modify CLI (<code>archive_tool/cli.py</code>):</li> <li>Add new argument to <code>argparse</code></li> <li> <p>Update <code>run_with_parsed_args()</code></p> </li> <li> <p>Update contract (<code>ha_backend/archive_contract.py</code>):</p> </li> <li>Add field to <code>ArchiveToolOptions</code> TypedDict</li> <li> <p>Update <code>validate_tool_options()</code></p> </li> <li> <p>Update backend (<code>ha_backend/jobs.py</code>):</p> </li> <li> <p>Add CLI flag construction in <code>run_persistent_job()</code></p> </li> <li> <p>Update job registry (<code>ha_backend/job_registry.py</code>):</p> </li> <li> <p>Add to <code>default_tool_options</code> if needed</p> </li> <li> <p>Add tests:</p> </li> <li><code>tests/test_archive_contract.py</code> - Config validation</li> <li><code>tests/test_jobs_persistent.py</code> - CLI construction</li> <li><code>tests/test_archive_tool_*.py</code> - archive_tool behavior</li> </ol> <p>See: <code>src/archive_tool/docs/documentation.md</code> (Development) for details</p>"},{"location":"reference/archive-tool/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/archive-tool/#docker-issues","title":"Docker Issues","text":"<p>Problem: \"Cannot connect to Docker daemon\"</p> <p>Solution: <pre><code>sudo systemctl start docker\ndocker ps  # Verify\n</code></pre></p> <p>Problem: Permission denied accessing Docker socket</p> <p>Solution: <pre><code>sudo usermod -aG docker $USER\n# Log out and back in\n</code></pre></p>"},{"location":"reference/archive-tool/#state-issues","title":"State Issues","text":"<p>Problem: Crawl won't resume</p> <p>Solution: <pre><code># Check state file\ncat output_dir/.archive_state.json\n\n# Force fresh start\narchive-tool --overwrite ...\n</code></pre></p> <p>Problem: WARCs not found</p> <p>Solution: <pre><code># Manually check\nfind output_dir -name \"*.warc.gz\"\n\n# Verify state points to correct dirs\ncat output_dir/.archive_state.json | jq '.temp_dirs'\n</code></pre></p>"},{"location":"reference/archive-tool/#monitoring-issues","title":"Monitoring Issues","text":"<p>Problem: Adaptive workers not triggering</p> <p>Check: 1. <code>--enable-monitoring</code> is set 2. <code>--enable-adaptive-workers</code> is set 3. Errors exceed threshold 4. Not already at <code>--min-workers</code></p>"},{"location":"reference/archive-tool/#performance-tuning","title":"Performance Tuning","text":""},{"location":"reference/archive-tool/#worker-count","title":"Worker Count","text":"<ul> <li>Default: 1 worker (conservative)</li> <li>Small sites: 1-2 workers</li> <li>Medium sites: 2-4 workers</li> <li>Large sites: 4-8 workers (watch resource usage)</li> </ul> <p>Factors: - Server CPU/memory - Network bandwidth - Site's rate limiting - Politeness requirements</p>"},{"location":"reference/archive-tool/#memory-usage","title":"Memory Usage","text":"<p>Docker container memory (per worker): - ~500MB base - +200-500MB per worker - +500MB-1GB for large sites</p> <p>Example: 4 workers \u2248 2-4GB RAM</p>"},{"location":"reference/archive-tool/#disk-io","title":"Disk I/O","text":"<p>WARCs write continuously: - 10-50MB/min for typical sites - 100-500MB/min for large sites</p> <p>Ensure: - Fast disk (SSD recommended) - Sufficient space (check <code>df -h</code> before starting) - No I/O bottlenecks (<code>iostat -x 1</code>)</p>"},{"location":"reference/archive-tool/#related-documentation","title":"Related Documentation","text":"<ul> <li>Full archive_tool docs: <code>src/archive_tool/docs/documentation.md</code> (Start here for details)</li> <li>Backend integration: ../architecture.md#5-archive_tool-integration</li> <li>Job execution: ../architecture.md#52-run_persistent_job</li> <li>CLI commands: cli-commands.md</li> <li>Debugging crawls: ../tutorials/debug-crawl.md</li> </ul>"},{"location":"reference/cli-commands/","title":"CLI Commands Reference","text":"<p>Complete reference for <code>ha-backend</code> command-line interface.</p>"},{"location":"reference/cli-commands/#installation","title":"Installation","text":"<p>The <code>ha-backend</code> command is installed when you install the package:</p> <pre><code>pip install -e .\n# or\nmake venv\n</code></pre> <p>Verify installation: <pre><code>ha-backend --help\n</code></pre></p>"},{"location":"reference/cli-commands/#command-categories","title":"Command Categories","text":"Category Commands Environment <code>check-env</code>, <code>check-archive-tool</code>, <code>check-db</code> Job Management <code>create-job</code>, <code>run-db-job</code>, <code>index-job</code>, <code>register-job-dir</code> Direct Execution <code>run-job</code> Inspection <code>list-jobs</code>, <code>show-job</code> Maintenance <code>retry-job</code>, <code>reset-retry-count</code>, <code>cleanup-job</code>, <code>replay-index-job</code> Annual Campaign <code>schedule-annual</code>, <code>annual-status</code>, <code>reconcile-annual-tool-options</code> Seeding <code>seed-sources</code> Worker <code>start-worker</code> Change Tracking <code>compute-changes</code>"},{"location":"reference/cli-commands/#environment-commands","title":"Environment Commands","text":""},{"location":"reference/cli-commands/#check-env","title":"check-env","text":"<p>Check environment configuration and ensure archive root exists.</p> <p>Usage: <pre><code>ha-backend check-env\n</code></pre></p> <p>Output: <pre><code>Archive root: /mnt/nasd/nobak/healtharchive/jobs\nArchive root exists: True\nArchive tool command: archive-tool\n</code></pre></p> <p>Exit codes: - <code>0</code> - Success - <code>1</code> - Archive root missing</p>"},{"location":"reference/cli-commands/#check-archive-tool","title":"check-archive-tool","text":"<p>Verify archive-tool is available and functional.</p> <p>Usage: <pre><code>ha-backend check-archive-tool\n</code></pre></p> <p>What it does: - Runs <code>archive-tool --help</code> - Validates command is available</p> <p>Exit codes: - <code>0</code> - archive-tool available - <code>1</code> - archive-tool not found or failed</p>"},{"location":"reference/cli-commands/#check-db","title":"check-db","text":"<p>Test database connectivity.</p> <p>Usage: <pre><code>ha-backend check-db\n</code></pre></p> <p>Output: <pre><code>Database connection successful\n</code></pre></p> <p>Exit codes: - <code>0</code> - Database reachable - <code>1</code> - Connection failed</p>"},{"location":"reference/cli-commands/#job-management-commands","title":"Job Management Commands","text":""},{"location":"reference/cli-commands/#create-job","title":"create-job","text":"<p>Create a new archive job using source defaults.</p> <p>Usage: <pre><code>ha-backend create-job --source SOURCE_CODE [--override JSON]\n</code></pre></p> <p>Arguments: - <code>--source</code>, <code>-s</code> (required) - Source code (<code>hc</code>, <code>phac</code>) - <code>--override</code> (optional) - JSON string with config overrides</p> <p>Examples:</p> <pre><code># Create Health Canada job with defaults\nha-backend create-job --source hc\n\n# Create with custom worker count\nha-backend create-job --source hc --override '{\"tool_options\": {\"initial_workers\": 2}}'\n\n# Create a \"search-first\" crawl (skip optional .zim build) with a larger Docker /dev/shm\nha-backend create-job --source hc --override '{\"tool_options\": {\"initial_workers\": 2, \"skip_final_build\": true, \"docker_shm_size\": \"1g\"}}'\n\n# Enable monitoring and stall detection\nha-backend create-job --source phac --override '{\n  \"tool_options\": {\n    \"enable_monitoring\": true,\n    \"stall_timeout_minutes\": 60\n  }\n}'\n</code></pre> <p>Output: <pre><code>Created job ID: 42\nName: hc-20260118\nOutput directory: /mnt/nasd/nobak/healtharchive/jobs/hc/20260118T210911Z__hc-20260118\nStatus: queued\n</code></pre></p> <p>Exit codes: - <code>0</code> - Job created successfully - <code>1</code> - Failed (invalid source, config validation error)</p>"},{"location":"reference/cli-commands/#run-db-job","title":"run-db-job","text":"<p>Execute a queued job by ID.</p> <p>Usage: <pre><code>ha-backend run-db-job --id JOB_ID\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID to run</p> <p>Example: <pre><code>ha-backend run-db-job --id 42\n</code></pre></p> <p>What it does: 1. Validates job status is <code>queued</code> or <code>retryable</code> 2. Sets status to <code>running</code> 3. Executes archive-tool subprocess 4. Updates status to <code>completed</code> or <code>failed</code></p> <p>Exit codes: - <code>0</code> - Crawl succeeded - <code>1</code> - Crawl failed or job invalid</p>"},{"location":"reference/cli-commands/#index-job","title":"index-job","text":"<p>Index WARCs from a completed job into the database.</p> <p>Usage: <pre><code>ha-backend index-job --id JOB_ID\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID to index</p> <p>Example: <pre><code>ha-backend index-job --id 42\n</code></pre></p> <p>What it does: 1. Discovers WARC files in job output directory 2. Parses WARC records 3. Extracts text, title, snippet 4. Creates Snapshot rows 5. Sets job status to <code>indexed</code></p> <p>Output: <pre><code>Indexing job 42...\nFound 245 WARC files\nIndexed 12,347 snapshots\nJob status: indexed\n</code></pre></p> <p>Exit codes: - <code>0</code> - Indexing succeeded - <code>1</code> - Failed (no WARCs, parsing error)</p>"},{"location":"reference/cli-commands/#register-job-dir","title":"register-job-dir","text":"<p>Attach an existing archive_tool output directory to a new database job.</p> <p>Usage: <pre><code>ha-backend register-job-dir --source SOURCE --output-dir PATH [--name NAME]\n</code></pre></p> <p>Arguments: - <code>--source</code> (required) - Source code - <code>--output-dir</code> (required) - Existing directory path - <code>--name</code> (optional) - Job name (default: derived from directory)</p> <p>Example: <pre><code>ha-backend register-job-dir \\\n  --source hc \\\n  --output-dir /mnt/nasd/nobak/healtharchive/jobs/hc/20260101T120000Z__hc-20260101\n</code></pre></p> <p>Use case: Import externally-run crawls into database</p> <p>Exit codes: - <code>0</code> - Job registered - <code>1</code> - Directory doesn't exist or validation failed</p>"},{"location":"reference/cli-commands/#direct-execution","title":"Direct Execution","text":""},{"location":"reference/cli-commands/#run-job","title":"run-job","text":"<p>Run archive-tool directly without database persistence.</p> <p>Usage: <pre><code>ha-backend run-job \\\n  --name NAME \\\n  --seeds URL [URL...] \\\n  [--initial-workers N] \\\n  [--output-dir DIR]\n</code></pre></p> <p>Arguments: - <code>--name</code> (required) - Job name - <code>--seeds</code> (required) - One or more seed URLs - <code>--initial-workers</code> (optional) - Worker count (default: 1) - <code>--output-dir</code> (optional) - Output directory (default: auto-generated)</p> <p>Example: <pre><code>ha-backend run-job \\\n  --name test-crawl \\\n  --seeds https://www.canada.ca/en/health-canada.html \\\n  --initial-workers 2\n</code></pre></p> <p>Use case: Quick testing without database overhead</p> <p>Exit codes: - <code>0</code> - Crawl succeeded - Non-zero - archive-tool exit code</p>"},{"location":"reference/cli-commands/#inspection-commands","title":"Inspection Commands","text":""},{"location":"reference/cli-commands/#list-jobs","title":"list-jobs","text":"<p>List recent jobs with summary information.</p> <p>Usage: <pre><code>ha-backend list-jobs [--limit N] [--status STATUS] [--source SOURCE]\n</code></pre></p> <p>Arguments: - <code>--limit</code> (optional) - Number of jobs to show (default: 20) - <code>--status</code> (optional) - Filter by status - <code>--source</code> (optional) - Filter by source code</p> <p>Examples: <pre><code># List 20 most recent jobs\nha-backend list-jobs\n\n# Show only failed jobs\nha-backend list-jobs --status failed\n\n# Show Health Canada jobs\nha-backend list-jobs --source hc\n\n# Show last 50 jobs\nha-backend list-jobs --limit 50\n</code></pre></p> <p>Output: <pre><code>ID  Name            Source  Status    Queued              Started             Finished            Pages\n42  hc-20260118     hc      indexed   2026-01-18 20:00    2026-01-18 20:05    2026-01-18 21:30    12,347\n41  phac-20260117   phac    completed 2026-01-17 19:00    2026-01-17 19:10    2026-01-17 20:45    8,234\n</code></pre></p>"},{"location":"reference/cli-commands/#show-job","title":"show-job","text":"<p>Display detailed information about a specific job.</p> <p>Usage: <pre><code>ha-backend show-job --id JOB_ID [--format {text|json}]\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID - <code>--format</code> (optional) - Output format (default: <code>text</code>)</p> <p>Examples: <pre><code># Human-readable output\nha-backend show-job --id 42\n\n# JSON output (for scripting)\nha-backend show-job --id 42 --format json\n</code></pre></p> <p>Output (text format): <pre><code>Job ID: 42\nName: hc-20260118\nSource: Health Canada (hc)\nStatus: indexed\nOutput Directory: /mnt/nasd/nobak/healtharchive/jobs/hc/20260118T210911Z__hc-20260118\n\nTimeline:\n  Queued:  2026-01-18 20:00:00\n  Started: 2026-01-18 20:05:00\n  Finished: 2026-01-18 21:30:00\n  Duration: 1h 25m\n\nCrawl Metrics:\n  Exit Code: 0\n  Status: success\n  Pages Crawled: 12,347\n  Pages Total: 12,500\n  Pages Failed: 153\n\nIndexing:\n  WARC Files: 245\n  Snapshots: 12,347\n\nCleanup:\n  Status: none\n</code></pre></p>"},{"location":"reference/cli-commands/#maintenance-commands","title":"Maintenance Commands","text":""},{"location":"reference/cli-commands/#retry-job","title":"retry-job","text":"<p>Retry a failed or index-failed job.</p> <p>Usage: <pre><code>ha-backend retry-job --id JOB_ID\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID to retry</p> <p>Example: <pre><code>ha-backend retry-job --id 42\n</code></pre></p> <p>What it does: - If job status is <code>failed</code>: Sets to <code>retryable</code> (for re-crawl) - If job status is <code>index_failed</code>: Sets to <code>completed</code> (for re-index)</p> <p>Exit codes: - <code>0</code> - Job marked for retry - <code>1</code> - Job not in retryable state</p>"},{"location":"reference/cli-commands/#reset-retry-count","title":"reset-retry-count","text":"<p>Reset a crawl job's retry budget by setting <code>retry_count</code> to a lower value.</p> <p>Safe-by-default: dry-run unless <code>--apply</code> is passed.</p> <p>Usage: <pre><code>ha-backend reset-retry-count --id JOB_ID [--apply] [--reason \"note\"]\n</code></pre></p> <p>Arguments: - <code>--id</code> - One or more Job IDs to modify - <code>--apply</code> - Persist changes (default: dry-run) - <code>--reason</code> - Optional note printed in output (required for multi-job apply) - <code>--new-count</code> - New value for <code>retry_count</code> (default: <code>0</code>) - <code>--min-retry-count</code> - Only match jobs with retry_count &gt;= this (default: <code>1</code>)</p> <p>Examples: <pre><code># Dry-run (prints what would change)\nha-backend reset-retry-count --id 42\n\n# Apply for one job\nha-backend reset-retry-count --id 42 --apply --reason \"storage recovered; re-attempt crawl\"\n\n# Bulk mode (requires --source, --status, and --limit)\nha-backend reset-retry-count --source hc --status failed retryable --limit 25 --apply --reason \"post-incident retry budget reset\"\n</code></pre></p> <p>Safety guardrails: - Skips jobs in <code>running</code> status. - Skips jobs whose lock file appears held (job runner likely still active). - Only supports statuses: <code>queued</code>, <code>retryable</code>, <code>failed</code>.</p>"},{"location":"reference/cli-commands/#cleanup-job","title":"cleanup-job","text":"<p>Clean up temporary crawl artifacts.</p> <p>Usage: <pre><code>ha-backend cleanup-job --id JOB_ID [--mode MODE] [--force]\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID - <code>--mode</code> (optional) - Cleanup mode (default: <code>temp</code>, only supported value) - <code>--force</code> (optional) - Force cleanup even if replay is enabled</p> <p>Example: <pre><code># Clean up temp directories and state file\nha-backend cleanup-job --id 42 --mode temp\n\n# Force cleanup (use with caution)\nha-backend cleanup-job --id 42 --mode temp --force\n</code></pre></p> <p>What it does: - Removes <code>.tmp*</code> directories - Removes <code>.archive_state.json</code> - Updates job: <code>cleanup_status = \"temp_cleaned\"</code>, <code>cleaned_at = now</code></p> <p>\u26a0\ufe0f Warning: This deletes WARCs if they're in <code>.tmp*</code> directories. Only run on indexed jobs where you don't need replay.</p> <p>Exit codes: - <code>0</code> - Cleanup succeeded - <code>1</code> - Failed (job not indexed, replay enabled without --force)</p>"},{"location":"reference/cli-commands/#replay-index-job","title":"replay-index-job","text":"<p>Create/refresh pywb collection index for a job.</p> <p>Usage: <pre><code>ha-backend replay-index-job --id JOB_ID\n</code></pre></p> <p>Arguments: - <code>--id</code> (required) - Job ID</p> <p>Example: <pre><code>ha-backend replay-index-job --id 42\n</code></pre></p> <p>What it does: - Creates pywb collection for job WARCs - Generates CDX index for fast replay - Enables browsing via pywb</p> <p>Prerequisites: - <code>HEALTHARCHIVE_REPLAY_BASE_URL</code> set - pywb installed and configured</p> <p>Exit codes: - <code>0</code> - Index created - <code>1</code> - Failed or replay not configured</p>"},{"location":"reference/cli-commands/#seeding","title":"Seeding","text":""},{"location":"reference/cli-commands/#seed-sources","title":"seed-sources","text":"<p>Initialize source records in the database.</p> <p>Usage: <pre><code>ha-backend seed-sources\n</code></pre></p> <p>What it does: - Inserts <code>Source</code> rows for <code>hc</code>, <code>phac</code>, and <code>cihr</code> - Idempotent (safe to run multiple times)</p> <p>Example: <pre><code>ha-backend seed-sources\n</code></pre></p> <p>Output: <pre><code>Seeded source: hc (Health Canada)\nSeeded source: phac (Public Health Agency of Canada)\nSeeded source: cihr (Canadian Institutes of Health Research)\n</code></pre></p> <p>Exit codes: - <code>0</code> - Sources seeded or already exist</p>"},{"location":"reference/cli-commands/#annual-campaign","title":"Annual Campaign","text":""},{"location":"reference/cli-commands/#schedule-annual","title":"schedule-annual","text":"<p>Plan or enqueue Jan 01 (UTC) annual campaign jobs for <code>hc</code>, <code>phac</code>, and <code>cihr</code>.</p> <p>Usage: <pre><code>ha-backend schedule-annual --year YEAR [--sources hc phac cihr] [--apply]\n</code></pre></p> <p>Examples: <pre><code># Show what would be created\nha-backend schedule-annual --year 2026\n\n# Actually create jobs\nha-backend schedule-annual --year 2026 --apply\n</code></pre></p> <p>Notes: - Dry-run by default - Idempotent for annual campaign metadata/name matches - Refuses to enqueue when a source already has an active non-indexed job</p>"},{"location":"reference/cli-commands/#annual-status","title":"annual-status","text":"<p>Report annual campaign progress and search-readiness for a given year.</p> <p>Usage: <pre><code>ha-backend annual-status --year YEAR [--json] [--sources hc phac cihr]\n</code></pre></p> <p>Examples: <pre><code>ha-backend annual-status --year 2026\nha-backend annual-status --year 2026 --json\n</code></pre></p>"},{"location":"reference/cli-commands/#reconcile-annual-tool-options","title":"reconcile-annual-tool-options","text":"<p>Reconcile existing annual jobs to source-specific crawl profiles.</p> <p>Usage: <pre><code>ha-backend reconcile-annual-tool-options --year YEAR [--sources ...] [--limit N] [--apply]\n</code></pre></p> <p>Examples: <pre><code># Dry-run reconciliation for all annual sources\nha-backend reconcile-annual-tool-options --year 2026\n\n# Apply only to HC annual jobs\nha-backend reconcile-annual-tool-options --year 2026 --sources hc --apply\n</code></pre></p> <p>What it does: - Reconciles legacy baseline tool options to per-source profiles - Preserves explicit non-baseline overrides - Enforces restart-budget floor and annual safety defaults</p>"},{"location":"reference/cli-commands/#worker","title":"Worker","text":""},{"location":"reference/cli-commands/#start-worker","title":"start-worker","text":"<p>Start the job processing worker loop.</p> <p>Usage: <pre><code>ha-backend start-worker [--poll-interval SECONDS] [--once]\n</code></pre></p> <p>Arguments: - <code>--poll-interval</code> (optional) - Seconds between polls (default: 30) - <code>--once</code> (optional) - Process one job then exit</p> <p>Examples: <pre><code># Run continuously with 30s polling\nha-backend start-worker\n\n# Poll every 60 seconds\nha-backend start-worker --poll-interval 60\n\n# Process one job and exit (for testing)\nha-backend start-worker --once\n</code></pre></p> <p>What it does: 1. Polls for jobs with status <code>queued</code> or <code>retryable</code> 2. Runs oldest job first 3. Crawls \u2192 Indexes \u2192 Repeats 4. Sleeps if no jobs found</p> <p>Exit: Press Ctrl+C to stop gracefully</p>"},{"location":"reference/cli-commands/#change-tracking","title":"Change Tracking","text":""},{"location":"reference/cli-commands/#compute-changes","title":"compute-changes","text":"<p>Compute change events between adjacent snapshots.</p> <p>Usage: <pre><code>ha-backend compute-changes [--limit N] [--source SOURCE]\n</code></pre></p> <p>Arguments: - <code>--limit</code> (optional) - Max snapshot groups to process - <code>--source</code> (optional) - Limit to specific source</p> <p>Example: <pre><code># Compute changes for all snapshots\nha-backend compute-changes\n\n# Process 100 page groups\nha-backend compute-changes --limit 100\n\n# Only Health Canada changes\nha-backend compute-changes --source hc\n</code></pre></p> <p>What it does: - Groups snapshots by <code>normalized_url_group</code> - Compares adjacent captures (by timestamp) - Generates <code>SnapshotChange</code> rows with diff metadata</p> <p>Exit codes: - <code>0</code> - Changes computed - <code>1</code> - Error</p>"},{"location":"reference/cli-commands/#global-options","title":"Global Options","text":"<p>All commands support:</p> <pre><code>ha-backend COMMAND --help  # Show command help\n</code></pre>"},{"location":"reference/cli-commands/#environment-variables","title":"Environment Variables","text":"<p>Commands respect these environment variables:</p> Variable Purpose Default <code>HEALTHARCHIVE_DATABASE_URL</code> Database connection <code>sqlite:///healtharchive.db</code> <code>HEALTHARCHIVE_ARCHIVE_ROOT</code> Base directory for jobs <code>/mnt/nasd/nobak/healtharchive/jobs</code> <code>HEALTHARCHIVE_TOOL_CMD</code> archive-tool command <code>archive-tool</code> <code>HEALTHARCHIVE_LOG_LEVEL</code> Logging level <code>INFO</code> <p>Set in <code>.env</code> file: <pre><code>HEALTHARCHIVE_DATABASE_URL=postgresql://user:pass@localhost/healtharchive\nHEALTHARCHIVE_ARCHIVE_ROOT=/data/healtharchive/jobs\nHEALTHARCHIVE_LOG_LEVEL=DEBUG\n</code></pre></p>"},{"location":"reference/cli-commands/#exit-codes","title":"Exit Codes","text":"<p>Standard exit codes: - <code>0</code> - Success - <code>1</code> - General error - <code>2</code> - Command-line usage error</p>"},{"location":"reference/cli-commands/#scripting-examples","title":"Scripting Examples","text":""},{"location":"reference/cli-commands/#process-a-job-end-to-end","title":"Process a job end-to-end","text":"<pre><code>#!/bin/bash\nset -e\n\n# Create job\nJOB_ID=$(ha-backend create-job --source hc | grep \"Created job ID:\" | awk '{print $4}')\necho \"Created job $JOB_ID\"\n\n# Run crawl\nha-backend run-db-job --id $JOB_ID\n\n# Index WARCs\nha-backend index-job --id $JOB_ID\n\n# Clean up\nha-backend cleanup-job --id $JOB_ID --mode temp\n\necho \"Job $JOB_ID complete\"\n</code></pre>"},{"location":"reference/cli-commands/#monitor-worker","title":"Monitor worker","text":"<pre><code>#!/bin/bash\n\nwhile true; do\n  clear\n  echo \"=== Job Status ===\"\n  ha-backend list-jobs --limit 10\n  sleep 10\ndone\n</code></pre>"},{"location":"reference/cli-commands/#retry-all-failed-jobs","title":"Retry all failed jobs","text":"<pre><code>#!/bin/bash\n\nha-backend list-jobs --status failed --limit 100 --format json | \\\n  jq -r '.[].id' | \\\n  while read job_id; do\n    echo \"Retrying job $job_id\"\n    ha-backend retry-job --id $job_id\n  done\n</code></pre>"},{"location":"reference/cli-commands/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture Guide: ../architecture.md</li> <li>Job Registry: ../architecture.md#4-job-registry--creation</li> <li>Worker Loop: ../architecture.md#9-worker-loop</li> <li>Data Model: data-model.md</li> <li>Live Testing: ../development/live-testing.md</li> </ul>"},{"location":"reference/data-model/","title":"Data Model Reference","text":"<p>Quick reference for HealthArchive database models.</p> <p>Full details: See Architecture Guide</p>"},{"location":"reference/data-model/#entity-relationship","title":"Entity Relationship","text":"<pre><code>erDiagram\n    Source ||--o{ ArchiveJob : has\n    Source ||--o{ Snapshot : has\n    ArchiveJob ||--o{ Snapshot : produces\n\n    Source {\n        int id PK\n        string code UK\n        string name\n        string base_url\n        bool enabled\n    }\n\n    ArchiveJob {\n        int id PK\n        int source_id FK\n        string name\n        string status\n        string output_dir\n        json config\n        int warc_file_count\n        int indexed_page_count\n    }\n\n    Snapshot {\n        int id PK\n        int job_id FK\n        int source_id FK\n        string url\n        string normalized_url_group\n        datetime capture_timestamp\n        string title\n        string snippet\n        string language\n    }</code></pre>"},{"location":"reference/data-model/#source","title":"Source","text":"<p>Represents a content origin (e.g., Health Canada, PHAC).</p> <p>Table: <code>sources</code></p> Field Type Nullable Description <code>id</code> Integer No Primary key <code>code</code> String(50) No Unique short code (<code>\"hc\"</code>, <code>\"phac\"</code>) <code>name</code> String(200) No Human-readable name <code>base_url</code> String(500) Yes Base URL of source <code>description</code> Text Yes Optional description <code>enabled</code> Boolean No Whether source is active (default: <code>true</code>) <code>created_at</code> DateTime No Creation timestamp <code>updated_at</code> DateTime No Last update timestamp <p>Indexes: - Unique on <code>code</code></p> <p>Relationships: - <code>jobs</code>: One-to-many \u2192 <code>ArchiveJob</code> - <code>snapshots</code>: One-to-many \u2192 <code>Snapshot</code></p>"},{"location":"reference/data-model/#archivejob","title":"ArchiveJob","text":"<p>Represents a single crawl job execution.</p> <p>Table: <code>archive_jobs</code></p> Field Type Nullable Description Identity <code>id</code> Integer No Primary key <code>source_id</code> Integer Yes Foreign key \u2192 <code>sources.id</code> <code>name</code> String(200) No Job name (used in ZIM naming) <code>output_dir</code> String(500) No Absolute path to job directory Lifecycle <code>status</code> String(50) No <code>queued</code>, <code>running</code>, <code>completed</code>, <code>failed</code>, <code>indexing</code>, <code>indexed</code>, etc. <code>queued_at</code> DateTime Yes When job was queued <code>started_at</code> DateTime Yes When crawl started <code>finished_at</code> DateTime Yes When crawl finished <code>retry_count</code> Integer No Number of retry attempts (default: 0) Configuration <code>config</code> JSON Yes Job configuration (seeds, tool_options, zimit args) Crawl Metrics <code>crawler_exit_code</code> Integer Yes Exit code from archive_tool process <code>crawler_status</code> String(50) Yes Summarized status (<code>\"success\"</code>, <code>\"failed\"</code>) <code>crawler_stage</code> String(50) Yes Last known stage <code>last_stats_json</code> JSON Yes Parsed crawl stats from logs <code>pages_crawled</code> Integer Yes Pages successfully crawled <code>pages_total</code> Integer Yes Total pages discovered <code>pages_failed</code> Integer Yes Pages that failed to crawl Indexing <code>warc_file_count</code> Integer No Number of WARC files discovered (default: 0) <code>indexed_page_count</code> Integer No Number of snapshots created (default: 0) File Paths <code>final_zim_path</code> String(500) Yes Path to ZIM file (if built) <code>combined_log_path</code> String(500) Yes Path to combined crawl log <code>state_file_path</code> String(500) Yes Path to <code>.archive_state.json</code> Cleanup <code>cleanup_status</code> String(50) No <code>\"none\"</code>, <code>\"temp_cleaned\"</code> (default: <code>\"none\"</code>) <code>cleaned_at</code> DateTime Yes When cleanup was performed Timestamps <code>created_at</code> DateTime No Record creation <code>updated_at</code> DateTime No Last update <p>Indexes: - Index on <code>source_id</code> - Index on <code>status</code> - Index on <code>queued_at</code></p> <p>Relationships: - <code>source</code>: Many-to-one \u2192 <code>Source</code> - <code>snapshots</code>: One-to-many \u2192 <code>Snapshot</code></p>"},{"location":"reference/data-model/#snapshot","title":"Snapshot","text":"<p>Represents a single captured web page.</p> <p>Table: <code>snapshots</code></p> Field Type Nullable Description Identity <code>id</code> Integer No Primary key <code>job_id</code> Integer Yes Foreign key \u2192 <code>archive_jobs.id</code> <code>source_id</code> Integer Yes Foreign key \u2192 <code>sources.id</code> URL &amp; Grouping <code>url</code> String(2000) No Full URL of captured page <code>normalized_url_group</code> String(2000) Yes Canonical URL for grouping Timing <code>capture_timestamp</code> DateTime No When page was captured (from WARC) HTTP &amp; Content <code>mime_type</code> String(100) Yes MIME type (usually <code>\"text/html\"</code>) <code>status_code</code> Integer Yes HTTP status code <code>title</code> String(500) Yes Extracted page title <code>snippet</code> Text Yes Short text preview <code>language</code> String(10) Yes ISO language code (<code>\"en\"</code>, <code>\"fr\"</code>) Storage/Replay <code>warc_path</code> String(500) No Path to WARC file <code>warc_record_id</code> String(200) Yes WARC record identifier <code>raw_snapshot_path</code> String(500) Yes Optional static HTML export path <code>content_hash</code> String(64) Yes Hash of HTML body (for deduplication) Timestamps <code>created_at</code> DateTime No Record creation <code>updated_at</code> DateTime No Last update <p>Indexes: - Index on <code>job_id</code> - Index on <code>source_id</code> - Index on <code>url</code> - Index on <code>normalized_url_group</code> - Index on <code>capture_timestamp</code> - Index on <code>status_code</code></p> <p>Relationships: - <code>job</code>: Many-to-one \u2192 <code>ArchiveJob</code> - <code>source</code>: Many-to-one \u2192 <code>Source</code></p>"},{"location":"reference/data-model/#job-status-lifecycle","title":"Job Status Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; queued: create-job\n    queued --&gt; running: worker starts\n    running --&gt; completed: crawl succeeds\n    running --&gt; failed: crawl fails\n    failed --&gt; retryable: retry if count &lt; MAX\n    retryable --&gt; running: worker retries\n    completed --&gt; indexing: index-job starts\n    indexing --&gt; indexed: indexing succeeds\n    indexing --&gt; index_failed: indexing fails\n    indexed --&gt; [*]\n    failed --&gt; [*]\n    index_failed --&gt; completed: retry-job (reindex)</code></pre> <p>Common status values: - <code>queued</code> - Job created, waiting for worker - <code>running</code> - Crawl in progress - <code>completed</code> - Crawl succeeded - <code>failed</code> - Crawl failed (terminal if retries exhausted) - <code>retryable</code> - Failed but can retry - <code>indexing</code> - WARC indexing in progress - <code>indexed</code> - Fully indexed and ready to serve - <code>index_failed</code> - Indexing failed</p>"},{"location":"reference/data-model/#config-json-schema","title":"Config JSON Schema","text":"<p>ArchiveJob.config structure:</p> <pre><code>{\n  \"seeds\": [\n    \"https://www.canada.ca/en/health-canada.html\"\n  ],\n  \"zimit_passthrough_args\": [\n    \"--profile\", \"social-media\"\n  ],\n  \"tool_options\": {\n    \"cleanup\": false,\n    \"overwrite\": false,\n    \"skip_final_build\": false,\n    \"enable_monitoring\": false,\n    \"enable_adaptive_workers\": false,\n    \"enable_adaptive_restart\": false,\n    \"enable_vpn_rotation\": false,\n    \"initial_workers\": 2,\n    \"log_level\": \"INFO\",\n    \"relax_perms\": true,\n    \"docker_shm_size\": \"1g\",\n    \"monitor_interval_seconds\": 30,\n    \"stall_timeout_minutes\": 30,\n    \"error_threshold_timeout\": 10,\n    \"error_threshold_http\": 10,\n    \"min_workers\": 1,\n    \"max_worker_reductions\": 2,\n    \"vpn_connect_command\": \"vpn connect ca\",\n    \"max_vpn_rotations\": 3,\n    \"vpn_rotation_frequency_minutes\": 60,\n    \"backoff_delay_minutes\": 15\n  }\n}\n</code></pre> <p>See: Job Registry for defaults per source</p>"},{"location":"reference/data-model/#database-configuration","title":"Database Configuration","text":"<p>Location: <code>src/ha_backend/models.py</code></p> <p>ORM: SQLAlchemy 2.0</p> <p>Migrations: Alembic (in <code>alembic/</code> directory)</p>"},{"location":"reference/data-model/#running-migrations","title":"Running Migrations","text":"<pre><code># Upgrade to latest\nalembic upgrade head\n\n# Downgrade one revision\nalembic downgrade -1\n\n# Show current revision\nalembic current\n\n# Show migration history\nalembic history\n</code></pre>"},{"location":"reference/data-model/#supported-databases","title":"Supported Databases","text":"<ul> <li>SQLite (default for dev): <code>sqlite:///healtharchive.db</code></li> <li>PostgreSQL (recommended for production): <code>postgresql://user:pass@host/dbname</code></li> </ul> <p>Environment variable: <code>HEALTHARCHIVE_DATABASE_URL</code></p>"},{"location":"reference/data-model/#common-queries","title":"Common Queries","text":""},{"location":"reference/data-model/#find-jobs-by-status","title":"Find jobs by status","text":"<pre><code>from ha_backend.models import ArchiveJob\nfrom ha_backend.db import get_session\n\nsession = get_session()\njobs = session.query(ArchiveJob).filter_by(status=\"queued\").all()\n</code></pre>"},{"location":"reference/data-model/#get-source-with-all-jobs","title":"Get source with all jobs","text":"<pre><code>from ha_backend.models import Source\n\nsession = get_session()\nsource = session.query(Source).filter_by(code=\"hc\").one()\nprint(f\"{source.name}: {len(source.jobs)} jobs\")\n</code></pre>"},{"location":"reference/data-model/#find-snapshots-by-url","title":"Find snapshots by URL","text":"<pre><code>from ha_backend.models import Snapshot\n\nsession = get_session()\nsnapshots = session.query(Snapshot).filter(\n    Snapshot.url.like(\"%health-canada%\")\n).limit(10).all()\n</code></pre>"},{"location":"reference/data-model/#related-documentation","title":"Related Documentation","text":"<ul> <li>Full Architecture Guide: architecture.md</li> <li>Job Creation: architecture.md#4-job-registry--creation</li> <li>Indexing Pipeline: architecture.md#6-indexing-pipeline</li> <li>CLI Commands: cli-commands.md</li> </ul>"},{"location":"tutorials/architecture-walkthrough/","title":"Architecture Walkthrough","text":"<p>Learn how HealthArchive works by following a web page from crawl to search result.</p> <p>Time: 20-30 minutes Skill Level: Beginner to intermediate Prerequisites: Basic understanding of web applications and databases</p>"},{"location":"tutorials/architecture-walkthrough/#overview-the-big-picture","title":"Overview: The Big Picture","text":"<p>HealthArchive preserves Canadian health government websites through a multi-stage pipeline:</p> <pre><code>graph LR\n    A[Web Pages] --&gt;|Crawl| B[WARCs]\n    B --&gt;|Index| C[Database]\n    C --&gt;|Serve| D[Search API]\n    D --&gt;|Display| E[Public Website]</code></pre> <p>Let's walk through what happens when we archive a page from Health Canada.</p>"},{"location":"tutorials/architecture-walkthrough/#step-1-job-creation","title":"Step 1: Job Creation","text":"<p>Everything starts with creating an ArchiveJob.</p> <pre><code>ha-backend create-job --source hc\n</code></pre> <p>What happens:</p> <ol> <li>Job Registry (<code>ha_backend/job_registry.py:312-332</code>) looks up configuration for source code <code>\"hc\"</code>:</li> <li>Default seeds: <code>[\"https://www.canada.ca/en/health-canada.html\"]</code></li> <li>Initial workers: <code>1</code></li> <li> <p>Tool options: monitoring, adaptive workers, etc.</p> </li> <li> <p>Job Creation (<code>ha_backend/job_registry.py:400-420</code>):</p> </li> <li>Generates job name: <code>hc-20260118</code> (using today's date)</li> <li>Creates output directory: <code>/mnt/nasd/nobak/healtharchive/jobs/hc/20260118T210911Z__hc-20260118</code></li> <li>Inserts <code>ArchiveJob</code> row with <code>status=\"queued\"</code></li> </ol> <p>Database state after creation:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 name       \u2502 status   \u2502 output_dir                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 42 \u2502 hc-20260118\u2502 queued   \u2502 /mnt/.../20260118T...hc-20260118\u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#step-2-worker-picks-up-the-job","title":"Step 2: Worker Picks Up the Job","text":"<p>The worker runs continuously, polling for queued jobs:</p> <pre><code>ha-backend start-worker --poll-interval 30\n</code></pre> <p>Worker Loop (<code>ha_backend/worker/main.py:1087-1140</code>):</p> <pre><code>sequenceDiagram\n    participant W as Worker\n    participant DB as Database\n    participant J as Jobs Module\n\n    loop Every 30 seconds\n        W-&gt;&gt;DB: SELECT next queued job\n        alt Job found\n            DB-&gt;&gt;W: Return job_id=42\n            W-&gt;&gt;J: run_persistent_job(42)\n            J--&gt;&gt;W: Return exit_code\n            alt Exit code != 0\n                W-&gt;&gt;DB: Set status=retryable\n            else Success\n                W-&gt;&gt;DB: Set status=completed\n                W-&gt;&gt;J: index_job(42)\n            end\n        else No jobs\n            W-&gt;&gt;W: Sleep 30 seconds\n        end\n    end</code></pre>"},{"location":"tutorials/architecture-walkthrough/#step-3-running-the-crawler","title":"Step 3: Running the Crawler","text":"<p>Job Execution (<code>ha_backend/jobs.py:439-560</code>):</p> <ol> <li> <p>Load job config from database:    <pre><code>{\n  \"seeds\": [\"https://www.canada.ca/en/health-canada.html\"],\n  \"tool_options\": {\n    \"initial_workers\": 2,\n    \"cleanup\": false,\n    \"enable_monitoring\": false,\n    \"skip_final_build\": true\n  }\n}\n</code></pre></p> </li> <li> <p>Build CLI command:    <pre><code>archive-tool \\\n  --name hc-20260118 \\\n  --output-dir /mnt/.../20260118T...hc-20260118 \\\n  --initial-workers 2 \\\n  https://www.canada.ca/en/health-canada.html\n</code></pre></p> </li> <li> <p>Execute as subprocess (<code>jobs.py:529-542</code>):</p> </li> <li>Spawns <code>archive-tool</code> process</li> <li>Streams output to logs</li> <li>Waits for completion</li> </ol>"},{"location":"tutorials/architecture-walkthrough/#step-4-archive-tool-crawls","title":"Step 4: Archive Tool Crawls","text":"<p>The archive_tool (separate subpackage) orchestrates Docker:</p> <pre><code>graph TD\n    AT[archive-tool] --&gt;|Spawns| D[Docker Container]\n    D --&gt;|Runs| Z[zimit crawler]\n    Z --&gt;|Writes| W[WARC files]\n    Z --&gt;|Builds| ZIM[ZIM file]\n    W --&gt;|Stored in| TD[.tmp_1/ directory]</code></pre> <p>Key files created (see <code>src/archive_tool/docs/documentation.md</code>):</p> <pre><code>output_dir/\n\u251c\u2500\u2500 .archive_state.json          # Tracks crawl state\n\u251c\u2500\u2500 .tmp_1/                      # Temporary crawl artifacts\n\u2502   \u2514\u2500\u2500 collections/\n\u2502       \u2514\u2500\u2500 crawl-20260118.../\n\u2502           \u2514\u2500\u2500 archive/\n\u2502               \u251c\u2500\u2500 rec-00000-20260118.warc.gz\n\u2502               \u251c\u2500\u2500 rec-00001-20260118.warc.gz\n\u2502               \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 zim/\n    \u2514\u2500\u2500 hc-20260118_2026-01-18.zim\n</code></pre> <p>What's in a WARC?</p> <p>WARC (Web ARChive) files contain: - HTTP request/response pairs - Headers and content - Capture timestamps - Record metadata</p> <p>Example WARC record: <pre><code>WARC/1.0\nWARC-Type: response\nWARC-Date: 2026-01-18T21:15:42Z\nWARC-Record-ID: &lt;urn:uuid:12345...&gt;\nContent-Type: application/http; msgtype=response\n\nHTTP/1.1 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 45678\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;&lt;title&gt;Health Canada&lt;/title&gt;&lt;/head&gt;\n  &lt;body&gt;...actual page content...&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"tutorials/architecture-walkthrough/#step-5-crawl-completes","title":"Step 5: Crawl Completes","text":"<p>When <code>archive-tool</code> exits:</p> <ol> <li>Exit code 0 (success):</li> <li>Worker sets <code>job.status = \"completed\"</code></li> <li>Worker sets <code>job.crawler_exit_code = 0</code></li> <li> <p>Worker proceeds to indexing</p> </li> <li> <p>Exit code != 0 (failure):</p> </li> <li>Worker checks <code>retry_count &lt; MAX_CRAWL_RETRIES</code></li> <li>If retries available: set <code>status = \"retryable\"</code></li> <li>Otherwise: set <code>status = \"failed\"</code></li> </ol> <p>Database state after successful crawl:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 name        \u2502 status    \u2502 crawler_exit \u2502 warc_count\u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 42 \u2502 hc-20260118 \u2502 completed \u2502 0            \u2502 NULL      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#step-6-warc-indexing","title":"Step 6: WARC Indexing","text":"<p>The worker automatically calls <code>index_job(42)</code> after successful crawl.</p> <p>Indexing Pipeline (<code>ha_backend/indexing/pipeline.py:743-778</code>):</p> <pre><code>graph TD\n    A[Discover WARCs] --&gt; B[Read WARC Records]\n    B --&gt; C[Extract Text]\n    C --&gt; D[Create Snapshots]\n    D --&gt; E[Save to Database]</code></pre>"},{"location":"tutorials/architecture-walkthrough/#61-warc-discovery","title":"6.1 WARC Discovery","text":"<p>Discovery process (<code>ha_backend/indexing/warc_discovery.py:660-689</code>):</p> <ol> <li>Load crawl state from <code>.archive_state.json</code></li> <li>Get temporary directories from state</li> <li>Find all <code>*.warc.gz</code> files in temp dirs</li> <li>Return list of WARC paths</li> </ol> <pre><code>warc_paths = discover_warcs_for_job(job)\n# Returns: [\n#   Path(\"/mnt/.../rec-00000-20260118.warc.gz\"),\n#   Path(\"/mnt/.../rec-00001-20260118.warc.gz\"),\n# ]\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#62-reading-warc-records","title":"6.2 Reading WARC Records","text":"<p>WARC Reader (<code>ha_backend/indexing/warc_reader.py</code>):</p> <pre><code>for record in iter_html_records(warc_path):\n    # record.url = \"https://www.canada.ca/en/health-canada.html\"\n    # record.capture_timestamp = datetime(2026, 1, 18, 21, 15, 42)\n    # record.body_bytes = b\"&lt;!DOCTYPE html&gt;...\"\n    # record.warc_record_id = \"&lt;urn:uuid:12345...&gt;\"\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#63-text-extraction","title":"6.3 Text Extraction","text":"<p>Text Extraction (<code>ha_backend/indexing/text_extraction.py</code>):</p> <pre><code># Decode HTML\nhtml = record.body_bytes.decode(\"utf-8\", errors=\"replace\")\n\n# Extract metadata\ntitle = extract_title(html)\n# Returns: \"Health Canada - Canada.ca\"\n\ntext = extract_text(html)\n# Returns: \"Health Canada\\nAbout Health Canada\\n...\"\n\nsnippet = make_snippet(text)\n# Returns: \"Health Canada works to help Canadians maintain...\"\n\nlanguage = detect_language(text, record.headers)\n# Returns: \"en\"\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#64-creating-snapshots","title":"6.4 Creating Snapshots","text":"<p>Mapping (<code>ha_backend/indexing/mapping.py:724-739</code>):</p> <pre><code>snapshot = Snapshot(\n    job_id=42,\n    source_id=1,  # hc\n    url=record.url,\n    normalized_url_group=normalize_url(record.url),\n    capture_timestamp=record.capture_timestamp,\n    title=title,\n    snippet=snippet,\n    language=language,\n    warc_path=str(record.warc_path),\n    warc_record_id=record.warc_record_id,\n    mime_type=\"text/html\",\n    status_code=200,\n)\nsession.add(snapshot)\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#65-database-result","title":"6.5 Database Result","text":"<p>After indexing all WARC records:</p> <pre><code>snapshots table:\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 job_id \u2502 source  \u2502 url                          \u2502 title                \u2502 language\u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1  \u2502 42     \u2502 hc      \u2502 https://www.canada.ca/...    \u2502 Health Canada        \u2502 en      \u2502\n\u2502 2  \u2502 42     \u2502 hc      \u2502 https://www.canada.ca/...    \u2502 About Health Canada  \u2502 en      \u2502\n\u2502 3  \u2502 42     \u2502 hc      \u2502 https://www.canada.ca/...    \u2502 Services             \u2502 en      \u2502\n\u2502 ...\u2502 ...    \u2502 ...     \u2502 ...                          \u2502 ...                  \u2502 ...     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\narchive_jobs table:\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 name        \u2502 status  \u2502 warc_count   \u2502 indexed_page_count \u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 42 \u2502 hc-20260118 \u2502 indexed \u2502 245          \u2502 12,347             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#step-7-serving-via-api","title":"Step 7: Serving via API","text":"<p>Now the indexed snapshots are searchable via the public API.</p>"},{"location":"tutorials/architecture-walkthrough/#71-search-request","title":"7.1 Search Request","text":"<pre><code>curl \"https://api.healtharchive.ca/api/search?q=vaccines&amp;sort=relevance\"\n</code></pre> <p>API Route (<code>ha_backend/api/routes_public.py:885-946</code>):</p> <pre><code>sequenceDiagram\n    participant C as Client\n    participant API as FastAPI\n    participant DB as Database\n\n    C-&gt;&gt;API: GET /api/search?q=vaccines\n    API-&gt;&gt;DB: Build search query\n    alt Postgres with FTS\n        DB-&gt;&gt;DB: Use ts_rank_cd + search_vector\n    else SQLite or no FTS\n        DB-&gt;&gt;DB: Substring match on title/snippet\n    end\n    DB--&gt;&gt;API: Return matching snapshots\n    API-&gt;&gt;API: Apply pagination (page=1, size=20)\n    API--&gt;&gt;C: Return SearchResponseSchema</code></pre>"},{"location":"tutorials/architecture-walkthrough/#72-query-building","title":"7.2 Query Building","text":"<p>For Postgres (with full-text search):</p> <pre><code>SELECT *\nFROM snapshots\nWHERE search_vector @@ websearch_to_tsquery('english', 'vaccines')\n  AND source_id = (SELECT id FROM sources WHERE code = 'hc')\n  AND (status_code IS NULL OR status_code BETWEEN 200 AND 299)\nORDER BY ts_rank_cd(search_vector, websearch_to_tsquery('english', 'vaccines')) DESC\nLIMIT 20 OFFSET 0;\n</code></pre> <p>For SQLite (substring matching):</p> <pre><code>SELECT *\nFROM snapshots\nWHERE (\n    LOWER(title) LIKE '%vaccines%'\n    OR LOWER(snippet) LIKE '%vaccines%'\n    OR LOWER(url) LIKE '%vaccines%'\n  )\n  AND source_id = (SELECT id FROM sources WHERE code = 'hc')\n  AND (status_code IS NULL OR status_code BETWEEN 200 AND 299)\nORDER BY\n    CASE\n        WHEN LOWER(title) LIKE '%vaccines%' THEN 3\n        WHEN LOWER(url) LIKE '%vaccines%' THEN 2\n        ELSE 1\n    END DESC,\n    capture_timestamp DESC\nLIMIT 20 OFFSET 0;\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#73-response-format","title":"7.3 Response Format","text":"<pre><code>{\n  \"results\": [\n    {\n      \"id\": 1,\n      \"title\": \"COVID-19 vaccines: Authorization and safety\",\n      \"sourceCode\": \"hc\",\n      \"sourceName\": \"Health Canada\",\n      \"language\": \"en\",\n      \"captureDate\": \"2026-01-18T21:15:42Z\",\n      \"originalUrl\": \"https://www.canada.ca/en/health-canada/services/drugs-health-products/covid19-industry/drugs-vaccines-treatments/vaccines.html\",\n      \"snippet\": \"Health Canada has approved COVID-19 vaccines for use in Canada...\",\n      \"rawSnapshotUrl\": \"/api/snapshots/raw/1\"\n    }\n  ],\n  \"total\": 127,\n  \"page\": 1,\n  \"pageSize\": 20\n}\n</code></pre>"},{"location":"tutorials/architecture-walkthrough/#step-8-viewing-archived-content","title":"Step 8: Viewing Archived Content","text":"<p>When a user clicks a search result, the frontend requests the archived HTML:</p> <pre><code>curl \"https://api.healtharchive.ca/api/snapshots/raw/1\"\n</code></pre> <p>Replay Process (<code>ha_backend/indexing/viewer.py:782-799</code>):</p> <pre><code>sequenceDiagram\n    participant C as Client\n    participant API as FastAPI\n    participant DB as Database\n    participant WARC as WARC File\n\n    C-&gt;&gt;API: GET /api/snapshots/raw/1\n    API-&gt;&gt;DB: SELECT snapshot WHERE id=1\n    DB--&gt;&gt;API: Return snapshot with warc_path\n    API-&gt;&gt;WARC: Open WARC file\n    API-&gt;&gt;WARC: Find record by warc_record_id\n    WARC--&gt;&gt;API: Return archived HTML\n    API--&gt;&gt;C: Return HTMLResponse</code></pre> <p>Output: The archived HTML exactly as it was captured, with a HealthArchive banner added for context.</p>"},{"location":"tutorials/architecture-walkthrough/#system-architecture-diagram","title":"System Architecture Diagram","text":"<p>Putting it all together:</p> <pre><code>graph TB\n    subgraph \"Crawl Phase\"\n        CLI[CLI: create-job] --&gt;|Insert| DB1[(Database)]\n        W[Worker Loop] --&gt;|Poll| DB1\n        W --&gt;|Execute| AT[archive-tool]\n        AT --&gt;|Docker| Z[zimit]\n        Z --&gt;|Write| WARC[WARC Files]\n    end\n\n    subgraph \"Index Phase\"\n        W2[Worker] --&gt;|Discover| WARC\n        WARC --&gt;|Read| IDX[Indexer]\n        IDX --&gt;|Extract| TXT[Text Extraction]\n        TXT --&gt;|Create| SNAP[Snapshots]\n        SNAP --&gt;|Insert| DB2[(Database)]\n    end\n\n    subgraph \"Serve Phase\"\n        FE[Frontend] --&gt;|API Request| API[FastAPI]\n        API --&gt;|Query| DB3[(Database)]\n        DB3 --&gt;|Results| API\n        API --&gt;|JSON| FE\n        FE --&gt;|Request Raw| API\n        API --&gt;|Read| WARC2[WARC Files]\n        WARC2 --&gt;|HTML| API\n        API --&gt;|HTML| FE\n    end\n\n    DB1 --&gt; DB2\n    DB2 --&gt; DB3</code></pre>"},{"location":"tutorials/architecture-walkthrough/#key-components-summary","title":"Key Components Summary","text":"Component Location Role Job Registry <code>ha_backend/job_registry.py</code> Source configs, job creation Worker <code>ha_backend/worker/main.py</code> Job polling and execution Jobs Module <code>ha_backend/jobs.py</code> Runs archive-tool subprocess Archive Tool <code>src/archive_tool/</code> Docker orchestration, crawling WARC Discovery <code>ha_backend/indexing/warc_discovery.py</code> Find WARC files WARC Reader <code>ha_backend/indexing/warc_reader.py</code> Stream WARC records Text Extraction <code>ha_backend/indexing/text_extraction.py</code> Extract title/text/snippet Indexer <code>ha_backend/indexing/pipeline.py</code> Orchestrate indexing Public API <code>ha_backend/api/routes_public.py</code> Search, stats, snapshots Admin API <code>ha_backend/api/routes_admin.py</code> Job management Database Models <code>ha_backend/models.py</code> ORM definitions"},{"location":"tutorials/architecture-walkthrough/#data-flow-summary","title":"Data Flow Summary","text":"<ol> <li>Create: CLI creates <code>ArchiveJob</code> \u2192 database</li> <li>Queue: Worker polls \u2192 finds queued job</li> <li>Crawl: Worker runs <code>archive-tool</code> \u2192 writes WARCs</li> <li>Discover: Indexer finds WARCs in output directory</li> <li>Extract: Parse WARCs \u2192 extract text and metadata</li> <li>Store: Create <code>Snapshot</code> rows \u2192 database</li> <li>Search: API queries snapshots \u2192 returns results</li> <li>View: API reads WARC \u2192 serves archived HTML</li> </ol>"},{"location":"tutorials/architecture-walkthrough/#next-steps","title":"Next Steps","text":"<p>Now that you understand the architecture:</p> <ol> <li>Try it locally: Follow Live Testing to run the pipeline</li> <li>Debug a crawl: See Debugging a Failed Crawl</li> <li>Dive deeper: Read the full Architecture Guide</li> <li>Explore the code: Browse the components in the table above</li> </ol>"},{"location":"tutorials/architecture-walkthrough/#further-reading","title":"Further Reading","text":"<ul> <li>Archive Tool Internals: <code>src/archive_tool/docs/documentation.md</code></li> <li>WARC Format: WARC specification</li> <li>Full-Text Search: PostgreSQL <code>tsvector</code> and <code>ts_rank_cd</code> docs</li> <li>FastAPI: FastAPI documentation</li> </ul>"},{"location":"tutorials/architecture-walkthrough/#questions","title":"Questions?","text":"<ul> <li>How do I add a new source? See <code>ha_backend/job_registry.py</code> and add to <code>SOURCE_CONFIGS</code></li> <li>How do I customize crawl options? Modify <code>default_tool_options</code> in source config</li> <li>How do I improve search ranking? Review search logic in <code>routes_public.py:885-946</code></li> <li>Where are WARCs stored long-term? In the job's output directory under <code>archive_root</code></li> </ul> <p>Still have questions? Check the How-To Guides or ask in GitHub Discussions.</p>"},{"location":"tutorials/debug-crawl/","title":"Debugging a Failed Crawl","text":"<p>A practical tutorial for diagnosing and fixing common crawl failures.</p> <p>Scenario: You created a crawl job, but it failed. Now what?</p> <p>Time: 15-30 minutes Prerequisites: Basic command line skills, access to the backend server</p>"},{"location":"tutorials/debug-crawl/#step-1-identify-the-failed-job","title":"Step 1: Identify the Failed Job","text":"<p>First, find the job ID and understand what went wrong.</p>"},{"location":"tutorials/debug-crawl/#check-job-status","title":"Check Job Status","text":"<pre><code>ha-backend list-jobs\n</code></pre> <p>Example output: <pre><code>ID  Name            Source  Status  Queued              Started             Finished            Pages\n42  hc-20260118     hc      failed  2026-01-18 20:00:00 2026-01-18 20:05:00 2026-01-18 20:45:00 0\n</code></pre></p>"},{"location":"tutorials/debug-crawl/#get-detailed-job-info","title":"Get Detailed Job Info","text":"<pre><code>ha-backend show-job --id 42\n</code></pre> <p>Look for these key fields: <pre><code>{\n  \"id\": 42,\n  \"name\": \"hc-20260118\",\n  \"status\": \"failed\",\n  \"crawler_exit_code\": 1,\n  \"crawler_status\": \"failed\",\n  \"pages_crawled\": 147,\n  \"pages_total\": 500,\n  \"pages_failed\": 12,\n  \"output_dir\": \"/mnt/nasd/nobak/healtharchive/jobs/hc/20260118T200500Z__hc-20260118\",\n  \"combined_log_path\": \"/mnt/.../archive_crawl_20260118T200511Z.combined.log\",\n  \"retry_count\": 0\n}\n</code></pre></p> <p>Key indicators: - <code>crawler_exit_code != 0</code> \u2192 Archive tool process failed - <code>crawler_status = \"failed\"</code> \u2192 Crawl did not complete successfully - <code>retry_count</code> \u2192 How many times we've already retried</p>"},{"location":"tutorials/debug-crawl/#step-2-check-the-crawl-logs","title":"Step 2: Check the Crawl Logs","text":"<p>Logs are your best friend for debugging. Let's examine them systematically.</p>"},{"location":"tutorials/debug-crawl/#find-the-log-file","title":"Find the Log File","text":"<p>The <code>combined_log_path</code> from <code>show-job</code> tells you where to look:</p> <pre><code>JOB_ID=42\nOUTPUT_DIR=$(ha-backend show-job --id $JOB_ID | jq -r '.outputDir')\nLOG_PATH=$(ha-backend show-job --id $JOB_ID | jq -r '.combinedLogPath')\n\n# View the log\nless \"$LOG_PATH\"\n</code></pre>"},{"location":"tutorials/debug-crawl/#common-log-patterns","title":"Common Log Patterns","text":""},{"location":"tutorials/debug-crawl/#1-permission-denied","title":"1. Permission Denied","text":"<pre><code>ERROR: Permission denied: '/mnt/nasd/nobak/healtharchive/jobs/hc/...'\n</code></pre> <p>Diagnosis: Output directory has wrong permissions</p> <p>Fix: <pre><code># Check permissions\nls -la \"$(dirname \"$OUTPUT_DIR\")\"\n\n# Fix ownership (if needed)\nsudo chown -R healtharchive:healtharchive \"$OUTPUT_DIR\"\nsudo chmod -R 755 \"$OUTPUT_DIR\"\n</code></pre></p> <p>Root cause: Often happens after manual operations as root user</p>"},{"location":"tutorials/debug-crawl/#2-docker-not-running","title":"2. Docker Not Running","text":"<pre><code>ERROR: Cannot connect to the Docker daemon at unix:///var/run/docker.sock\n</code></pre> <p>Diagnosis: Docker service is down</p> <p>Fix: <pre><code># Check Docker status\nsudo systemctl status docker\n\n# Start Docker if stopped\nsudo systemctl start docker\n\n# Verify Docker works\ndocker ps\n</code></pre></p> <p>Prevention: Enable Docker to start on boot: <pre><code>sudo systemctl enable docker\n</code></pre></p>"},{"location":"tutorials/debug-crawl/#3-out-of-disk-space","title":"3. Out of Disk Space","text":"<pre><code>ERROR: No space left on device\n</code></pre> <p>Diagnosis: Disk full</p> <p>Fix: <pre><code># Check disk usage\ndf -h /mnt/nasd/nobak\n\n# Find large directories\ndu -sh /mnt/nasd/nobak/healtharchive/jobs/* | sort -rh | head -10\n\n# Clean up old jobs (carefully!)\nha-backend cleanup-job --id OLD_JOB_ID --mode temp\n\n# Or manually remove old temp directories\nrm -rf /mnt/nasd/nobak/healtharchive/jobs/hc/*/.tmp_*\n</code></pre></p> <p>See: <code>operations/playbooks/crawl/cleanup-automation.md</code></p>"},{"location":"tutorials/debug-crawl/#4-network-timeout","title":"4. Network Timeout","text":"<pre><code>WARNING: Request timeout for https://www.canada.ca/...\nERROR: Max retries exceeded\n</code></pre> <p>Diagnosis: Network connectivity issues or slow responses</p> <p>Fix: <pre><code># Test connectivity\ncurl -I https://www.canada.ca/en/health-canada.html\n\n# Check DNS\ndig www.canada.ca\n\n# If VPN is enabled, check VPN status\ntailscale status\n</code></pre></p> <p>Workaround: Increase timeouts in job config (see Step 5)</p>"},{"location":"tutorials/debug-crawl/#5-crawl-stalled","title":"5. Crawl Stalled","text":"<pre><code>INFO: Pages crawled: 147/500 (29%)\n... [no new log entries for 30+ minutes]\n</code></pre> <p>Diagnosis: Crawl made progress but stopped advancing</p> <p>Possible causes: - Site became unresponsive - Workers all blocked on slow pages - Memory issues in Docker container</p> <p>Fix: <pre><code># Check if Docker container is still running\ndocker ps\n\n# Check Docker logs\ndocker logs $(docker ps -q --filter ancestor=ghcr.io/openzim/zimit)\n\n# Check system resources\nhtop\n\n# If stalled, kill and retry\ndocker stop $(docker ps -q --filter ancestor=ghcr.io/openzim/zimit)\nha-backend retry-job --id 42\n</code></pre></p> <p>See: Real incident report: operations/incidents/2026-01-09-annual-crawl-hc-job-stalled.md</p>"},{"location":"tutorials/debug-crawl/#6-zimit-errors","title":"6. Zimit Errors","text":"<pre><code>zimit: error: unrecognized arguments: --bad-flag\n</code></pre> <p>Diagnosis: Invalid passthrough arguments to zimit</p> <p>Fix: Check job config: <pre><code>ha-backend show-job --id 42 | jq '.config.zimit_passthrough_args'\n</code></pre></p> <p>Remove invalid flags and recreate job with correct config</p> <p>Reference: See zimit documentation for valid flags</p>"},{"location":"tutorials/debug-crawl/#step-3-inspect-the-job-directory","title":"Step 3: Inspect the Job Directory","text":"<p>Sometimes logs don't tell the full story. Let's check the filesystem.</p>"},{"location":"tutorials/debug-crawl/#navigate-to-output-directory","title":"Navigate to Output Directory","text":"<pre><code>cd \"$OUTPUT_DIR\"\nls -lah\n</code></pre> <p>Expected structure: <pre><code>.archive_state.json           # Crawl state tracking\n.tmp_1/                       # Temporary crawl artifacts\n\u251c\u2500\u2500 collections/\n\u2502   \u2514\u2500\u2500 crawl-20260118.../\n\u2502       \u2514\u2500\u2500 archive/\n\u2502           \u251c\u2500\u2500 rec-00000-20260118.warc.gz\n\u2502           \u2514\u2500\u2500 ...\narchive_crawl_....log         # Individual stage logs\narchive_crawl_....combined.log # Aggregated log\nzim/                          # ZIM output (if built)\n</code></pre></p>"},{"location":"tutorials/debug-crawl/#check-crawl-state","title":"Check Crawl State","text":"<pre><code>cat .archive_state.json | jq '.'\n</code></pre> <p>Key fields: <pre><code>{\n  \"run_mode\": \"Fresh\",\n  \"temp_dir_counter\": 1,\n  \"temp_dirs\": [\".tmp_1\"],\n  \"current_stage\": \"crawl\",\n  \"is_complete\": false\n}\n</code></pre></p> <p>Indicators: - <code>is_complete: false</code> \u2192 Crawl didn't finish - <code>current_stage</code> \u2192 What stage failed</p>"},{"location":"tutorials/debug-crawl/#check-warc-files","title":"Check WARC Files","text":"<pre><code># Count WARC files\nfind .tmp_1 -name \"*.warc.gz\" | wc -l\n\n# Check sizes\nfind .tmp_1 -name \"*.warc.gz\" -exec ls -lh {} \\; | head -5\n\n# Verify WARCs are readable\nwarcio check .tmp_1/collections/*/archive/rec-00000*.warc.gz\n</code></pre> <p>Red flags: - 0 WARC files \u2192 Crawl never started - Very small WARCs (&lt; 1KB) \u2192 Likely corrupt - WARC validation errors \u2192 Damaged files</p>"},{"location":"tutorials/debug-crawl/#step-4-check-system-resources","title":"Step 4: Check System Resources","text":"<p>Resource exhaustion is a common cause of failures.</p>"},{"location":"tutorials/debug-crawl/#memory","title":"Memory","text":"<pre><code># Current memory usage\nfree -h\n\n# Memory usage during crawl (if still running)\ndocker stats\n</code></pre> <p>Fix for memory issues: - Reduce <code>initial_workers</code> in job config - Add swap space - Upgrade server RAM</p>"},{"location":"tutorials/debug-crawl/#cpu","title":"CPU","text":"<pre><code># CPU load\nuptime\n\n# Top processes\nhtop\n</code></pre> <p>Fix for high CPU: - Reduce worker count - Check for competing processes - Consider time-based scheduling</p>"},{"location":"tutorials/debug-crawl/#disk-io","title":"Disk I/O","text":"<pre><code># Check I/O wait\niostat -x 1 5\n\n# Disk usage\ndf -h\n\n# Inode usage (can be exhausted even with space available)\ndf -i\n</code></pre>"},{"location":"tutorials/debug-crawl/#step-5-retry-with-adjustments","title":"Step 5: Retry with Adjustments","text":"<p>Now that you've identified the issue, let's fix it and retry.</p>"},{"location":"tutorials/debug-crawl/#simple-retry-no-changes","title":"Simple Retry (No Changes)","text":"<p>If the issue was transient (network blip, temporary resource exhaustion):</p> <pre><code>ha-backend retry-job --id 42\n</code></pre> <p>This sets <code>status = \"retryable\"</code> and the worker will pick it up.</p>"},{"location":"tutorials/debug-crawl/#retry-with-modified-config","title":"Retry with Modified Config","text":"<p>If you need to change job settings, create a new job with overrides:</p> <pre><code>ha-backend create-job --source hc \\\n  --override '{\"tool_options\": {\"initial_workers\": 2, \"enable_monitoring\": true, \"stall_timeout_minutes\": 60}}'\n</code></pre> <p>Common overrides: - <code>initial_workers: 2</code> \u2192 More parallelism (or <code>1</code> if resource-constrained) - <code>enable_monitoring: true</code> \u2192 Enable stall detection - <code>stall_timeout_minutes: 60</code> \u2192 Abort if no progress for 60 mins - <code>error_threshold_timeout: 50</code> \u2192 Tolerate more timeouts before adaptations - <code>error_threshold_http: 50</code> \u2192 Tolerate more HTTP/network errors before adaptations - <code>backoff_delay_minutes: 2</code> \u2192 Shorten post-adaptation sleep on single-worker hosts - <code>page_limit: 1000</code> \u2192 Limit crawl scope for development/testing (avoid for annual campaign completeness)</p>"},{"location":"tutorials/debug-crawl/#resume-existing-crawl","title":"Resume Existing Crawl","text":"<p>Archive tool can resume from existing state:</p> <pre><code># The output_dir still exists, so just retry\nha-backend retry-job --id 42\n</code></pre> <p>Archive tool will detect <code>.archive_state.json</code> and resume.</p> <p>Resume behavior (see <code>src/archive_tool/docs/documentation.md</code>): - <code>run_mode: \"Resume\"</code> if state indicates incomplete crawl - Reuses existing WARCs - Continues from last checkpoint</p>"},{"location":"tutorials/debug-crawl/#step-6-verify-the-fix","title":"Step 6: Verify the Fix","text":"<p>After retrying, monitor the job closely.</p>"},{"location":"tutorials/debug-crawl/#watch-job-progress","title":"Watch Job Progress","text":"<pre><code># Poll job status\nwatch -n 30 'ha-backend show-job --id 42 | jq \".status, .pagesCrawled, .pagesTotal\"'\n</code></pre>"},{"location":"tutorials/debug-crawl/#tail-the-logs","title":"Tail the Logs","text":"<pre><code>tail -f \"$LOG_PATH\"\n</code></pre> <p>Look for: - <code>INFO: Pages crawled: X/Y</code> \u2192 Progress increasing - <code>INFO: Crawl stage completed successfully</code> \u2192 Success - No new ERROR lines</p>"},{"location":"tutorials/debug-crawl/#check-metrics","title":"Check Metrics","text":"<p>If you have Prometheus metrics enabled:</p> <pre><code>curl -H \"X-Admin-Token: $HEALTHARCHIVE_ADMIN_TOKEN\" \\\n  https://api.healtharchive.ca/metrics | grep healtharchive_jobs\n</code></pre>"},{"location":"tutorials/debug-crawl/#step-7-post-mortem-for-serious-failures","title":"Step 7: Post-Mortem (For Serious Failures)","text":"<p>If this was a significant failure (e.g., production annual crawl), document it.</p>"},{"location":"tutorials/debug-crawl/#create-incident-note","title":"Create Incident Note","text":"<pre><code>cp docs/_templates/incident-template.md \\\n   docs/operations/incidents/$(date +%Y-%m-%d)-brief-description.md\n</code></pre> <p>Fill in: - Timeline of events - Impact (pages missed, data loss) - Root cause - Resolution steps - Preventive measures</p> <p>Example: operations/incidents/2026-01-09-annual-crawl-hc-job-stalled.md</p>"},{"location":"tutorials/debug-crawl/#update-runbooks","title":"Update Runbooks","text":"<p>If you discovered a new failure mode or solution:</p> <ol> <li>Update the relevant playbook or this tutorial</li> <li>Add to troubleshooting FAQ</li> <li>Submit a PR</li> </ol>"},{"location":"tutorials/debug-crawl/#common-failure-scenarios-solutions","title":"Common Failure Scenarios &amp; Solutions","text":"Symptom Likely Cause Fix Prevention <code>exit_code: 1</code>, \"Permission denied\" Wrong file permissions <code>chown</code>/<code>chmod</code> output dir Use dedicated user, avoid sudo <code>exit_code: 125</code>, \"Docker not found\" Docker not running <code>systemctl start docker</code> Enable Docker on boot \"No space left\" in logs Disk full Cleanup old jobs Monitor disk usage, automate cleanup Crawl stalled, no progress Network issues or slow site Enable monitoring, retry Use <code>stall_timeout_minutes</code> 0 WARCs created Crawl failed immediately Check seeds, Docker logs Validate seeds before creating job WARCs exist but index fails WARC corruption Re-crawl or skip corrupt files Verify WARC integrity post-crawl <code>retry_count: 3</code>, still failing Persistent issue Manual intervention needed Review config, escalate"},{"location":"tutorials/debug-crawl/#debugging-checklist","title":"Debugging Checklist","text":"<p>Use this checklist for systematic debugging:</p> <ul> <li> Identify job ID and get detailed status (<code>show-job</code>)</li> <li> Read crawl logs (<code>combined_log_path</code>)</li> <li> Check for common error patterns (permissions, Docker, disk, network)</li> <li> Inspect job directory structure and files</li> <li> Verify WARC files exist and are valid</li> <li> Check system resources (memory, CPU, disk)</li> <li> Identify root cause</li> <li> Apply fix (permissions, config, cleanup)</li> <li> Retry job with adjustments</li> <li> Monitor retry for success</li> <li> Document incident if significant</li> <li> Update runbooks/playbooks with learnings</li> </ul>"},{"location":"tutorials/debug-crawl/#advanced-debugging","title":"Advanced Debugging","text":""},{"location":"tutorials/debug-crawl/#enable-debug-logging","title":"Enable Debug Logging","text":"<p>Create job with verbose logging:</p> <pre><code>ha-backend create-job --source hc \\\n  --override '{\"tool_options\": {\"log_level\": \"DEBUG\"}}'\n</code></pre>"},{"location":"tutorials/debug-crawl/#run-archive-tool-manually","title":"Run Archive Tool Manually","text":"<p>For deep debugging, run archive-tool outside the backend:</p> <pre><code>archive-tool \\\n  --name debug-crawl \\\n  --output-dir /tmp/debug-crawl \\\n  --initial-workers 1 \\\n  --log-level DEBUG \\\n  https://www.canada.ca/en/health-canada.html\n</code></pre>"},{"location":"tutorials/debug-crawl/#inspect-docker-container","title":"Inspect Docker Container","text":"<p>If the container is still running:</p> <pre><code># Get container ID\nCONTAINER_ID=$(docker ps -q --filter ancestor=ghcr.io/openzim/zimit)\n\n# Check logs\ndocker logs $CONTAINER_ID\n\n# Exec into container\ndocker exec -it $CONTAINER_ID /bin/bash\n\n# Inside container:\n# - Check /output/\n# - Review zimit logs\n# - Inspect environment\n</code></pre>"},{"location":"tutorials/debug-crawl/#check-database-state","title":"Check Database State","text":"<pre><code># Connect to database\nsqlite3 healtharchive.db  # or psql for Postgres\n\n# Check job status\nSELECT id, name, status, crawler_exit_code, pages_crawled, pages_total\nFROM archive_jobs\nWHERE id = 42;\n\n# Check if any snapshots were indexed\nSELECT COUNT(*) FROM snapshots WHERE job_id = 42;\n</code></pre>"},{"location":"tutorials/debug-crawl/#getting-help","title":"Getting Help","text":"<p>If you're still stuck:</p> <ol> <li>Check existing incidents: operations/incidents/</li> <li>Review playbooks: operations/playbooks/</li> <li>Search GitHub issues: github.com/jerdaw/healtharchive-backend/issues</li> <li>Ask for help: Open a new issue with:</li> <li>Job ID and status output</li> <li>Relevant log excerpts</li> <li>Steps you've already tried</li> <li>Consult archive-tool docs: <code>src/archive_tool/docs/documentation.md</code></li> </ol>"},{"location":"tutorials/debug-crawl/#related-resources","title":"Related Resources","text":"<ul> <li>Architecture Guide: architecture.md</li> <li>Archive Tool Documentation: <code>src/archive_tool/docs/documentation.md</code></li> <li>Incident Response: operations/playbooks/core/incident-response.md</li> </ul>"},{"location":"tutorials/debug-crawl/#conclusion","title":"Conclusion","text":"<p>Most crawl failures fall into a few categories: - Permissions: Fix ownership and modes - Resources: Free up disk, memory, or adjust worker count - Configuration: Correct invalid options or seeds - Network: Retry or adjust timeouts</p> <p>With systematic debugging, you can identify and fix most issues quickly. Document significant failures so the next operator can benefit from your learnings!</p>"},{"location":"tutorials/first-contribution/","title":"Your First Contribution","text":"<p>This tutorial walks you through making your first code contribution to HealthArchive, from setup to merged pull request.</p> <p>Time: 30-45 minutes Prerequisites: - Git installed - Python 3.11+ installed - Basic command line knowledge - GitHub account</p>"},{"location":"tutorials/first-contribution/#step-1-fork-and-clone","title":"Step 1: Fork and Clone","text":"<ol> <li>Fork the repository on GitHub:</li> <li>Visit github.com/jerdaw/healtharchive-backend</li> <li> <p>Click \"Fork\" in the top-right corner</p> </li> <li> <p>Clone your fork:    <pre><code>git clone https://github.com/YOUR-USERNAME/healtharchive-backend.git\ncd healtharchive-backend\n</code></pre></p> </li> <li> <p>Add upstream remote (to sync with the main repo later):    <pre><code>git remote add upstream https://github.com/jerdaw/healtharchive-backend.git\n</code></pre></p> </li> <li> <p>Verify remotes:    <pre><code>git remote -v\n# Should show:\n# origin    https://github.com/YOUR-USERNAME/healtharchive-backend.git (fetch)\n# origin    https://github.com/YOUR-USERNAME/healtharchive-backend.git (push)\n# upstream  https://github.com/jerdaw/healtharchive-backend.git (fetch)\n# upstream  https://github.com/jerdaw/healtharchive-backend.git (push)\n</code></pre></p> </li> </ol>"},{"location":"tutorials/first-contribution/#step-2-set-up-your-development-environment","title":"Step 2: Set Up Your Development Environment","text":"<ol> <li>Create a virtual environment and install dependencies:    <pre><code>make venv\n</code></pre></li> </ol> <p>This command:    - Creates <code>.venv/</code> directory    - Installs all Python dependencies    - Installs development tools (pytest, ruff, mypy, pre-commit)</p> <ol> <li> <p>Activate the virtual environment:    <pre><code>source .venv/bin/activate\n</code></pre></p> </li> <li> <p>Copy the example environment file:    <pre><code>cp .env.example .env\n</code></pre></p> </li> <li> <p>Edit <code>.env</code> with your local paths (optional, defaults work for most cases):    <pre><code># Example local settings\nHEALTHARCHIVE_DATABASE_URL=sqlite:///$(pwd)/.dev-healtharchive.db\nHEALTHARCHIVE_ARCHIVE_ROOT=$(pwd)/.dev-archive-root\nHEALTHARCHIVE_LOG_LEVEL=INFO\n</code></pre></p> </li> <li> <p>Source the environment:    <pre><code>source .env\n</code></pre></p> </li> <li> <p>Run database migrations:    <pre><code>alembic upgrade head\n</code></pre></p> </li> <li> <p>Seed initial data:    <pre><code>ha-backend seed-sources\n</code></pre></p> </li> </ol>"},{"location":"tutorials/first-contribution/#step-3-verify-your-setup","title":"Step 3: Verify Your Setup","text":"<p>Run the fast CI checks to ensure everything works:</p> <pre><code>make ci\n</code></pre> <p>This runs: - <code>ruff check</code> (linting) - <code>mypy</code> (type checking) - <code>pytest</code> (tests)</p> <p>Expected output: All checks should pass \u2705</p> <p>If anything fails, review the error messages and check: - Python version is 3.11+ - Virtual environment is activated - Dependencies installed correctly</p>"},{"location":"tutorials/first-contribution/#step-4-find-a-good-first-issue","title":"Step 4: Find a Good First Issue","text":"<ol> <li>Browse issues labeled <code>good first issue</code>:</li> <li> <p>Visit github.com/jerdaw/healtharchive-backend/issues?q=is:issue+is:open+label:\"good+first+issue\"</p> </li> <li> <p>Pick an issue that interests you:</p> </li> <li>Look for clear descriptions</li> <li>Check if anyone is already working on it</li> <li> <p>Comment on the issue to claim it</p> </li> <li> <p>No good first issues available? Try:</p> </li> <li>Fix a typo in documentation</li> <li>Add a test for an existing function</li> <li>Improve error messages or log output</li> </ol> <p>For this tutorial, we'll add a simple CLI command as an example.</p>"},{"location":"tutorials/first-contribution/#step-5-create-a-feature-branch","title":"Step 5: Create a Feature Branch","text":"<ol> <li> <p>Sync with upstream (ensure you have latest changes):    <pre><code>git checkout main\ngit pull upstream main\n</code></pre></p> </li> <li> <p>Create a new branch (use a descriptive name):    <pre><code>git checkout -b add-version-command\n</code></pre></p> </li> </ol> <p>Branch naming conventions:    - <code>add-*</code> for new features    - <code>fix-*</code> for bug fixes    - <code>docs-*</code> for documentation changes    - <code>refactor-*</code> for code refactoring</p>"},{"location":"tutorials/first-contribution/#step-6-make-your-change","title":"Step 6: Make Your Change","text":"<p>Let's add a simple <code>--version</code> command to the CLI.</p> <ol> <li> <p>Open <code>src/ha_backend/cli.py</code> in your editor</p> </li> <li> <p>Add a version command (example change):    <pre><code>@cli.command()\ndef version():\n    \"\"\"Display version information.\"\"\"\n    import ha_backend\n    from ha_backend.logging_config import setup_logging\n\n    setup_logging()\n    logger = logging.getLogger(__name__)\n\n    # You would import from a version module in a real implementation\n    version_string = \"0.1.0\"  # Placeholder\n    logger.info(f\"HealthArchive Backend version {version_string}\")\n    print(f\"HealthArchive Backend v{version_string}\")\n</code></pre></p> </li> <li> <p>Test your change manually:    <pre><code>ha-backend version\n# Should output: HealthArchive Backend v0.1.0\n</code></pre></p> </li> </ol>"},{"location":"tutorials/first-contribution/#step-7-write-tests","title":"Step 7: Write Tests","text":"<p>Every code change needs tests. Let's write a test for our new command.</p> <ol> <li>Create or update test file: <code>tests/test_cli_&lt;your_feature&gt;.py</code></li> </ol> <pre><code>\"\"\"Tests for version CLI command.\"\"\"\nimport subprocess\n\n\ndef test_version_command():\n    \"\"\"Test that version command runs and outputs version info.\"\"\"\n    result = subprocess.run(\n        [\"ha-backend\", \"version\"],\n        capture_output=True,\n        text=True,\n    )\n\n    assert result.returncode == 0\n    assert \"HealthArchive Backend\" in result.stdout\n    assert \"0.1.0\" in result.stdout\n</code></pre> <ol> <li> <p>Run the test:    <pre><code>pytest tests/test_cli_&lt;your_feature&gt;.py -v\n</code></pre></p> </li> <li> <p>Verify it passes:    <pre><code>tests/test_cli_version.py::test_version_command PASSED\n</code></pre></p> </li> </ol>"},{"location":"tutorials/first-contribution/#step-8-run-all-checks","title":"Step 8: Run All Checks","text":"<p>Before submitting, ensure all checks pass:</p> <pre><code>make ci\n</code></pre> <p>This runs: 1. Format check: <code>ruff format --check .</code> 2. Lint: <code>ruff check .</code> 3. Type check: <code>mypy .</code> 4. Tests: <code>pytest</code></p> <p>Fix any failures before proceeding.</p> <p>Common fixes: - Formatting issues: Run <code>make format</code> to auto-fix - Linting issues: Fix manually or run <code>ruff check --fix .</code> - Type errors: Add type hints or fix type mismatches - Test failures: Fix the code or update tests</p>"},{"location":"tutorials/first-contribution/#step-9-commit-your-changes","title":"Step 9: Commit Your Changes","text":"<ol> <li> <p>Stage your changes:    <pre><code>git add src/ha_backend/cli/main.py tests/test_cli_version.py\n</code></pre></p> </li> <li> <p>Commit with a clear message:    <pre><code>git commit -m \"feat: add version command to CLI\n\n- Add 'ha-backend version' command\n- Display version information\n- Add test coverage for version command\n\nCloses #123\"\n</code></pre></p> </li> </ol> <p>Commit message conventions:    - First line: <code>type: short description</code> (50 chars or less)    - Types: <code>feat</code>, <code>fix</code>, <code>docs</code>, <code>test</code>, <code>refactor</code>, <code>chore</code>    - Body: Explain what and why (not how)    - Footer: Reference issues with <code>Closes #123</code></p>"},{"location":"tutorials/first-contribution/#step-10-push-and-create-pull-request","title":"Step 10: Push and Create Pull Request","text":"<ol> <li> <p>Push your branch to your fork:    <pre><code>git push origin add-version-command\n</code></pre></p> </li> <li> <p>Create a pull request on GitHub:</p> </li> <li>Visit your fork on GitHub</li> <li>Click \"Compare &amp; pull request\" button</li> <li> <p>Fill in the PR template:</p> <ul> <li>Title: Clear, concise description</li> <li>Description: What changed and why</li> <li>Testing: How you verified it works</li> <li>Checklist: Complete all items</li> </ul> </li> <li> <p>Example PR description:    <pre><code>## Summary\nAdds a `--version` command to display backend version information.\n\n## Changes\n- Added `version` command to CLI\n- Added test coverage in `tests/test_cli_version.py`\n\n## Testing\n- \u2705 Manual testing: `ha-backend version` outputs correct version\n- \u2705 All CI checks pass\n- \u2705 New tests added and passing\n\n## Related Issues\nCloses #123\n</code></pre></p> </li> <li> <p>Wait for CI checks to run on your PR (takes ~5 minutes)</p> </li> <li> <p>Address review feedback if requested</p> </li> </ol>"},{"location":"tutorials/first-contribution/#step-11-respond-to-review-feedback","title":"Step 11: Respond to Review Feedback","text":"<p>When maintainers review your PR, they may request changes:</p> <ol> <li> <p>Make the requested changes:    <pre><code># Still on your feature branch\nvim src/ha_backend/cli/main.py\n</code></pre></p> </li> <li> <p>Commit the changes:    <pre><code>git add .\ngit commit -m \"fix: address review feedback\n\n- Improve version output formatting\n- Add docstring details\"\n</code></pre></p> </li> <li> <p>Push updates:    <pre><code>git push origin add-version-command\n</code></pre></p> </li> </ol> <p>The PR will update automatically!</p> <ol> <li>Reply to review comments to acknowledge feedback</li> </ol>"},{"location":"tutorials/first-contribution/#step-12-merge-and-celebrate","title":"Step 12: Merge and Celebrate! \ud83c\udf89","text":"<p>Once approved:</p> <ol> <li>A maintainer will merge your PR</li> <li>Your contribution is now part of HealthArchive!</li> <li>Your GitHub profile will show the contribution</li> </ol> <p>Optional: Clean up your local branches: <pre><code>git checkout main\ngit pull upstream main\ngit branch -d add-version-command\n</code></pre></p>"},{"location":"tutorials/first-contribution/#tips-for-success","title":"Tips for Success","text":""},{"location":"tutorials/first-contribution/#code-quality","title":"Code Quality","text":"<ul> <li>\u2705 Follow existing code style and patterns</li> <li>\u2705 Add type hints to all functions</li> <li>\u2705 Write clear docstrings</li> <li>\u2705 Keep changes focused and small</li> </ul>"},{"location":"tutorials/first-contribution/#testing","title":"Testing","text":"<ul> <li>\u2705 Write tests for all new code</li> <li>\u2705 Ensure tests are deterministic (no flaky tests)</li> <li>\u2705 Use fixtures for test data</li> <li>\u2705 Test edge cases and error conditions</li> </ul>"},{"location":"tutorials/first-contribution/#communication","title":"Communication","text":"<ul> <li>\u2705 Ask questions if unclear</li> <li>\u2705 Keep PR descriptions detailed</li> <li>\u2705 Respond to feedback promptly</li> <li>\u2705 Be respectful and collaborative</li> </ul>"},{"location":"tutorials/first-contribution/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>\u274c Don't commit directly to <code>main</code></li> <li>\u274c Don't include unrelated changes in one PR</li> <li>\u274c Don't skip writing tests</li> <li>\u274c Don't ignore CI failures</li> </ul>"},{"location":"tutorials/first-contribution/#next-steps","title":"Next Steps","text":"<p>After your first contribution:</p> <ol> <li>Tackle more issues: Graduate to <code>help wanted</code> issues</li> <li>Learn the architecture: Read the Architecture Walkthrough</li> <li>Improve the docs: Documentation PRs are always welcome</li> <li>Review others' PRs: Great way to learn and help the community</li> </ol>"},{"location":"tutorials/first-contribution/#getting-help","title":"Getting Help","text":"<ul> <li>Questions about the code? Ask in the issue thread</li> <li>CI failures? Check the Testing Guidelines</li> <li>Architecture questions? Read the Architecture Guide</li> <li>Still stuck? Open a discussion on GitHub</li> </ul>"},{"location":"tutorials/first-contribution/#resources","title":"Resources","text":"<ul> <li>Development Setup</li> <li>Testing Guidelines</li> <li>Architecture Guide</li> <li>Documentation Guidelines</li> </ul> <p>Welcome to the HealthArchive community! \ud83d\ude80</p>"}]}